{"pages":[{"title":"NICO","text":"non-iid朝向分类图像：数据集和基线训练和测试数据之间的独立同分布假设是众多图像分类方法的基础。在非真实性普遍的情况下，这种性质在实际中很难得到保证，导致模型性能不稳定。然而，在文献中，非I.I.D图像分类问题在很大程度上未被研究。一个关键原因是缺乏设计良好的数据集来支持相关研究。在本文中，我们构建并发布了一个名为nico的非i.i.d.图像数据集，它使用上下文有意识地创建非真实性。与其他数据集相比，扩展的分析证明NICO可以支持各种非I.I.D情况，具有足够的灵活性。同时，我们提出了一个具有convnet结构的基线模型，用于一般非i.i.d.图像分类，其中测试数据的分布未知，但与训练数据不同。实验结果表明，nico可以从零开始支持convnet模型的训练，而批处理平衡模块可以帮助convnet在非i.i.d.环境下更好地运行。 Keywords: Non-I.I.D., Dataset, Context, Bias, ConvNet, Batch Balancing. 1. Introduction近年来，机器学习取得了显著的进展，主要得益于深层神经网络的发展[1，2，3，4，5，6]。机器学习模型的一个基本假设是，训练和测试数据应该包含依赖和相同分布的样本（i.i.d.）。然而，这种理想假设在实际情况下是脆弱的，我们很难对测试数据分布施加约束。这意味着最小化训练数据经验误差的模型不一定能很好地处理测试数据，从而导致非i.i.d.学习的挑战。当训练样本不足以逼近训练分布本身时，问题更为严重。对于学术研究和工业应用来说，如何开发出对分布变化具有鲁棒性的非身份识别学习方法至关重要。 基准数据集为竞争方法提供了一个共同的基础，对于促进研究方向的发展总是很重要的。以图像分类为例，这是一项重要的学习任务。它的开发得益于基准数据集，如pascal voc[7]、mscoc[8]和imagenet[9]。尤其是ImageNet，一个大规模且结构良好的图像数据集，它成功地展示了深度学习的能力，并在随后显著地促进了深度卷积神经网络的发展。在这些数据集上，通过随机数据分割很容易建立I.I.D.图像分类设置。但是它们没有提供一个显式的选项来模拟非i.i.d.设置。能够很好地支持非身份识别图像分类研究的数据集仍处于空白状态。 在本文中，我们构建并发布了一个专为非i.i.d.图像分类而设计的数据集NICO（non-i.d.图像上下文数据集）。其基本思想是用主要概念main concept和上下文context标记图像。例如，在“狗”的类别中，图像被分为不同的上下文，例如“草地”、“汽车”、“海滩”，这意味着“狗”分别在草地、汽车或海滩上。有了这些上下文，通过在某些上下文中训练模型并在其他不可见上下文中测试它，可以很容易地设计非i.i.d.设置。同时，通过调整训练和测试数据中不同情境的比例，可以显著地控制分布转移的程度。到目前为止，nico共包含19个类、188个上下文和近25000个图像。规模还在不断扩大，目前的规模已经能够从零开始支持深卷积网络的训练。 nico数据集可以支持但不限于两种典型的非i.d.图像分类设置。一种是针对非I.I.D.图像分类，其中测试数据分布已知，但不同于训练数据分布。另一种是一般的非I.I.D.图像分类，其中测试数据分布未知，不同于训练数据分布。显然，后者更现实，更具挑战性。在一个环境中学习的模型可能会应用到许多其他环境中。在这种情况下，模型在未知分布变化环境中的鲁棒性是一个非常有利的特性。它在医疗和安全等风险敏感应用中尤其重要。 由于缺乏结构良好、尺度合理的数据集，目前还没有提出卷积神经网络模型来解决一般的非i.i.d.图像分类问题。本文提出了一种新的用于一般非i.i.d.图像分类的cnn模型CNBB（convnet-with-batch-balancing），实验结果表明，该模型能在一定程度上克服非i.d.图像分类带来的负面影响。 2. Non-I.I.D. Image Classiﬁcation2.1. Problem Deﬁnition我们首先对non-iid图像分类进行正式定义，如下所示： Problem 1. (Non-I.I.D. Image Classiﬁcation) 根据训练数据 $D_{\\text {train}} = \\left(X_{\\text {train}}, Y_{\\text {train}}\\right), \\text {where } X_{\\text {train}} \\in \\mathbb{R}^{n \\times(c \\times h \\times w)} 代表图像， Y_{\\text {train}} \\in \\mathbb{R}^{n \\times 1}代表labels$ 。任务是学习一个特征抽取器$g_{\\varphi}(\\cdot)$和一个分类器$f_{\\theta}(\\cdot)$ 。$\\begin{array}{l}{\\text { so that } f_{\\theta}\\left(g_{\\varphi}(\\cdot)\\right) \\text { can predict the labels of testing data } D_{\\text {test }}=\\left(X_{\\text {test }}, Y_{\\text {test }}\\right) \\text { precisely, }} \\ {\\text { where } g_{\\varphi}(\\cdot) \\in \\mathbb{R}^{n \\times p} \\text { and } \\psi\\left(D_{\\text {train }}\\right) \\neq \\psi\\left(D_{\\text {test }}\\right) .}\\end{array}$ 此外，根据测试数据先验知识的可用性，我们进一步定义了两个不同的任务。 一种是目标Non-IID图像分类，其中测试数据分布是已知的。 另一种是一般的Non-IID图像分类，对应于测试数据分布未知的更现实的场景。 为了直观地量化训练数据集$\\psi\\left(D_{\\text {train}}\\right)$和测试数据集$\\psi\\left(D_{\\text {test}}\\right)$之间的分布变化程度，我们将非I.I.D.指数定义如下：Non-I.I.D. Index Deﬁnition 1. Non-I.I.D. Index (NI) 给出一个特征提取器$g_{\\varphi}(\\cdot)$, 一个类 $C$, 一个分布偏离的程度在训练数据$D_{t r a i n}^{C}$ 和 测试数据$D_{test}^{C}$ 被定义为： $N I(C)=\\left|\\frac{\\overline{g_{\\varphi}\\left(X_{\\text {train}}^{C}\\right)}-\\overline{g_{\\varphi}\\left(X_{\\text {test}}^{C}\\right)}}{\\sigma\\left(g_{\\varphi}\\left(X^{C}\\right)\\right)}\\right|_{2}$ 其中$X^{C}=X_{\\text {train}}^{C} \\cup X_{\\text {test}}^{C}, \\overline{(\\cdot)}$代表表示一阶矩, $\\sigma(\\cdot)$ 代表是用于标准化特征比例的标准, $|\\cdot|_{2}$ 表示2-范数 2.2. Existence of Non-IIDness在实际的情况下，I.I.D.假设不能严格满足，也就是说，在先前的数据集中普遍存在非Idess（10）。这里我们以imagenet为例。ImageNet是一个层次结构，其中每个类（例如Dog）包含多个子类（例如不同种类的Dog）。 对于每个子类，它提供图像的训练和测试（验证）子集。为了验证imagenet中的non-iid性，我们选择了10个常见的动物类（如狗、猫）并使用从这些类中随机抽取的10个实例化子类（如拉布拉多、波斯语）构建了一个新的数据集。使用训练和测试子集，我们训练和评估convnet的图像分类任务。本文使用的convnet的结构类似于alexnet（详见附录），我们将convnet的最后一个fc层作为特征抽取器g’。请注意，模型结构用于所有后续分析（包括对nico的分析）以进行公平比较，因此通过权衡绩效和所需的培训数据量表进行选择。 但作为一个具有充分学习能力的基础模型，具体的模型结构并不影响结论。我们重复这个收集过程3次，得到3个新的数据集（数据集A、数据集B和数据集C），分别计算每个类的NI和测试误差。作为一个例子，我们在图1中绘制了dataseta的结果。 图1：数据集A中每个类的ni（用条形表示）和测试误差（用曲线表示）。 我们发现： 所有类的NI都高于零，这意味着即使在imagenet这样的大规模数据集中，训练和测试数据之间的不一致性也是普遍存在的。 不同的类具有不同的NI值，较高的NI值对应较高的测试误差。 NI与检验误差之间的强相关性可以通过它们的高pearson相关系数（r=0.95）和小p_value（2e-15）得到进一步证明。这个展示和统计分析很好地支持了一个合理的结论，即NI量化的分布变化程度是影响分类性能的关键因素。虽然NI的数值是以特定的特征抽取器为条件的，但是如果特征抽取器固定，我们可以通过在训练数据和测试数据之间进行一些干预来分析分布偏差的趋势。在后面的章节中，我们使用NI对我们构造的新数据集进行了实证分析，以证明NICO可以地支持各种非独立的情形。 图2:ImageNet构建的3个不同数据集中每个类的NI。不同的数据集用不同的子类实例化相同的类。 2.3. Limitations of Existing Datasets在计算机视觉研究的整个发展过程中，基准数据集在为算法评估提供共同基础和推动新方向方面一直发挥着至关重要的作用。具体来说，对于图像分类任务，我们可以枚举几个里程碑数据集，如pascal voc、mscoc和imagenet。然而，现有的基准数据集不能很好地支持非I.I.D图像分类。首先，尽管在ImageNet和其他数据集中显示出非IID性，如图1所示，但每个类的训练和测试数据之间的总体分布变化程度相对较小，从非I.I.D.图像分类的角度来看，这些数据集的挑战性较小。更重要的是，在现有的数据集中，没有明确的方法来控制训练和测试数据之间的分布转移程度。如图2所示，如果我们在ImageNet中用不同的子类实例化同一个类并获得3个结构相同的数据集，那么给定类的NI在不同的数据集之间是相当不稳定的。如果没有一种可控的方法来模拟不同程度的非真实性，就无法对这些数据集上的竞争方法进行公平和系统的评估。也就是说，需要一个专门为non-iid图像分类设计的数据集。 3. The NICO Dataset在本节中，我们将介绍数据集的属性和收集过程，然后介绍该数据集支持的不同非i.i.d.设置下的初步经验结果。 3.1. Context for Non-I.I.D. Images生成非i.i.d.图像的基本思想是用概念和上下文标签丰富图像的标签。与以前的数据集仅用主要概念（如狗）标记图像不同，我们还标记概念出现在的具体上下文（如草地上）。然后，通过训练和测试一个概念在不同上下文中的模型，很容易模拟一个非i.i.d.环境。一个良好的non-iid图像分类模型有望在培训和测试环境中表现良好。 在nico中，我们主要包含两种上下文。一个是概念（或对象）的属性，如颜色、动作和形状。“语境+概念”对的例子包括白熊、爬山猴和双层车等。另一种语境是概念的背景或场景。“context+concept”对的示例包括雪地上的猫、马边的人和日出时的飞机等。nico数据集中不同上下文的示例如图3所示。 图3:nico中的上下文示例。第一排的图片是动物的狗，被分配到不同的背景下。第二排和第三排分别对应动物的马和车辆的船。 3.2. Data Collection and Statistics参考imagenet、mscoco和其他经典数据集[12,13]，我们首先定义了两个超类：动物和车辆。 对于每个超类，我们从mscoco中的272个候选类中选择类，标准是超类中选择的类应该具有较大的类间差异。 对于上下文选择context，我们利用yfcc100m[14]broswer，首先导出给定概念（即类标签）的频繁共现标记列表。然后我们过滤掉只在一些概念中出现的标签。最后，我们手动筛选所有标签，并选择与上下文定义一致的标签（即对象属性或背景和场景）。 http://www.yfcc100m.org/ 为了能够方便快捷地访问定义yfcc100m数据集特定子集的查询类型，我们提供了yfcc100m浏览器，该浏览器旨在实时过滤和浏览整个数据集（包含1亿张图像和视频）。完整数据集的子集可以通过简单的关键字搜索进行检索，并直接进行审查。由于我们的技术选择，尽管数据集很大，但性能足够高，可以在几秒钟内查看查询结果，从而实现流畅的浏览体验。 在获得概念和上下文标记之后，我们将给定的概念标记及其每个上下文标记连接起来形成一个查询，将查询输入到google和bing图像搜索的api中，并收集排名靠前的图像作为候选图像。最后，在筛选阶段，我们根据以下标准选择最终数据集中的图像： 图像的内容应该正确反映其概念和背景。 给定一个类，每个上下文中的图像数量应该足够，并且在上下文之间尽可能保持平衡。 请注意，我们不进行图像注册或通过对象集中进行过滤，因此所选图像比ImageNet中的图像更真实、更自然。 NICO数据集将不断更新和扩展。到目前为止，有两个超类：动物类和车辆类，其中动物类10个，车辆类9个。每个类有9或10个上下文。每个类的上下文平均数量大小在83到215之间，类的平均大小约为1300个图像，与imagenet类似。nico数据集中总共有25000个图像。由于nico是一个层次结构，所以很容易扩展。关于nico的更多统计数据见表1。数据集可以通过link 5下载，也可以通过link 6下载。 https://www.dropbox.com/sh/8mouawi5guaupyb/AAD4fdySrA6fn3PgSmhKwFgva?dl=0 https://pan.baidu.com/s/1277mgM-Nju6REd5h3xXlrA 3.3. Supported Non-I.I.D. SettingsSetting 1. Minimum bias.​ 给定一个类，我们可以忽略上下文，将类的所有图像随机分成训练和测试子集作为正样本。然后我们可以随机抽取属于其他类别的图像作为负样本进行训练和测试。在这个设置中，随机抽样的方式导致数据集中的训练和测试分布之间的最小分布偏移，这模拟了一个近似I.I.D.的场景。 Setting 2. Proportional bias.​ 给定一个类，当对正样本进行抽样时，我们使用所有上下文进行训练和测试，但是每个上下文在训练和测试子集中所占的百分比是不同的。例如，我们可以让一个上下文在训练数据中占大多数，而在测试中占少数，这与视觉概念服从幂律分布的自然现象是一致的[15]，负采样过程与设置1相同。在此设置中，可以通过调整每个上下文的训练和测试子集之间的比例差来调整分布偏移的级别。 Setting 3. Compositional bias.​ 给定一个类，并不是正样本所属的测试上下文都同时出现在训练子集中，这样的设置在真实场景中很常见，因为由于采样时间和空间的限制，可用的数据集在本质上不可能包含所有的潜在上下文。从观察到的上下文到看不到的上下文的分布变化通常很大。训练中测试上下文的数量越少，通常会导致更高的分布偏移，通过组合偏移和比例偏移相结合，可以进一步实现更激进的分布偏移。 Setting 4. Adversarial bias. 对抗的​ 给定一个类，正采样过程与设置3相同。对于负样本，我们倾向于从没有（或已经）包含在正训练样本中的上下文中选择负样本，以形成负训练（或测试）子集。在这种情况下，分布转移甚至高于设置3，并且在I.I.D.假设下开发的现有分类模型更容易混淆。 以上4个设置用于生成非I.I.D.训练和测试子集。在每个设置下，我们可以通过假设测试子集的分布是已知的或未知的，来进行有针对性的或一般的非i.i.d.图像分类。 3.4. Empirical Analysis 经验性分析为了验证nico支持非i.d图像分类的有效性，我们进行了一系列的实证分析。值得注意的是，在每个设置中，只有训练或测试数据的分布发生变化，而convnet的结构和训练数据的大小保持不变。 3.4.1. Minimum Bias Setting在这个设置中，我们随机抽取8000张图片进行训练，并分别从动物和车辆超类中抽取2000张图片进行测试。各等级的平均检测准确率和NI分别为49.6%、3.85对于动物超类；63.0%、3.20对于车辆超类。我们可以发现，即使在构造训练和测试子集时没有明显的偏差（由于随机抽样），nico中的ni也远高于imagenet中的ni。这是因为nico中的图像通常是非图标图像，具有丰富的上下文信息和非规范视角，从图像分类的角度来看，这更具挑战性。 图5：组合偏差设置中的ni：相对于训练数据中使用的上下文数量，车辆超类中所有类的平均ni。 图6：组合偏误和比例偏误的组合设置中的ni：相对于训练数据的各种占主导比例，车辆超类中所有类别的平均ni，其中测试数据中的上下文在训练中完全不可见。 3.4.2. Proportional Bias Setting在这个设置中，我们让所有的上下文同时出现在训练和测试数据中，并为动物超类中的每个类随机选择训练数据（或测试数据）中的一个主上下文。这种实验环境符合自然现象，即除了少数常见的视觉环境外，大多数视觉环境都是罕见的[15]。 3.4.3. Compositional Bias Setting与比例偏差设置相比，组合偏差设置模拟了从训练数据中获得的知识不足以描述整个分布的情况。为此，我们在构造训练数据时为给定类选择一个子集上下文，并用所有上下文测试模型。通过改变训练数据中观察到的上下文数目，我们可以模拟不同程度的信息丢失和分布偏移。 从图5中我们可以发现，当我们观察到训练数据中的更多上下文时，NI持续下降。通过结合比例偏差和成分偏差的概念，可以实现更激进的分布变化。在车辆超类中给定一个特定的类，我们选择7个上下文进行训练，另外3个上下文进行测试，并进一步让一个上下文控制训练数据。通过这样做，我们可以在训练和测试数据之间获得比前两个设置更严重的非I.I.D.条件，如图6中的结果所示。 3.4.4. Adversarial Bias Setting给定一个目标类，如果上下文只出现在训练数据的负样本和测试数据的正样本中，我们将其定义为混淆上下文。在这个实验中，我们选择动物超类中的四个类作为目标类，并在图7中报告ni w.r.t的各种混淆上下文。实验结果表明，混杂语境的数量对不同类别学生的英语学习记忆有着一致的影响。在给定任何目标类的情况下，我们都可以通过添加更多的混淆上下文来模拟更严酷的分布变化，并进一步混淆convnet。 图8:nico支持的不同设置的动物超类平均ni范围。 最后，我们在图8中显示了不同非I.I.D.设置下的镍范围。我们可以看到nico中ni的水平显著高于imagenet，并且从最小偏向到对抗性偏向有明显的上升趋势。 4.General Non-I.I.D. Image Classiﬁcation在这一部分中，我们提出了一个新的non-iid图像分类模型。 在非I.I.D.图像分类的文献中，大多数以前的方法是针对目标非I.I.D.图像分类提出的。针对训练数据和测试数据之间的分布匹配、特征空间变换和不变特征学习问题，提出了域自适应和协变量移位方法[16,17,18,19]。 这些方法可以获得良好的性能，但由于需要测试数据分布的先验知识，因此在实际应用中不太可行。另一方面，提出了几种方法来满足目标非i.d.图像分类中测试数据信息的需求。例如，领域泛化方法[20，21]仅使用训练数据来学习领域不可知的模型或不变表示。然而，这些关于转移学习的方法[22]要求训练数据具有多个域，并且我们知道每个样本属于哪个域。此外，这些方法的性能在很大程度上依赖于训练数据的多样性。 近年来，越来越多的人对一般的非独立学习产生了越来越多的关注。在因果关系文献[23]中，解决选择偏差的理想模型是基于因果变量制定政策，这些变量在不同领域保持稳定[24]。基于一个观察数据来估计治疗结果的因果ect的常用方法包括倾向性得分匹配[25，26]、马尔可夫毯子[27，28]和混杂平衡[29，30]等[31]。最近[32]利用因果关系进行预测建模。通过全局共焦平衡，可以准确识别对未知分布漂移不敏感的稳定特征进行预测。[33]提出了一种称为CRLR 7的因果正则逻辑回归，用于一般的非I.I.D.图像分类，并在相对较小的数据集中取得了良好的性能。然而，由于缺乏结构良好、规模合理的数据集，这些方法无法充分利用强大的表示学习技术（如convnets），因此不利于大规模的图像分类任务。 本文在nico的帮助下，将全局共焦平衡（the notion of global confounder balancing）的概念推广到convnet中，提出了一个新的模型CNBB，convnet with Batch Balancing。 4.1. ConvNet with Batch BalancingCRLR的核心思想是全局共焦平衡global confounder balancing，它将每个特征依次设置为处理变量，并学习一组最优的样本权重，以平衡处理组和控制组对任何处理变量的分布。在那里之后，特征之间的相关性将被分离，并且它们在类标签上的真实效果可以更准确地估计。 为了将全球共焦平衡的概念引入深度学习，我们主要面临两个挑战： 共焦平衡方法假设特征为二进制形式，而我们通常在convnet中具有连续特征。 对于全局共焦平衡，我们需要在一次迭代中为所有训练样本学习一组新的样本权重。 这对于convnet来说是不可行的，因为我们不能同时将所有的训练数据输入到模型中。 为了克服这些挑战，我们引入了特征二值化的量化损失，并提出了一种批量共焦平衡方法。具体来说，给定一批训练图像，我们定义量化损失如下： $\\left.\\operatorname{Loss} q=-\\sum_{i=1}^{n} | g_{\\varphi}\\left(x_{i}\\right)\\right) |_{2}^{2}$ , （1） 5. Conclusion and Future Works本文介绍了一种新的数据集NICO，用于促进non-iid图像分类的研究。据我们所知，nico是第一个结构良好的非i.i.d.图像数据集，具有合理的规模以支持convnets的培训。通过结合语境的思想，nico可以提供各种非身份识别设置，并有意识地创建不同层次的非身份。我们还提出了一个具有convnet结构的简单基线模型，用于一般的非i.i.d.图像分类问题，其中测试数据与训练数据具有不可知的分布变化。实验结果清楚地证明了nico在训练convnets方面的能力以及该模型在各种非i.i.d.环境下的优越性。 我们未来的工作将集中在以下几个方面。一是NICO的质量和数量不断提高。将探索正交背景、去噪图像和适当的对象面积比，使nico更可控，以唯一地调整偏差和对非i.i.d的响应。我们将从各个层面扩大数据集的规模，以满足足够的需求。其次，更多关于不同形式的非i.i.d的设置有望被利用。因此，如果需要，可以在nico中添加其他视觉概念，并将详细介绍使用nico满足新设置的方法。第三，将设计更有效的模型，以解决非身份识别图像分类的不同设置中的问题。","link":"/NICO.html"},{"title":"LEAF","text":"Leaf：联合设置的基准现代联邦网络，例如由可穿戴设备、移动电话或自动车辆组成的网络，每天都会生成大量的数据。这些丰富的数据有助于学习能够改善每个设备上用户体验的模型。然而，在联合设置中的学习在机器学习管道的所有阶段都提出了新的挑战。当机器学习社区开始应对这些挑战时，我们正处于关键时刻，以确保这一领域的发展是以现实世界的假设为基础的。为此，我们提出了Leaf，一个用于在联合环境中学习的模块化基准测试框架。Leaf包括一套开放源码的联邦数据集、严格的评估框架和一套参考实现，所有这些都旨在捕获实际联邦环境的障碍和复杂性。 1 Introduction随着越来越多的数据在远程设备的联邦网络上生成，人们越来越有兴趣使用这些数据的模型来增强设备上应用程序的能力[16、17、23]。然而，了解联邦网络中生成的数据会带来一些新的障碍： 统计：数据是以异构方式在每个设备上生成的，每个设备都与一个不同的（尽管可能相关）底层数据生成分布相关联。此外，数据点的数量通常在不同的设备上有很大的差异。 系统：联邦场景中的设备数量通常是大于典型分布式设置（如数据中心计算）中节点数量的数量级。每个设备在存储、计算和通信容量方面都可能有很大的限制，而且由于硬件、网络连接和电源的变化，这些容量在设备之间也可能有所不同。因此，联邦设置可能会遇到通信瓶颈，使传统分布式数据中心设置中遇到的瓶颈相形见绌，可能需要更快的设备推断。 隐私和安全：最后，个人生成数据的敏感特性要求对联邦数据进行操作的方法，以平衡隐私和安全问题与更传统的考虑因素，如统计准确性、可扩展性和效率。 最近的工作已经提出了应对这些挑战的不同方法，但在实验评估方面，这些努力中的许多都没有达到。作为一个例子，考虑联邦学习范式，它集中于直接在联邦网络上的培训模型[16，23，21]。集中于联邦学习的实验工作广泛使用三种类型的数据集： （1）不提供联邦场景的现实模型但常用的数据集，例如mnist、mnist fashion或cifar-10的人工分区[16、10、7、3、9、25、27]； （2）实际但专有联邦数据集，例如[16]中未命名社交网络的数据、[15]中的众包语音命令以及[4]中华为的专有数据； （3）源自公开数据但不容易再次发生的现实联邦数据集，例如：《FaceScrub》（19），《莎士比亚》（16），《Reddit》（10，18，3）。 在这项工作中，我们的目标是弥合数据集之间的鸿沟，这些数据集是流行的和可访问的，用于基准标记，以及那些现实地捕捉联邦场景的特征，但要么是专有的，要么是难以处理的。此外，除了建立一套联邦数据集之外，我们还提出了评估方法和复制结果的清晰方法。为此，我们提出了Leaf，一个模块化的基准测试框架，用于在大规模分布式的远程设备联合网络中学习。我们注意到，虽然联合学习是Leaf的一个典型应用程序，但是框架实际上包含了广泛的潜在学习设置，例如设备上学习或模型推理、多任务学习、元学习、转移学习、终身学习以及对公平有影响的个性化学习模式。例如，多任务学习（MTL）社区可以通过将每个设备视为不同的任务从Leaf中获益。Leaf的数据集将允许研究人员和实践者在任务和样本数量众多的情况下测试MTL方法，这与传统的MTL数据集（例如，流行的地雷探测[30、20、29、23]、计算机调查[2、1、11]和伦敦教育管理学院l[20，14，1，2，11]个数据集，每个数据集最多有200个任务）。类似地，这些设备也可以自然地解释为元学习设置中的任务，而不是普通基准中考虑的人工生成的任务，如Omniglot[12、6、26、24]和MiniImageNet[22、6、26、24]。 2 LEAFLeaf是一个用于联合设置的开放源代码基准框架。它由（1）一套开放源代码数据集，（2）一组统计和系统度量，以及（3）一组参考实现组成。如图1所示，Leaf的模块化设计允许这三个组件轻松地集成到不同的实验管道中。我们现在详细介绍Leaf的核心组件。 图1：Leaf模块以及他们之间的连接关系。Datasets模块预处理数据并且转化成标准json格式，可以让任意ML管道使用。Reference Implementations是一个成长的，包含用在联邦设置的公共方法的仓库：每一个实现产生了大量不同的统计和metrics衡量指标的日志。这个日志(或者任何用合适的格式生成的日志)可以用来聚合 和 分析这些metrics衡量指标在各个方面。Leaf在Metrics来执行这个操作。 数据集：我们策划了一套真实的联邦数据集。我们关注数据集，其中（1）数据具有自然的键生成过程（其中每个键引用特定设备）；（2）数据是从数千到数百万个设备的网络生成的；以及（3）数据点的数量在设备之间发生了倾斜。目前，leaf包\u0010含三个数据集： 联邦扩展mnist（femnist），它是流行mnist[13]数据集的类似（但更具挑战性）基准。它是根据数字/字符的写入器，对扩展mnist[5]中的数据进行分区而建立的。 sentiment140[8]，一个自动生成的情感分析数据集，根据其中的表情符号注释推文。在这个数据集中，每个设备都是不同的Twitter用户 莎士比亚，根据威廉·莎士比亚的完整作品建立的数据集[28，16]。在这里，每个角色在每个戏剧中的发言被认为是一个不同的装置。 我们在表1中提供了这些数据集的统计数据。在Leaf中，我们为每个数据集提供所有必要的预处理脚本，以及用于原型设计和最终测试的小/完整版本。接下来，我们计划添加来自不同领域（如音频、视频）的数据集，并增加机器学习任务的范围（如文本到语音、翻译、压缩等）。 度量：需要严格的评估度量来适当地评估学习解决方案在联合场景中的行为。目前，leaf建立了一组专门为此目的选择的初始度量。例如，我们引入了更好地捕捉整个设备性能分布的指标：第10和第90百分位的性能和数据中按自然层次分层的性能（例如，莎士比亚数据集中的“play”）。我们还引入了一些指标，这些指标说明了从边缘设备所需的计算资源的数量，包括触发器的数量和下载/上载的字节数。最后，Leaf还认识到了指定如何在设备之间加权精度的重要性，例如，每个设备是否同等重要，或者每个数据点是否同等重要（意味着超级用户/设备获得优先待遇）。值得注意的是，考虑分层系统和准确度度量对于评估一种方法是否会系统地排除用户组（例如，因为他们有低端设备）和/或在人群中的某些部分表现不佳（例如，因为他们产生了更少的数据）。 参考实现：为了促进再现性，leaf还包含一组面向联邦场景的算法参考实现。目前，这个集合仅限于联邦学习范式，特别是包括了minibatch sgd和fedavg的参考实现[16]。我们的目标是在更广泛的研究团体的帮助下，为Leaf提供更多方法和范例的实现。 3 LEAF in action我们将展示Leaf在运行中。 leaf支持可复制的科学：为了证明leaf支持的可复制性，我们集中于定性地复制[16]在下一个字符预测任务的莎士比亚数据集上获得的结果。特别值得注意的是，对于这个特定的数据集，fedavg方法会随着本地时间段的增加而出现惊人的差异。因此，在部署fedavg等方法之前，这是一个需要了解的关键设置。为了展示leaf如何允许这种场景的快速原型化，我们在莎士比亚的数据中使用了参考fedavg实现和子样本118设备（大约占总数的5%），这可以通过我们的框架轻松完成。结果如图2所示，当我们增加epoch的数量时，在训练损失方面我们确实看到类似的差异行为。 图2：莎士比亚数据集的子样本上fedavg的收敛行为。我们在所有实验中使用每轮0.8的学习率和10个设备。我们能够达到与[16]中获得的结果相当的测试精度。我们还定性地复制了训练损失的差异，这是在大量的epochs(E)观察到的。 Leaf提供了粒度系统和统计指标：如图3所示，在同时为多个客户机提供服务时，我们建议的系统和统计指标非常重要。对于统计指标，我们展示了在Sentiment140中改变每个用户最小样本数的效果（我们将其表示为k）。我们看到，尽管数据不足的用户（即k=3）的性能中值只有轻微下降，但25%的用户（框底）的性能大幅下降。同时，对于系统度量，我们为femnist运行minibatchsgd和fedavg，并计算达到精度阈值0.35所需的系统花费的资源。我们根据所有设备的总故障次数和上传到网络的总字节数来描述预算。当涉及通信与局部计算之间的权衡时，我们的结果表明，改进了FEDAVG的系统配置文件，尽管我们注意到，在一般情况下，方法可能在这两个维度上有所不同，因此根据当前遇到的问题考虑这两个方面很重要。 图3:SENT140和FEMINIST的统计和系统分析。k是每个用户的最小样本数，E是每个设备使用的本地数据的百分比，C是每轮选择的客户端数。橙色线代表设备精度的中位数，绿色三角形代表平均值，方框代表25%和75%，胡须代表10%到90%。我们对所有实验使用3*10e-4的学习率。 4 Conclusion我们介绍Leaf，一个用于在联合环境中学习的模块化基准测试框架，或者以大规模分布的设备网络为标志的生态系统。适用于此类环境的学习模式包括联合学习、多任务学习、元学习和设备上学习。Leaf允许这些领域的研究人员和实践者在比以前的基准更现实的假设下对新提出的解决方案进行推理。我们打算保持Leaf最新的数据集、度量标准和开源解决方案，以促进这一领域的知情和有根据的进展。 Githubhttps://github.com/TalwalkarLab/leaf model: 为了运行这些引用实现，在为各自的数据集运行./preprocess.sh脚本时必须使用-t sample标记。模拟的客户端总数等于各自数据集的培训数据中的用户总数。为了获得最佳的模型性能，可以使用类似于各自数据集自述文件中“大型数据集”选项中列出的参数来生成数据。main.py支持这些附加标签：–模型：模型的名称；选项列在各自的数据集文件夹中，例如，femnist的cnn；默认为各自数据集文件夹中的第一个模型。–num_rounds：要模拟的轮数–评估每轮：评估每轮–每轮客户数：每轮培训的客户数–批量大小：客户机训练数据时的批量大小–num epoch：客户机在数据上训练时的epoch数-T：模拟时间：小、中或大；时间越长，精度越高；对于大运行，使用与各自数据集自述文件中“大数据集”选项中列出的参数类似的参数生成数据，以获得最佳模型性能；默认值：大-lr：本地优化器的学习率。运行分类器之后，打开metrics.ipynb查看上次运行的系统和统计指标。模型生成的度量存储在metrics.json中，其中包含以下“key:value”对：数据集：数据集的名称num_轮数：模拟的轮数eval_every:integer x，这样模型每x轮评估一次精度：length-num_rounds（id，groups，num_samples，accs）列表，其中ID是对应于在各自回合中测试的客户机的客户机ID字符串列表，组是一个包含每个客户机的组名的列表（如果客户机没有关联的组，则列出NONE）。num_samples是一个包含每个客户机的测试样本数量的列表，以及ACCS是一个包含每个客户评估的准确性的列表。客户机计算：以客户机ID为键，length-num-rounds列表为值的字典；列表中的元素是int，表示相应客户机在相应的一轮中计算的浮点数。bytes_written：以客户机ID为键，length-num_rounds列表为值的字典；列表中的元素是int，表示resepctive客户机在相应的一轮中写入服务器的字节数。字节读取：以客户机ID为键，length-num-rounds列表为值的字典；列表中的元素是int，表示resepctive客户机在相应的一轮中从服务器读取的字节数。 小可爱呀小可爱 我的小可爱呀","link":"/LEAF.html"},{"title":"","text":"EMNIST: an extension of MNIST to handwritten lettersMNIST数据集已成为学习，分类和计算机视觉系统的标准基准。任务的可理解性和直观性，相对较小的大小和存储要求以及数据库本身的可访问性和易用性，是其广泛采用的原因。 MNIST数据库来自称为NIST特殊数据库19的较大数据集，该数据库包含数字，大写和小写手写字母。本文介绍了完整NIST数据集的一个变体，我们称为扩展MNIST（EMNIST），它遵循用于创建MNIST数据集的相同转换范例。结果是一组数据集，这些数据集构成更具挑战性的涉及字母和数字的分类任务，并且与原始MNIST任务共享相同的图像结构和参数，从而可以与所有现有分类器和系统直接兼容。通过比较转换后的NIST数字和MNIST数字的分类结果，提供基准结果以及对转换过程的验证。 找句子 - 方法论的描述 - 如何配图 介绍好d的基准和标准化问题的重要性不可低估，尤其是在竞争激烈且节奏飞快的领域，例如机器学习和计算机视觉。 这些任务提供了一种快速，定量和公正的手段来分析和比较不同的学习方法和技术。 这使研究人员可以快速了解方法和算法的性能和特殊性，尤其是当任务是一种直观且概念上很简单的任务时。 由于单个数据集可能仅涵盖特定任务，因此存在各种基准任务套件，对于允许采用更全面的方法来评估和表征算法或系统的性能至关重要。 在机器学习社区中，有一些标准化的数据集已被广泛使用并变得高度竞争。 这些数据包括MNIST数据集[1]，CIFAR-10和CIFAR-100 [2]数据集，STL-10数据集[3]和街景房号（SVHN）数据集[4]。 MNIST数据集由10类手写数字分类任务组成，并于1998年首次引入，它仍然是计算机视觉和神经网络社区中最广为人知和使用的数据集。 然而，一个好的数据集需要代表一个足够具有挑战性的问题，以使其既有用又确保其寿命[5]。 面对使用深度学习和卷积神经网络实现的越来越高的准确度，MNIST可能遭受了痛苦。 多个研究小组已发表了99.7％以上的准确度[6] – [10]，该分类准确度可以质疑数据集标签。 因此，与有意义或具有挑战性的基准相比，它已成为测试和验证分类系统的更多手段。 MNIST数据集的可访问性几乎肯定有助于其广泛使用。 整个数据集相对较小（与最新的基准数据集相比），可以自由访问和使用，并且以完全简单的方式进行编码和存储。 编码不使用复杂的存储结构，压缩或专有数据格式。 因此，从任何平台或通过任何编程语言访问并包含数据集都非常容易。 MNIST数据库是一个更大的数据集的子集，称为NIST特殊数据库19 [11]。 该数据集包含手写数字和字母，代表更大，更广泛的分类任务，并可能添加更复杂的任务，例如作者识别，转录任务和案例检测。 与MNIST相比，NIST数据集仍然难以访问和使用。 NIST数据集在收集时由于更高的成本和更高的存储量而受到驱动，最初是以非常有效和紧凑的方式存储的。 尽管提供了访问数据的源代码，但是在现代计算平台上使用它仍然具有挑战性。 因此，NIST最近发布了NIST数据集的第二版[12]。 数据集的第二版更易于访问，但是数据集的结构以及其中包含的图像与MNIST有所不同，并且不直接兼容。 NIST数据集偶尔在神经网络系统中使用。 许多分类器仅使用数字类[13]，[14]，而其他分类器也处理字母类[15] – [18]。 每篇论文都以略有不同的方式解决了制定分类任务的任务，这些基本方面的变化包括要包括的类数，训练和测试分割以及图像的预处理。 为了支持该数据集的使用，显然需要创建一套定义明确的数据集，以彻底指定分类任务的性质和数据集的结构，从而允许在各组数据之间进行简单直接的比较。 结果。 本文介绍了这样的数据集，称为扩展修改NIST（EMNIST）。 这些数据集源自NIST特殊数据库19，旨在代表神经网络和学习系统的更具挑战性的分类任务。 通过直接匹配原始MNIST数据集中发现的图像规范，数据集组织和文件格式，这些数据集被设计为现有网络和系统的直接替代品。 本文介绍了这些数据集，记录了用于创建图像的转换过程，并给出了该数据集的一组基准结果。 然后将这些结果用于进一步表征和验证数据集。 A. MNIST和NIST数据集 NIST特殊数据库19 [11]包含从500多个作者那里收集的手写数字和字符。数据集包含手写样本收集表格的二进制扫描，以及从表格中提取的单独分段和标记的字符。字符包括数字以及大写和小写字母。该数据库于1995年作为一个完整的集合发布[11]，然后在2016年9月使用一种更现代的文件格式重新发布[12]。数据集本身包含并取代了许多以前发布的NIST手写数据集，例如特殊数据库1、3和7。 MNIST数据集来自NIST特殊数据库1和3中包含的一小部分数字，并使用[1]中概述的方法进行了转换。 NIST特别数据库19代表了该系列数据集中的手写字符的最终集合，其中包含其他手写数字以及大写和小写手写字母的广泛集合。 NIST数据集的作者和整理者还建议，特殊数据库7（包含在特殊数据库19中）中包含的数据将专门用作测试集，因为这些样本是从高中生那里收集的，这构成了一个更具挑战性的问题。 NIST数据集旨在提供多种光学字符识别任务，因此可以在五个独立的组织中显示字符数据，称为数据层次结构。内容如下： 按页面：此层次结构包含手写样本表格的未处理全页二进制扫描。 其他层次结构中使用的字符数据是通过一组标准的表格收集的，要求作者填写这些表格。 3699张表格已完成。 按作者：此层次结构包含由作者组织的单独分段的手写字符图像。 它允许诸如作者识别之类的任务，但由于每个分组都包含来自多个类别的数字，因此几乎没有提供任何分类益处。 按字段：此组织包含按出现的收集表单上的字段排序的数字和字符。 这对于分割数字类（因为它们出现在其自己的独立字段中）主要有用。 按类别：从分类的角度来看，这是最有用的组织，因为它包含按类别排列的分段数字和字符。 有62个类别，包括[0-9]，[a-z]和[A-Z]。 数据也分为建议的训练和测试集。 按合并：此数据层次结构解决了手写数字分类中的一个有趣问题，即某些大写和小写字母之间的相似性。 确实，当检查“按类别”数据集上的完整分类任务所导致的混淆矩阵时，这些影响通常显而易见。 数据集上的此变体合并了某些类，从而创建了47个类的分类任务。 根据NIST的建议，合并类别适用于字母C，I，J，K，L，M，O，P，S，U，V，W，X，Y和Z。 本文所介绍的转换过程和所提供的代码适用于所有层次结构，但逐页层次结构除外，因为它包含根本不同的图像。但是，这项工作的主要重点在于按类别和按合并组织，因为它们涵盖了与标准MNIST数据集分类任务直接兼容的分类任务。 表I显示了NIST特殊数据库19版本中指定的原始培训和测试集的细目分类。“按类别”和“按合并”层次结构都包含814,255个手写字符，其中包括建议的731,668个训练样本和82,587个测试样本。但是应注意，样本总数中几乎有一半是手写数字。 “按作者分类”类提供了一个有趣的机会，可以从根本上制定新的分类任务，例如从手写样本中识别作者，但这超出了本文的范围。 II. METHODOLOGY 本文介绍了EMNIST数据集，然后将基于OPIUM的分类器应用于基于这些数据集的分类任务。 分类器的目的是提供一种验证和表征数据集的方法，同时还提供基准分类结果。 通过使用这些分类结果来解释EMNIST数据集的性质和组织。 为了最大程度地提高此数据集的可重复性和可访问性，本节仔细概述了将原始NIST图像转换为与原始MNIST数据集的图像直接兼容的格式所使用的步骤。所使用的转换过程试图重现[1]中概述的用于创建原始MNIST数据集（也是从NIST数字创建的）的步骤，然后将相同的处理步骤应用于NIST Special Database的全部内容19.第II-A节介绍了此转换过程以及为更好地转换数据集中的字母而进行的修改。 第II-E节介绍了用于在这些新数据集上创建初始基准测试结果的分类器。这些分类器应用于由EMNIST数据集定义的分类任务，该数据集包含字母，数字和两者的组合的不同分类。之所以选择基于OPIUM的分类器，是因为它们基于伪逆网络解决方案，因此提供了确定性的单步分析解决方案。 A.转换过程 NIST特殊数据库19最初于1995年发布，它使用了基于CCITT 4组算法的编码和压缩方法[19]，并将压缩后的图像打包为专有文件格式。 尽管数据库的初始版本包括提取和处理数据集的代码，但仍难以在现代系统上编译和运行这些实用程序。 为了直接解决此问题，该数据集的第二版于2016年9月发布，其中包含使用PNG文件格式编码的相同数据。 本文介绍的工作利用原始数据集，包括提取和转换这些文件的代码和指令。 用于创建降采样的28×28像素图像的后处理技术与数据集的第二版直接兼容。 转换过程将在NIST数据集中找到的128×128像素二进制图像转换为具有8位灰度分辨率的28×28像素图像，这些图像与MNIST数据集中的数字特征相匹配。 转换过程的概述如图1所示。如I-A部分所述，NIST数据集包含814、255个图像，分布在四个不同的层次结构中，这些图像会影响数据的标记和组织。 对于此工作，仅使用“按类”和“按合并”层次结构。 两者的转换过程相同，只有类标签（和类标签的数量）发生变化。 转换方法遵循与用于MNIST数据集的转换过程相同的总体范例，并在文献[1]中进行了概述，但是使用了不同的下采样方法来更好地处理NIST数据集中字符形状和大小的变化。 ，因此感兴趣区域的大小存在显着差异。原始的MNIST转换技术将数字下采样到20×20像素或32×32像素帧，然后再将其放入最终的28×28像素帧，而本文中使用的技术尝试利用最大可用空间量。 为了执行转换，将所提取的感兴趣区域的中心放在长度等于最大尺寸的正方形框中，并保留所提取的感兴趣区域的纵横比。然后在此方形框架上填充2个空像素边框，以防止数字和字符接触边框。最后，使用双三次插值算法将图像降采样为28×28像素，从而产生强度光谱，然后将其缩放到8位范围。 B.训练和测试拆分 特殊数据库19中使用的笔迹数据是从人口普查雇员和高中学生那里收集的。随数据集一起提供的规范中建议使用学生语料库中的手写数字作为测试集。尽管认为学生笔迹代表着一项艰巨的任务，因此应将其用作看不见的测试集的论点是有道理的，但它的确引起了人们的质疑，即两组参与者之间是否有足够的相似性和一致性。 因此，原始的MNIST数据集使用了与两个数据集版本一起提供的用户指南中指定和推荐的培训和测试的不同内容。因此，EMNIST数据集的创建遵循原始MNIST论文[1]中使用的相同方法，在该方法中，将原始训练和测试集进行了组合，并绘制了新的随机训练和测试集。由此产生的培训和测试数据集包含来自高中生和普查员工的样本。 C.EMNIST数据集 NIST特殊数据库19包含两种分割的手写字符排列，非常适合创建类似于MNIST的新分类任务。 这是在I-A节中介绍的“按类别”和“按合并”层次结构，它们都包含相同的图像数据，但具有不同的类别标签。 使用第II-A节中描述的方法将基础图像全部转换为28×28像素表示。 这两个数据集代表NIST手写字符的完整补充，但每个类的样本数量不均匀。 由于收集过程的性质，数字样本比字母样本要多得多，并且数据集中字母部分中每个类别的样本数量大约等于英语中的频率。 结果，产生了这些数据集的另外四个子集以专门解决这些问题。 图2总结了组成EMNIST数据集的六个数据集的内容。该图显示了六个数据集中每个类别的包含类别和每个类别的样本数量。此外，它还显示了每个数据集的训练和测试拆分。 EMNIST按类别和EMNIST按合并数据集都包含完整的814,255个字符，并且仅在分配的类数上有所不同。因此，字母类别中的样本分布在两个数据集之间有所不同。在两个数据集中，数字类中的样本数量保持不变。 EMNIST平衡数据集旨在包含最广泛的数据集，因为它包含所有“按合并”类的平衡子集。选择47类数据集而不是按类别数据集，以避免纯粹由大写字母和小写字母之间的误分类导致的分类错误。 EMNIST Letters数据集试图通过合并所有大写和小写类以形成平衡的26类分类任务来进一步减少由大小写混淆引起的错误。同样，EMNIST Digits类包含数字数据集的平衡子集，每个子集包含28,000个样本。 最后，EMNIST MNIST数据集旨在与原始MNIST数据集的大小和规格完全匹配。它旨在替代原始MNIST数据集，该数据集包含通过II-A节中概述的转换过程创建的数字。它主要用于根据原始MNIST数据集验证和表征转换过程。 D.验证分区 许多迭代训练算法都使用验证分区来评估训练期间网络的当前性能。 这需要与看不见的测试集分开，以保持结果的完整性。 EMNIST数据集中的平衡数据集不包含针对每个班级的单独验证集，而是包含训练集的特殊平衡子集，专门用于验证任务。 表II汇总了EMNIST数据集，并指出了哪些类在训练集中包含验证子集。 在这些数据集中，训练集的最后一部分（与测试集大小相等）被留作验证集。 另外，该子集也得到平衡，以使每个子集包含相等数量的样本。 如果不使用验证集，则可以将训练集用作一个连续集。 E.分类器 这项工作中提供的分类结果旨在为提供的数据集形成基准。 如[20]中所述，一个简单的三层ELM网络被用来进行分类，并且期望更深，更复杂的网络将提供更好的分类性能。 此外，还对数据集进行了线性分类器的训练。 该分类器表示无隐藏层的网络的解析伪逆解。 由于数据集的大小，不能单步计算ELM所需的伪逆，而是使用在线伪逆更新方法（OPIUM）来训练网络[21]。 该方法迭代地为输出权重计算精确的伪逆解，并使网络可以处理任何大小的数据集。 基于OPIUM的分类网络已在一系列隐藏层大小上进行了测试。在所有测试中，使用了随机训练顺序并将其保持恒定。进行了具有不同随机输入权重和相同隐藏层大小的多次试验，并在适当时报告了平均分类精度和标准差。线性分类器虽然使用相同的迭代伪逆技术进行训练，但不包含隐藏层，因此每个数据集仅获得一个结果。 本文的目的是介绍和表征数据集。选择基于OPIUM的训练方法是因为它们可以生成分析解决方案，并且不需要对数据集进行多次迭代。给定相同的网络结构和训练顺序，这些网络的结果是确定的。本文中的结果并不代表最先进的技术，而是作为指导性基准以及探索和验证数据集的手段。","link":"/EMNIST an extension of MNIST to handwritten letters.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"1两数之和","text":"给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。 你可以假设每种输入只会对应一个答案。但是，你不能重复利用这个数组中同样的元素。 1234给定 nums = [2, 7, 11, 15], target = 9因为 nums[0] + nums[1] = 2 + 7 = 9所以返回 [0, 1] 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/two-sum著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 方法一：暴力法暴力法很简单，遍历每个元素 x，并查找是否存在一个值与target−x 相等的目标元素。 12345678910111213141516class Solution { public int[] twoSum(int[] nums, int target) { int[] indexs = new int[2]; for(int i=0; i&lt; nums.length; i++) { for(int j=i+1; j &lt; nums.length; j++) { if(nums[i] + nums[j] == target) { indexs[0] = i; indexs[1] = j; return indexs; } } } throw IllegalArgumentException(\"No two sumsolution\"); }} 对于每个元素，我们试图通过遍历数组的其余部分来寻找它所对应的目标元素，这将耗费 O(n) 的时间。因此时间复杂度为 $O(n^2)$。 方法二：两遍哈希表为了对运行时间复杂度进行优化，我们需要一种更有效的方法来检查数组中是否存在目标元素。如果存在，我们需要找出它的索引。保持数组中的每个元素与其索引相互对应的最好方法是什么？哈希表。 通过以空间换取速度的方式，我们可以将查找时间从 O(n)降低到 O(1)。哈希表正是为此目的而构建的，它支持以 近似 恒定的时间进行快速查找。我用“近似”来描述，是因为一旦出现冲突，查找用时可能会退化到 O(n)。但只要你仔细地挑选哈希函数，在哈希表中进行查找的用时应当被摊销为 O(1)。 一个简单的实现使用了两次迭代。在第一次迭代中，我们将每个元素的值和它的索引添加到表中。然后，在第二次迭代中，我们将检查每个元素所对应的目标元素（target - nums[i]）是否存在于表中。注意，该目标元素不能是 nums[i] 本身！ 12345678910111213141516class Solution { public int[] twoSum(int[] nums, int target) { // 1. 循环nums数组放入map 2. 循环nums找到nums[i]与target-nums[i] Map&lt;Integer, Integer&gt; map = new Map&lt;&gt;(); for(int i=0; i&lt;nums.length; i++) { map.put(nums[i], i); } for(int i=0; i&lt;nums.length; i++) { int complement = target - nums[i]; if(map.containsKey(complement) &amp;&amp; map.get(complement) != i) { return new int[] {i, map.get(complement)}; } } throw new IllegalArgumentException(\"NO two sum\"); }} 方法三：一遍哈希表事实证明，我们可以一次完成。在进行迭代并将元素插入到表中的同时，我们还会回过头来检查表中是否已经存在当前元素所对应的目标元素。如果它存在，那我们已经找到了对应解，并立即将其返回。 123456789101112131415class Solution { public int[] twoSum(int[] nums, int target) { // 1. 循环nums, 判断存在 Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for(int i=0; i&lt;nums.length; i++) { int complement = target - nums[i]; if(map.containsKey(complement) &amp;&amp; map.get(complement) != i) { // return new int[] {i, map.get(complement)}; return new int[] {map.get(complement), i}}; } map.put(nums[i], i); } throw new IllegalArgumentException(\"No two sum\"); }}","link":"/2019/11/07/1两数之和/"},{"title":"","text":"A generic framework for privacy preserving deep learning一个隐私保护深度学习的通用框架 Abstract我们详细介绍了一个新的隐私框架，以保护深度学习并讨论其优点。该框架重视所有权和数据的安全处理，并基于the chain of commands and tensors来引入了有价值的表示。这种抽象允许人们实现复杂的隐私保护结构，如联合学习，安全多方计算和差异隐私，同时仍然向最终用户公开熟悉的深度学习API。我们报告基于Boston Housing和Pima Indian Diabetes数据集的早期结果。虽然除差异隐私之外的隐私功能不会影响预测准确性，框架的当前实现方面导致显着的性能开销，将在开发的后期阶段解决。里程碑，第一个通用框架基于隐私保护的深度学习框架。 1 Introduction安全多方计算（SMPC）作为在不信任环境中执行操作而不泄露数据的方式正变得越来越流行。在机器学习模型的情况下，SMPC将保护模型权重，同时允许多个工作节点使用自己的数据集参与训练阶段，比如联合学习。但是，已经证明，安全训练的模型仍然容易受到逆向工程攻击，这些攻击可以直接从模型中提取有关数据集的敏感信息。标记为差分专用（DP）方法的另一组方法解决了这个问题，并且可以有效地保护数据。我们为每个PyTorch用户提供透明的隐私保护框架，保护深度学习，从直观的界面中使用FL，MPC和DP。我们展示了框架支持MPC和DP解决方案的各种实现的能力，并报告了在联合学习环境中分别为MPC和DP实例化SPDZ和时刻会计方法时获得的结果。 我们的主要贡献如下： 建立了一个标准化协议在worker之间的通信，协调联合学习 让我们开发一个基于Tensor的链式抽象模型，可以重写操作 例如发送和共享一个Tensor 在两个worker之间。 最后，我们提供了元素， 运用框架实现 最近提出的dp和mcp算法。通过这样做，我们打算帮助推广在机器学习中的隐私保护技术通过研究人员和数据科学家每天使用的可得到通用工具。FL, MPC和DP作为可插拔的模块。 2 A standardized framework to abstract operations on Tensors2.1 The chain structure执行转换或向其他worker发送Tensor可以表示为一系列操作，每个操作都由一个特殊的类来体现。为此，我们创建了一个名为SyftTensor的抽象类。SyftTensors旨在表示数据的状态或转换，并且可以链接在一起。链结构总是在其头部具有PyTorch张量，并且使用子属性向下访问SyftTensors所体现的变换或状态，并使用父属性向上访问。 上图显示了Tensor chain的一般结构，其中SyftTensors被一些子类的实例替换，这些子类都具有特定的角色，比如接下来将描述的LocalTensor类。所有操作首先应用于Torch张量，这使得可以使用native Torch interface，然后通过chain 转发到子属性。 SyftTensor有两个重要的子类。首先，LocalTensor是在实例化Torch张量时自动创建的。(Its role is to perform on the Torch tensor the native operation corresponding to the overloaded operation.=&gt;它的作用是在Torch张量上执行(相对应的被重载过的)native operation)。例如，如果命令是add，则LocalTensor将在头张量上执行native Torch命令native_add。该链有两个节点，它循环使得LocalTensor子节点引用包含数据的head node tensor，而不需要重新创建子张量对象，这会降低性能。?? trask:So this particular aspect we’re sortof getting rid of in the 1,0 refactorBasically in order to create new tensor types in a way that works with the rest of pytorch, we have to wrap them in an actualy pytorch tensorbut we still need a pytorch tensor to store the dataso the canonical chain isWrapper (FloatTensor) -&gt; LocalTensor (our class) -&gt; FloatTensor (actually holds the data)what that sentence is saying that (to save memory), we re-use the same tensor for the wrapper and for the data tensor (The one on the far right) 所以这个特殊的方面我们在1.0重构中有点摆脱基本上为了以与pytorch的其余部分一起工作的方式创建新的张量类型，我们必须将它们包装在一个实际的pytorch张量中但是我们仍然需要一个pytorch张量来存储数据所以规范链是包装器（FloatTensor） - &gt; LocalTensor（我们的类） - &gt; FloatTensor（实际上保存数据）那个句子说的是（为了节省内存），我们为包装器和数据张量（最右边的那个）重复使用相同的张量。 the chain structure: the main reason is to make it easier to extend Tensor types - and to have multiple layers of abstraction Which can be used independently of each other.链结构：主要原因是更容易扩展Tensor类型 - 并且具有多个抽象层，可以彼此独立使用。 Second, the PointerTensor which is created when a tensor is sent to a remote worker.发送和返回张量就像在张量上调用方法send（worker）和get（）一样简单。当发生这种情况时，整个链被发送给worker并由双节点链代替：tensor，现在为空，PointerTensor指向谁拥有数据和远程存储位置。这次，指针没有子节点。上图说明了在发送给remote worker时如何修改链以及如何在这些链中使用LocalTensor和PointerTensor。 2.2 From virtual to real context execution of federated learning为了简化调试复杂的操作链，该框架开发了Virtual Workers的概念。虚拟工作者都在同一台计算机上，不通过网络进行通信。它们只是复制命令链并公开与实际工作者相同的接口以便相互通信。 截至目前，联合学习环境中的Network worker在框架中有两个实现。一个构建在普通网络套接字上，而另一个支持Web套接字。 Web Socket workers允许在浏览器中实例化多个工作程序，每个工作程序都在其自己的选项卡中。在实际寻址不在同一台机器上的远程工作程序之前，这在构建联合学习应用程序时为我们提供了另一层次的粒度。Web Socket工作者对于围绕基于浏览器的笔记本电脑的数据科学生态系统也是一个非常好的选择。 3 Towards a Secure MPC framework3.1 Building an MPCTensor第2节中介绍的元素构成了创建MPCTensor所需的构建块。可以使用PointerTensors列表完成拆分和发送共享Tensor。我们框架中提出的MPC工具箱实现了[3,2]中的SPDZ协议。MPC工具箱包括基本操作，例如加法和乘法，还包括预处理工具，用于生成例如用于乘法的三元组，以及包括矩阵乘法在内的神经网络的更具体操作。由于MPC，对卷积网络的传统元素进行了一些调整：如[2]中所述，我们使用平均池而不是最大池和近似高度sigmoid而不是relu作为激活函数。由于SPDZ协议假设数据以整数形式给出，我们在链中添加了一个名为FixedPrecisionTensor的节点，该节点将浮点数转换为固定精度数。此节点将值编码为整数并存储小数点的位置。实现SPDZ的张量的完整结构在图中进行了总结。 与[2]提出的MPC协议不同，player在我们的框架中并不相同，因为一个是模型的所有者（称为localworker）。他通过控制所有其他参与者（remote worker）的训练程序来充当领导者。为了在处理数据时减轻这种集中偏差，localworker可以创建远程共享张量，这个张量基于他无法看到的数据和不拥有。 实际上，我们希望remote worker能够在一般环境中保存自己的一些数据，例如，当医院提供医学图像来训练模型时。然后，多个参与者有兴趣看到执行正确执行，这在推理阶段尤其重要，因为许多因素可能会导致破坏预测。 到目前为止，目前的实施还没有一种机制来确保每个玩家都诚实地行事。一个有趣的改进是实现秘密共享值的MAC认证，如下所述[2] 3.2 Applying Differential Privacy我们基于[1]的工作实现了差异隐私，其提供了适度（“单个数字”）隐私预算内的深度神经网络的训练方法。为了实现这一目标，本文提供了用于仔细调整所需噪声的隐私损失的新估计，以及提高私人培训效率的新算法. 特别是，我们实施了随机梯度下降（SGD）：而不是以相同的方式在数据集和epoch上迭代，训练由阶段组成，每个阶段包括从数据集的N个项目中抽样L个项目并使用它们来升级模型.我们直接重用了[1]提供的privacy accountant，但实施了我们自己的sanitizer，它可以剪切渐变并增加高斯噪声。这样，本地工作人员将获得用于更新模型的安全梯度，该模型不能公开关于数据集的信息。 我们的框架还提供了由联合学习环境指导的一些改进。首先，当抽样时，我们随机选择一个worker并在自己的数据中进行抽样。其次，对远程工作人员进行sanitizer，以便有效地确保数据隐私。 [5]中描述的方法提出了另一种方法，通过使用预训练和未发表模型（教师）的嘈杂和聚合投票训练最终模型（称为学生模型）来确保差异隐私。它目前正在实施，并将作为我们框架中的另一个DP Tensor进行整合。 4 Results and discussion表1报告了在规范的Boston Housing数据集上训练神经网络所需的执行时间，使用了我们框架的三个下降。性能分析表示使用Web Socket工作器而不是Virtual Workers的开销相当小，从而验证了他们的笔记本开发工具的用途。这是由于在不同本地选项卡之间进行通信时网络延迟较低。然而，我们比使用常规PyTorch慢46倍。我们在第二个实验中观察到相同的性能开销，该实验使用Pima Indian Diabetes数据集训练分类器检测糖尿病，这是一个包含768行和8列的小数据集[6]。 表2显示了如何增加？以牺牲数据隐私为代价改进模型。与基线模型中的20-24相比，DP模型实现了25-30 MSE，但随着我们实现（0.5,10-5） - 差异隐私，隐私保证仍然很强。这些结果与计算机视觉应用文献中报道的结果一致[1]。 对于Boston Housing数据集，基线模型每批花费约19.8ms，而差异私有模型花费约30.0ms，这对于隐私等功能来说是非常合理的开销（+ 50％）。我们可以做的最后一个观察是，启用DP后收敛速度要慢得多。在50次采样的第一阶段，MSE的值保持在500的范围内。然后MSE开始下降并稳定地达到10-50 MSE值。有两个原因可以解释这种行为：首先，梯度削波会降低最后一层更新的效率，其次，高斯噪声会干扰梯度建议的更新，因此梯度更低，因此效率更低。注意，提高梯度限幅也会增加高斯噪声的方差。 5 Conclusions我们引入了一个基于PyTorch构建的隐私保护联合学习框架。该设计依赖于在local 和remote worker之间交换的张量链。我们的张量实现支持PyTorch API的命令，并在同一框架内结合MPC和DP功能。 仍有许多问题需要解决，其中最重要的是减少训练时间。效率尚未得到解决，但目前的开销表明，纯粹的Python框架存在改进的空间，而高级Python API则依赖于优化的低级库。另一个问题与保护MPC有关，以确保检测并防止恶意企图破坏数据或模型。 在满足提交的匿名化要求后，本文中涉及的所有代码示例将在GitHub存储库中提供。 [2]Ivan Damgård, Valerio Pastro, Nigel Smart, and Sarah Zakarias. Multiparty computation from somewhat homomorphic encryption. In Reihaneh Safavi-Naini and Ran Canetti, editors, Advances in Cryptology – CRYPTO 2012, pages 643–662, Berlin, Heidelberg, 2012. Springer Berlin Heidelberg. ISBN 978-3-642-32009-5.我们提出了一种通用的多方计算协议，可以防止主动攻击者腐败n个玩家中的n-1个。该协议可用于计算任何有限域上的安全运算电路。我们的协议包括一个预处理阶段，它既独立于要计算的函数和输入，也是一个更有效的在线阶段，在这个阶段进行实际计算。在线阶段是无条件安全的，并且具有线性的总计算（和通信）复杂度n，即参与者的数量，其中早期工作在n中是二次的。此外，每个玩家所做的工作只是一个很小的常数因子，大于计算电路所需的一个小的常数因子。我们证明这对于大型领域的计算是最佳的。实际上，对于3个玩家，可以在0.05毫秒内完成安全的64位乘法。我们的预处理基于一个有点同态的密码系统。我们扩展了Brakerski等人的方案，以便我们可以执行分布式解密并在一个密文中并行处理许多值。我们的预处理阶段的计算复杂性由公钥操作主导，我们需要每安全乘法𝑂（𝑛2/𝑠）操作，其中s是随密码系统的安全参数增加的参数。此模型的早期工作需要Ω（𝑛2）操作。实际上，预处理在大约13毫秒内为3个玩家准备了一个安全的64位乘法。","link":"/2018/12/07/A-generic-framwork-for-privacy-preserving-deep-learning/"},{"title":"2两数相加","text":"给出两个 非空 的链表用来表示两个非负的整数。其中，它们各自的位数是按照 逆序 的方式存储的，并且它们的每个节点只能存储 一位 数字。 如果，我们将这两个数相加起来，则会返回一个新的链表来表示它们的和。 您可以假设除了数字 0 之外，这两个数都不会以 0 开头。 示例： 输入：(2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)输出：7 -&gt; 0 -&gt; 8原因：342 + 465 = 807 方法：初等数学思路 我们使用变量来跟踪进位，并从包含最低有效位的表头开始模拟逐位相加的过程。 哑结点其实就是放在第一个存放数据结点之前、头结点之后的结点。加入哑结点之后就可以使所有数据结点都有前驱结点，这样就会方便执行链表的一些操作。 123456789101112131415161718192021222324252627282930/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */class Solution { public ListNode addTwoNumber(ListNode l1, ListNode l2) { // 创建一个链表 带头结点！！！ 1. 变量跟踪进位 ListNode dummyHead = new ListNode(0); ListNode p = l1, q = l2, curr = dummyHead; int carry = 0; while(p != null || q != null){ int x=(p != null) ? p.val : 0; int y=(q != null) ? q.val : 0; int sum = x+y+carry; carry = sum/10; curr.next = new ListNode(sum%10); curr = curr.next; if (p!=null) p = p.next; if (q!=null) q = q.next; } if (carry &gt; 0) { curr.next = new ListNode(carry); } return dummyHead.next; } }","link":"/2019/11/09/2两数相加/"},{"title":"BlockchainTutorial","text":"How to learn Blockchain确定方向从事区块链开发也有很多方向，如:区块链应用开发人员、区块链架构师、底层核心开发、共识算法研究等等。方向不同，需要学习的内容就不一样，如果做基于区块链应用开发，只需要了解一门编程语言（nodejs, Go, Python, C++ 等）, 大概了解区块链的原理，不一定要深入，当能理解越深入开发应用就越顺。如果做区块链基础开发，就需要了解加密算法，P2P通信，共识算法等等。 投入时间学习-动手实践由于区块链涉及的技术很多，可以相对各个技术有一个概念了解，再逐步深入原理。总=(深入)&gt;分=(总结)&gt;总；逐步深入的过程也是解答疑问的过程，需要我们善用Google搜索。如果觉得已经理解一个概念或原理时，可以尝试动手实现它，如在理解挖矿后，可以写代码模拟挖矿过程。实践检验学习，画出脑图，印在脑中，记录下来学习是一个长期的过期，没有捷径，必须得多读书，读代码，写代码。 学习资源介绍比特币:一种点对点的电子现金系统-英文比特币:一种点对点的电子现金系统-中文以太坊白皮书-英文以太坊白皮书-中文区块链技术指南-电子书区块链开发指南-纸书比特币 - 官网以太坊 - github超级账本HyperledgerETHFANS - 社区深入浅出区块链 08年次贷危机；中心化的思想 =&gt; 比特币，revolution，技术改变世界，去中心化。 Blockchain introduction什么是比特币 比特币是一种基于分布式网络的数字货币。比特币系统（广义的比特币）则是用来构建这种数字货币的网络系统，是一个分布式的点对点网络系统。 本文立足于狭义的区块链货币。 数字货币是什么凯恩斯在《货币论》上讲，货币可以承载债务，价格的一般等价物。货币的本质是等价物，它可以是任何东西，如：一张纸，一个数字，只要人们认可它的价值。人民币，美元等作为国家信用货币，其价值由国家主权背书。而数字货币是一种不依赖信用和实物的新型货币，它的价值由大家的共识决定。比特币就是一种数字货币。（我们在网银，微信，支付宝的金额，准确来讲，它是信用货币的数字化，不是数字货币，不过央行也在研究比特币，准备发行数字货币） 运行原理在银行系统的数据库里记录着跟我们身份id对应的财产，下文称这样的记录为账本，如张三的卡10月1日转入1w, 余额10w。比特币系统也同样有这样的账本，不同银行由单一的组织负责记录,比特币的记账由所有运行系统的人（即节点，可以简单理解为一台电脑）共同参与记录，每个节点都保存（同步）一份完整的账本。同时使用简单多数原则，来保证账本的一致性。举个例子：如果有人在自己电脑上把自己的余额从1万改为1百万，他这个账本和大多数人的账本不一致，就会被比特币系统认为是无效的。 比特币使用区块链技术来支撑整个系统的运行：区块链记账原理比特币所有权问题比特币如何挖矿 进阶阅读：分析比特币网络：一种去中心化、点对点的网络架构，可以详细了解比特币网络。比特币区块结构Merkle树及简单支付验证分析，可以详细了解区块结构如何验证交易。 区块链记账原理区块链(1.0)是一个基于密码学安全的分布式账本，是一个方便验证，不可篡改的账本。通常认为与智能合约相结合的区块链为区块链2.0, 如以太坊是典型的区块链2.0很多人只了解过比特币，不知道区块链，比特币实际是一个使用了区块链技术的应用，只是比特币当前太热，把区块链技术的光芒给掩盖了。区块链才是未来，期望各位开发人员少关心币价，多关心技术。本文将讲解区块链1.0技术是如何实现的。 哈希函数在讲区块链记账之前，先说明一下哈希函数。哈希函数：Hash(原始信息) = 摘要信息原始信息可以是任意的信息, hash之后会得到一个简短的摘要信息 哈希函数有几个特点: - 同样的原始信息用同一个哈希函数总能得到相同的摘要信息 - 原始信息任何微小的变化都会哈希出面目全非的摘要信息 - 从摘要信息无法逆向推算出原始信息 举例说明：Hash(张三借给李四100万，利息1%，1年后还本息 …..) = AC4635D34DEF账本上记录了AC4635D34DEF这样一条记录。 可以看出哈希函数有4个作用： 简化信息 很好理解，哈希后的信息变短了。 标识信息 可以使用AC4635D34DEF来标识原始信息，摘要信息也称为原始信息的id。 隐匿信息 账本是AC4635D34DEF这样一条记录，原始信息被隐匿。 验证信息 假如李四在还款时欺骗说，张三只借给李四10万，双方可以用AC4635D34DEF来验证原始信息。 哈希函数的这4个作用在区块链技术里有广泛的运用。（哈希函数是一组函数或算法） 区块链记账方法假设有一个账页序号为0的账页交易记录如下: 账号 入账 出账 余额 备注说明 王二 100 190 收到xx货款 张三 100 30 xxxx 李四 120 90 170 xxxx 记账时间为：2018-11-11 10:22:02区块链在记账是会把账页信息（包含序号、记账时间、交易记录）作为原始信息进行Hash, 得到一个Hash值，如：787635ACD, 用函数表示为：Hash(序号0、记账时间、交易记录) = 787635ACD账页信息和Hash值组合在一起就构成了第一个区块。 比特币系统里约10分钟记一次账，即每个区块生成时间大概间隔10分钟 在记第2个账页的时候，会把上一个块的Hash值和当前的账页信息一起作为原始信息进行Hash,即： Hash(上一个Hash值、序号1、记账时间、交易记录) = 456635BCD 这样第2个区块不仅包含了本账页信息，还间接的包含了第一个区块的信息。依次按照此方法继续记账，则最新的区块总是间接包含了所有之前的账页信息。所有这些区块组合起来就形成了区块链，这样的区块链就构成了一个便于验证（只要验证最后一个区块的Hash值就相当于验证了整个账本），不可更改（任何一个交易信息的更改，会让所有之后的区块的Hash值发生变化，这样在验证时就无法通过）的总账本。 记账有成本，想了解节点为什么要记账，请看这篇：比特币如何挖矿（挖矿原理）-工作量证明","link":"/2018/10/29/BlockchainTutorial/"},{"title":"C++入门","text":"简介 C++，使用 CLion编程; 首先，Mac OS 自带的g++编译器版本比较低，许多c++的新特性都不支持，所以可以先在终端下下载g++ 的新版本，如g++7，代开Clion后在preference里找到Cmake并修改参数如下： -D CMAKE_CXX_COMPILER=/usr/local/bin/g++-7 与服务器交互配置： https://www.cnblogs.com/pugang/p/9734547.html CLion 学习之读懂 CMakeLists.txt有当我们使用 CLion 自动生成一个简单的 C 项目，目录如下： 123├── CMakeLists.txt├── cmake-build-debug└── main.c main.c 123456#include &lt;stdio.h&gt;int main() { printf(\"Hello, World!\\n\"); return 0;} CMakeLists.txt 1234567cmake_minimum_required(VERSION 3.8)project(demo1)set(CMAKE_C_STANDARD 99)set(SOURCE_FILES main.c)add_executable(demo1 ${SOURCE_FILES}) CMakeLists.txt是什么呢？ CLion 的编译肯定是依赖这个文件了。有了这个文件，我们就可以直接使用 ide 里面的 run 和 debug 了，相比使用 gdb 来断点调试，开发效率简直有质的飞跃了哈。（不过，很多问题需要线上追踪的话，只能 gdb 了） CMake 是一个跨平台的自动化建构系统，它使用一个名为 CMakeLists.txt 的文件来描述构建过程，可以产生标准的构建文件，如 Unix 的 Makefile 或Windows Visual C++ 的 projects/workspaces 。 CMakeLists.txt 语法12345678910111213# 限定了 CMake 的版本cmake_minimum_required(VERSION 3.8)# 该命令表示项目的名称是 main project(demo1)# set(变量 值)set(CMAKE_C_STANDARD 99)set(SOURCE_FILES main.c)# ${xxx} 引用上面定义的 xxx 变量# add_executable 表示把变量 SOURCE_FILES 编译成一个名称为 demo1 的可执行文件。add_executable(demo1 ${SOURCE_FILES})","link":"/2019/08/31/CPP入门/"},{"title":"Docker三剑客","text":"前言理解Docker的三个核心概念：镜像，容器，仓库； 开发人员使用Dockerfile定制镜像，使用docker-compose在.yml文件中配置所有容器的部署方法、文件映射、容器链接等，运行docker-compose up执行安装容器并且自动部署。 运维人员从仓库下载容器，运行脚本。 因此下面将介绍具体的操作流程。 Docker Compose 项目Docker Compose 是 Docker 官方编排（Orchestration）项目之一，负责快速的部署分布式应用。 Compose 简介Compose 项目是 Docker 官方的开源项目，负责实现对 Docker 容器集群的快速编排。从功能上看，跟 OpenStack 中的 Heat 十分类似。 Compose 定位是 「定义和运行多个 Docker 容器的应用（Defining and running multi-container Docker applications）」，其前身是开源项目 Fig。 我们知道使用一个 Dockerfile 模板文件，可以让用户很方便的定义一个单独的应用容器。然而，在日常工作中，经常会碰到需要多个容器相互配合来完成某项任务的情况。例如要实现一个 Web 项目，除了 Web 服务容器本身，往往还需要再加上后端的数据库服务容器，甚至还包括负载均衡容器等。 Compose 恰好满足了这样的需求。它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。Compose 中有两个重要的概念： 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。 Compose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。 Compose 项目由 Python 编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。因此，只要所操作的平台支持 Docker API，就可以在其上利用 Compose 来进行编排管理。 安装与卸载Compose 支持 Linux、macOS、Windows 10 三大平台。 Compose 可以通过 Python 的包管理工具 pip 进行安装，也可以直接下载编译好的二进制文件使用，甚至能够直接在 Docker 容器中运行。 前两种方式是传统方式，适合本地环境下安装使用；最后一种方式则不破坏系统环境，更适合云计算场景。 Docker for Mac 、Docker for Windows 自带 docker-compose 二进制文件，安装 Docker 之后可以直接使用。 123$ docker-compose --versiondocker-compose version 1.17.1, build 6d101fb 使用###术语首先介绍几个术语。 服务 (service)：一个应用容器，实际上可以运行多个相同镜像的实例。 项目 (project)：由一组关联的应用容器组成的一个完整业务单元。 可见，一个项目可以由多个服务（容器）关联而成，Compose 面向项目进行管理。 场景最常见的项目是 web 网站，该项目应该包含 web 应用和缓存。 下面我们用 Python 来建立一个能够记录页面访问次数的 web 网站。 web 应用新建文件夹，在该目录中编写 app.py 文件 12345678910111213from flask import Flaskfrom redis import Redisapp = Flask(__name__)redis = Redis(host=&apos;redis&apos;, port=6379)@app.route(&apos;/&apos;)def hello(): count = redis.incr(&apos;hits&apos;) return &apos;Hello World! 该页面已被访问 {} 次。\\n&apos;.format(count)if __name__ == &quot;__main__&quot;: app.run(host=&quot;0.0.0.0&quot;, debug=True) Dockerfile编写 Dockerfile 文件，内容为 12345FROM python:3.6-alpineADD . /codeWORKDIR /codeRUN pip install redis flaskCMD [&quot;python&quot;, &quot;app.py&quot;] docker-compose.yml编写 docker-compose.yml 文件，这个是 Compose 使用的主模板文件。 12345678910version: &apos;3&apos;services: web: build: . ports: - &quot;5000:5000&quot; redis: image: &quot;redis:alpine&quot; 运行 compose 项目$ docker-compose up 此时访问本地 5000 端口，每次刷新页面，计数就会加 1。","link":"/2018/11/28/Docker三剑客/"},{"title":"Feddataset","text":"IntroductionInvestigate current larger federated learning projects’ datasets. motivated by both image classification and language modeling tasks, models2000 models, presented CIFAR10; a large language modeling task intial study: three model families on two datasets; 1) MNIST 2NN：A simple multilayer-perceptron with 2-hidden layers with 200 units each using ReLu activations (199,210 total parameters), which we refer to as the MNIST 2NN. 2) A CNN with two 5x5 convolution layers (the first with 32 channels, the second with 64, each followed with 2x2 max pooling), a fully connected layer with 512 units and ReLu activation, and a final softmax output layer (1,663,370 total parameters). FL paperCommunication-Efficient Learning of Deep Networks from Decentralized DataarXiv:1602.05629v3 datasetstwo ways of partitioning the MNIST data over clients: IID, where the data is shuffled, and then partitioned into 100 clients each receiving 600 examples, Non-IID, where we first sort the data by digit label, divide it into 200 shards of size 300, and assign each of 100 clients 2 shards. most client will only have example of two digits; Balanced, we performed additional experiments on unbalanced versions of these datasets, and found them to in fact be slightly easier for FedAvg. Table 1: Effect of the client fraction C on the MNIST 2NN with E = 1 and CNN with E = 5. Note C = 0.0 corre- sponds to one client per round; since we use 100 clients for the MNIST data, the rows correspond to 1, 10 20, 50, and 100 clients. Each table entry gives the number of rounds of communication necessary to achieve a test-set accuracy of 97% for the 2NN and 99% for the CNN, along with the speedup relative to the C = 0 baseline. Five runs with the large batch size did not reach the target accuracy in the allowed time. language modeling taskThe Complete Works ofWilliam Shakespeare: William Shakespeare. The Complete Works of William Shakespeare. Publically available at https: //www.gutenberg.org/ebooks/100. 对于语言建模，我们从威廉·莎士比亚的完整作品中建立了一个数据集[32]。我们为每个游戏中的每个说话角色构建一个客户数据集，至少有两行。这产生了一个包含1146个客户端的数据集。 《A Berkeley View of Systems Challenges for AI》 Berkeley: Shared learning on confidential data 微众银行AI团队“联邦迁移学习”新方法和它打造的开源“联邦学习”框架FATE(Federated AI Technology Enabler) 针对金融行业: Secure Federated Transfer Learning: 1) NUS-WIDE data set ([seng Chua et al. 2009] seng Chua, T.; Tang, J.; Hong, R.; Li,Nus-wide: A real-world web image database from national university of singapore. In CIVR)–由Flickr图像的数百个低级特征以及它们的关联标签和地面实况标签组成。 2) Kaggle’s Default-of-Credit-Card-Clients ([Kaggle ] Kaggle. Default of credit card clients dataset:https://www.kaggle.com/uciml/default-of-credit- card-clients-dataset.–包括重叠样本的数量，隐藏的共同表示的维度和特征的数量 SecureBoost: A Lossless Federated Learning Framework 1) Credit: https://www.kaggle.com/c/GiveMeSomeCredit/data – 它涉及分类用户是否会遭受严重财务问题的问题。它包含总共150000个实例和10个属性。 2) https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset pysyftpytorch Boston Housing and Pima Indian Diabetes datasets: https://archive.ics.uci.edu/ml/support/diabetes Vincent Sigillito. Pima indian diabetes dataset. Obtained from UCI, 1990. https://www.kaggle.com/kumargh/pimaindiansdiabetescsv Gboard的联盟学习 Federated Learning Of Out-Of-Vocabulary Words the public Reddit dataset: Rami Al-Rfou, Marc Pickett, Javier Snaider, Yun- hsuan Sung, Brian Strope, and Ray Kurzweil. 2016. Conversational contextual cues: The case of person- alization and history for response ranking. arXiv preprint arXiv:1606.00372.； https://bigquery.cloud.google.com/ Applied Federated Learning: Improving Google Keyboard Query Suggestions Multi-Institutional-DL-Modeling-wo-Sharing-Patient-Data宾夕法尼亚大学，医学:联盟学习在现实世界医学成像中的第一个概念验证应用 BraTS 2018 training dataset [6-9]： 1) the actual BraTS distribution, i.e. the real-world data distribution, and 2) simulated distributions of 4 to 32 institutions, in steps of powers of two. https://www.med.upenn.edu/sbia/brats2018/data.html 蚂蚁金服用于中和信贷 共享学习分为两个方案，一是基于TEE的可信计算平台方案，二是基于MPC的多方联合建模方案（非常类似于联邦学习） 蚂蚁金服给出了一些实验数据，实验数据虽然没有给出明确的出处，但是总体感觉是用了一些推荐系统研究用的开放数据集 创业公司https://s20.ai/home Healthcare；分析病患计费记录来发现欺诈报销或者计费案例; Smart Grid; Genomics; https://splitlearning.github.io/ Reducing leakage in distributed deep learning for sensitive health data Some example classes from images of colorectal histology dataset https://www.kaggle.com/kmader/colorectal-histology-mnist and mnist Split learning for health: Distributed deep learning without sharing raw patient data CIFAR 10 and CIFAR 100 datasets Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption https://archive.ics.uci.edu/ml/datasets.php 华为https://arxiv.org/pdf/1807.08127.pdf","link":"/2019/08/05/Feddataset/"},{"title":"CNN学习","text":"CNN学习快速学习背景知识在前面的文章中，我们介绍了全连接神经网络，以及它的训练和使用。我们用它来识别了手写数字，然而，这种结构的网络对于图像识别任务来说并不是很合适。本文将要介绍一种更适合图像、语音识别任务的神经网络结构——卷积神经网络(Convolutional Neural Network, CNN)。说卷积神经网络是最重要的一种神经网络也不为过，它在最近几年大放异彩，几乎所有图像、语音识别领域的重要突破都是卷积神经网络取得的，比如谷歌的GoogleNet、微软的ResNet等，打败李世石的AlphaGo也用到了这种网络。本文将详细介绍卷积神经网络以及它的训练算法，以及动手实现一个简单的卷积神经网络。 一个新的激活函数–ReLU最近几年卷积神经网络中，激活函数往往不选择sigmoid或tanh函数，而是选择relu函数。Relu函数的定义是：Relu函数图像如下图所示: Relu函数作为激活函数，有下面几大优势： 速度快 和sigmoid函数需要计算指数和倒数相比，relu函数其实就是一个max(0,x)，计算代价小很多。 减轻梯度消失问题 回忆一下计算梯度的公式其中，是sigmoid函数的导数。在使用反向传播算法进行梯度计算时，每经过一层sigmoid神经元，梯度就要乘上一个。从下图可以看出，函数最大值是1/4。因此，乘一个会导致梯度越来越小，这对于深层网络的训练是个很大的问题。而relu函数的导数是1，不会导致梯度变小。当然，激活函数仅仅是导致梯度减小的一个因素，但无论如何在这方面relu的表现强于sigmoid。使用relu激活函数可以让你训练更深的网络。 稀疏性 通过对大脑的研究发现，大脑在工作的时候只有大约5%的神经元是激活的，而采用sigmoid激活函数的人工神经网络，其激活率大约是50%。有论文声称人工神经网络在15%-30%的激活率时是比较理想的。因为relu函数在输入小于0时是完全不激活的，因此可以获得一个更低的激活率。 全连接网络 VS 卷积网络全连接神经网络之所以不太适合图像识别任务，主要有以下几个方面的问题： 参数数量太多 考虑一个输入10001000像素的图片(一百万像素，现在已经不能算大图了)，输入层有10001000=100万节点。假设第一个隐藏层有100个节点(这个数量并不多)，那么仅这一层就有(10001000+1)100=1亿参数，这实在是太多了！我们看到图像只扩大一点，参数数量就会多很多，因此它的扩展性很差。 没有利用像素之间的位置信息 对于图像识别任务来说，每个像素和其周围像素的联系是比较紧密的，和离得很远的像素的联系可能就很小了。如果一个神经元和上一层所有神经元相连，那么就相当于对于一个像素来说，把图像的所有像素都等同看待，这不符合前面的假设。当我们完成每个连接权重的学习之后，最终可能会发现，有大量的权重，它们的值都是很小的(也就是这些连接其实无关紧要)。努力学习大量并不重要的权重，这样的学习必将是非常低效的。 网络层数限制 我们知道网络层数越多其表达能力越强，但是通过梯度下降方法训练深度全连接神经网络很困难，因为全连接神经网络的梯度很难传递超过3层。因此，我们不可能得到一个很深的全连接神经网络，也就限制了它的能力。 那么，卷积神经网络又是怎样解决这个问题的呢？主要有三个思路： 局部连接 这个是最容易想到的，每个神经元不再和上一层的所有神经元相连，而只和一小部分神经元相连。这样就减少了很多参数。 权值共享 一组连接可以共享同一个权重，而不是每个连接有一个不同的权重，这样又减少了很多参数。 下采样 可以使用Pooling来减少每层的样本数，进一步减少参数数量，同时还可以提升模型的鲁棒性。 对于图像识别任务来说，卷积神经网络通过尽可能保留重要的参数，去掉大量不重要的参数，来达到更好的学习效果。 接下来，我们将详述卷积神经网络到底是何方神圣。 卷积神经网络是啥首先，我们先获取一个感性认识，下图是一个卷积神经网络的示意图： 网络架构如上图所示，一个卷积神经网络由若干卷积层、Pooling层、全连接层组成。你可以构建各种不同的卷积神经网络，它的常用架构模式为：INPUT -&gt; [[CONV]*N -&gt; POOL?]*M -&gt; [FC]*K 也就是N个卷积层叠加，然后(可选)叠加一个Pooling层，重复这个结构M次，最后叠加K个全连接层。 对于上图展示的卷积神经网络：INPUT -&gt; CONV -&gt; POOL -&gt; CONV -&gt; POOL -&gt; FC -&gt; FC按照上面描述：INPUT -&gt; [[CONV]*1 -&gt; POOL]*2 -&gt; [FC]*2 即N=1, M=2, K=2 三维的层结构从图1我们可以发现卷积神经网络的层结构和全连接神经网络的层结构有很大不同。全连接神经网络每层的神经元是按照一维排列的，也就是排成一条线的样子；而卷积神经网络每层的神经元是按照三维排列的，也就是排成一个长方体的样子，有宽度、高度和深度。 对于图1展示的神经网络，我们看到输入层的宽度和高度对应于输入图像的宽度和高度，而它的深度为1。接着，第一个卷积层对这幅图像进行了卷积操作(后面我们会讲如何计算卷积)，得到了三个Feature Map。这里的”3”可能是让很多初学者迷惑的地方，实际上，就是这个卷积层包含三个Filter，也就是三套参数，每个Filter都可以把原始输入图像卷积得到一个Feature Map，三个Filter就可以得到三个Feature Map。至于一个卷积层可以有多少个Filter，那是可以自由设定的。也就是说，卷积层的Filter个数也是一个超参数。我们可以把Feature Map可以看做是通过卷积变换提取到的图像特征，三个Filter就对原始图像提取出三组不同的特征，也就是得到了三个Feature Map，也称做三个通道(channel)。 继续观察图1，在第一个卷积层之后，Pooling层对三个Feature Map做了下采样(后面我们会讲如何计算下采样)，得到了三个更小的Feature Map。接着，是第二个卷积层，它有5个Filter。每个Fitler都把前面下采样之后的3个Feature Map卷积在一起，得到一个新的Feature Map。这样，5个Filter就得到了5个Feature Map。接着，是第二个Pooling，继续对5个Feature Map进行下采样，得到了5个更小的Feature Map。 图1所示网络的最后两层是全连接层。第一个全连接层的每个神经元，和上一层5个Feature Map中的每个神经元相连，第二个全连接层(也就是输出层)的每个神经元，则和第一个全连接层的每个神经元相连，这样得到了整个网络的输出。 至此，我们对卷积神经网络有了最基本的感性认识。接下来，我们将介绍卷积神经网络中各种层的计算和训练。 卷积神经网络输出值的计算卷积层输出值的计算我们用一个简单的例子来讲述如何计算卷积，然后，我们抽象出卷积层的一些重要概念和计算方法。 假设有一个5*5的图像，使用一个3*3的filter进行卷积，想得到一个3*3的Feature Map，如下所示： 为了清楚的描述卷积计算过程，我们首先对图像的每个像素进行编号，用表示图像的第i行第i列元素；对filter的每个权重进行编号，用表示第m行第n列权重，用表示filter的偏置项；对Feature Map的每个元素进行编号，用表示Feature Map的第i行第j列元素；用f表示激活函数(这个例子选择relu函数作为激活函数)。然后，使用下列公式计算卷积：例如，对于Feature Map左上角元素来说，其卷积计算方法为：计算结果如下图所示:接下来，Feature Map的元素的卷积计算方法为：计算结果如下图所示：可以依次计算出Feature Map中所有元素的值。下面的动画显示了整个Feature Map的计算过程：上面的计算过程中，步幅(stride)为1。步幅可以设为大于1的数。例如，当步幅为2时，Feature Map计算如下：我们注意到，当步幅设置为2的时候，Feature Map就变成2*2了。这说明图像大小、步幅和卷积后的Feature Map大小是有关系的。事实上，它们满足下面的关系：在上面两个公式中，是卷积后Feature Map的宽度；是卷积前图像的宽度；F是filter的宽度；P是Zero Padding数量，Zero Padding是指在原始图像周围补几圈0，如果P的值是1，那么就补1圈0；S是步幅；是卷积后Feature Map的高度；是卷积前图像的宽度。式2和式3本质上是一样的。以前面的例子来说，图像宽度=5，filter宽度F=3，Zero Padding的值P=0，步幅S=2，则说明Feature Map宽度是2。同样，我们也可以计算出Feature Map高度也是2。前面我们已经讲了深度为1的卷积层的计算方法，如果深度大于1怎么计算呢？其实也是类似的。如果卷积前的图像深度为D，那么相应的filter的深度也必须为D。我们扩展一下式1，得到了深度大于1的卷积计算公式：在式4中，D是深度；F是filter的大小(宽度或高度，两者相同)；表示filter的第d层第m行第n列权重；表示图像的第d层第i行第j列像素；其它的符号含义和式1是相同的，不再赘述。我们前面还曾提到，每个卷积层可以有多个filter。每个filter和原始图像进行卷积后，都可以得到一个Feature Map。因此，卷积后Feature Map的深度(个数)和卷积层的filter个数是相同的。 下面的动画显示了包含两个filter的卷积层的计算。我们可以看到7*7*3输入，经过两个3*3*3filter的卷积(步幅为2)，得到了3*3*2的输出。另外我们也会看到下图的Zero padding是1，也就是在输入元素的周围补了一圈0。Zero padding对于图像边缘部分的特征提取是很有帮助的。以上就是卷积层的计算方法。这里面体现了局部连接和权值共享：每层神经元只和上一层部分神经元相连(卷积计算规则)，且filter的权值对于上一层所有神经元都是一样的。对于包含两个3*3*3的fitler的卷积层来说，其参数数量仅有(3*3*3+1)*2=56个，且参数数量与上一层神经元个数无关。与全连接神经网络相比，其参数数量大大减少了。 用卷积公式来表达卷积层计算(暂时跳过)式4的表达很是繁冗，最好能简化一下。就像利用矩阵可以简化表达全连接神经网络的计算一样，我们利用卷积公式可以简化卷积神经网络的表达。下面我们介绍二维卷积公式。设矩阵A，B，其行、列数分别为、、、，则二维卷积公式如下：且使得满足条件我们可以把上式写成如果我们按照式5来计算卷积，我们可以发现矩阵A实际上是filter，而矩阵B是待卷积的输入，位置关系也有所不同：从上图可以看到，A左上角的值与B对应区块中右下角的值相乘，而不是与左上角的相乘。因此，数学中的卷积和卷积神经网络中的『卷积』还是有区别的，为了避免混淆，我们把卷积神经网络中的『卷积』操作叫做互相关(cross-correlation)操作。卷积和互相关操作是可以转化的。首先，我们把矩阵A翻转180度，然后再交换A和B的位置（即把B放在左边而把A放在右边。卷积满足交换率，这个操作不会导致结果变化），那么卷积就变成了互相关。如果我们不去考虑两者这么一点点的区别，我们可以把式5代入到式4：其中，是卷积层输出的feature map。同式4相比，式6就简单多了。然而，这种简洁写法只适合步长为1的情况。 Pooling层输出值的计算Pooling层主要的作用是下采样，通过去掉Feature Map中不重要的样本，进一步减少参数数量。Pooling的方法很多，最常用的是Max Pooling。Max Pooling实际上就是在nn的样本中取最大值，作为采样后的样本值。下图是22 max pooling：除了Max Pooing之外，常用的还有Mean Pooling——取各样本的平均值。对于深度为D的Feature Map，各层独立做Pooling，因此Pooling后的深度仍然为D。 全连接层零基础入门深度学习(3) - 神经网络和反向传播算法 卷积神经网络的训练和全连接神经网络相比，卷积神经网络的训练要复杂一些。但训练的原理是一样的： 利用链式求导计算损失函数对每个权重的偏导数（梯度）， 然后根据梯度下降公式更新权重。 训练算法依然是反向传播算法。 神经网络和反向传播算法整个算法分为三个步骤： 前向计算每个神经元的输出值（j表示网络的第j个神经元，以下同）； 反向计算每个神经元的误差项，在有的文献中也叫做敏感度(sensitivity)。它实际上是网络的损失函数对神经元加权输入的偏导数，即: 计算每个神经元连接权重的梯度（表示从神经元i连接到神经元j的权重），公式为，其中，表示神经元i的输出。 最后，根据梯度下降法则更新每个权重即可。对于卷积神经网络，由于涉及到局部连接、下采样的等操作，影响到了第二步误差项的具体计算方法，而权值共享影响了第三步权重的梯度的计算方法。接下来，我们分别介绍卷积层和Pooling层的训练算法。 卷积层的训练对于卷积层，我们先来看看上面的第二步，即如何将误差项传递到上一层；然后再来看看第三步，即如何计算filter每个权值的梯度。 卷积层误差项的传递最简单情况下误差项的传递我们先来考虑步长为1、输入的深度为1、filter个数为1的最简单的情况。假设输入的大小为3*3，filter大小为2*2，按步长为1卷积，我们将得到2*2的feature map。如下图所示：在上图中，为了描述方便，我们为每个元素都进行了编号。用表示第层第i行第j列的误差项；用表示filter第m行第n列权重，用表示filter的偏置项；用表示第层第i行第j列神经元的输出；用表示第行神经元的加权输入；用表示第l层第i行第j列的误差项；用表示第层的激活函数。它们之间的关系如下：上式中，、、都是数组，是由组成的数组，conv表示卷积操作。在这里，我们假设第中的每个值都已经算好，我们要做的是计算第层每个神经元的误差项。根据链式求导法则：从上面三个例子，我们发挥一下想象力，不难发现，计算，相当于把第l层的sensitive map周围补一圈0，在与180度翻转后的filter进行cross-correlation，就能得到想要结果，如下图所示：因为卷积相当于将filter旋转180度的cross-correlation，因此上图的计算可以用卷积公式完美的表达：上式中的表示第层的filter的权重数组。也可以把上式的卷积展开，写成求和的形式：现在，我们再求第二项。因为所以这一项极其简单，仅求激活函数的导数就行了。将第一项和第二项组合起来，我们得到最终的公式：也可以将式7写成卷积的形式：其中，符号表示element-wise product，即将矩阵中每个对应元素相乘。注意式8中的、、都是矩阵。以上就是步长为1、输入的深度为1、filter个数为1的最简单的情况，卷积层误差项传递的算法。下面我们来推导一下步长为S的情况. 卷积步长为S时的误差传递我们先来看看步长为S与步长为1的差别。如上图，上面是步长为1时的卷积结果，下面是步长为2时的卷积结果。我们可以看出，因为步长为2，得到的feature map跳过了步长为1时相应的部分。因此，当我们反向计算误差项时，我们可以对步长为S的sensitivity map相应的位置进行补0，将其『还原』成步长为1时的sensitivity map，再用式8进行求解。 输入层深度为D时的误差传递当输入深度为D时，filter的深度也必须为D，层的通道只与filter的通道的权重进行计算。因此，反向计算误差项时，我们可以使用式8，用filter的第通道权重对第层sensitivity map进行卷积，得到第层通道的sensitivity map。如下图所示： filter数量为N时的误差传递filter数量为N时，输出层的深度也为N，第i个filter卷积产生输出层的第i个feature map。由于第层每个加权输入都同时影响了第层所有feature map的输出值，因此，反向计算误差项时，需要使用全导数公式。也就是，我们先使用第d个filter对第层相应的第d个sensitivity map进行卷积，得到一组N个层的偏sensitivity map。依次用每个filter做这种卷积，就得到D组偏sensitivity map。最后在各组之间将N个偏sensitivity map 按元素相加，得到最终的N个层的sensitivity map：以上就是卷积层误差项传递的算法，如果读者还有所困惑，可以参考后面的代码实现来理解。 卷积层filter权重梯度的计算我们要在得到第层sensitivity map的情况下，计算filter的权重的梯度，由于卷积层是权重共享的，因此梯度的计算稍有不同。如上图所示，是第层的输出，是第l层filter的权重，是第l层的sensitivity map。我们的任务是计算的梯度，即。为了计算偏导数，我们需要考察权重对的影响。权重项通过影响的值，进而影响。我们仍然通过几个具体的例子来看权重项对的影响，然后再从中总结出规律。从上面的公式看出，由于权值共享，权值对所有的都有影响。是每一个的函数，而每一个又是的函数，根据全导数公式，计算就是要把每个偏导数都加起来：例2，计算：通过查看与的关系，我们很容易得到：实际上，每个权重项都是类似的，我们不一一举例了。现在，是我们再次发挥想象力的时候，我们发现计算规律是：也就是用sensitivity map作为卷积核，在input上进行cross-correlation，如下图所示：最后，我们来看一看偏置项的梯度。通过查看前面的公式，我们很容易发现：也就是偏置项的梯度就是sensitivity map所有误差项之和。对于步长为S的卷积层，处理方法与传递误差项是一样的，首先将sensitivity map『还原』成步长为1时的sensitivity map，再用上面的方法进行计算。获得了所有的梯度之后，就是根据梯度下降算法来更新每个权重。这在前面的文章中已经反复写过，这里就不再重复了。 至此，我们已经解决了卷积层的训练问题，接下来我们看一看Pooling层的训练。 Pooling层的训练无论max pooling还是mean pooling，都没有需要学习的参数。因此，在卷积神经网络的训练中，Pooling层需要做的仅仅是将误差项传递到上一层，而没有梯度的计算。 Max Pooling误差项的传递如下图，假设第层大小为4*4，pooling filter大小为2*2，步长为2，这样，max pooling之后，第l层大小为2*2。假设第l层的值都已经计算完毕，我们现在的任务是计算第层的值。我们用表示第l-1层的加权输入；用表示第l层的加权输入。我们先来考察一个具体的例子，然后再总结一般性的规律。对于max pooling：也就是说，只有区块中最大的才会对的值产生影响。我们假设最大的值是，则上式相当于：那么，我们不难求得下面几个偏导数：因此：而：现在，我们发现了规律：对于max pooling，下一层的误差项的值会原封不动的传递到上一层对应区块中的最大值所对应的神经元，而其他神经元的误差项的值都是0。如下图所示(假设、、、为所在区块中的最大输出值)： Mean Pooling误差项的传递我们还是用前面屡试不爽的套路，先研究一个特殊的情形，再扩展为一般规律。同理我们发现了规律：对于mean pooling，下一层的误差项的值会平均分配到上一层对应区块中的所有神经元。如下图所示：上面这个算法可以表达为高大上的克罗内克积(Kronecker product)的形式，有兴趣的读者可以研究一下。其中，n是pooling层filter的大小，、都是矩阵。 至此，我们已经把卷积层、Pooling层的训练算法介绍完毕，加上上一篇文章讲的全连接层训练算法，您应该已经具备了编写卷积神经网络代码所需要的知识。为了加深对知识的理解，接下来，我们将展示如何实现一个简单的卷积神经网络。 卷积神经网络的实现现在，我们亲自动手实现一个卷积神经网络，以便巩固我们所学的知识。 首先，我们要改变一下代码的架构，『层』成为了我们最核心的组件。这是因为卷积神经网络有不同的层，而每种层的算法都在对应的类中实现。 这次，我们用到了在python中编写算法经常会用到的numpy包。为了使用numpy，我们需要先将numpy导入： 1import numpy as np 卷积层的实现卷积层初始化我们用ConvLayer类来实现一个卷积层。下面的代码是初始化一个卷积层，可以在构造函数中设置卷积层的超参数。 123456789101112131415161718192021222324# 用ConvLayer类来实现一个卷积层。 class ConvLayer(object): # 下面的代码是初始化一个卷积层，可以在构造函数中设置卷积层的超参数 def __init__(self, input_width, input_height, channel_number, filter_width, filter_height, filter_number, zero_padding, stride, activator, learning_rate): self.input_width = input_width # 输入宽度 self.input_height = input_height # 输入高度 self.channel_number = channel_number # 通道数=输入的深度=过滤器的深度 self.filter_width = filter_width # 过滤器的宽度 self.filter_height = filter_height # 过滤器的高度 self.filter_number = filter_number # 过滤器的数量。 self.zero_padding = zero_padding # 补0圈数 self.stride = stride # 步幅 self.output_width = int(ConvLayer.calculate_output_size(self.input_width, filter_width, zero_padding,stride)) # 计算输出宽度 self.output_height = int(ConvLayer.calculate_output_size(self.input_height, filter_height, zero_padding,stride)) # 计算输出高度 self.output_array = np.zeros((self.filter_number,self.output_height, self.output_width)) # 创建输出三维数组。每个过滤器都产生一个二维数组的输出 self.filters = [] # 卷积层的每个过滤器 for i in range(filter_number): self.filters.append(Filter(filter_width,filter_height, self.channel_number)) self.activator = activator # 使用rule激活器 self.learning_rate = learning_rate # 学习速率 calculate_output_size函数用来确定卷积层输出的大小，其实现如下： 1234# 确定卷积层输出的大小 @staticmethod def calculate_output_size(input_size,filter_size, zero_padding, stride): return (input_size - filter_size + 2 * zero_padding) / stride + 1 Filter类保存了卷积层的参数以及梯度，并且实现了用梯度下降算法来更新参数。 1234567891011121314151617181920212223# Filter类保存了卷积层的参数以及梯度，并且实现了用梯度下降算法来更新参数。 class Filter(object): def __init__(self, width, height, depth): self.weights = np.random.uniform(-1e-4, 1e-4,(depth, height, width)) # 随机初始化卷基层权重一个很小的值， self.bias = 0 # 初始化偏量为0 self.weights_grad = np.zeros(self.weights.shape) # 初始化权重梯度 self.bias_grad = 0 # 初始化偏量梯度 def __repr__(self): return 'filter weights:\\n%s\\nbias:\\n%s' % (repr(self.weights), repr(self.bias)) # 读取权重 def get_weights(self): return self.weights # 读取偏量 def get_bias(self): return self.bias # 更新权重和偏量 def update(self, learning_rate): self.weights -= learning_rate * self.weights_grad self.bias -= learning_rate * self.bias_grad 我们对参数的初始化采用了常用的策略，即：权重随机初始化为一个很小的值，而偏置项初始化为0。 ReluActivator类实现了relu激活函数，其中，forward方法实现了前向计算，而backward方法则是计算导数。比如，relu函数的实现如下: 123456# rule激活器 class ReluActivator(object): def forward(self, weighted_input): # 前向计算，计算输出 return max(0, weighted_input) def backward(self, output): # 后向计算，计算导数 return 1 if output &gt; 0 else 0 IdentityActivator类实现f(x)=x激活函数，其中，forward方法实现了前向计算，而backward方法则是计算导数。 123456# IdentityActivator激活器.f(x)=x class IdentityActivator(object): def forward(self, weighted_input): # 前向计算，计算输出 return weighted_input def backward(self, output): # 后向计算，计算导数 return 1 卷积层前向计算的实现ConvLayer类的forward方法实现了卷积层的前向计算（即计算根据输入来计算卷积层 123456789# 计算卷积层的输出。输出结果保存在self.output_array def forward(self, input_array): self.input_array = input_array # 多个通道的图片，每个通道为一个二维图片 self.padded_input_array = padding(input_array,self.zero_padding) # 先将输入补足0 for i in range(self.filter_number): #每个过滤器都产生一个二维数组的输出 filter = self.filters[i] conv(self.padded_input_array,filter.get_weights(), self.output_array[i],self.stride, filter.get_bias()) # element_wise_op函数实现了对numpy数组进行按元素操作，并将返回值写回到数组中 element_wise_op(self.output_array,self.activator.forward) 上面的代码里面包含了几个工具函数。element_wise_op函数实现了对numpy数组进行按元素操作，并将返回值写回到数组中，代码如下： 1234# 对numpy数组进行逐个元素的操作。op为函数。element_wise_op函数实现了对numpy数组进行按元素操作，并将返回值写回到数组中 def element_wise_op(array, op): for i in np.nditer(array,op_flags=['readwrite']): i[...] = op(i) # 将元素i传入op函数，返回值，再修改i conv函数实现了2维和3维数组的卷积，代码如下： 123456789101112# 计算一个过滤器的卷积运算，输出一个二维数据。每个通道的输入是图片，但是可能不是一个通道，所以这里自动适配输入为2D和3D的情况。 def conv(input_array,kernel_array,output_array,stride, bias): output_width = output_array.shape[1] # 获取输出的宽度。一个过滤器产生的输出一定是一个通道 output_height = output_array.shape[0] # 获取输出的高度 kernel_width = kernel_array.shape[-1] # 过滤器的宽度。有可能有多个通道。多通道时shape=[深度、高度、宽度]，单通道时shape=[高度、宽度] kernel_height = kernel_array.shape[-2] # 过滤器的高度。有可能有多个通道。多通道时shape=[深度、高度、宽度]，单通道时shape=[高度、宽度] for i in range(output_height): for j in range(output_width): juanjiqu = get_patch(input_array, i, j, kernel_width,kernel_height, stride) # 获取输入的卷积区。（单通道或多通道） # 这里是对每个通道的两个矩阵对应元素相乘求和，再将每个通道的和值求和 kernel_values= (np.multiply(juanjiqu,kernel_array)).sum() # 卷积区与过滤器卷积运算。1，一个通道内，卷积区矩阵与过滤器矩阵对应点相乘后，求和值。2、将每个通道的和值再求和。 output_array[i][j] = kernel_values + bias #将卷积结果加上偏量 padding函数实现了zero padding操作： 123456789101112131415161718# 为数组增加Zero padding。zp步长，自动适配输入为2D和3D的情况 def padding(input_array, zp): if zp == 0: # 如果不补0 return input_array else: if input_array.ndim == 3: # 如果输入有多个通道 input_width = input_array.shape[2] # 获取输入的宽度 input_height = input_array.shape[1] # 获取输入的宽度 input_depth = input_array.shape[0] # 获取输入的深度 padded_array = np.zeros((input_depth,input_height + 2 * zp,input_width + 2 * zp)) # 先定义一个补0后大小的全0矩阵 padded_array[:,zp: zp + input_height,zp: zp + input_width] = input_array # 每个通道上，将中间部分替换成输入，这样就变成了原矩阵周围补0 的形式 return padded_array elif input_array.ndim == 2: # 如果输入只有一个通道 input_width = input_array.shape[1] # 获取输入的宽度 input_height = input_array.shape[0] # 虎丘输入的高度 padded_array = np.zeros((input_height + 2 * zp,input_width + 2 * zp)) # 先定义一个补0后大小的全0矩阵 padded_array[zp: zp + input_height,zp: zp + input_width] = input_array # 将中间部分替换成输入，这样就变成了原矩阵周围补0 的形式 return padded_array 卷积层反向传播算法的实现现在，是介绍卷积层核心算法的时候了。我们知道反向传播算法需要完成几个任务： 将误差项传递到上一层。 计算每个参数的梯度。 更新参数。以下代码都是在ConvLayer类中实现。我们先来看看将误差项传递到上一层的代码实现。 12345678910111213141516171819202122232425262728293031# 将误差项传递到上一层。sensitivity_array: 本层的误差。activator: 上一层的激活函数 def bp_sensitivity_map(self, sensitivity_array,activator): # 公式9 # 根据卷积步长，对原始sensitivity map进行补0扩展，扩展成如果步长为1的输出误差形状。再用公式8求解 expanded_error_array = self.expand_sensitivity_map(sensitivity_array) # print(sensitivity_array) # full卷积，对sensitivitiy map进行zero padding # 虽然原始输入的zero padding单元也会获得残差，但这个残差不需要继续向上传递，因此就不计算了 expanded_width = expanded_error_array.shape[2] # 误差的宽度 zp = int((self.input_width + self.filter_width - 1 - expanded_width) / 2) # 计算步长 padded_array = padding(expanded_error_array, zp) #补0操作 # 初始化delta_array，用于保存传递到上一层的sensitivity map self.delta_array = self.create_delta_array() # 对于具有多个filter的卷积层来说，最终传递到上一层的sensitivity map相当于所有的filter的sensitivity map之和 for i in range(self.filter_number): # 遍历每一个过滤器。每个过滤器都产生多通道的误差，多个多通道的误差叠加 filter = self.filters[i] # 将滤波器每个通道的权重权重翻转180度。 flipped_weights=[] for oneweight in filter.get_weights(): # 这一个滤波器下的每个通道都进行180翻转 flipped_weights.append(np.rot90(oneweight, 2)) flipped_weights = np.array(flipped_weights) # 计算与一个filter对应的delta_array delta_array = self.create_delta_array() for d in range(delta_array.shape[0]): # 计算每个通道上的误差，存储在delta_array的对应通道上 # print(&apos;大小：\\n&apos;,flipped_weights[d]) conv(padded_array[i], flipped_weights[d],delta_array[d], 1, 0) self.delta_array += delta_array # 将每个滤波器每个通道产生的误差叠加 # 将计算结果与激活函数的偏导数做element-wise乘法操作 derivative_array = np.array(self.input_array) # 复制一个矩阵，因为下面的会改变元素的值，所以深复制了一个矩阵 element_wise_op(derivative_array,activator.backward) # 逐个元素求偏导数。 self.delta_array *= derivative_array # 误差乘以偏导数。得到上一层的误差 expand_sensitivity_map方法就是将步长为S的sensitivity map『还原』为步长为1的sensitivity map，代码如下 123456789101112131415# 对步长为S的sensitivitymap相应的位置进行补0，将其『还原』成步长为1时的sensitivitymap，再用式8进行求解 def expand_sensitivity_map(self, sensitivity_array): depth = sensitivity_array.shape[0] # 获取误差项的深度 # 确定扩展后sensitivity map的大小，即计算stride为1时sensitivity map的大小 expanded_width = (self.input_width - self.filter_width + 2 * self.zero_padding + 1) expanded_height = (self.input_height - self.filter_height + 2 * self.zero_padding + 1) # 构建新的sensitivity_map expand_array = np.zeros((depth, expanded_height, expanded_width)) # 从原始sensitivity map拷贝误差值，每有拷贝的位置，就是要填充的0 for i in range(self.output_height): for j in range(self.output_width): i_pos = i * self.stride j_pos = j * self.stride expand_array[:, i_pos, j_pos] = sensitivity_array[:, i, j] return expand_array create_delta_array是创建用来保存传递到上一层的sensitivity map的数组 123# 创建用来保存传递到上一层的sensitivity map的数组。（上一层的输出也就是这一层的输入。所以上一层的误差项的维度和这一层的输入的维度相同） def create_delta_array(self): return np.zeros((self.channel_number,self.input_height, self.input_width)) 接下来，是计算梯度的代码 123456789101112# 计算梯度。根据误差值，计算本层每个过滤器的w和b的梯度 def bp_gradient(self, sensitivity_array): # 处理卷积步长，对原始sensitivity map进行扩展 expanded_error_array = self.expand_sensitivity_map(sensitivity_array) for i in range(self.filter_number): # 每个过滤器产生一个输出 # 计算每个权重的梯度 filter = self.filters[i] for d in range(filter.weights.shape[0]): # 过滤器的每个通道都要计算梯度 conv(self.padded_input_array[d],expanded_error_array[i],filter.weights_grad[d], 1, 0) # 公式（31、32中间） # 计算偏置项的梯度 filter.bias_grad = expanded_error_array[i].sum() # 公式（34） 最后，是按照梯度下降算法更新参数的代码，这部分非常简单 1234# 按照梯度下降，更新权重 def update(self): for filter in self.filters: filter.update(self.learning_rate) # 每个过滤器 卷积层的梯度检查为了验证我们的公式推导和代码实现的正确性，我们必须要对卷积层进行梯度检查。下面是代吗实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091def init_test(): a = np.array( # 作为输入 [[[0, 1, 1, 0, 2], [2, 2, 2, 2, 1], [1, 0, 0, 2, 0], [0, 1, 1, 0, 0], [1, 2, 0, 0, 2]], [[1, 0, 2, 2, 0], [0, 0, 0, 2, 0], [1, 2, 1, 2, 1], [1, 0, 0, 0, 0], [1, 2, 1, 1, 1]], [[2, 1, 2, 0, 0], [1, 0, 0, 1, 0], [0, 2, 1, 0, 1], [0, 1, 2, 2, 2], [2, 1, 0, 0, 1]]]) b = np.array( # 作为输出误差 [[[0, 1, 1], [2, 2, 2], [1, 0, 0]], [[1, 0, 2], [0, 0, 0], [1, 2, 1]]]) cl = ConvLayer(5, 5, 3, 3, 3, 2, 1, 2, IdentityActivator(), 0.001) cl.filters[0].weights = np.array( # 初始化第一层卷积层权重 [[[-1, 1, 0], [0, 1, 0], [0, 1, 1]], [[-1, -1, 0], [0, 0, 0], [0, -1, 0]], [[0, 0, -1], [0, 1, 0], [1, -1, -1]]], dtype=np.float64) cl.filters[0].bias = 1 # 初始化第一层卷积层偏重 cl.filters[1].weights = np.array( # 初始化第二层卷积层权重 [[[1, 1, -1], [-1, -1, 1], [0, -1, 1]], [[0, 1, 0], [-1, 0, -1], [-1, 1, 0]], [[-1, 0, 0], [-1, 0, 1], [-1, 0, 0]]], dtype=np.float64) return a, b, cl # 测试前向传播 def test(): a, b, cl = init_test() cl.forward(a) # 对输出进行以一次前向预测 # print(cl.output_array) # 测试后向传播 def test_bp(): a, b, cl = init_test() cl.backward(a, b, IdentityActivator()) # 对输出误差后向传播 cl.update() # 跟新权重 # print(cl.filters[0]) # 查看说过更新一次后的滤波器权重 # print(cl.filters[1]) # 查看说过更新一次后的滤波器权重 # 梯度检查 def gradient_check(): # 设计一个误差函数，取所有节点输出项之和 error_function = lambda o: o.sum() # 计算forward值 a, b, cl = init_test() cl.forward(a) # 对输入进行一次预测 # 求取sensitivity map sensitivity_array = np.ones(cl.output_array.shape,dtype=np.float64) # 计算梯度 cl.backward(a, sensitivity_array,IdentityActivator()) # 检查梯度 epsilon = 10e-4 for d in range(cl.filters[0].weights_grad.shape[0]): for i in range(cl.filters[0].weights_grad.shape[1]): for j in range(cl.filters[0].weights_grad.shape[2]): cl.filters[0].weights[d, i, j] += epsilon cl.forward(a) err1 = error_function(cl.output_array) cl.filters[0].weights[d, i, j] -= 2 * epsilon cl.forward(a) err2 = error_function(cl.output_array) expect_grad = (err1 - err2) / (2 * epsilon) cl.filters[0].weights[d, i, j] += epsilon print(&apos;weights(%d,%d,%d): expected - actural %f - %f&apos; % (d, i, j, expect_grad, cl.filters[0].weights_grad[d, i, j])) 上面代码值得思考的地方在于，传递给卷积层的sensitivity map是全1数组，留给读者自己推导一下为什么是这样（提示：激活函数选择了identity函数：）。运行上面梯度检查的代码，我们得到的输出如下，期望的梯度和实际计算出的梯度一致，这证明我们的算法推导和代码实现确实是正确的。 以上就是卷积层的实现。 Max Pooling层的实现max pooling层的实现相对简单，我们直接贴出全部代码如下： 12345678910111213141516171819202122232425262728293031# Max Pooling层的实现。就是一个卷积区域取最大值，形成输出。除了Max Pooing之外，常用的还有Mean Pooling——取各样本的平均值。 class MaxPoolingLayer(object): def __init__(self, input_width, input_height, channel_number, filter_width, filter_height, stride): self.input_width = input_width self.input_height = input_height self.channel_number = channel_number self.filter_width = filter_width self.filter_height = filter_height self.stride = stride self.output_width = int((input_width -filter_width) / self.stride + 1) self.output_height = int((input_height -filter_height) / self.stride + 1) self.output_array = np.zeros((self.channel_number,self.output_height, self.output_width)) # 前向计算。 def forward(self, input_array): for d in range(self.channel_number): for i in range(self.output_height): for j in range(self.output_width): self.output_array[d, i, j] = (get_patch(input_array[d], i, j,self.filter_width,self.filter_height,self.stride).max()) # 获取卷积区后去最大值 # 后向传播更新w和b def backward(self, input_array, sensitivity_array): self.delta_array = np.zeros(input_array.shape) for d in range(self.channel_number): for i in range(self.output_height): for j in range(self.output_width): patch_array = get_patch(input_array[d], i, j,self.filter_width,self.filter_height,self.stride) # 获取卷积区 k, l = get_max_index(patch_array) # 获取最大值的位置 self.delta_array[d,i * self.stride + k,j * self.stride + l] = sensitivity_array[d, i, j] # 更新误差 全连接层的实现和上一篇文章类似，在此就不再赘述了。至此，你已经拥有了实现了一个简单的卷积神经网络所需要的基本组件。对于卷积神经网络，现在有很多优秀的开源实现，因此我们并不需要真的自己去实现一个。贴出这些代码的目的是为了让我们更好的了解卷积神经网络的基本原理。 卷积神经网络的应用MNIST手写数字识别LeNet-5是实现手写数字识别的卷积神经网络，在MNIST测试集上，它取得了0.8%的错误率。LeNet-5的结构如下：关于LeNet-5的详细介绍，网上的资料很多，因此就不再重复了。感兴趣的读者可以尝试用我们自己实现的卷积神经网络代码去构造并训练LeNet-5（当然代码会更复杂一些）。","link":"/2018/05/09/CNN学习/"},{"title":"FEDERATED LEARNING: STRATEGIES FOR IMPROVING COMMUNICATION EFFICIENCY","text":"联合学习：提高通信效率的策略摘要 联合学习是一种机器学习设置，其目标是培训高质量的集中模型，同时培训数据仍然分布在大量客户端上，每个客户端都具有不可靠且相对较慢的网络连接。我们考虑针对此设置的学习算法，其中在每一轮中，每个客户端独立地基于其本地数据计算对当前模型的更新，并将该更新传送到中央服务器，其中聚合客户端更新以计算新的全局模式。此设置中的典型客户端是移动电话，通信效率至关重要。在本文中，我们提出了两种降低上行链路通信成本的方法：结构化更新，我们直接从使用较少数量的变量参数化的受限空间中学习更新，例如：低级别或随机掩码;另外是sketch update，我们学习完整的模型更新，然后在将其发送到服务器之前使用量化，随机旋转和子采样的组合进行压缩。在卷积网络和循环网络上的实验表明，所提出的方法可以将通信成本降低两个数量级。 1 Introduction随着数据集变得越来越大，模型越来越复杂，训练机器学习模型越来越需要在多台机器上分配模型参数的优化。现有的机器学习算法专为高度受控的环境（如数据中心）而设计，在这些环境中，数据以均衡的方式在i.i.d.中分布。时尚和高吞吐量网络可用。 最近，联邦学习（以及相关的分散方法）（McMahan＆Ramage，2017; Konecn’y等，2016; McMahan等，2017; Shokri＆Shmatikov，2015）已​​被提议作为替代设置：共享全球模型在中央服务器的协调下训练，来自参与设备的联合。参与设备（客户端）通常数量很大并且具有缓慢或不稳定的互联网连接。当培训数据来自用户与移动应用程序的交互时，就会出现联合学习的主要动机示例。联合学习使移动电话能够协作学习共享预测模型，同时将所有培训数据保留在设备上，从而将机器学习的能力与在云中存储数据的需求脱钩。训练数据在本地保存在用户的移动设备上，并且设备用作对其本地数据执行计算的节点，以便更新全局模型。通过对设备进行模型培训，这不仅仅是使用可以在移动设备上进行预测的本地模型。上述框架不同于传统的分布式机器学习（Reddi等，2016; Ma等，2017; Shamir等，2014; Zhang＆Lin，2015; Dean等，2012; Chilimbi等，2014 ）由于客户数量非常多，高度不平衡和非iid每个客户端上可用的数据，以及相对较差的网络连接。在这项工作中，我们的重点是最后一个约束，因为这些不可靠和不对称的连接对实际的联邦学习提出了特殊的挑战。 为简单起见，我们考虑联合学习的同步算法，其中典型的一轮包括以下步骤：1.选择现有客户端的子集，每个客户端都下载当前模型。2.子集中的每个客户端都根据其本地数据计算更新的模型。3.模型更新从选定的客户端发送到服务器。4.服务器聚合这些模型（通常通过平均）来构建改进的全局模型。 上述框架的简单实现要求每个客户端在每轮中将完整模型（或完整模型更新）发送回服务器。对于大型模型，由于多种因素，这一步骤很可能成为联邦学习的瓶颈。一个因素是互联网连接速度的不对称属性：上行链路通常比下行链路慢得多。美国平均宽带速度下载为55.0Mbps，而上传速度为18.9Mbps，一些互联网服务提供商明显更加不对称，例如，X网络速率低于125Mbps，低于15Mbps（speedtest.net，2016）。另外，现有的模型压缩方案如Han等人。 （2015）可以减少下载当前模型所需的带宽，并建立加密协议，以确保在平均数百或数千个其他更新（Bonawitz等，2017）之前不会检查个别客户的更新（进一步增加）需要上传的位数。 因此，研究可以降低上行链路通信成本的方法很重要。在本文中，我们研究了两种通用方法：•结构化更新，我们直接从受限空间学习更新，可以使用较少数量的变量进行参数化。•素描更新，我们学习完整模型更新，然后在发送到服务器之前对其进行压缩。 在第2节和第3节中详细解释的这些方法可以组合，例如，首先学习结构化更新并绘制草图;我们不会在这项工作中尝试这种组合。 在下文中，我们正式描述了这个问题。联邦学习的目标是从存储在大量客户端的数据中学习具有包含在实矩阵中的参数的模型1W∈Rd1×d 2。我们首先提供了一个未通信的联邦学习版本。在t≥0的轮次中，服务器将当前模型W t分配给n t个客户端的子集S t。这些客户端根据其本地数据独立更新模型。让更新的局部模型为W t 1，W t 2，…. 。 。 ，W t n t，因此对于i∈St，客户i的更新可写为H t i：= W t i -W t。这些更新可以是在客户端上计算的单个梯度，但通常是更复杂计算的结果，例如，在客户端的本地数据集上采用的多步骤随机梯度下降（SGD）。在任何情况下，每个选定的客户端然后将更新发送回服务器，其中通过聚合所有客户端更新来计算全局更新： 2. STRUCTURED UPDATE第一种类型的通信有效更新限制了更新H t i具有预先指定的结构。本文考虑了两种类型的结构：低秩和随机掩模。重要的是要强调我们直接训练这种结构的更新，而不是用一个特定结构的对象来近似/描绘一般更新 - 这将在第3节中讨论。 低等级。我们将对局部模型H ti∈Rd 1×d 2的每次更新强制为最多为k的秩的低秩矩阵，其中k是固定数。为此，我们将H t i表示为两个矩阵的乘积：H t i = A t i B t i，其中A ti∈Rd 1×k，B ti∈Rk×d 2。在随后的计算中，我们随机生成A t i并在局部训练过程中考虑常数，并且我们仅优化B t i。注意，在实际实现中，在这种情况下，可以以随机种子的形式压缩，并且客户端仅需要将训练的B t i发送到服务器。这种方法立即在通信中节省了d 1 / k的因子。我们在每轮中重新生成矩阵A t i，并为每个客户端独立生成矩阵。 我们还尝试了固定B t i和训练A t i，以及训练A t i和B t i;也没有表现得那么好。我们的方法似乎与Denil等人所考虑的最佳技术一样。 （2013年），无需任何手工制作的功能。这种观察的直观解释如下。我们可以将B t i解释为投影矩阵，并将A t i解释为重建矩阵。修复A t i并优化B t i类似于询问“给定给定的随机重建，什么是将恢复大多数信息的投影？”。在这种情况下，如果重建是全等级，则存在恢复由前k个特征向量跨越的空间的投影。然而，如果我们随机地对投影进行搜索并搜索重建，我们可能是不幸的，并且可能已经投射出重要的子空间，这意味着没有尽可能好的重建，或者很难找到。 随机面具。我们将更新H t i限制为稀疏矩阵，遵循预定义的随机稀疏模式（即，随机掩码）。在每轮中重新生成模式并且为每个客户端独立地生成模式。与低秩方法类似，稀疏模式可以由随机种子完全指定，因此仅需要发送H t i的非零条目的值以及种子。 3. SKETCHED UPDATE第二种类型的更新解决了通信成本，我们称之为草绘，首先在没有任何约束的情况下计算本地训练期间的全部H t i，然后在发送到服务器之前以（有损）压缩形式对更新进行近似或编码。服务器在进行聚合之前解码更新。这种草绘方法在许多领域都有应用（Woodruff，2014）。我们尝试使用多个工具来执行草图绘制，它们是相互兼容的，可以联合使用： 子采样。每个客户端仅传送矩阵H，而不是发送H ti，矩阵H由Hti的（缩放的）值的随机子集形成。然后，服务器对二次采样的更新进行平均，产生全局更新H t。这可以这样做，使得采样更新的平均值是真实平均值的无偏估计：E [H t] = H t。与随机掩码结构更新类似，掩码针对每一轮中的每个客户端独立随机化，并且掩码本身可以存储为同步种子。 概率量化。压缩更新的另一种方法是量化权重。我们首先描述了将每个标量量化为一位的算法。考虑更新Hti，令h =（h 1，…，h d 1×d 2）= vec（H t i），并且令h max = max j（h j），h min = min j（h j）。由~h表示的h的压缩更新生成如下：","link":"/2019/04/19/FEDERATED-LEARNING-STRATEGIES-FOR-IMPROVINGCOMMUNICATION-EFFICIENCY/"},{"title":"Communication-Efficient Learning of Deep Networks from Decentralized Data","text":"title: 基于分散数据的深度网络通信高效学习 Abstract现代移动设备可以访问适合学习模型的大量数据，这反过来可以极大地改善设备上的用户体验。例如，语言模型可以改善语音识别和文本输入，图像模型可以自动选择好照片。然而，这种丰富的数据通常是隐私敏感的，数量大或两者兼而有之，这可能妨碍使用传统方法记录到数据中心并在那里进行训练。我们提倡一种替代方案，即将训练数据分布在移动设备上，并学习共享模型是通过聚合本地计算的更新模型得到。我们将这种分散的方法称为联合学习。 我们提出了一种基于迭代模型平均的联合学习深度网络的实用方法，并进行了广泛的经验评估，考虑了五种不同的模型结构和四种数据集。这些实验证明该方法对于非平衡和非IID数据分布是稳健的，这是该设置的一个定义特征。通信成本是主要的约束条件，与同步随机梯度下降相比，我们显示所需通信轮次减少了10-100倍。 1 Introduction手机和平板电脑越来越多地成为许多人的主要计算设备[30,2]。 这些设备上的强大传感器（包括摄像头，麦克风和GPS），加上它们经常携带的事实，意味着它们可以访问前所未有的大量数据，其中大部分是私有的。在这些数据上学到的模型有望通过为更智能的应用程序提供动力来大大提高可用性，但数据的敏感性意味着将其存储在集中位置存在风险和责任。 我们研究了一种学习技术，该技术允许用户集体获得从这些丰富数据训练的共享模型的好处，而无需集中存储它。我们称之为联合学习，因为学习任务是由一个由中央服务器(server)协调的参与设备（我们称之为客户端client）的松散联合解决的。 每个客户端都有一个从未上传到服务器的本地训练数据集。相反，每个客户端计算服务器维护的当前全局模型的更新，并且仅传递此更新。 这是2012年白宫关于消费者数据隐私的报告[39]提出的重点收集或数据最小化原则的直接应用。由于这些更新特定于改进当前模型，因此一旦应用它们就没有理由存储它们。 这种方法的主要优点是模型训练与直接访问原始训练数据的需要脱钩。显然，仍然需要一些信任的服务器协调训练。但是，对于可以根据每个客户端上可用数据指定训练目标的应用程序，联合学习可以通过将攻击面限制为仅设备而非设备和云来显着降低隐私和安全风险。 我们的主要贡献： 确定移动设备分散数据训练问题是一个重要的研究方向; 选择可应用于此设置的简单实用的算法; 对一些方法进行广泛的实证评估。 更具体地说，我们引入了FederatedAveraging算法，该算法将每个客户端上的本地随机梯度下降（SGD）与服务器用来执行模型平均。我们对该算法进行了广泛的实验，证明它对非平衡和非IID数据分布具有鲁棒性，并且可以减少在分散数据上训练深度网络所需的通信量。 Federated Learning联合学习的理想问题具有以下特性：1）对来自移动设备的真实数据的训练提供了优于数据中心通常可用的代理数据训练的明显优势。2）该数据对隐私敏感或规模大（与模型的大小相比），因此最好不要仅仅为了模型训练（使用聚焦​​收集原则）而将其记录到数据中心。3）对于监督任务，可以从用户交互中自然地推断出数据上的标签。 许多支持移动设备智能行为的模型符合上述标准。作为两个例子，我们考虑图像分类，例如预测哪些照片最有可能在未来多次被观看或共享;和语言模型，可以通过改进解码，下一个单词预测，甚至预测整个回复来改善触摸屏键盘上的语音识别和文本输入[10]。这些任务的潜在训练数据（用户拍摄的所有照片以及他们在移动键盘上键入的所有内容，包括密码，URL，消息等）都可能对隐私敏感。绘制这些示例的分布也可能与容易获得的代理数据集大不相同：聊天和文本消息中的语言使用通常与标准语言语料库（例如，维基百科和其他Web文档）大不相同;人们拍摄手机的照片可能与典型的Flickr照片完全不同。最后，这些问题的标签可以直接使用：输入的文本是自我标记的，用于学习语言模型，照片标签可以通过自然用户与照片应用程序的交互来定义（照片被删除，共享或查看）。 这两项任务都非常适合学习神经网络。对于图像分类，前馈深度网络，特别是卷积网络，众所周知提供最先进的结果[26,25]。对于语言建模任务，递归神经网络，特别是LSTM，已经取得了最先进的结果[20,5,22]。 Privacy？？与持久数据的数据中心训练相比，联合学习具有明显的隐私优势。持有“匿名”数据集仍然可以通过与其他数据的连接将用户隐私置于风险之中[37]。相反，为联合学习传输的信息是改进特定模型所必需的最小更新（当然，隐私利益的强度取决于更新的内容）更新本身可以（并且应该）是短暂的。它们永远不会包含比原始训练数据更多的信息（通过数据处理不等式），并且通常包含的内容要少得多。此外，聚合算法不需要更新源，因此可以在不通过诸如Tor [7]的混合网络或通过可信第三方识别元数据的情况下发送更新。我们将在本文末尾简要讨论将联合学习与安全多方计算和差异隐私相结合的可能性。 Federated Optimization我们将联合学习中隐含的优化问题称为联合优化，将连接（和对比）绘制为分布式优化。联合优化具有几个关键属性，可将其与典型的分布式优化问题区分开来： Non-IID 给定客户端上的训练数据通常基于特定用户对移动设备的使用，因此任何特定用户的本地数据集将不代表真实分布。 Unbalanced 同样，一些用户将比其他用户更多地使用服务或应用程序，从而导致不同数量的本地培训数据。 Massively distributed 我们希望参与优化的客户端数量远远大于每个客户端的平均示例数量。 Limited communication移动设备经常处于脱机状态或连接缓慢或昂贵。 这项工作中，我们的重点是优化的非IID和不平衡属性，以及通信约束的关键性质。部署的联合优化系统还必须解决许多实际问题：随着数据的添加和删除而变化的客户端数据集;客户可用性与复杂方式的本地数据分布相关（例如，美国英语使用者的电话可能会在不同时间插入英国英语的发言者）;客户端不响应或发送损坏更新。 这些问题超出了目前的工作范围;相反，我们使用适合实验的受控环境，但仍解决客户端可用性以及不平衡和非IID数据的关键问题。我们假设一个同步更新方案，进行多轮通信。有一组固定的$K$客户端，每个客户端都有一个固定的本地数据集。在每轮的开始，随机选择部分$C$个客户端，并且服务器将当前全局算法状态发送到这些客户端中的每一个（例如，当前模型参数）。我们只选择一小部分客户来提高效率，因为我们的实验显示，在超过某一点时添加更多客户的收益递减。然后，每个选定的客户端基于全局状态及其本地数据集执行本地计算，并向服务器发送更新。然后，服务器将这些更新应用于其全局状态，并重复该过程。 虽然我们关注非凸神经网络目标，但我们考虑的算法适用于任何有限目标的式子： $\\min _ { w \\in \\mathbb { R } ^ { d } } f ( w ) \\quad \\text { where } \\quad f ( w ) \\stackrel { \\text { def } } { = } \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } f _ { i } ( w )$ （1） 对于机器学习问题，我们把 $f _ { i } ( w ) =\\ell \\left( x _ { i } , y _ { i } ; w \\right) $ 由 损失函数 和模型参数 $w$ 组成。 我们假设数据被分区有$K$个客户端上，$p_{k}$是客户端$k$上的数据点索引集合, $n _ { k } = \\left| \\mathcal { p } _ { k } \\right|$.大小因此我们重写式子(1)为:$f ( w ) = \\sum _ { k = 1 } ^ { K } \\frac { n _ { k } } { n } F _ { k } ( w ) \\quad$ where $\\quad F _ { k } ( w ) = \\frac { 1 } { n _ { k } } \\sum _ { i \\in \\mathcal { P } _ { k } } f _ { i } ( w )$ 总数据集大小是n；左边是式子是说1..K个客户端训练数据，客户端K的数据集是$n _ { k }$, 加权平均$F _ { k } ( w )$; 同时右边是的式子是指每一个$F _ { k } ( w )$有平均梯度 如果通过在客户端上随机均匀地分布训练样例来形成分区$p_{k}$；然后我们将会有 ;就是client k的期望。这是通常由分布式优化算法做出的IID假设;我们指的是这不成立的情况($F _ { k }$坏的分布预测) 在数据中心优化中，通信成本相对较小，计算成本占主导地位，最近的重点主要是使用GPU降低这些成本。相反，在联合优化中，通信成本占主导地位-我们通常会受到1MB/s或更低的上传带宽的限制。此外，客户通常只会在充电，插入和未计量的Wi-Fi连接时自愿参与优化。此外，我们希望每个客户每天只参加少量的更新轮次。另一方面，由于任何单个设备上的数据集与总数据集大小相比较小，并且现代智能手机具有相对较快的处理器（包括GPU），与许多模型类型的通信成本相比，计算变得基本上是免费的。 因此，我们的目标是使用额外的计算，以减少训练模型所需的通信轮次数。我们可以通过两种主要方式添加计算：1）增加并行性，我们使用更多客户端独立工作在每个通信轮次之间2）增加每个客户端的计算，而不是像梯度计算那样执行简单的计算，每个客户端在每个通信轮次之间执行更复杂的计算。我们研究了这两种方法，但是我们实现的加速主要是因为在每个客户端上添加更多计算；一旦在客户端上使用最低级别的并行性。 Related WorkMcDonald等人已经研究了通过迭代平均局部训练的模型进行的分布式训练[28]感知器和Povey[31]用于语音识别DNN。张[42]研究了一种采用“软”平均的异步方法“soft” averaging。这些工作仅考虑群集/数据中心设置（最多16个workers，基于快速网络的挂钟时间），并且不考虑非平衡和非IID的数据集，这些属性对联合学习设置至关重要。我们将这种算法类型调整为联合设置并执行适当的实证评估，这些评估会提出与数据中心设置相关的不同问题，并且需要不同的方法。 使用与我们相似的动机，Neverova等。 [29]还讨论了在设备上保留敏感用户数据的优势。 Shokri和Shmatikov [35]的工作在几个方面有关：他们专注于训练深层网络，强调隐私的重要性，并通过在每轮通信中共享一部分参数来解决通信成本;然而，他们也不考虑不平衡和非IID数据，并且实证评估是有限的。 在凸设置中，分布式优化和估计问题已经引起了人们的重视[4,15,33]，并且一些算法确实专注于通信效率[45,34,40,27,43]。除了假设凸性之外，这个现有工作通常要求客户端的数量远小于每个客户端的示例数量，数据以IID方式分布在客户端上，并且每个节点具有相同的数量数据点 - 联合优化设置中违反了所有这些假设。异步分布式SGD也已应用于训练神经网络，例如Dean等人。 [12]，但这些方法在联合设置中需要大量的更新。分布式一致性算法（例如，[41]）放宽了IID假设，但对于很多客户端的通信约束优化仍然不是很好。 我们考虑是单次的简单的平均对参数化的算法家族来说，其中每一个client解决model的最小损失在本地数据上，这些模型将会被平均分配到最终模型上。在IID数据的凸案例中已经广泛研究了这种方法，并且众所周知，在最坏的情况下，生成的全局模型几乎等于单个客户端上训练模型的训练效果。 理解随机梯度下降，首先要知道梯度下降法，故先介绍梯度下降法： 梯度下降法大多数机器学习或者深度学习算法都涉及某种形式的优化。优化指的是改变 $x$以最小化或最大化某个函数 $f(x)$的任务。 我们通常以最小化 $f(x)$ 指代大多数最优化问题。 最大化可经由最小化算法最小化 $-f(x)$ 来实现。 我们把要最小化或最大化的函数称为目标函数或准则。 当我们对其进行最小化时，我们也把它称为代价函数、损失函数或误差函数。下面，我们假设一个损失函数为$J ( \\theta ) = \\frac { 1 } { 2 } \\sum _ { i = 1 } ^ { m } \\left( h _ { \\theta } ( x ) - y \\right) ^ { 2 }$;其中$h _ { \\theta } ( x ) = \\theta _ { 0 } + \\theta _ { 1 } x _ { 1 } + \\theta _ { 2 } x _ { 2 } \\ldots + \\theta _ { n } x _ { n }$;然后要使得最小化它 梯度下降：我们知道曲面上方向导数的最大值的方向就代表了梯度的方向，因此我们在做梯度下降的时候，应该是沿着梯度的反方向进行权重的更新，可以有效的找到全局的最优解。这个 $\\theta_i$ 的更新过程可以描述为[a表示的是步长或者说是学习率（learning rate）] 随机梯度下降：在每次更新时用1个样本，可以看到多了随机两个字，随机也就是说我们用样本中的一个例子来近似我所有的样本，来调整θ，因而随机梯度下降是会带来一定的问题，因为计算得到的并不是准确的一个梯度，对于最优化问题，凸问题，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近。但是相比于批量梯度，这样的方法更快，更快收敛，虽然不是全局最优，但很多时候是我们可以接受的，所以这个方法用的也比上面的多。 2 The FederatedAveraging Algorithm最近深度学习的众多成功应用几乎完全依赖于随机梯度下降（SGD）的变体进行优化;事实上，许多进步可以被理解为通过使用简单的基于梯度的方法使模型的结构（以及因此损失函数）更适合于优化。因此，我们很自然地从SGD开始构建用于联合优化的算法。 SGD可以自然地应用于联合优化问题，其中每轮通信完成一个batch数量梯度计算（例如在随机选择的客户端上）。这种方法计算效率很高，但需要大量的训练才能生成出良好的模型(即使使用像批量标准化这样的高级方法，Ioffe和Szegedy也会在尺寸为60的minibatches上训练MNIST 50000步).我们在CIFAR-10实验中考虑了这个基线。 在联合设置中，涉及更多客户端的挂钟时间成本很低，因此对于我们的基线，我们使用大批量同步SGD;experiments by Chen et al.表明这种方法在数据中心设置中是最先进的，它在性能上优于异步方法。为了要在联合设置中应用此方法，我们在每一轮中选择一小部分$C$客户，并计算这些客户持有的所有数据的损失梯度。因此，$C$部分控制着global batch size的大小，C=1对应于全批（非随机）梯度下降。我们将此基线算法称为FederatedSGD（或FedSGD）。FedSGD的典型实现：C=1, 一个固定的学习率η每个client k计算$g {k} = \\nabla F (w{t})$;当前模型的本地数据的平均梯度$w_{t}$;每个中心服务器聚合这些梯度并且引用这些更新; 因为$\\sum _ { k = 1 } ^ { K } \\frac { n _ { k } } { n } g _ { k } = \\nabla f \\left( w _ { t } \\right)$。一个相等的更新$\\forall k , w _ { t + 1 } ^ { k } \\leftarrow w _ { t } - \\eta g _ { k }$。也就是说，每个客户端使用其本地数据在当前模型上本地采用梯度下降的一步，然后服务器对所得模型进行加权平均。 一旦以这种方式编写算法，我们可以通过迭代本地更新为每个客户端添加更多计算$w ^ { k } \\leftarrow w ^ { k } - \\eta \\nabla F _ { k } \\left( w ^ { k } \\right)$在平均步骤之前多次。我们称这种方法为FederatedAveraging（或FedAvg）。 计算量由三个关键参数控制：$C$:每轮执行计算的客户端的比例;$E$:然后每个客户在每轮上对其本地数据集进行的训练传递次数$B$:用于客户端更新的本地小批量大小。我们写B =∞表示完整的本地数据集被视为单个小批量。因此，在该算法族的一个两端，我们可以取B =∞和E = 1，这与FedSGD完全对应。对于具有$n_{k}$个本地示例的客户端，每轮本地更新的数量由给出$u _ { k } = E \\frac { n _ { k } } { B }$; 算法1中给出了完整的伪代码 图1: 通过平均两个模型的参数生成的模型的完整MNIST训练集的损失 w and w 0 using θw + (1 − θ)w 0 for 50 evenly spaced values θ ∈ [−0.2, 1.2]; 使用SGD在不同的小数据集上训练模型w和w 0。对于左图，使用不同的随机种子初始化w和w 0;对于右图，使用共享种子。请注意不同的y轴刻度。水平线给出了由w或w 0实现的最佳损耗（它们非常接近与对应于θ= 0和θ= 1处的垂直线）。通过共享初始化，对模型进行平均可以显着减少总训练集的损失（比任何一个父模型的损失要好得多）。 θ是参数，控制调节；总的损失函数： 对于一般的非凸目标，参数空间中的平均模型可能产生任意不良的模型。遵循Goodfellow等人的方法 [17]。当我们对从不同初始条件训练的两个MNIST数字识别模型进行平均时，我们看到了这种不良行为（图1，左）。对于该图，父模型w和w 0均在来自MNIST训练集的600个实例的非重叠IID样本上进行训练。训练是通过SGD进行的，固定学习率为0.1，对于50个50（或E = 20次通过大小为600的minibatch）的240个更新。这大约是模型开始过拟合其本地数据集的训练量。 最近的研究表明，在实践中，足够过度参数化的NN的损失表面出乎意料地得到了很好的支持，特别是不像以前认为的那样容易出现局部最小值。事实上，当我们从相同的随机初始化开始两个模型然后再次在不同的数据子集上独立训练时（如上所述），我们发现原始的参数平均效果出奇的好(Figure 1, right):这两个模型的平均值, $\\frac { 1 } { 2 } w + \\frac { 1 } { 2 } w ^ { \\prime }$;在整个MNIST训练集上实现的损失显着低于通过独立训练任何一个小数据集所获得的最佳模型。虽然图1从随机初始化开始，但请注意每轮FedAvg使用共享的起始模型，因此同样的直觉也适用。 3 Experimental Results4 Conclusions and Future Work我们的实验表明联合学习可以变得切实可行，因为FedAvg使用相对较少的几轮通信训练高质量模型，如各种模型架构的结果所示：多层感知器，两个不同的卷积NN，双层字符LSTM和大规模字级LSTM。 虽然联合学习提供了许多实用的隐私保护，但通过差异隐私[14,13,1]提供更强有力的保证，安全的多方计算[18]，或者它们的组合是未来工作的一个有趣方向。请注意，这两类技术最自然地应用于FedAvg等同步算法。 这项工作之后，Bonawitz等人。 [6]为联邦学习引入了一种有效的安全聚合协议，Konecn’y等。 [23]提出了进一步降低通信成本的算法。 [6] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Anto- nio Marcedone, H. Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for federated learning on user-held data. In NIPS Workshop on Private Multi-Party Machine Learning, 2016.用于用户持有数据的联合学习的实用安全聚合。在NIPS私人多方机器学习研讨会上 [23] JakubKoneˇcn´y, H. Brendan McMahan, Felix X. Yu, Peter Richtarik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efﬁciency. In NIPS Workshop on Private Multi-Party Machine Learning, 2016.联合学习：提高沟通效率的策略。在NIPS私人多方机器学习研讨会","link":"/2019/01/05/Communication-Efficient-Learning-of-Deep-Networks-from-Decentralized-Data/"},{"title":"Golang总结","text":"编译问题go build；go install； go getValue go run: 运行 命令源码文件 go build： 在当前目录下(compile+link)生成可执行文件,注意: go build指令会调用所有引用包的源码，重新编译，而不是直接使用pkg里的编译后的文件，如果在$GOROOT或者$GOPATH下没有找到import引入的项目源码，就会报错 go install: 编译源代码，编译并安装代码包或者源码文件; go install 命令只比 go build 命令多做了一件事，即：安装编译后的结果文件到指定目录。如果可执行文件(package ‘’mian’’且包含main方法),则会编译生成可执行文件到$GOPATH/bin目录下，可执行文件import引入其他包，就会被编译. 到$GOPATH/pkg/$GOOS_$GOARCH目录下. 1. 安装代码包会在当前工作区的 pkg 的平台相关目录下生成归档文件（即 .a 文件）。 2.安装命令源码文件会在当前工作区的 bin 目录（如果 GOPATH 下有多个工作区，就会放在 GOBIN 目录下）生成可执行文件。 go get： git clone到 $GOPATH\\src + go install","link":"/2019/08/15/Golang总结/"},{"title":"FLandEncryption","text":"Papers and Code Asynchronous Federated Optimization https://arxiv.org/pdf/1903.03934 Towards Federated Learning at Scale: System Design https://arxiv.org/pdf/1902.01046 Robust and Communication-Efficient Federated Learning from Non-IID Data https://arxiv.org/pdf/1903.02891 One-Shot Federated Learning https://arxiv.org/pdf/1902.11175 High Dimensional Restrictive Federated Model Selection with multi-objective Bayesian Optimization over shifted distributions https://arxiv.org/pdf/1902.08999 Federated Machine Learning: Concept and Applications https://arxiv.org/pdf/1902.04885 Agnostic Federated Learning https://arxiv.org/pdf/1902.00146 Peer-to-peer Federated Learning on Graphs https://arxiv.org/pdf/1901.11173 Federated Collaborative Filtering for Privacy-Preserving Personalized Recommendation Systemhttps://arxiv.org/pdf/1901.09888 SecureBoost: A Lossless Federated Learning Framework https://arxiv.org/pdf/1901.08755 Federated Reinforcement Learning https://arxiv.org/pdf/1901.08277 Lifelong Federated Reinforcement Learning: A Learning Architecture for Navigation in Cloud Robotic Systemshttps://arxiv.org/pdf/1901.06455 Federated Learning via Over-the-Air Computation https://arxiv.org/pdf/1812.11750 Broadband Analog Aggregation for Low-Latency Federated Edge Learning (Extended Version)https://arxiv.org/pdf/1812.11494 Multi-objective Evolutionary Federated Learning https://arxiv.org/pdf/1812.07478 Federated Optimization for Heterogeneous Networks https://arxiv.org/pdf/1812.06127 Efficient Training Management for Mobile Crowd-Machine Learning: A Deep Reinforcement Learning Approachhttps://arxiv.org/pdf/1812.03633 No Peek: A Survey of private distributed deep learning https://arxiv.org/pdf/1812.03288 A Hybrid Approach to Privacy-Preserving Federated Learning https://arxiv.org/pdf/1812.03224 Applied Federated Learning: Improving Google Keyboard Query Suggestions https://arxiv.org/pdf/1812.02903 Differentially Private Data Generative Models https://arxiv.org/pdf/1812.02274 Protection Against Reconstruction and Its Applications in Private Federated Learning https://arxiv.org/pdf/1812.00984 Split learning for health: Distributed deep learning without sharing raw patient data https://arxiv.org/pdf/1812.00564 Beyond Inferring Class Representatives: User-Level Privacy Leakage From Federated Learninghttps://arxiv.org/pdf/1812.00535 LoAdaBoost:Loss-Based AdaBoost Federated Machine Learning on medical Data https://arxiv.org/pdf/1811.12629 Analyzing Federated Learning through an Adversarial Lens https://arxiv.org/pdf/1811.12470 Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data https://arxiv.org/pdf/1811.11479 Biscotti: A Ledger for Private and Secure Peer-to-Peer Machine Learning https://arxiv.org/pdf/1811.09904 Dancing in the Dark: Private Multi-Party Machine Learning in an Untrusted Setting https://arxiv.org/pdf/1811.09712 Weekly Dig in Privacy-Preserving Machine Learning15 February 2019Papers Secure Evaluation of Quantized Neural Networks TensorSCONE: A Secure TensorFlow Framework using Intel SGX Achieving GWAS with Homomorphic Encryption Bonus A Marauder’s Map of Security and Privacy in Machine Learning, a lecture on security and privacy. By Nicolas Papernot. 8 February 2019Paper CodedPrivateML: A Fast and Privacy-Preserving Framework for Distributed Machine LearningInteresting solution for offloading/out-sourcing model training to set of workers while ensuring strong privacy guarantees; based on Lagrange coded computations. Towards Federated Learning at Scale: System Design Bonus A Simple Explanation for the Existence of Adversarial Examples with Small Hamming DistanceSome of the greatest minds from cryptography join in on adversarial examples: “We develop a simple mathematical framework which enables us to think about this baffling phenomenon [and] explain why we should expect to find targeted adversarial examples in arbitrarily deep neural networks.” 1 February 2019Papers Privacy-preserving semi-parallel logistic regression training with Fully Homomorphic Encryption CaRENets: Compact and Resource-Efficient CNN for Homomorphic Inference on Encrypted Medical ImagesSecure predictions using FHE with careful packing. Differentially Private Markov Chain Monte Carlo Improved Accounting for Differentially Private Learning News Videos from Hacking Deep Learning 2 online, including talks on adversarily attacks and privacy. Via @BIUCrypto. Videos from CCS’18 online, including presentation of ABY3. Via @lzcarl. Bonus Deep Learning to Evaluate Secure RSA Implementations Turbospeedz: Double Your Online SPDZ! Improving SPDZ using Function Dependent Preprocessing 18 January 2019News Simons Institute program on Data Privacy: Foundations and Applications kicked off this week with several workshops around differential privacy. 11 January 2019Papers Secure Computation for Machine Learning With SPDZLooks at regression tasks using the general-purpose reference implementation and with active security. Secure Two-Party Feature SelectionPrivacy-preserving chi-squared test for binary feature selection from Paillier encryption. Contamination Attacks and Mitigation in Multi-Party Machine LearningMaking models more robust to tainted training data by minimizing the ability to predict the providing parties. News Program for SP’19 is out with four accepted papers on differential privacy. Via @IEEESSP. Bonus Excellent summary of what happened last year in the world of privacy-preserving machine learning by Dropout Labs. Real World Crypto happened this week, with (temporary?) recordings available on YouTube. Especially the talk on Deploying MPC for Social Good has received significant attention, while the talk on Foreshadow attack on Intel SGXfurthermore reminded us that enclaves are not perfect yet. 31 December 2018Papers Fast Secure Comparison for Medium-Sized Integers and Its Application in Binarized Neural Networks Low Latency Privacy Preserving Inference News Google AI team releases new TensorFlow Privacy library for training machine learning models with differential privacy for training data. Via @NicolasPapernot. 14 December 2018Papers Applied Federated Learning: Improving Google Keyboard Query SuggestionsUpdate on concrete use of federated learning at Google; no secure computation nor differential privacy but including thoughts on dealing with unseen training data. When Homomorphic Cryptosystem Meets Differential Privacy: Training Machine Learning Classifier with Privacy Protection Differentially Private User-based Collaborative Filtering Recommendation Based on K-means Clustering Privacy Partitioning: Protecting User Data During the Deep Learning Inference PhaseOptimising for privacy loss at early layers suggests pragmatic approach for protecting privacy of prediction inputs without cryptography nor DP. A Review of Homomorphic Encryption Libraries for Secure Computation Private Polynomial Computation from Lagrange Encoding News NeurIPS workshop on Privacy Preserving Machine Learning happened this week with a very interesting selection of papers. Intel’s HE Transformer for nGraph released as open source! Bonus Scaling Shared Model Governance via Model Splitting 30 November 2018Papers nGraph-HE: A Graph Compiler for Deep Learning on Homomorphically Encrypted Data“One of the biggest accelerators in deep learning has been frameworks that allow users to describe networks and operations at a high level while hinding details … A key challenge for building large-scale privacy-preserving ML systems using HE has been the lack of such a framework; as a result data scientists face the formidable task of becoming experts in deep learning, cryptography, and software engineering”. Amen! CHET: Compiler and Runtime for Homomorphic Evaluation of Tensor Programs“In many respects, programming FHE applications today is akin to low-level assembly … Our central hypothesis is that future applications will benefit from a compiler and runtime that targets a compact and well-reasoned interface”. Amen! Also describes several ways on which the compiler can optimize encrypted computations. Faster CryptoNets: Leveraging Sparsity for Real-World Encrypted InferenceSolid work on using weights quantization and other ML techniques to adapt neural networks for the encrypted setting, significantly improving performance relative to CryptoNets. Interestingly, second degree approximations of the Swish activation function are used over ReLUs and squaring. Gives plenty of references for those not coming from a ML background. Privacy-Preserving Collaborative Preduction using Random ForestsTrain models locally on independent data sets and apply ensemble techniques to serve private predictions using these. FALCON: A Fourier Transform Based Approach for Fast and Secure Convolutional Neural Network PredictionsPrivate predictions via FHE and GC. Interestingly, values are first convert to the frequency domain using the FFT and there’s a protocol for softmax. The AlexNet Moment for Homomorphic Encryption: HCNN, the First Homomorphic CNN on Encrypted Data with GPUs A Fully Private Pipeline for Deep Learning on Electronic Health Records Distributed and Secure ML with Self-tallying Multi-party Aggregation News List of accepted papers for NeurIPS’18 privacy workshop is out! Via @mortendahlcs. 31 October 2018Papers Privado: Practical and Secure DNN Inference 28 September 2018Papers Encrypted Databases for Differential Privacy 27 July 2018Papers Efficient Logistic Regression on Large Encrypted Data Round-Efficient Protocols for Secure Multiparty Fixed-Point Arithmetic 27 June 2018 Slalom: Fast, Verifiable and Private Execution of Neural Networks in Trusted Hardware TAPAS: Tricks to Accelerate (encrypted) Prediction As a Service DeepObfuscation: Securing the Structure of Convolutional Neural Networks via Knowledge Distillation ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models 25 May 2018Papers Logistic Regression over Encrypted Data from Fully Homomorphic Encryption From Keys to Databases – Real-World Applications of Secure Multi-Party ComputationJana: private SQL databases Sharemind: secure analytics Partisia, Sepior: auctions and key management Unbound Technology: enterprise secrets Minimising Communication in Honest-Majority MPC by Batchwise Multiplication Verification SPDZ2k: Efficient MPC mod 2^k for Dishonest MajorityTo improve efficiency of MPC it is interesting to perform operations over rings that fit closely with native CPU instructions, as opposed to over e.g. a prime field. Doing so is straight forward when the attacker is honest-but-curious, and this paper addresses the case when he is fully malicious. News GDPR has come into effect! Slides from UCL course on privacy enhancing technologies available. Via @emilianoucl. Keystone: An Open-source Secure Hardware Enclave. Via @Daeinar. The reference SPDZ implementation is being prepared for production. Via @SmartCryptology. Next week’s TPMPC workshop will be live-streamed if you happen to be elsewhere than Aarhus! Via @claudiorlandi. Bonus Cautious Deep Learning 18 May 2018Small but good: we only dug up one paper this week but it comes with very interesting claims. Papers SecureNN: Efficient and Private Neural Network TrainingFollowing recent approachs but reporting significant performance improvements via specialized protocols for the 3 and 4-server setting: the claimed cost of encrypted training is in some cases only 13-33 times that of training on cleartext data. Big factor in this is the avoidance of bit-decomposition and garbled circuits when computing comparisons and ReLUs. 11 May 2018If anyone had any doubt that private machine learning is a growing area then this week might take care of that. PapersSecure multiparty computation: ABY3: A Mixed Protocol Framework for Machine LearningOne of big guys in secure computation for ML is back with new protocols in the 3-server setting for training linear regression, logistic regression, and neural network models. Impressive performance improvements for both training and prediction. EPIC: Efficient Private Image Classification (or: Learning from the Masters)An update to work from last year on efficient private image classification using SPDZ and support vector machines. Includes great overview of recent related work. Homomorphic encryption: Unsupervised Machine Learning on Encrypted DataImplements K-means privately using fully homomorphic encryption and a bit-wise rational encoding, with suggestions for tweaking K-means to make it more practical for this setting. The TFHE library (see below) is used for experiments. TFHE: Fast Fully Homomorphic Encryption over the TorusProclaimed as the fastest FHE library currently available, this paper is the extended version of previous descriptions of the underlying scheme and optimizations. Homomorphic Secret Sharing: Optimizations and ApplicationsFurther work on a hybrid scheme between homomorphic encryption and secret sharing: operations can be performed locally by each share holder as in the former, yet a final combination is needed in the end to recover the result as in the latter: “this enables a level of compactness and efficiency of reconstruction that is impossible to achieve via standard FHE”. Secure enclaves: SecureCloud: Secure Big Data Processing in Untrusted CloudsAn joint European research project to develop a platform for pusing critical applications to untrusted cloud environments, using secure enclaves and supporting big data. Envisioned use cases from finance, health care, and smart grids. SecureStreams: A Reactive Middleware Framework for Secure Data Stream ProcessingPresents concrete work done in the above SecureCloud project, namely a high-level Lua-based framework for privately processing streams at scale using dataflow programming and secure enclaves. Differential privacy: Privately Learning High-Dimensional DistributionsTackles the problem that privacy “comes almost for free when data is low-dimensional but comes at a steep price when data is high-dimensional” as measured in amount of samples needed. Two mechanisms are presented for learning respectively a multivariate Gaussian and a product distribution. SynTF: Synthetic and Differentially Private Term Frequency Vectors for Privacy-Preserving Text MiningA differentially private mechanism is used to prevent author re-identification in texts used for training models where anomymized feature vectors can be used instead of the actual body text. Concrete experiments include topic classification of newsgroups postings. Distributed Differentially-Private Algorithms for Matrix and Tensor FactorizationCorrelated noise is used to privately perform two common operations via a centralized but curious party or directly between data holders, respectively. Interestingly, the correlated noise is not uniform as in typical secure aggregation settings. Bonus An Empirical Analysis of Anonymity in Zcash A little reminder that anonymity is hard. 27 April 2018Papers Towards Dependable Deep Convolutional Neural Networks (CNNs) with Out-distribution Learning“in this paper we propose to add an additional dustbin class containing natural out-distribution samples” “We show that such an augmented CNN has a lower error rate in the presence of adversarial examples because it either correctly classifies adversarial samples or rejects them to a dustbin class.” Weak labeling for crowd learning“weak labeling for crowd learning is proposed, where the annotators may provide more than a single label per instance to try not to miss the real label” Decentralized learning with budgeted network load using Gaussian copulas and classifier ensembles“In this article, we place ourselves in a context where the amount of transferred data must be anticipated but a limited portion of the local training sets can be shared. We also suppose a minimalist topology where each node can only send information unidirectionally to a single central node which will aggregate models trained by the nodes” “Using shared data on the central node, we then train a probabilistic model to aggregate the base classifiers in a second stage.” Securing Distributed Machine Learning in High DimensionsSome results towards the issue of input pollution in federated learning, where a fraction of gradient providers may give arbitrarily malicious inputs to an aggregation protocol. “The core of our method is a robust gradient aggregator based on the iterative filtering algorithm for robust mean estimation”. 20 April 2018Papers Nothing Refreshes Like a RePSI: Reactive Private Set IntersectionPSI was several applications in private data processing, including object linking in advertising and data augmentation. This paper takes a step towards mitigating exhaustive attacks where a party learns too much by simply asking for many intersections. News Sharemind, one of the biggest and earliest players pushing MPC to industry, has launched a new privacy servicebased on secure computation using secure enclaves with the promise that it can handle big data. Via @positium. Interesting interview with Lea Kissner, the head of Google’s privacy team NightWatch. Few details are given but “She recently tried to obscure some data using cryptography, so that none of it would be visible to Google upon upload … but it turned out that [it] would require more spare computing power than Google has” sounds like techniques that could be related to MPC or HE. Via @rosa. Google had two AI presentations at this year’s RSA conference, one on fraud detection and one on adversarial techniques. Via @goodfellow_ian. Bonus Privacy-Preserving Multibiometric Authentication in Cloud with Untrusted Database ProvidersRelevant application of secure computation to authentication using sensitive data. Relative black box use of existing protocols yet experimental performance &lt;1sec. Private Anonymous Data AccessInteresting mix of private information retrieval and oblivious RAM: “We consider a scenario where a server holds a huge database that it wants to make accessible to a large group of clients while maintaining privacy and anonymity … with the goal of getting the best of both worlds: allow many clients to privately and anonymously access the database as in PIR, while having an efficient server as in ORAM”. Adversarial Attacks Against Medical Deep Learning SystemsA discussion around some of the concrete consequences the medical profession may face from adversarial examples in machine learning systems with a warning of “caution in employing deep learning systems in clinical settings”. 13 April 2018Papers Differentially Private Confidence Intervals for Empirical Risk MinimizationAddresses the question of computing confidence intervals in a private manner, using either DP or concentrated DP. Gives concrete examples and experiments using logistic regression and SVM. News Facebook host privacy summit but seem a bit sparse on details. Via @sweis. Bonus PowerHammer: Exfiltrating Data from Air-Gapped Computers through Power LinesMore work on leaking data from air-gapped computers through obscure side-channels, this time through power lines by varying the CPU utilization, achieving bit rates of 10-1000 bit/sec for different attacks. 30 March 2018Papers Private Nearest Neighbors Classification in Federated DatabasesGreat read on custom MPC protocols allowing k-NN classification of a sample (such as document classification with cosine similarity) using a distributed data set, without leaking neither sample nor data set. This includes feature extraction, similarity computation, and top-k selection. Chiron: Privacy-preserving Machine Learning as a ServiceInteresting look at protecting both privacy of training data and model specifics via secure enclaves. The technology is promising despite having experienced a few issues recently and e.g. avoids use of heavy cryptography. Locally Private Bayesian Inference for Count ModelsWhen applying differential privacy one may either ignore the fact that noise has been added to the data or try to take it into account; the latter is done here with good illustrations of the improvements this can give. Hiding in the Crowd: A Massively Distributed Algorithm for Private Averaging with Malicious AdversariesInteresting peer-to-peer protocol for privately computing the exact average of a distributed data set via gossiping directly between the peers. No heavy cryptography is used in case of honest peers, with a PHE-based extension for detecting malicious cheating. Comparing Population Means under Local Differential Privacy Cloud-based MPC with Encrypted DataGives two schemes for private Model Predictive Control by a central authority (who might have a better understanding of the environment than individual sensors), one based on PHE and another on MPC. 16 March 2018Papers Model-Agnostic Private Learning via StabilityMore work on ensuring privacy of training data via differential private query mechanisms. Compared to paper from a few weeks ago, this one focuses on “algorithms that are agnostic to the underlying learning problem [with] formal utility guarantees [and] provable accuracy guarantees”. Homomorphic Encryption for Speaker Recognition: Protection of Biometric Templates and Vendor Model ParametersThe Paillier cryptosystem is used to securely evaluate simplified similarity functions so users don’t leak biometric information during authentication. Performance numbers included. Efficient Determination of Equivalence for Encrypted DataReminder that even a simpler task such as privately linking identities and records together is relevant in industry. Bonus The Morning Paper: When coding style survives compilation Anonymity is hard! Random forests can be trained to identify your coding style from source code as well as compiled programs. 9 March 2018News The 2018 Gödel Prize is awarded to Oded Regev for his paper On lattices, learning with errors, random linear codes, and cryptography. This had a huge influence on later work in cryptography, not least homomorphic encryption. Via @hoonoseme. OpenMined is now maintaining a list of papers and tools around private machine learning: https://github.com/OpenMined/awesome-ai-privacy! Via @iamtrask. Lab41 has released a Python wrapper around Microsoft’s SEAL homomorphic encryption library: https://github.com/Lab41/PySEAL. Via @mortendahl. The list of accepted contributed talks for this year’s Theory and Practice of MPC workshop has been announced. This is the definitive annual event dedicated to secure multi-party computation. Via @claudiorlandi. Papers Generating Differentially Private Datasets Using GANsInteresting idea of using GANs to produce artificial differential privacy-preserving datasets from sensitive data that are safe to release for further training purposes. This is done on the client side, meaning there’s no need for a trusted aggregator. Faster Homomorphic Linear Transformations in HElibThe mesters are at it again, giving algorithmic improvements to perhaps the most well-known homomorphic encryption library and thereby making it 30-75 times faster. Logistic Regression Model Training based on the Approximate Homomorphic EncryptionPrivate fitting of several logisictic regression models on smaller genomic data sets using the HEAAN homomorphic encryption scheme. Approach is somewhat typical gradient descent and sigmoid polynomial approximation but with significant concrete performance improvements over other work using HEAAN. Blogs The Building Blocks of Interpretability Nothing to do with private machine learning, yet this is so neat that it warrents a mention. Go play! 2 March 2018News @mvaria‘s talk about a real-world application of MPC at this year’s ENIGMA conference is online and well worth a watch: https://www.youtube.com/watch?v=d9rMokeYx9I. Via @lcyqn. Papers Scalable Private Learning with PATEFollow-up work to the celebrated Student-Teacher way of ensuring privacy of training data via differential privacy, now with better privacy bounds and hence less added noise. This is partially achieved by switching to Gaussian noise and more advanced (trusted) aggregation mechanisms. Privacy-Preserving Logistic Regression TrainingFitting a logistic model from homomorphically encrypted data using the Newton-Raphson iterative method, but with a fixed and approximated Hessian matrix. Performance is evaluated on the iDASH cancer detection scenario. Privacy-Preserving Boosting with Random Linear Classifiers for Learning from User-Generated DataPresents the SecureBoost framework for mixing boosting algorithms with secure computation. The former uses randomly generated linear classifiers at the base and the latter comes in three variants: RLWE+GC, Paillier+GC, and SecretSharing+GC. Performance experiments on both the model itself and on the secure versions are provided. Machine learning and genomics: precision medicine vs. patient privacyNon-technical paper illustrating that secure computation techniques are finding their way into otherwise unrelated research areas, and hitting home-run with “data access restrictions are a burden for researchers, particularly junior researchers or small labs that do not have the clout to set up collaborations with major data curators”. Blogs Uber’s differential privacy .. probably isn’t @frankmcsherry looks at Uber’s SQL differential privacy project and shares experience gained from implementing these things in Microsoft’s PINQ. 23 February 2018Papers The Secret Sharer: Measuring Unintended Neural Network Memorization &amp; Extracting SecretsConcrete study of what a model can leak about sensitive information in the traning data. Perhaps not surprisingly, “only by developing and training a differential private model are we able to … protect against the extraction of secrets”. Doing Real Work with FHE: The Case of Logistic RegressionThe heavyweights of homomorphic encryption apply HElib to logistic regression with a focus on implementing “optimized versions of many bread and butter FHE tools. These tools include binary arithmetic, comparisons, partial sorting, and low-precision approximation of complicated functions such as reciprocals and logarithms”. On the Connection between Differential Privacy and Adversarial Robustness in Machine Learning … Reading in the Dark: Classifying Encrypted Digits with Functional EncryptionDevelops a functional encryption scheme for “efficient computation of quadratic polynomials on encrypted vectors” and applies this to private MNIST prediction (i.e. using a model trained on unencrypted data) via suitable quadractic models.","link":"/2019/08/07/FLandEncryption/"},{"title":"Java入门","text":"基础入门 语言介绍 面向对象；编译型，解释型； 程序代码经过编译之后转换为Java字节码的中间语言，Java虚拟机(JVM)对字节码解释并运行；编译只进行一次，解释是每次程序运行时都会进行；编译后的字节码采用一种针对JVM优化过的机器码形式保存，虚拟机将字节码解释为机器码，然后在计算机上运行。 Java版本 Java SE 桌面版 Java EE 企业版，分布式网络程序，网站，核心EJB(企业Java组件模型) Java ME 嵌入式， 学好Java 明确目标，大方向，按照方向努力学习，认真研究 书不用太多，基础打好。系统学习 了解设计模式，编写程序代码，要学习设计模式！可读性 不要死记语法，理解，类比记忆 多实践，多思考，多请教，手动编写，运行程序，分析结构；编程思想，请教他人，不耻下问！！！ ！！多阅读源代码，多了解他人的程序代码，分析编程者的编程思想和设计模式 Java语言的特性 简单：与C++类似；自动回收 面相对象 解释器：java字节码解释器 多线程","link":"/2019/04/29/Java入门/"},{"title":"Java核心技术笔记1","text":"简介Java核心技术卷1，笔记，记录 知识点，第一步，思想，掌握语法，使用。 少而精 #","link":"/2019/05/12/Java核心技术笔记1/"},{"title":"Java快排和二分","text":"快排 要点：两端开始“探测”，设置的基准数是最左边的数，所以需要让哨兵j先出动， 分治想法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class QuickSort { public static void main(String[] args) { int[] arr = new int[]{10,7,2,4,7,62,3,4,2,1,8,100,19}; QuickSort qs = new QuickSort(); int[] res = qs.quickSort(arr, 0, arr.length-1); for (int i=0; i&lt;arr.length; i++) { System.out.println(res[i]); } } public int[] quickSort(int[] arr, int start, int end) { if (start &gt; end) { return null; } if (start == end) { return arr; } int len = arr.length; int index = partition(arr, start, end); if (index &gt; start) quickSort(arr, start, index-1); if (index &lt; end) quickSort(arr, index+1, end); return arr; } private int partition(int[] arr, int start, int end) { if (arr == null || start &gt; end || arr.length == 0){ return -1; } int baseVal = arr[start]; int i,j; for (i=start, j=end;i&lt;j; ) { while (arr[j] &gt; baseVal &amp;&amp; i &lt;j) { j--; } while (arr[i] &lt;= baseVal &amp;&amp; i &lt;j) { i++; } int temp=0; if (arr[i]&gt;baseVal &amp;&amp; arr[j] &lt;=baseVal) { temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; } } int temp = arr[start]; arr[start] = arr[i]; arr[i] = temp; return i; }} 二分 要点 边界条件 12345678910111213141516171819private int binarySearch(int[] nums, int target) { if (nums.length == 0) { return 0; } int len = nums.length; int l = 0, r = len-1; while (l&lt;=r) { int m = (l+r)/2; int middle= l+ (r-l)/2 ; if (target == nums[m]) { return m; } else if(target &gt; nums[m]) { l = m+1; } else { r = m-1; } } return -1;}","link":"/2020/06/08/Java快排和二分/"},{"title":"Java的String和char数组","text":"谈Java的String和char数组的关系继承关系12java.lang.Object java.lang.String 原型声明public final class String extends Object implements Serializable, Comparable&lt;String&gt;, charSequence 构造方法\\123456789Stirng()String(byte[] bytes, int ofset, int length, String charSetName)String(byte[] bytes)Stirng(char[] value)String(StringBuilder builder) 在以上的构造方法中，我们强调String(char[] value)\\ 它说明我们可以通过一个char数组来构造一个新的String字符串\\ 1String str = new String(new char[]{&apos;a,&apos;, &apos;b&apos;, &apos;c&apos;}) 在Java中char数组转换为String类型的方法有二\\ 1）String.valueOf(char[] arr)\\ 2）new String(char[] arr)\\ 注意，不能直接把char数组调用toString()方法来完成转换\\ 123char[] arr = new char[]{&apos;a&apos;, &apos;e&apos;, &apos;i&apos;, &apos;o&apos;, &apos;u&apos;};String str = arr.toString();System.out.println(str); // [C@1ee0005 而要把一个String类实例转换为char数组则要（只能）调用str.toCharArray()方法（因为char数组没有所谓的构造方法，所以不能通过把String类型参数传递给char构造方法来构造一个新的char数组，但是String是一个类，所以可以）\\ 1public char[] toCharArray() 该方法返回一个新分配的字符数组，该数组的长度是此字符串的长度，其内容被初始化为包含这个字符串所表示的字符序列\\ 12345String str = \"aeiou\";char[] arr = str.toCharArray();System.out.println(arr); // aeiouSystem.out.println(arr.toString()); // [C@1ee0005 Java Character 类https://www.runoob.com/java/java-character.html Character 类用于对单个字符进行操作。 Character 类在对象中包装一个基本类型 char 的值 1234567char ch = 'a'; // Unicode 字符表示形式char uniChar = '\\u039A'; // 字符数组char[] charArray ={ 'a', 'b', 'c', 'd', 'e' }; 然而，在实际开发过程中，我们经常会遇到需要使用对象，而不是内置数据类型的情况。为了解决这个问题，Java语言为内置数据类型char提供了包装类Character类。 Character类提供了一系列方法来操纵字符。你可以使用Character的构造方法创建一个Character类对象，例如： 1Character ch = new Character('a'); 在某些情况下，Java编译器会自动创建一个Character对象。 例如，将一个char类型的参数传递给需要一个Character类型参数的方法时，那么编译器会自动地将char类型参数转换为Character对象。 这种特征称为装箱，反过来称为拆箱。 123456// 原始字符 'a' 装箱到 Character 对象 ch 中Character ch = 'a'; // 原始字符 'x' 用 test 方法装箱// 返回拆箱的值到 'c'char c = test('x'); Character类的方法： 序号 方法与描述 1 isLetter() 是否是一个字母 2 isDigit() 是否是一个数字字符 3 isWhitespace() 是否是一个空白字符 4 isUpperCase() 是否是大写字母 5 isLowerCase() 是否是小写字母 6 toUpperCase() 指定字母的大写形式 7 toLowerCase() 指定字母的小写形式 8 toString() 返回字符的字符串形式，字符串的长度仅为1 Character 构造函数自版本 9 以来被弃用。 翻译：它很少被适当的使用这个构造函数。静态工厂Character.valueOf(char)通常是一个更好的选择,因为它可能会产生更好的空间和时间性能 12Character ch = new Character('a'); // Java9 以前Character ch = Character.valueOf('a'); // Java9 以后 Java String 类在 Java 中字符串属于对象，Java 提供了 String 类来创建和操作字符串。 创建字符串创建字符串最简单的方式如下: String greeting = &quot;菜鸟教程&quot;; 在代码中遇到字符串常量时，这里的值是 “菜鸟教程“”，编译器会使用该值创建一个 String 对象。 和其它对象一样，可以使用关键字和构造方法来创建 String 对象。 String 类有 11 种构造方法，这些方法提供不同的参数来初始化字符串，比如提供一个字符数组参数: 注意:String 类是不可改变的，所以你一旦创建了 String 对象，那它的值就无法改变了（详看笔记部分解析）。 如果需要对字符串做很多修改，那么应该选择使用 StringBuffer &amp; StringBuilder 类。 这里可以根据 jdk 的源码来分析。 字符串实际上就是一个 char 数组，并且内部就是封装了一个 char 数组。 字符串长度用于获取有关对象的信息的方法称为访问器方法。 String 类的一个访问器方法是 length() 方法，它返回字符串对象包含的字符数。 SN(序号) 方法描述 1 char charAt(int index) 返回指定索引处的 char 值。 2 int compareTo(Object o) 把这个字符串和另一个对象比较。 3 int compareTo(String anotherString) 按字典顺序比较两个字符串。 4 int compareToIgnoreCase(String str) 按字典顺序比较两个字符串，不考虑大小写。 5 String concat(String str) 将指定字符串连接到此字符串的结尾。 6 boolean contentEquals(StringBuffer sb) 当且仅当字符串与指定的StringBuffer有相同顺序的字符时候返回真。 7 static String copyValueOf(char[] data) 返回指定数组中表示该字符序列的 String。 8 static String copyValueOf(char[] data, int offset, int count) 返回指定数组中表示该字符序列的 String。 9 boolean endsWith(String suffix) 测试此字符串是否以指定的后缀结束。 10 boolean equals(Object anObject) 将此字符串与指定的对象比较。 11 boolean equalsIgnoreCase(String anotherString) 将此 String 与另一个 String 比较，不考虑大小写。 12 byte[] getBytes() 使用平台的默认字符集将此 String 编码为 byte 序列，并将结果存储到一个新的 byte 数组中。 13 byte[] getBytes(String charsetName) 使用指定的字符集将此 String 编码为 byte 序列，并将结果存储到一个新的 byte 数组中。 14 void getChars(int srcBegin, int srcEnd, char[] dst, int dstBegin) 将字符从此字符串复制到目标字符数组。 15 int hashCode() 返回此字符串的哈希码。 16 int indexOf(int ch) 返回指定字符在此字符串中第一次出现处的索引。 17 int indexOf(int ch, int fromIndex) 返回在此字符串中第一次出现指定字符处的索引，从指定的索引开始搜索。 18 int indexOf(String str) 返回指定子字符串在此字符串中第一次出现处的索引。 19 int indexOf(String str, int fromIndex) 返回指定子字符串在此字符串中第一次出现处的索引，从指定的索引开始。 20 String intern() 返回字符串对象的规范化表示形式。 21 int lastIndexOf(int ch) 返回指定字符在此字符串中最后一次出现处的索引。 22 int lastIndexOf(int ch, int fromIndex) 返回指定字符在此字符串中最后一次出现处的索引，从指定的索引处开始进行反向搜索。 23 int lastIndexOf(String str) 返回指定子字符串在此字符串中最右边出现处的索引。 24 int lastIndexOf(String str, int fromIndex) 返回指定子字符串在此字符串中最后一次出现处的索引，从指定的索引开始反向搜索。 25 int length() 返回此字符串的长度。 26 boolean matches(String regex) 告知此字符串是否匹配给定的正则表达式。 27 boolean regionMatches(boolean ignoreCase, int toffset, String other, int ooffset, int len) 测试两个字符串区域是否相等。 28 boolean regionMatches(int toffset, String other, int ooffset, int len) 测试两个字符串区域是否相等。 29 String replace(char oldChar, char newChar) 返回一个新的字符串，它是通过用 newChar 替换此字符串中出现的所有 oldChar 得到的。 30 String replaceAll(String regex, String replacement) 使用给定的 replacement 替换此字符串所有匹配给定的正则表达式的子字符串。 31 String replaceFirst(String regex, String replacement) 使用给定的 replacement 替换此字符串匹配给定的正则表达式的第一个子字符串。 32 String[] split(String regex) 根据给定正则表达式的匹配拆分此字符串。 33 String[] split(String regex, int limit) 根据匹配给定的正则表达式来拆分此字符串。 34 boolean startsWith(String prefix) 测试此字符串是否以指定的前缀开始。 35 boolean startsWith(String prefix, int toffset) 测试此字符串从指定索引开始的子字符串是否以指定前缀开始。 36 CharSequence subSequence(int beginIndex, int endIndex) 返回一个新的字符序列，它是此序列的一个子序列。 37 String substring(int beginIndex) 返回一个新的字符串，它是此字符串的一个子字符串。 38 String substring(int beginIndex, int endIndex) 返回一个新字符串，它是此字符串的一个子字符串。 39 char[] toCharArray() 将此字符串转换为一个新的字符数组。 40 String toLowerCase() 使用默认语言环境的规则将此 String 中的所有字符都转换为小写。 41 String toLowerCase(Locale locale) 使用给定 Locale 的规则将此 String 中的所有字符都转换为小写。 42 String toString() 返回此对象本身（它已经是一个字符串！）。 43 String toUpperCase() 使用默认语言环境的规则将此 String 中的所有字符都转换为大写。 44 String toUpperCase(Locale locale) 使用给定 Locale 的规则将此 String 中的所有字符都转换为大写。 45 String trim() 返回字符串的副本，忽略前导空白和尾部空白。 46 static String valueOf(primitive data type x) 返回给定data type类型x参数的字符串表示形式。 length() 方法，length 属性和 size() 方法的区别: 1、length() 方法是针对字符串来说的，要求一个字符串的长度就要用到它的length()方法； 2、length 属性是针对 Java 中的数组来说的，要求数组的长度可以用其 length 属性； 3、Java 中的 size() 方法是针对泛型集合说的, 如果想看这个泛型有多少个元素, 就调用此方法来查看! 这个例子来演示这两个方法和一个属性的用法： String str1 = “hello world”; 和 String str3 = “hello world”; 都在编译期间生成了字面常量和符号引用，运行期间字面常量 “hello world” 被存储在运行时常量池（当然只保存了一份）。通过这种方式来将 String 对象跟引用绑定的话，JVM 执行引擎会先在运行时常量池查找是否存在相同的字面常量，如果存在，则直接将引用指向已经存在的字面常量；否则在运行时常量池开辟一个空间来存储该字面常量，并将引用指向该字面常量。 众所周知，通过 new 关键字来生成对象是在堆区进行的，而在堆区进行对象生成的过程是不会去检测该对象是否已经存在的。因此通过 new 来创建对象，创建出的一定是不同的对象，即使字符串的内容是相同的。 数组Arrays 类java.util.Arrays 类能方便地操作数组，它提供的所有方法都是静态的。 具有以下功能： 给数组赋值：通过 fill 方法。 对数组排序：通过 sort 方法,按升序。 比较数组：通过 equals 方法比较数组中元素值是否相等。 查找数组元素：通过 binarySearch 方法能对排序好的数组进行二分查找法操作。","link":"/2020/06/08/Java的String和char数组/"},{"title":"LeetCode-双指针","text":"思想双指针主要用于遍历数组，两个指针指向不同的元素，从而协同完成任务。 如何写程序：分析程序代码：入口，分叉逻辑(各个分支情况列举)，出口考虑(各种边界条件,特别是=的情况) 控制结束的条件，初始，各个分支情况流程逻辑；结束时的条件: 正常结束，异常结束，边界值考虑 167,Two sumGiven an array of integers that is already sorted in ascending order, find two numbers such that they add up to a specific target number. The function twoSum should return indices of the two numbers such that they add up to the target, where index1 must be less than index2. 123456789101112131415161718192021class Solution {public: vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; numbers, int target) { int index1 = 0, index2 = numbers.size()-1; vector&lt;int&gt; list; while (index1 &lt; index2) { int sum = numbers[index1] + numbers[index2]; if(sum == target) { list.push_back(index1+1); list.push_back(index2+1); return list; } else if(sum &gt; target) { index2--; } else if (sum &lt; target) { index1++; } } return list; }}; 解： 使用双指针，一个指针指向值较小的元素，一个指针指向值较大的元素。指向较小元素的指针从头向尾遍历，指向较大元素的指针从尾向头遍历。指针 不一定是真的指针，标示符； ##633. Sum of Square Numbers (Easy) 两数平方和 Given a non-negative integer c, your task is to decide whether there’re two integers a and b such that $a^{2}+b^{2}=c$. 初始化想法，但是边界的条件，int 的范围不够 123456789101112131415161718class Solution {public: bool judgeSquareSum(int c) { int i = 0; int j = (int)sqrt(c); while (i &lt;= j) { int sqrtSum = i*i +j*j; if(sqrtSum == c) { return true; } else if(sqrtSum &gt; c){ j--; } else { i++; } } return false; }}; 双指针 不一定是针对数据，主要需要协同工作.注意 int的取值范围；改进之后如下： 1234567891011121314151617181920class Solution {public: bool judgeSquareSum(int c) { int i = 0; int j = sqrt(c); while(i &lt;= j) { int squreI = c - j*j; if(squreI == i*i) { return true; } else if(i*i &gt; squreI) { j--; } else { i++; } } return false; }}; 345. Reverse Vowels of a String反转字符串中的元音字符 Write a function that takes a string as input and reverse only the vowels of a string. Note:The vowels does not include the letter “y”. 1234567891011121314151617181920212223242526272829303132333435class Solution {public: string reverseVowels(string s) { int front = 0, rare = s.length()-1; while(front &lt; rare) { if(isVowels(s[front]) == false) { front++; } if(isVowels(s[rare]) == false) { rare--; } if( isVowels(s[front])&amp;&amp; isVowels(s[rare])) { char c = s[front]; s[front] = s[rare]; s[rare] = c; front++; rare--; } } // cout &lt;&lt; s; return s; } bool isVowels(char c) { if(c==&apos;a&apos; ||c==&apos;e&apos; ||c==&apos;i&apos; ||c==&apos;o&apos; ||c==&apos;u&apos; ||c==&apos;A&apos; ||c==&apos;E&apos; ||c==&apos;I&apos; ||c==&apos;O&apos; ||c==&apos;U&apos;) { return true; } return false; }}; 680. Valid Palindrome II (Easy)回文字符串 Given a non-empty string s, you may delete at most one character. Judge whether you can make it a palindrome. Note: The string will only contain lowercase characters a-z. The maximum length of the string is 50000. 123456789101112131415161718192021222324252627282930313233343536373839class Solution {public: bool validPalindrome(string s) { int start = 0, end = s.length()-1; int flag = 0; // &gt;=2 -&gt; no char c; while(start &lt; end) { if(s[start] == s[end]) { start++; end--; } else { if(s[start] == s[end-1]) { c = s[end]; end--; flag++; }else if(s[start+1] == s[end]) { c = s[start]; start++; flag++; } else if(s[start] != s[end-1]&amp;&amp;s[start+1] != s[end]){ flag = 2; } if(flag &gt;= 2) { break; } } } if(flag &gt;= 2) { return false; } else { return true; } }}; 注： 分类情况，写清楚，分类清楚，代码的逻辑才会写的不会有问题。 本题 的算法数据没有，实现步骤没有问题，入口，各种分支逻辑，出口边界条件；但是算法的 思想，没有考虑全面性，要是不存在。 1234567891011121314151617181920212223242526272829303132333435class Solution {public: bool validPalindrome(string s) { int start = 0, end = s.length()-1; int flag = 0; // &gt;=2 -&gt; no char c; while(start &lt;= end &amp;&amp; s[start] == s[end]) { start++; end--; } if(start &gt; end) { //索引相遇表示回文串 return true; } else { //未相遇 string s1 = s.substr(start, end-start); string s2 = s.substr(start+1, end-start); return isPalindrome(s1) || isPalindrome(s2); } } // 判断回文串 bool isPalindrome(string s) { int left = 0; int right = s.length()-1; while(left &lt;= right &amp;&amp; s[left] == s[right]) { left++; right--; } if(left &gt; right) { return true; } else { return false; } }}; 88. Merge Sorted Array (Easy)归并两个有序数组 Given two sorted integer arrays nums1 and nums2, merge nums2 into nums1 as one sorted array. Note: The number of elements initialized in nums1 and nums2 are m and n respectively. You may assume that nums1 has enough space (size that is greater or equal to m + n) to hold additional elements from nums2. 1234567891011121314151617181920212223242526class Solution {public: void merge(vector&lt;int&gt;&amp; nums1, int m, vector&lt;int&gt;&amp; nums2, int n) { int index = n+m-1; int index1 = m-1; int index2 = n-1; while(index1&gt;=0&amp;&amp;index2 &gt;=0) { if(nums2[index2] &gt; nums1[index1]) { nums1[index] = nums2[index2]; index--; index2--; } else { nums1[index] = nums1[index1]; index--; index1--; } } while(index2&gt;=0) { nums1[index] = nums2[index2]; index--; index2--; } }}; 注：双指针，协同完成一个任务，确定开始，逻辑流程，结束条件。从开始麻烦，可换个角度考虑，从尾部开始。 141. Linked List Cycle (Easy)判断链表是否存在环 Given a linked list, determine if it has a cycle in it. To represent a cycle in the given linked list, we use an integer pos which represents the position (0-indexed) in the linked list where tail connects to. If pos is -1, then there is no cycle in the linked list. 12345678910111213141516171819202122232425262728293031323334/** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */class Solution {public: bool hasCycle(ListNode *head) { if(head == NULL) return false; ListNode* slow = head; ListNode* fast = head; // while(slow != NULL &amp;&amp; fast != NULL) { // slow = slow-&gt;next; // fast = fast-&gt;next-&gt;next; // if(slow == fast) { // return true; // } // } // slow-&gt;next不能保证fast-&gt;next不为空啊 while(fast-&gt;next != NULL &amp;&amp; fast-&gt;next-&gt;next != NULL) { slow = slow-&gt;next; fast = fast-&gt;next-&gt;next; if(slow == fast) { return true; } } return false; }}; 注：判断链表回环问题，主要是判断尾部是否为空但是这样会死循环；采用快慢指针。 524. Longest Word in Dictionary through Deleting (Medium)最长子序列 Given a string and a string dictionary, find the longest string in the dictionary that can be formed by deleting some characters of the given string. If there are more than one possible results, return the longest word with the smallest lexicographical order. If there is no possible result, return the empty string. Example 1: 12345Input:s = &quot;abpcplea&quot;, d = [&quot;ale&quot;,&quot;apple&quot;,&quot;monkey&quot;,&quot;plea&quot;]Output: &quot;apple&quot; Note: All the strings in the input will only contain lower-case letters. The size of the dictionary won’t exceed 1,000. The length of all the strings in the input won’t exceed 1,000. 分析程序代码：入口，分叉逻辑(各个分支情况列举)，出口考虑(各种边界条件) 12345678910111213141516171819202122232425262728293031323334353637383940class Solution {public: string findLongestWord(string s, vector&lt;string&gt;&amp; d) { int sIndex = 0, dIndex = 0; string maxString = d[0]; for(int i = 0; i &lt; d.size(); i++) { sIndex = 0; dIndex = 0; while(sIndex &lt; s.length() &amp;&amp; dIndex &lt; d[i].length()) { if(s[sIndex] == d[i][dIndex]) { sIndex++; dIndex++; } else { sIndex++; } // 是否存在 dIndex未走完,存在则抛弃 if(sIndex == s.length()-1 &amp;&amp; dIndex != d[i].length()-1) { // sIndex = 0; // dIndex = 0; break; } if(sIndex == s.length()-1 || dIndex == d[i].length()-1) { //匹配最长最小子串 if(maxString.length() &lt; d[i].length()) { maxString = d[i]; } else if(maxString.length() == d[i].length()){ if(maxString &gt; d[i]) { maxString = d[i]; } } // sIndex = 0; // dIndex = 0; } } } return maxString; }}; 注：控制的变量最好放在一处控制，不要分散开来。 控制结束的条件，初始，各个分支情况流程逻辑；结束时的条件，正常结束，异常结束，边界值 12345678910111213141516171819202122232425262728293031class Solution {public: string findLongestWord(string s, vector&lt;string&gt;&amp; d) { int sIndex = 0, dIndex = 0; string maxString = \"\"; for(int i = 0; i &lt; d.size(); i++) { sIndex = 0; dIndex = 0; while(sIndex &lt; s.length() &amp;&amp; dIndex &lt; d[i].length()) { if(s[sIndex] == d[i][dIndex]) { sIndex++; dIndex++; } else { sIndex++; } } if(dIndex == d[i].length()) { //匹配最长最小子串 if(maxString.length() &lt; d[i].length()) { maxString = d[i]; } else if(maxString.length() == d[i].length()){ if(maxString &gt; d[i]) { maxString = d[i]; } } } } return maxString; }};","link":"/2019/05/11/LeetCode-双指针/"},{"title":"LeetCode-排序","text":"思想如何写程序：分析程序代码：入口，分叉逻辑(各个分支情况列举)，出口考虑(各种边界条件,特别是=的情况) 控制结束的条件，初始，各个分支情况流程逻辑；结束时的条件:(正常结束，异常结束，边界值) 快速选择：用于求解Kth Element问题，也就是第K个元素的问题。 可以使用快速排序的partition()进行实现。需要先打乱数组，否则最坏情况下时间复杂度是$O\\left(N^{2}\\right)$. 堆用于求解TopK Elements问题，也是就是K个最小元素的问题。可以维护一个大小为K的最小堆，最小堆中的元素就是最小元素。最小堆要用大顶堆来实现，大顶堆表示堆顶元素是堆中最大元素。这是因为我们要得到k个最小的元素，因此当遍历到一个新的元素时，需要知道这个新元素是否比堆中的最大元素更小，更小的话就把堆中的最大元素去除，并将新元素添加到堆中。所以我们需要很容易的得到最大元素并移除最大元素，大顶堆可以很好的满足这个要求。 堆也是用于与求解Kth Element问题，得到了大小为k的最小堆之后，因为使用了大顶堆来实现，因此堆顶元素就是第K大的元素。 快速选择也可以求解TopK Elements问题，因为找到Kth Element之后，遍历一次数组，所有小于等于KthElement的元素都是TopK Elements。 可以看到，快速选择和堆排序都可以求解 Kth Element 和 TopK Elements 问题。第K个元素和前K元素 215. Kth Largest Element in an Array (Medium)Kth Element Find the kth largest element in an unsorted array. Note that it is the kth largest element in the sorted order, not the kth distinct element. Note:You may assume k is always valid, 1 ≤ k ≤ array’s length. 题目描述：找到倒数第 k 个的元素。 solution 1： 1234567class Solution {public: int findKthLargest(vector&lt;int&gt;&amp; nums, int k) { sort(nums.begin(), nums.end()); return nums[nums.size() - k]; }}; solution 2: 12345678910class Solution { public: int findKthLargest(vector&lt;int&gt;&amp; nums, int k) { priority_queue&lt;int&gt; q(nums.begin(), nums.end()); for(int i = 0; i &lt; k, i++) { q.pop(); } return q.top(); }}; 熟悉快排：核心思想是每次都要先找一个中枢点Pivot，然后遍历其他所有的数字，像这道题从大往小排的话，就把大于中枢点的数字放到左半边，把小于中枢点的放在右半边，这样中枢点是整个数组中第几大的数字就确定了，虽然左右两部分各自不一定是完全有序的，但是并不影响本题要求的结果，因为左半部分的所有值都大于右半部分的任意值，所以我们求出中枢点的位置，如果正好是k-1，那么直接返回该位置上的数字；如果大于k-1，说明要求的数字在左半部分，更新右边界，再求新的中枢点位置；反之则更新右半部分，求中枢点的位置； 桶排序1. 出现频率最多的 k 个元素 Top K Frequent Elements Given a non-empty array of integers, return the k most frequent elements. 1234567891011121314151617181920212223242526public List&lt;Integer&gt; topKFrequent(int[] nums, int k) { Map&lt;Integer, Integer&gt; frequencyForNum = new HashMap&lt;&gt;(); for (int num : nums) { frequencyForNum.put(num, frequencyForNum.getOrDefault(num, 0) + 1); } List&lt;Integer&gt;[] buckets = new ArrayList[nums.length + 1]; for (int key : frequencyForNum.keySet()) { int frequency = frequencyForNum.get(key); if (buckets[frequency] == null) { buckets[frequency] = new ArrayList&lt;&gt;(); } buckets[frequency].add(key); } List&lt;Integer&gt; topK = new ArrayList&lt;&gt;(); for (int i = buckets.length - 1; i &gt;= 0 &amp;&amp; topK.size() &lt; k; i--) { if (buckets[i] == null) { continue; } if (buckets[i].size() &lt;= (k - topK.size())) { topK.addAll(buckets[i]); } else { topK.addAll(buckets[i].subList(0, k - topK.size())); } } return topK;} 设置若干个桶，每个桶存储出现频率相同的数。桶的下标表示数出现的频率，即第 i 个桶中存储的数出现的频率为 i。把数都放到桶之后，从后向前遍历桶，最先得到的 k 个数就是出现频率最多的的 k 个数。 1234567891011121314151617181920212223242526public List&lt;Integer&gt; topKFrequent(int[] nums, int k) { Map&lt;Integer, Integer&gt; frequencyForNum = new HashMap&lt;&gt;(); for (int num : nums) { frequencyForNum.put(num, frequencyForNum.getOrDefault(num, 0) + 1); } List&lt;Integer&gt;[] buckets = new ArrayList[nums.length + 1]; for (int key : frequencyForNum.keySet()) { int frequency = frequencyForNum.get(key); if (buckets[frequency] == null) { buckets[frequency] = new ArrayList&lt;&gt;(); } buckets[frequency].add(key); } List&lt;Integer&gt; topK = new ArrayList&lt;&gt;(); for (int i = buckets.length - 1; i &gt;= 0 &amp;&amp; topK.size() &lt; k; i--) { if (buckets[i] == null) { continue; } if (buckets[i].size() &lt;= (k - topK.size())) { topK.addAll(buckets[i]); } else { topK.addAll(buckets[i].subList(0, k - topK.size())); } } return topK;} 2. 按照字符出现次数对字符串排序451. Sort Characters By Frequency (Medium) Given a string, sort it in decreasing order based on the frequency of characters. 1234567891011121314151617181920212223242526public String frequencySort(String s) { Map&lt;Character, Integer&gt; frequencyForNum = new HashMap&lt;&gt;(); for (char c : s.toCharArray()) frequencyForNum.put(c, frequencyForNum.getOrDefault(c, 0) + 1); List&lt;Character&gt;[] frequencyBucket = new ArrayList[s.length() + 1]; for (char c : frequencyForNum.keySet()) { int f = frequencyForNum.get(c); if (frequencyBucket[f] == null) { frequencyBucket[f] = new ArrayList&lt;&gt;(); } frequencyBucket[f].add(c); } StringBuilder str = new StringBuilder(); for (int i = frequencyBucket.length - 1; i &gt;= 0; i--) { if (frequencyBucket[i] == null) { continue; } for (char c : frequencyBucket[i]) { for (int j = 0; j &lt; i; j++) { str.append(c); } } } return str.toString();}","link":"/2019/05/14/LeetCode-排序/"},{"title":"Mac终端命令大全","text":"常用命令查看在终端里输入ls就可以查看文件和文件夹，但隐藏的文件就无法查看，使用ls -a即可。 创建目录命令：mkdir使用说明：输入命令随后空格再输入目录名即可。案例：创建一个test目录则对应命令就是mkdir test 进入目录cd命令进入目录比如要进入test目录则就是cd test 返回上一级目录命令：cd.. 删除目录（空目录）命令：rmdir案例：rmdir test（rmdir命令后面空格在加上目录名）注意：此删除不会出现在废纸篓里 删除目录（非空或者空目录都可以删除）推荐使用命令： rm -rf案例：rm -rf test（rm -rf命令后面空格在加上目录名）注意：此删除不会出现在废纸篓里 创建文件命令:touch笔者感觉这个命令还是挺靠谱的。Mac如果不用第三方工具（sublime等）是无法可视化创建一个txt文件。而用这个命令即可创建。示例（创建一个test为名的txt文件）：touch test.txt 删除文件命令：rm案例（删除一个test.txt文件）：rm test.txt注意：此删除不会出现在废纸篓里 拷贝命令：cp案例（拷贝一个test.txt文件并重新命名为test2.txt）：cp test.txt test2.txt 查找命令：find案例（查找当前目录下所有的txt文件）：find *.txt 显示当前的目录命令：pwd 打开成可视化的文件夹命令：open说明：如果是open .则是打开当前命令里的目录，如果要打开指定的文件夹或者文件则格式为open 要打开的路径即可。 Mac 打开、编辑 .bash_profile 文件一般在Mac上配置环境变量时经常要创建、编辑 .bash_profile文件。创建该文件时一般都会选择在当前用户目录下，即Mac下的.bash_profile 文件的路径是 /Users/YourMacUserName/.bash_profile(如果该文件已经创建过的话) 1、创建 .bash_profile （1） 启动终端Terminal （2） 进入当前用户的home目录： cd ~ 或 cd /Users/YourMacUserName （3）输入touch .bash_profile2、查看 、编辑 .bash_profile 文件 （1）终端输入 open -e .bash_profile （2）编辑 （3）关闭即可保存修改3、更新刚配置的环境变量 输入source .bash_profile MAC 设置环境变量PATH 和 查看PATH理论篇Mac系统的环境变量，加载顺序为：/etc/profile /etc/paths ~/.bash_profile ~/.bash_login ~/.profile ~/.bashrc /etc/profile和/etc/paths是系统级别的，系统启动就会加载，后面几个是当前用户级的环境变量。后面3个按照从前往后的顺序读取，如果/.bash_profile文件存在，则后面的几个文件就会被忽略不读了，如果/.bash_profile文件不存在，才会以此类推读取后面的文件。~/.bashrc没有上述规则，它是bash shell打开的时候载入的。PATH的语法为如下12#中间用冒号隔开export PATH=$PATH:&lt;PATH 1&gt;:&lt;PATH 2&gt;:&lt;PATH 3&gt;:------:&lt;PATH N&gt; 上述文件的科普 /etc/paths （全局建议修改这个文件 ）编辑 paths，将环境变量添加到 paths文件中 ，一行一个路径Hint：输入环境变量时，不用一个一个地输入，只要拖动文件夹到 Terminal 里就可以了。 /etc/profile （建议不修改这个文件 ）全局（公有）配置，不管是哪个用户，登录时都会读取该文件。 /etc/bashrc （一般在这个文件中添加系统级环境变量）全局（公有）配置，bash shell执行时，不管是何种方式，都会读取此文件 .profile 文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行.并从/etc/profile.d目录的配置文件中搜集shell的设置; 使用注意：如果你有对/etc/profile有修改的话必须得重启你的修改才会生效，此修改对每个用户都生效。 ./bashrc 每一个运行bash shell的用户执行此文件.当bash shell被打开时,该文件被读取.使用注意 对所有的使用bash的用户修改某个配置并在以后打开的bash都生效的话可以修改这个文件，修改这个文件不用重启，重新打开一个bash即可生效。 ./bash_profile 该文件包含专用于你的bash shell的bash信息,当登录时以及每次打开新的shell时,该文件被读取.（每个用户都有一个.bashrc文件，在用户目录下）使用注意 需要需要重启才会生效，/etc/profile对所有用户生效，~/.bash_profile只对当前用户生效。 source ./.bash_profile 或者 ./.profile 环境信息生效 操作篇全局设置 创建一个文件：sudo touch /etc/paths.d/mysql 用 vim 打开这个文件（如果是以 open -t 的方式打开，则不允许编辑）：sudo vim /etc/paths.d/mysql 编辑该文件，键入路径并保存（关闭该 Terminal 窗口并重新打开一个，就能使用 mysql 命令了）/usr/local/mysql/bin source 相应的文件 生效配置环境单个用户设置 cd ~ vim ~/.bash_profile （任意一个文件中添加用户级环境变量）: export PATH=/opt/local/bin:/opt/local/sbin:$PATH 把上述代码添加到~/.bash_profile上。 source 相应的文件 生效配置环境查看PATHecho $PATHscreen命令整理常用指令 screen代表增加一个虚拟终端 ctrl + A + D 挂起虚拟终端，进入真终端 kill xxxx 结束虚拟终端 screen -r 进入虚拟终端 Ctr-a k 杀死某一个窗口 一个全屏窗口管理器，用他可以轻松在一个物理终端上获得多个虚拟终端的效果。 简单来说，Screen是一个可以在多个进程之间多路复用一个物理终端的窗口管理器,这意味着你能够使用一个单一的终端窗口运行多终端的应用。Screen中有会话的概念，用户可以在一个screen会话中创建多个screen窗口，在每一个screen窗口中就像操作一个真实的telnet/SSH连接窗口那样。 screen可以看作窗口管理器的命令行版本，因为它是运行在一个linux终端上。在screen中启动的每一个会话都有一个id。可以切换。它的特性：1、可恢复 在远程登录linux的时候，如果终端长时间没有动作会被机器强制断线，一旦终端的通信中断，那么这个会话上的所有内容都会丢失。在screen中的会话，只要screen本身没有终止，其内部运行的会话都可以恢复。这一点对于远程登录用户特别有用。2、多窗口 可以在一次远程登录终端中保持多个窗口，并可切换。 Session（会话） window（窗口） 前者包含多个后者。Ctr+c就是新建了一个window（可以在window里恢复一个session，那样会增加复杂度） 所有的窗口使用exit退出之后，session会自己退出 合理的使用方法是，每一项任务都是一个会话，每个会话里都有许多功能窗口 Screen –ls查看本机运行的screen会话 Screen –SU &lt;name&gt;新建一个会话 Ctr-a d离开一个会话进入真终端（会话进入detached挂起，不影响各window里进程的运行） Screen –rU &lt;name or id&gt;恢复某个会话（会话进入attached） Ctr-a c 新建一个窗口。 Ctr-a k 杀死某一个窗口 Ctr-a n/f 多个active窗口之间进行切换 Ctr-a w 显示窗口列表 Ctr-a 数字 切换至某个窗口 Ctr-a a最近使用的两个窗口间切换 在一个会话中，exit可以退出一个窗口。退出到底可以退出一个会话。中文乱码问题： secureCRT 设置字体、字符集、编码screen的设置问题。C+a ：defencoding utf8 C+a : encoding utf8 utf8 常用命令详解基本命令列出文件ls 参数 目录名 例: 看看驱动目录下有什么:ls /System/Library/Extensions参数 -w 显示中文，-l 详细信息， -a 包括隐藏文件 转换目录cd 例：想到驱动目录下溜达一圈 cd /System/Library/Extensions 建立新目录mkdir 目录名 例：在驱动目录下建一个备份目录 backup mkdir /System/Library/Extensions/backup在桌面上建一个备份目录 backup mkdir /User/用户名/Desktop/backup 拷贝文件cp 参数 源文件 目标文件例：想把桌面的Natit.kext 拷贝到驱动目录中 cp -R /User/用户名/Desktop/Natit.kext /System/Library/Extensions参数R表示对目录进行递归操作，kext在图形界面下看起来是个文件，实际上是个文件夹。把驱动目录下的所有文件备份到桌面backupcp -R /System/Library/Extensions/* /User/用户名/Desktop/backup 删除文件rm 参数 文件 例：想删除驱动的缓存 rm -rf /System/Library/Extensions.kextcache rm -rf /System/Library/Extensions.mkext参数－rf 表示递归和强制，千万要小心使用，如果执行了 rm -rf / 你的系统就全没了 移动文件mv 文件 例：想把AppleHDA.Kext 移到桌面 mv /System/Library/Extensions/AppleHDA.kext /User/用户名/Desktop想把AppleHDA.Kext 移到备份目录中 mv /System/Library/Extensions/AppleHDA.kext /System/Library/Extensions/backup 文本编辑nano 文件名 例：编辑natit Info.plist nano /System/Library/Extensions/Natit.kext/Info.plist 目录操作命令名 功能描述 使用举例 mkdir 创建一个目录 mkdir dirname rmdir 删除一个目录 rmdir dirname mvdir 移动或重命名一个目录 mvdir dir1 dir2 cd 改变当前目录 cd dirname pwd 显示当前目录的路径名 pwd ls 显示当前目录的内容 ls -la 文件操作命令名 功能描述 使用举例 cat 显示或连接文件 cat filename od 显示非文本文件的内容 od -c filename cp 复制文件或目录 cp file1 file2 rm 删除文件或目录 rm filename mv 改变文件名或所在目录 mv file1 file2 find 使用匹配表达式查找文件 find . -name &quot;*.c&quot; -print file 显示文件类型 file filename 选择操作命令名 功能描述 使用举例 head 显示文件的最初几行 head -20 filename tail 显示文件的最后几行 tail -15 filename cut 显示文件每行中的某些域 cut -f1,7 -d: /etc/passwd colrm 从标准输入中删除若干列 colrm 8 20 file2 diff 比较并显示两个文件的差异 diff file1 file2 sort 排序或归并文件 sort -d -f -u file1 uniq 去掉文件中的重复行 uniq file1 file2 comm 显示两有序文件的公共和非公共行 comm file1 file2 wc 统计文件的字符数、词数和行数 wc filename nl 给文件加上行号 nl file1 &gt;file2 进程操作命令名 功能描述 使用举例 ps 显示进程当前状态 ps u kill 终止进程 kill -9 30142 时间操作命令名 功能描述 使用举例 date 显示系统的当前日期和时间 date cal 显示日历 cal 8 1996 time 统计程序的执行时间 time a.out 网络与通信操作命令名 功能描述 使用举例 telnet 远程登录 telnet hpc.sp.net.edu.cn rlogin 远程登录 rlogin hostname -l username rsh 在远程主机执行指定命令 rsh f01n03 date ftp 在本地主机与远程主机之间传输文件 ftpftp.sp.net.edu.cn rcp 在本地主机与远程主机 之间复制文件 rcp file1 host1:file2 ping 给一个网络主机发送 回应请求 ping hpc.sp.net.edu.cn mail 阅读和发送电子邮件 mail write 给另一用户发送报文 write username pts/1 mesg 允许或拒绝接收报文 mesg n Korn Shell 命令 命令名 功能描述 使用举例 history 列出最近执行过的 几条命令及编号 history r 重复执行最近执行过的 某条命令 r -2 alias 给某个命令定义别名 alias del=rm -i unalias 取消对某个别名的定义 unalias del 其它命令命令名 功能描述 使用举例 uname 显示操作系统的有关信息 uname -a clear 清除屏幕或窗口内容 clear env 显示当前所有设置过的环境变量 env who 列出当前登录的所有用户 who whoami 显示当前正进行操作的用户名 whoami tty 显示终端或伪终端的名称 tty stty 显示或重置控制键定义 stty -a du 查询磁盘使用情况 du -k subdirdf /tmp 显示文件系统的总空间和可用空间 w 显示当前系统活动的总信息","link":"/2018/05/13/Mac终端命令大全/"},{"title":"综述-NeuralArchitectureSearch","text":"综述- Nerual Architecture Search神经网络架构搜索，顾名思义，就是让机器自己去学习如何构架一个神经网络，得到比人类专家手工设计的网络更好的效果。这个思路是非常一脉相承的，就像机器学习到深度学习，也是完成了人类专家手工提取特征到由机器自己学习特征这样的步骤转换。 而现在的深度学习网络架构，虽然获得了不错的效果，但是细究起来，其实是没有非常牢固的理论根据的，依靠的是人类的先验与设计。 所以，在考虑有限制的空间内获得效果更好的网络这一问题上，架构搜索能够给出一些答案。 Introduction of NASTODO 主参考论文：Neural Architecture Search: A Survey 【pdf】 作者：Thomas Elsken, Jan Hendrik Metzen, Frank Hutter 首先，我们先整体讨论一下神经网络架构搜索到底是怎样进行的。通常来说，我们需要考虑三个方面：搜索空间、搜索策略和评价策略。 搜索空间（SearchSpace）: 搜索空间定义了搜索的范围，其实就是在哪搜索。通过结合一些过去研究者架构设计方面的经验，可以通过减小搜索空间和简化搜索过程来提高搜索的性能。当然，这样同时也引入了人为的主观臆断，可能会妨碍寻找到超越当前人类知识的新的架构构建块（building blocks）。 搜索策略（SearchStrategy）：搜索策略定义的则怎样去搜索。一方面，我们希望能快速找到性能良好的架构，另一方面，也应避免过早收敛到次优架构（sub optimal architeture）区域。 性能评估策略（Performaceestimation strategy）：NAS 的目标是希望能够自动的在给定的数据集上找到一个高性能的架构。性能评估则是指评估此性能的过程：最简单的方式是按照通常的方式对一个标准架构训练和验证来获得结果，但遗憾的是这样的计算成本太高了，并且同时限制了可以搜索的网络架构的数量。因此，最近的许多研究都集中在探索新的方法来降低这些性能评估的成本。 搜索空间（Search Space） 如图所示的就是一个相对简单的搜索空间，称为链式神经网络（Chain-stuctured Neural Network），很简单就是一个链表，第$ 𝑖−1$层的输出作为第 $i$ 层的输入，可以表示为$ 𝐴=𝐿{n}∘…𝐿{1}∘𝐿_{0}$。而针对于这样一个 search space，我们就需要考虑这些参数： 网络的(最大)层数𝑛 每一层执行的操作类型，比如 pooling， convolution 这样的基础操作，或者更高级的一些操作，如depthwise separable convolutions 或 dilated convolutions。 每一层与这个操作相关的 hyperparameters，比如对于一个一般的 convolutional 层来说，有 filter 的 numbers，keneral 的 size 和 strides 的 length，而对于 fully-connected 层来说就说是 units 的 number 了。 而需要注意的是，与每层相关的 hyperparameters 是取决于这一层的操作类型，因此对于 Search Space 的参数化的结果并不是一个固定的长度（fixed-length），而是一个条件空间（conditioanl space）。 最近的很多关于 NAS 的研究中都引入了人工设计出的如跳跃连接(skip connections)这样的架构元素，可以用来构建如图 2 右所示的复杂的多分支网络（multi-branch networks）。对于这样的结构，第 i 层的输入不再仅仅是前一层的输入，而需要表示为一个组合函数的形式。 这种 cell-based 的 search space 也应用在了很多后来的研究中。然而，当使用基于 cell 的搜索空间时，出现了一种新的选择问题，即如何选择元架构（Micro-architecture）：应该使用多少 cells 以及如何连接它们来构建实际模型？ 理想情况下，Meta-architecture 也应作为 NAS 的一部分自动优化;否则，如果大多数复杂性已经由 meta-architecture 解决，那么很容易就把问题变成进行 meta-architecture engineer，这样对 Cell 的搜索就变得过于简单了。 搜索策略（Search Strategy）到现在，已经有许多不同的搜索策略用于 NAS，主要有如下这些： 随机搜索（random search） 贝叶斯优化（Bayesian optimazation） 进化方法（evolutionaray methods） 强化学习（Reinforcement Learning, RL） 梯度方法（gradient-based methods） 自 2013 年开始，Bayesian optimazation（BO）就在 NAS 研究中取得了一些成功，基于 BO 发现了当时最优的 vison architectures，在 CIFAR-10 上取得最优 architeture，以及实现了第一个超过人类专家赢得竞赛的 automaticallytuned 神经网络。 在 16 年 Google Brain 发表的 [10] 中通过强化学习的搜索策略在 CIFAR-10 和 Penn Treebank 数据集上取得很好表现后，NAS 成为机器学习社区的主流研究课题之一。 关于其他几个算法，会在后续介绍，这边简单介绍一下进化算法和贝叶斯优化。 贝叶斯优化（BO）是用于 hyperparameters 优化中最流行的方法，但是由于 typical BO toolboxes 是基于高斯过程且主要集中于低维连续性优化问题，所以它并没有被很多组应用于 NAS 中。[17] 中派生了 kennel function 来使用基于 GP 的 BO 方法，但还没有实现过最优的结果。相比之下，一些研究中使用基于树的模型或随机森林来搜索非常高维的条件空间时实现了在很多问题上的最优性能，这些方法同时优化神经架构和它们的 hyperparameters。虽然缺乏完整的比较，但初步证据表明这些方法可以超过进化算法 [18]。 关于 进化算法： 更近期的一些研究中有不是使用基于梯度的方法来优化权重，而使用进化算法来优化神经结构本身。是用进化算法演化一组模型，在每个进化步骤中，对来自群体的至少一个模型进行采样，并将它们作为父母通过繁衍或者突变来产生后代。在 NAS 中，突变表示本地化的操作，例如添加或移除层，改变层的超参数，添加跳跃连接，以及改变训练超参数。在对后代进行训练之后，评估它们的适应性（例如，在验证集上的表现）后再将它们添加到种群中。 性能评估策略（Performace Estimation Strategy）上一节讨论的搜索策略旨在找到某些性能度量（如准确度）最大化的架构 A，为了引导它们的搜索过程，这些策略需要考虑如何评判给定架构的性能高低。最简单的方法是在训练数据上训练 A 并评估其在验证数据上的表现。然而，从头开始训练每个要评估的架构经常会产生大约数千 GPU 天的计算需求 为了加快搜索过程，需要在相对较少的评估的基础上并在相对较大的搜索空间中进行良好的预测 Lower Fidelit Estimates 这种 lower fidelities 可以表示为较短的训练时间，对训练子集的训练，使用较低分辨率的图像，或者是用更少的 filter。虽然这些 lower fidelities 降低了计算成本，但与此同时它们也会在测试中引入偏差，因为性能通常会被低估。当然只要搜索策略只依赖于相对排名时，这不会有什么问题。 Learning Curve Extrapolation 提出方法来推断初始学习曲线并终止那些预测表现不佳以加速架构搜索过程。另外一些研究者则通过基于架构的超参数来预测哪些部分学习曲线最有希望。 Weight Inheritance/ Network Morphisms ??基于之前已经训练过的其他架构的权重来初始化新架构的权重。实现这一目标的一种方法，称为 network morphisms，它允许修改架构的同时保持网络所代表的功能不变。这就允许连续的增加网络容量并保持高性能而无需从头开始训练。对几个时期的持续训练也可以利用 network morphisms 引入额外容量。 One-Shot Models/ Weight Sharing ??它将所有架构都视为一个超级图的不同子图，并在架构之间共享权重。那么只需要一次性训练单个模型的权重，然后通过继承权重来评估架构（它们只是一次性模型的子图），而无需再进行任何单独的训练。比如ENAS、Darts。 NAS &amp; NASNet &amp; ENASNASNeural Architecture Search with Reinforcement Learning 【pdf】 它是NSANet的前置，比较详细的介绍了Google的NAS的整体设计，包括具体的目标函数以及实现skip connection的具体方法，主要是提出了用强化学习的方法来完成搜索神经网络架构。 通过一个controllerRNN在搜索空间（search space）中得到一个网络结构（论文中称为child network），然后用这个网络结构在数据集上训练，在验证集上测试得到准确率R，再将这个准确率回传给controller，controller继续优化得到另一个网络结构，如此反复进行直到得到最佳的结果，整个过程称为Neural Architecture Search。 How to use a controller RNN to generate an CNN model控制器生成的是网络架构中的超参数，NAS中假定预测的inference中只含卷积层。 对于Layer N，控制器可以预测该层的filter width，filter height，stride height，stride width，filter的个数，每一个预测都是通过softmax分类，并作为下一个预测的输入。实验中的停止条件是，层的个数超过了某个指定的数值。当然这个数值在训练的过程中也可以变化。 最终得到的是LSTM产生的一个序列化的tokens，记为$θ_{c}$。 How to train a controller RNN with REINFORCE将NAS与强化学习做一个对应，那么控制器LSTM就是Agent，控制器预测产生$θ{c}$对应为活动$𝑎{1:T}$，子网络在验证集上的准确率R作为reward signal。 那么，就得到了目标函数： $\\boldsymbol{J}\\left(\\boldsymbol{\\theta}{c}\\right)=\\boldsymbol{E}{P\\left(a_{1 : T} ; \\theta_{c}\\right)}[\\boldsymbol{R}]$ 鉴于R是不连续的，我们采用policy gradient来迭代更新$θ_{c}$，具体采用了REINFORCE规则： $\\nabla_{\\theta_{c}} J\\left(\\theta_{c}\\right)=\\sum_{t=1}^{T} E_{P\\left(a_{1 : T} ; \\theta_{c}\\right)}\\left[\\nabla_{\\theta_{c}} \\log P\\left(a_{t} | a_{(t-1) : 1} ; \\theta_{c}\\right) R\\right]$ 对上述公式作一个先验近似： $\\frac{1}{m} \\sum_{k=1}^{m} \\sum_{t=1}^{T} \\nabla \\theta_{c} \\log P\\left(a_{t} | a_{(t-1) : 1} ; \\theta_{c}\\right) R_{k}$ 由于baseline fuction b不依赖与当下action，本式依然是一个无偏估计。b采用的之前架构准确率的指数加权平均（exponential moving average）。 How to add skip connections and other layer typesNAS采用注意力机制。具体来说，通过添加在第N层添加N-1个anchor，来确定是否要与之前的某一层跳跃连接。anchor通过两层（本层和上一层）的hidden state，用sigmoid来判断： $\\mathrm{P}(\\text { Layer j is an input to layer i })=\\operatorname{sigmoid}\\left(v^{\\mathrm{T}} \\tanh \\left(W_{\\mathrm{prev}} h_{j}+W_{\\mathrm{curr}} h_{i}\\right)\\right)$ 公式中的$𝑊{𝑝}𝑟𝑒𝑣$、$𝑊{𝑐}𝑢𝑟𝑟$和$𝑣$是可训练变量。这些行为依然是概率分布的，因此REINFORCE方法不会有大的改动。 由下图可以形象的了解anchor point是怎样添加跳跃连接的。 本篇的实验结果并无太多参考意义，因为很快作者就出了NASNet。NAS最大的意义还是在提出了一种神经网络搜索的框架结构。 实验中采用了2层35个隐藏单元的LSTM，使用800个GPU训练了28天，一共22400 GPU-Hours，一共训练了12800个架构。 NASNet论文： Learning Transferable Architectures for Scalable Image Recognition 【pdf】 相较于NAS的优化： 一种新的搜索空间- NASNet search space—-a generic convolutional cell 一种新的正则化技术，ScheduledDropPath，使得NASNet模型的泛化能力更好(实验细节，不太重要) NASNet cell不再进行整个网络的搜索，而是定义了cell单元作为building block。通过堆叠cell形成最后的网络。 具体讨论的话，对于CNN网络，采用了两种cell，normal cell和reduction cell。normal cell的输出保持维度不变，而reduction cell用于收缩维度，具体来说就是步长为2。两种单元是不同的架构，堆叠在一起形成了最后的网络架构。 控制器RNN输出的是两种cell的超参，最后以下图形式堆叠为子网络，进行训练和验证，得到验证集准确率R回馈到控制器。 在NASNet的搜索空间中，每个cell的有两个初始隐藏状态输入$ℎ𝑖{i}$和$ℎ{𝑖−1}$，分别是前两个层的输出。控制器RNN会跟根据这两个initial hidden states预测卷积cell的剩余结构。 控制器对每个cell的预测分为B个bolck（此处的block可以理解为一个操作），每个block又有5个不同的softmax分类器进行5个预测步骤。 在算法中，用之前已经存在的隐藏态作为序列分块的可能输入来填入新建的hidden state。B=5时效果较好，即一个cell中包含5个操作。 为了使RNN能同时预测Normal Cell和Reduction Cell，我们简单设定控制器有2*5B个预测步骤。前5B给Normal Cell，后5B给Reduction Cell。 实验结果在具体训练阶段，控制器采用Proximal Policy Optimazation（PPO）策略，用RNN控制一个全局工作序列系统来生成一个子网络的备选池。 训练时长为500个GPU训练4天，总计2000GPU时。比初始NAS快了7倍，但是还是非常奢侈。 训练得到的卷积cell： CIFAR-10上的训练效果： cell泛化到ImageNet的图像分类任务： ENAS论文：Efficient Neural Architecture Search via Parameter Sharing 【pdf】【code-tf】【code-python】 在ENAS中，控制器通过在大型计算图中搜索最优子图来发现神经网络结构。利用策略梯度对控制器进行训练，通过权重共享，快速选择验证集上期望报酬最大的子图。 最关键是利用子模型之间共享参数，大大减少了训练时长。采用了控制器参数与子网络参数相互迭代的方式进行训练。 本处为了通篇逻辑，也使用ENAS中CNN cell的训练过程来描述。RNN网络训练过程请参考原文。 搜索空间基本沿用NASNet。 用一个例子来说明搜索空间的机制，这里是B = 4个节点(参见图5)。 节点1,2是输入节点，因此不需要为它们做任何决策。设ℎ1h1,ℎ2h2为这些节点的输出。 在节点3:控制器采样前两个节点和两个操作。在左上角的图5中，它对节点1、节点2分别选取了操作sep conv 5x5和identity。这意味着$h_{3}=\\operatorname{sep}{c} o n v{5} x 5\\left(h_{2}\\right)+i d\\left(h_{2}\\right)$。 在节点4:控制器采样节点3、节点1为 avg pooling3x3和sep conv 3x3。这意味着$h_{4}=a v g_{p} o o l_{3} x 3\\left(h_{3}\\right)+\\operatorname{sep}{c} o n v{3} \\times 3\\left(h_{1}\\right)$。 由于除h4之外的所有节点都被用作至少另一个节点的输入，因此惟一的松散端h4被视为单元的输出。如果有多个松散的端点，它们将沿着深度维度连接起来，形成单元格的输出。 架构搜索与训练权重迭代进行在ENAS,有两套可学的参数:控制器LSTM的参数θ,和子模型的共享参数ω。ENAS的培训过程包括两个交叉阶段。 第一阶段通过一整个训练集训练子模型共享参数$w$。在本文的Penn Treebank实验中，ω是训练大约400 steps,每个minibatch使用64个样本,采用梯度反向传播+梯度截断（Truncated Gradient）进行优化，每隔35 steps进行一次隔断。同时,在CIFAR-10上,ω是45,000张训练图像,分为minibatches大小128,∇ω计算使用标准的反向传播。 第二阶段训练参数控制器LSTM的参数θ, 对于固定数量的步骤,通常在我们的实验设置为2000。这两个阶段在ENAS的培训期间交替进行。 Deriving Architectures. 如何在训练好的ENAS模型中派生出全新的结构。首先从训练好的策略π(m, θ)中采样几个模型。针对每个模型，通过验证集的一个minibatch计算reward。选取其中reward最高的模型从零开始预训练。 实验结果 Latest architecture search algorithms上述NAS三篇论文是一脉相承，一点点改进过来的。其实我最开始接触的论文是Darts，当时感觉这个结构是全新的。再一路看下来，会发现Darts也是在ENAS上又做了搜索策略部分的革新，整体的架构也还是继承下来的。","link":"/2019/06/03/NeuralArchitectureSearch/"},{"title":"Docker快速入门","text":"简介本文记录的是作为一个新手，从了解 Docker 是什么、Docker 技术包含哪些概念到上手使用、安装以及发布 Docker 镜像的整个过程。作者在学习过程中参阅了诸多文档和教程，在此一并感谢，与此同时本文结尾也列出了参考文献的链接，供读者进一步参考。遵循简介、入门、上手到深入的顺序，本文根据个人学习实践过程进行书写，结构如下： Docker 简介 1.1 Docker 概念扫盲：什么是 Docker？ 1.2 Docker 和虚拟机的区别与特点 Docker 基本概念 2.1 核心概念：镜像、容器与仓库 2.2 Docker 三剑客 Docker 安装与使用 3.1 Docker 安装、运行与加速 3.2 Hello World 镜像使用与发布 4.1 镜像的获取与使用 4.2 通过 commit 命令理解镜像构成 4.3 利用 Dockerfile 定制镜像 总结 参考 其中第一章与第二章非常详细地介绍了 Docker 的相关概念与基本组成，主要是概论介绍等文字描述，第三章与第四章偏重于上手实践，从 Docker 环境安装、运行加速、镜像使用、镜像构成到镜像定制与发布，分解了各步骤的流程，通过教程加注解的形式加深读者印象。其中建立镜像的示例代码可以从 GitHub 打包下载。 一、Docker 简介谈到 Docker，不论我们是否实践过，都应该对它或多或少有一个印象，即“环境一次创建，多端一致性运行”，因为它正解决了曾经困扰我们已久“这段代码在我电脑上运行没问题啊”的烦恼。首先，简单介绍一下 Docker 技术是什么。 1.1 Docker 概念扫盲：什么是 Docker？Docker 是一个开放源代码软件项目，项目主要代码在2013年开源于 GitHub。它是云服务技术上的一次创新，让应用程序布署在软件容器下的工作可以自动化进行，借此在 Linux 操作系统上，提供一个额外的软件抽象层，以及操作系统层虚拟化的自动管理机制。 Docker 利用 Linux 核心中的资源分脱机制，例如 cgroups，以及 Linux 核心名字空间（name space），来创建独立的软件容器（containers），属于操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。Docker 在容器的基础上进行了进一步的封装，从文件系统、网络互联到进程隔离等等，极大的简化了容器的创建和维护，使得其比虚拟机技术更为轻便、快捷。Docker 可以在单一 Linux 实体下运作，避免因为创建一个虚拟机而造成的额外负担。 1.2 Docker 和虚拟机的区别与特点对于新手来说，第一个觉得困惑的地方可能就是不清楚 Docker 和虚拟机之间到底是什么关系。以下两张图分别介绍了虚拟机与 Docker 容器的结构。 对于虚拟机技术来说，传统的虚拟机需要模拟整台机器包括硬件，每台虚拟机都需要有自己的操作系统，虚拟机一旦被开启，预分配给他的资源将全部被占用。每一个虚拟机包括应用，必要的二进制和库，以及一个完整的用户操作系统。![](./Docker快速入门/images/DockerStructure.png 容器技术和我们的宿主机共享硬件资源及操作系统，可以实现资源的动态分配。容器包含应用和其所有的依赖包，但是与其他容器共享内核。容器在宿主机操作系统中，在用户空间以分离的进程运行。容器内没有自己的内核，也没有进行硬件虚拟 具体来说与虚拟机技术对比，Docker 容器存在以下几个特点： 更快的启动速度：因为 Docker 直接运行于宿主内核，无需启动完整的操作系统，因此启动速度属于秒级别，而虚拟机通常需要几分钟去启动。 更高效的资源利用率：由于容器不需要进行硬件虚拟以及运行完整操作系统等额外开销，Docker 对系统资源的利用率更高。 更高的系统支持量：Docker 的架构可以共用一个内核与共享应用程序库，所占内存极小。同样的硬件环境，Docker 运行的镜像数远多于虚拟机数量，对系统的利用率非常高。 持续交付与部署：对开发和运维人员来说，最希望的就是一次创建或配置，可以在任意地方正常运行。使用 Docker 可以通过定制应用镜像来实现持续集成、持续交付、部署。开发人员可以通过 Dockerfile 来进行镜像构建，并进行集成测试，而运维人员则可以直接在生产环境中快速部署该镜像，甚至进行自动部署。 更轻松的迁移：由于 Docker 确保了执行环境的一致性，使得应用的迁移更加容易。Docker 可以在很多平台上运行，无论是物理机、虚拟机、公有云、私有云，甚至是笔记本，其运行结果是一致的。因此用户可以很轻易的将在一个平台上运行的应用，迁移到另一个平台上，而不用担心运行环境的变化导致应用无法正常运行的情况。 更轻松的维护与扩展：Docker 使用的分层存储以及镜像的技术，使得应用重复部分的复用更为容易，也使得应用的维护更新更加简单，基于基础镜像进一步扩展镜像也变得非常简单。此外，Docker 团队同各个开源项目团队一起维护了一大批高质量的 官方镜像，既可以直接在生产环境使用，又可以作为基础进一步定制，大大的降低了应用服务的镜像制作成本。 更弱的隔离性：Docker 属于进程之间的隔离，虚拟机可实现系统级别隔离。 更弱的安全性：Docker 的租户 root 和宿主机 root 等同，一旦容器内的用户从普通用户权限提升为 root 权限，它就直接具备了宿主机的 root 权限，进而可进行无限制的操作。虚拟机租户 root 权限和宿主机的 root 虚拟机权限是分离的，并且利用硬件隔离技术可以防止虚拟机突破和彼此交互，而容器至今还没有任何形式的硬件隔离，这使得容器容易受到攻击。 二、Docker 基本概念##2.1 核心概念：镜像、容器与仓库 Docker 主要包含三个基本概念，分别是镜像、容器和仓库，理解了这三个概念，就理解了 Docker 的整个生命周期。以下简要总结一下这三点，详细介绍可以移步Docker 从入门到实践对应章节。 镜像：Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。 容器：容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的命名空间容器可以被创建、启动、停止、删除和暂停等等，说到镜像与容器之间的关系，可以类比面向对象程序设计中的类和实例。 仓库：镜像构建完成后，可以很容易的在当前宿主机上运行，但是，如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry 就是这样的服务。一个 Docker Registry 中可以包含多个仓库；每个仓库可以包含多个标签；每个标签对应一个镜像，其中标签可以理解为镜像的版本号。 2.2 Docker 三剑客 docker-compose：Docker 镜像在创建之后，往往需要自己手动 pull 来获取镜像，然后执行 run 命令来运行。当服务需要用到多种容器，容器之间又产生了各种依赖和连接的时候，部署一个服务的手动操作是令人感到十分厌烦的。 dcoker-compose 技术，就是通过一个 .yml 配置文件，将所有的容器的部署方法、文件映射、容器连接等等一系列的配置写在一个配置文件里，最后只需要执行 docker-compose up 命令就会像执行脚本一样的去一个个安装容器并自动部署他们，极大的便利了复杂服务的部署。 docker-machine：Docker 技术是基于 Linux 内核的 cgroup 技术实现的，那么问题来了，在非 Linux 平台上是否就不能使用 docker 技术了呢？答案是可以的，不过显然需要借助虚拟机去模拟出 Linux 环境来。 docker-machine 就是 docker 公司官方提出的，用于在各种平台上快速创建具有 docker 服务的虚拟机的技术，甚至可以通过指定 driver 来定制虚拟机的实现原理（一般是 virtualbox） docker-swarm：swarm 是基于 docker 平台实现的集群技术，他可以通过几条简单的指令快速的创建一个 docker 集群，接着在集群的共享网络上部署应用，最终实现分布式的服务。 三、Docker 安装与使用Docker 按版本划分，可以区分为 CE 和 EE。CE 即社区版（免费，支持周期三个月），EE 即企业版，强调安全，付费使用。 3.1 Docker 安装、运行与加速本部分只介绍 macOS 上的安装，其余操作系统上的使用方法可以参见官网。 Docker for Mac 要求系统最低为 macOS 10.10.3 Yosemite。如果系统不满足需求，可以安装 Docker Toolbox。 安装方法有两种：homebrew 和手动下载安装。Homebrew-Cask 已经支持 Docker for Mac，如果使用该方式，你只需要运行如下命令：brew cask install docker若是手动下载，也很容易。从这里选择合适你的版本下载 dmg 文件，然后拖进应用中即可。 启动应用之后，你可以在命令行查看 Docker 版本（Docker 不同技术对应的版本用不同命令进行查看）： 12345678docker --version&gt; Docker version 18.03.0-ce, build 0520e24docker-compose --version&gt; docker-compose version 1.20.1, build 5d8c71bdocker-machine --version&gt; docker-machine version 0.14.0, build 89b8332 当然了，由于某些原因，国内从 Docker Hub 上拉取内容会非常缓慢，这个时候就可以配置一个镜像加速器环境。详情说明可以移步Docker 中国官方镜像加速，对于 macOS 用户，在任务栏点击应用图标 -&gt; Perferences… -&gt; Daemon -&gt; Registry mirrors，在列表中填写加速器地址 https://registry.docker-cn.com 即可。修改完成之后，点击 Apply &amp; Restart 即可。 3.2 Hello World大多数编程语言以及一些软件的第一个示例都是 Hello Wolrd，Docker 也不例外，运行 docker run hello-world 验证一下吧（该命令下 Docker 会在本地查找镜像是否存在，但因为环境刚装上所以肯定不存在，之后 Docker 会去远程 Docker registry server 下载这个镜像），以下为运行信息。 1234567891011121314151617181920212223242526Unable to find image &apos;hello-world:latest&apos; locallylatest: Pulling from library/hello-worldd1725b59e92d: Pull completeDigest: sha256:0add3ace90ecb4adbf7777e9aacf18357296e799f81cabc9fde470971e499788Status: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/ 网上有不少文章说到 macOS 的特殊配制，即由于 Docker 引擎使用了特定于 Linux 内核的特性，需要安装一个轻量级的虚拟机（如 VirtualBox）来保证在 macOS 上正常运行 Docker\b\b。于是建议下载官方提供的 Boot2Docker 来运行 Docker 守护进程。但是查看了项目动态后发现，由于 Docker for Mac 和 Docker for Windows 的发布，这一操作已不被推荐，即两个客户端环境中已经继承了该功能。 Boot2Docker is officialy in maintenance mode – it is recommended that users transition from Boot2Docker over to Docker for Mac or Docker for Windows instead. From https://github.com/boot2docker/boot2docker 所以在看到任何教程准备上手之前，最好看看教程本身依赖的环境版本以及软件最新的发布动态，这个习惯对于本文的作者来说也不例外。 四、镜像使用与发布4.1 镜像的获取与使用之前提到 Docker Hub 上有大量优秀的镜像，下面先来说说如何获取镜像。和 git 类似，docker 也使用 pull 命令，其格式如下：docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签] 其中 Docker 镜像仓库地址若不写则默认为 Docker Hub，而仓库名是两段式名称，即 &lt;用户名&gt;/&lt;软件名&gt;。对于 Docker Hub，如果不给出用户名，则默认为 library，也就是官方镜像。我们拉取一个 ubuntu 16.04 镜像试试： 1234567891011docker pull ubuntu:16.04# 输出信息16.04: Pulling from library/ubuntud3938036b19c: Pull completea9b30c108bda: Pull complete67de21feec18: Pull complete817da545be2b: Pull completed967c497ce23: Pull completeDigest: sha256:9ee3b83bcaa383e5e3b657f042f4034c92cdd50c03f73166c145c9ceaea9ba7cStatus: Downloaded newer image for ubuntu:16.04 上面的命令中没有给出 Docker 镜像仓库地址，因此将会从 Docker Hub 获取镜像。而镜像名称是 ubuntu:16.04，因此将会获取官方镜像 library/ubuntu 仓库中标签为 16.04 的镜像。这里需要说明的是，由于 Docker for Mac 的支持即前文所述的原因，在 Docker 容器中使用的操作系统不必与主机操作系统相匹配。比如，你可以在 CentOS 主机中运行 Ubuntu，反之亦然。好了，下载完毕我们通过如下命令运行一个容器：123docker run -it --rm \\ ubuntu:16.04 \\ bash 运行完成后你便可以使用诸多命令来测试环境了，比如再来一遍 Hello Wolrd：echo &quot;Hello World&quot; 或者查看系统信息： 1234567891011121314cat /etc/os-release# 输出信息AME=&quot;Ubuntu&quot;VERSION=&quot;16.04.4 LTS (Xenial Xerus)&quot;ID=ubuntuID_LIKE=debianPRETTY_NAME=&quot;Ubuntu 16.04.4 LTS&quot;VERSION_ID=&quot;16.04&quot;HOME_URL=&quot;http://www.ubuntu.com/&quot;SUPPORT_URL=&quot;http://help.ubuntu.com/&quot;BUG_REPORT_URL=&quot;http://bugs.launchpad.net/ubuntu/&quot;VERSION_CODENAME=xenialUBUNTU_CODENAME=xenial docker run 就是运行容器的命令，这里简要的说明一下上面用到的参数，详细用法可以查看 help。 -it：这是两个参数，一个是 -i 表示交互式操作，一个是 -t 为终端。我们这里打算进入 bash 执行一些命令并查看返回结果，因此我们需要交互式终端。 --rm：这个参数是说容器退出后随之将其删除。默认情况下，为了排障需求，退出的容器并不会立即删除，除非手动 docker rm。 ubuntu:16.04：这是指用 ubuntu:16.04 镜像为基础来启动容器。 bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用 bash。 Docker 镜像还有一些常用操作，比如： docker image ls - 列出本地已下载镜像 docker image rm [选项] &lt;镜像1&gt; [&lt;镜像2&gt; ...] - 删除镜像 docker logs &lt;id/container_name&gt; - 查看容器日志 docker ps - 列出当前所有正在运行的容器 docker ps -l - 列出最近一次启动的容器 docker search image_name - 从 Docker Hub 检索镜像 docker history image_name - 显示镜像历史 docker push new_image_name - 发布镜像 4.2 通过 commit 命令理解镜像构成 docker commit 命令除了学习之外，还有一些特殊的应用场合，比如被入侵后保存现场等。但是，不要使用 docker commit定制镜像，定制镜像应该使用 Dockerfile 来完成。 以下通过定制化一个 web 服务器，来说明镜像的构成。 docker run --name webserver -d -p 4000:80 nginx 首先，以上命令会用 nginx 镜像启动一个容器，命名为 webserver，并映射在 4000 端口，接下来我们用浏览器去访问它，效果如下所示。 现在，假设我们非常不喜欢这个欢迎页面，我们希望改成欢迎 Docker 的文字，我们可以使用 docker exec 命令进入容器，修改其内容。 1234docker exec -it webserver bashroot@7603bd94b5e:/# echo &apos;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&apos; &gt; /usr/share/nginx/html/index.htmlroot@27603bd94b5e:/# exitexit 这时刷新浏览器，我们发现效果变了。我们修改了容器的文件，也就是改动了容器的存储层。我们可以通过 docker diff 命令看到具体的改动。 1234567891011121314docker diff webserver# 输出C /rootA /root/.bash_historyC /runA /run/nginx.pidC /usr/share/nginx/html/index.htmlC /var/cache/nginxA /var/cache/nginx/client_tempA /var/cache/nginx/fastcgi_tempA /var/cache/nginx/proxy_tempA /var/cache/nginx/scgi_tempA /var/cache/nginx/uwsgi_temp 当我们运行一个容器的时候（如果不使用卷的话），我们做的任何文件修改都会被记录于容器存储层里。而 Docker 提供了一个 docker commit 命令，可以将容器的存储层保存下来成为镜像。换句话说，就是在原有镜像的基础上，再叠加上容器的存储层，并构成新的镜像。以后我们运行这个新镜像的时候，就会拥有原有容器最后的文件变化。 假设以上操作已经符合我们的定制了，接下来就将它保存下来。commit 语法格式如下： docker commit [选项] &lt;容器ID或容器名&gt; [&lt;仓库名&gt;[:&lt;标签&gt;]]我们在命令行输入（其中 author、message 等信息可以省略）： 123456docker commit \\--author &quot;fallenk liu_lf@zju.edu.cn&quot; \\--message &quot;modify:Nginx default page to Hello Docker&quot; \\webserver \\nginx:v2sha256:6acf14e575436f39e7f9ab4cf1f25cb5654a1da2ad5dc8e87bd84e50ffbd3f94 接下来我们可以在本地镜像列表中看到这个新镜像： 1234567891011121314docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEnginx v2 6acf14e57543 57 seconds ago 109MBdistributed-ai-common latest 014f5e339e9f 6 hours ago 433MB&lt;none&gt; &lt;none&gt; 0d5f904acc10 6 hours ago 433MB&lt;none&gt; &lt;none&gt; 5312319017e9 7 hours ago 361MBipfs/ipfs-cluster latest 5b0dbf973d5c 2 weeks ago 49.9MBdadarek/wait-for-dependencies latest a525977c5119 2 weeks ago 4.03MBnginx latest 62f816a209e6 2 weeks ago 109MBgolang alpine 57915f96905a 3 weeks ago 310MBipfs/go-ipfs release 0cc220aa3738 3 weeks ago 43.2MBubuntu 16.04 4a689991aa24 5 weeks ago 116MBhello-world latest 4ab4c602aa5e 2 months ago 1.84kB liulifeng@Mac  ~  也可以查看这个镜像最新版的历史记录： 12345docker history nginx:v2IMAGE CREATED CREATED BY SIZE COMMENT6acf14e57543 2 minutes ago nginx -g daemon off; 164B modify:Nginx default page to Hello Docker62f816a209e6 2 weeks ago /bin/sh -c #(nop) CMD [&quot;nginx&quot; &quot;-g&quot; &quot;daemon… 0B&lt;missing&gt; 2 weeks ago /bin/sh -c #(nop) STOPSIGNAL [SIGTERM] 0B 当然也可以查看当前主机的所有容器，并终止我们启动的 webserver： 1234567891011121314# 查看所有容器docker container ls -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb5d1c94dc4f1 hello-world &quot;/hello&quot; 10 seconds ago Exited (0) 11 seconds ago musing_wilsonfb5adbea6106 nginx &quot;nginx -g &apos;daemon of…&quot; 15 minutes ago Up 15 minutes 0.0.0.0:4000-&gt;80/tcp webserverbf9ee212201b ubuntu:16.04 &quot;bash&quot; 17 minutes ago Exited (0) 17 minutes ago# 终止 webserver$ docker container stop webserverwebserver# 再次查看所有容器，确保 webserver 终止$ docker container ls -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES27603bd94b5e nginx &quot;nginx -g &apos;daemon of…&quot; 21 minutes ago Exited (0) 27 seconds ago webserverfdb4b0c9a5e1 hello-world &quot;/hello&quot; 2 hours ago Exited (0) 2 hours ago angry_bhabha 接下来我们运行这个新镜像，记得浏览器访问应该打开 http://localhost:4001： 1234docker run --name web2 -d -p 4001:80 nginx:v2# 输出190e998ff582aac8a3dd4188040c499270e2891ae05b74f54c456d450fc06950 但是实际环境中并不推荐如上所示的 commit 用法，具体可以查看《Docker 从入门到实践》中的描述，总结下来为三点： 由于 commit 命令的执行，有很多文件被改动或添加了。这还仅仅是最简单的操作，如果是安装软件包、编译构建，那会有大量的无关内容被添加进来，如果不小心清理，将会导致镜像极为臃肿。 使用 commit 生成的镜像也被称为黑箱镜像，换句话说，就是除了制作镜像的人知道执行过什么命令、怎么生成的镜像，别人根本无从得知。 不符合分层存储的概念，即除当前层外，之前的每一层都是不会发生改变的。 本节最后再说一点，细心的读者肯定观察到我们启动 webserver 的时候 docker run 命令用到了 -d 参数，那么不用这个命令会有什么效果呢？我们来试一下： 1234567docker run ubuntu:16.04 /bin/sh -c &quot;while true; do echo hello world; sleep 1; done&quot;# 输出hello worldhello worldhello worldhello world 容器会把输出的结果 (STDOUT) 打印到宿主机上面。是的，这个参数的作用就是让 Docker 容器在后台以守护态（Daemonized）形式运行，即后台运行。 -d Run container in background and print container ID 如果我们在如上命令中加入 -d 命令，那么这些 hello world 信息就需要通过如下命令来查看了，其中 ID 是容器的名称：docker logs ID 4.3 利用 Dockerfile 定制镜像利用 Dockerfile，之前提及的无法重复的问题、镜像构建透明性的问题、体积的问题就都会解决。Dockerfile 是一个文本文件，其内包含了一条条的指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。在本机上登陆你的 Docker 账户，以便之后发布使用，如果没有请到 https://cloud.docker.com/ 注册一个。本机登陆命令： docker login所谓定制镜像，那一定是以一个镜像为基础，在其上进行定制。本部分以一个简单的 web 服务器为例，记录定制镜像的操作步骤。首先，新建一个文件夹 newdocker 并进入： mkdir newdocker &amp;&amp; cd newdocker然后在当前目录新建一个 Dockerfile 文件，内容如下所示： 12345678910111213141516171819202122# Dockerfile# 使用 Python 运行时作为基础镜像FROM python:2.7-slim# 设置 /app 为工作路径WORKDIR /app# 将当前目录所有内容复制到容器的 /app 目录下ADD . /app# 安装 requirements.txt 中指定的包RUN pip install --trusted-host pypi.python.org -r requirements.txt# 对容器外开放80端口EXPOSE 80# 定义环境变量ENV NAME World# 当容器启动时运行 app.py CMD [&quot;python&quot;, &quot;app.py&quot;] 以上内容中，我们用 FROM 指定基础镜像，在 Docker Store 上有非常多的高质量的官方镜像，服务类镜像如 nginx、redis、mongo、mysql、httpd、php、tomcat 等；各种语言应用镜像如 node、openjdk、python、ruby、golang 等；操作系统镜像如 ubuntu、debian、centos、fedora、alpine。除了选择现有镜像为基础镜像外，Docker 还存在一个特殊的镜像，名为 scratch。直接 FROM scratch 会让镜像体积更加小巧。 RUN 指令是用来执行命令行命令的。由于我们的例子中只有一行 RUN 代码，我们来看看多行 RUN 的情况，如下代码所示： 1234567RUN apt-get updateRUN apt-get install -y gcc libc6-dev makeRUN wget -O redis.tar.gz &quot;http://download.redis.io/releases/redis-3.2.5.tar.gz&quot;RUN mkdir -p /usr/src/redisRUN tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1RUN make -C /usr/src/redisRUN make -C /usr/src/redis install Dockerfile 中每一个指令都会建立一层，RUN 也不例外。每一个 RUN 的行为，就和刚才我们手工建立镜像的过程一样：新建立一层，在其上执行这些命令，执行结束后，commit 这一层的修改，构成新的镜像。 而上面的这种写法，创建了 7 层镜像。这是完全没有意义的，而且很多运行时不需要的东西，都被装进了镜像里，比如编译环境、更新的软件包等等。结果就是产生非常臃肿、非常多层的镜像，不仅仅增加了构建部署的时间，也很容易出错。 这是很多初学 Docker 的人常犯的一个错误 正确的写法应该这样： 123456789101112RUN buildDeps=&apos;gcc libc6-dev make&apos; \\ &amp;&amp; apt-get update \\ &amp;&amp; apt-get install -y $buildDeps \\ &amp;&amp; wget -O redis.tar.gz &quot;http://download.redis.io/releases/redis-3.2.5.tar.gz&quot; \\ &amp;&amp; mkdir -p /usr/src/redis \\ &amp;&amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \\ &amp;&amp; make -C /usr/src/redis \\ &amp;&amp; make -C /usr/src/redis install \\ &amp;&amp; rm -rf /var/lib/apt/lists/* \\ &amp;&amp; rm redis.tar.gz \\ &amp;&amp; rm -r /usr/src/redis \\ &amp;&amp; apt-get purge -y --auto-remove $buildDeps 值得注意的是最后添加了清理工作的命令，删除了为了编译构建所需要的软件，清理了所有下载、展开的文件，并且还清理了 apt 缓存文件。 好了，以上内容走的有些远，我们回到 Dockerfile。由于以上代码用到了 app.py 和 requirements.txt 两个文件，接下来我们来进一步完善，requirements.txt 内容如下： 12FlaskRedis app.py 内容如下： 123456789101112131415161718192021222324from flask import Flaskfrom redis import Redis, RedisErrorimport osimport socket# Connect to Redisredis = Redis(host=\"redis\", db=0, socket_connect_timeout=2, socket_timeout=2)app = Flask(__name__)@app.route(\"/\")def hello(): try: visits = redis.incr(\"counter\") except RedisError: visits = \"&lt;i&gt;cannot connect to Redis, counter disabled&lt;/i&gt;\" html = \"&lt;h3&gt;Hello {name}!&lt;/h3&gt;\" \\ \"&lt;b&gt;Hostname:&lt;/b&gt; {hostname}&lt;br/&gt;\" \\ \"&lt;b&gt;Visits:&lt;/b&gt; {visits}\" return html.format(name=os.getenv(\"NAME\", \"world\"), hostname=socket.gethostname(), visits=visits)if __name__ == \"__main__\": app.run(host='0.0.0.0', port=80) 我们不探讨以上代码具体的实现，其实简单来看我们可以知道这是用 python 写的一个简单的 web 服务器。其中需要注意的地方是由于没有运行 Redis （我们只安装了 Python 库，而不是 Redis 本身），所以在运行镜像时应该期望在这部分产生错误消息（见下文图）。到这里，当前目录应该是这样的： 123ls# 输出Dockerfile app.py requirements.txt 接着，我们在当前目录开始构建这个镜像吧。 12# -t 可以给这个镜像取一个名字（tag）docker build -t friendlyhello . 以上代码用到了 docker build 命令，其格式如下： docker build [选项] &lt;上下文路径/URL/-&gt;值得注意的是上下文路径这个概念。在本例中我们用 . 定义，那么什么是上下文环境呢？这里我们需要了解整 build 的工作原理。 Docker 在运行时分为 Docker 引擎（也就是服务端守护进程）和客户端工具。Docker 的引擎提供了一组 REST API，被称为 Docker Remote API，而如 docker 命令这样的客户端工具，则是通过这组 API 与 Docker 引擎交互，从而完成各种功能。因此，虽然表面上我们好像是在本机执行各种 docker 功能，但实际上，一切都是使用的远程调用形式在服务端（Docker 引擎）完成。 docker build 得知这个路径后，会将路径下的所有内容打包，然后上传给 Docker 引擎。当我们代码中使用到了诸如 COPY 等指令时，服务端会根据该指令复制调用相应的文件。但由于整个文件中 构建完毕，新镜像放在哪里呢？ 12345678910docker image ls# 输出REPOSITORY TAG IMAGE ID CREATED SIZEfriendlyhello latest 73ff8ff81235 5 minutes ago 151MBnginx v2 48bcb7b903a4 About an hour ago 109MBubuntu 16.04 c9d990395902 4 days ago 113MBhello-world latest e38bc07ac18e 5 days ago 1.85kBnginx latest b175e7467d66 6 days ago 109MBpython 2.7-slim b16fde09c92c 3 weeks ago 139MB 接下来，我们运行它（若是要后台运行则记得加上 -d 参数）： docker run -p 4000:80 friendlyhello 你可以通过浏览器查看效果，也可以通过 curl 命令获得相同的内容： 1234curl http://localhost:4000# 输出结果&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; 98750b60e766&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; &lt;i&gt;cannot connect to Redis, counter disabled&lt;/i&gt; 接下来我们发布这个镜像到 Docker Hub。首先，你可以给这个镜像打上一个 tag（可选项）： 12345# tag 格式 docker tag image username/repository:tag# 本例中的运行命令docker tag friendlyhello fallenk/friendlyhello:v1 接着将它发布： 12345# push 格式docker push username/repository:tag# 本例中的运行命令docker push hijiangtao/friendlyhello:v1 至此，大功告成。现在你可以在任意一台配有 docker 环境的机器上运行该镜像了。docker run -p 4000:80 fallenk/friendlyhello:v1如果我们经常更新镜像，可以尝试使用自动创建（Automated Builds）功能来简化我们的流程，这一块可以结合 GitHub 一起操作，对于需要经常升级镜像内程序来说，这会十分方便。本文中所述的镜像就托管在 GitHub。使用自动构建前，首先要要将 GitHub 或 Bitbucket 帐号连接到 Docker Hub。如果你使用了这种方式，那么每次你 git commit 之后都应该可以在你的 docker cloud 上看到类似下方的镜像构建状态。 五、总结整体来说，Docker 是一个越看越有意思的概念，值得我们深入了解。但是其中也存在很多技巧，比如就 Docker 技术三剑客来说，这些是官方插件，简化了 Docker在多服务和多机器环境下的功能配置，但若没有弄懂 Docker 之前就去了解反而会让自己越看越绕。另一方面则是随着时间的推移，很多曾经 Docker 推荐的方式正在被淘汰，比如在非 Linux 环境下的 Boot2Docker，这也需要我们学习者在了解的过程中对做好区分，否则也会事倍功半。 本文着重总结了 Docker 的概念与简单的上手实践，对于 Docker 的广泛运用场景没有做过多介绍，但其实我们可以利用 Docker 做很多有意思的事情，作为一个可以用于流水线管理、能够应用以离、具有快速高效部署等诸多特点的技术，往好的方面想可以允许我们快速实现应用在线上部署的表现一致化，远离“这段代码在我电脑上运行没问题啊”的烦恼并减轻硬件服务器负担，往不好的方面想可以通过服务器超售大赚一笔… 开发流程开发人员：开发docker镜像，使用Dockerfile定制镜像，再使用docker-compose在.yml文件配置容器的部署方法、文件映射、容器连接等一系列的配置写在一个配置文件中。定制完成后发布。其他人获取到镜像后，运行。 六、参考Docker)Docker 从入门到实践Docker容器与虚拟机区别Docker三剑客实践之部署集群Get Started with Docker","link":"/2018/11/27/Docker快速入门/"},{"title":"OpenGL快速入门","text":"学习\b方向目的(学习什么) - 结构(新知识的整体框架，运行流程) - 使用(如何使用，入门)\b网址 学会：核心是现代OpenGL；理解图形编程和OpenGL背后的原理。首先讨论核心的图形学概念，OpenGL怎样将像素绘制到屏幕上，以及如何利用黑科技做出一些很酷的效果。 第一节前置知识OpenGL是一个图形API，并不是一个独立的平台，它需要一个编程语言来工作，在这里我们使用的是C++, 学习C++ ，网址; 学习线性代数、几何、三角学。 结构 入门 2.光照 3.模型加载 3.高级OpenGL 4. 高级光照 5.特效 6.PBR 7.实战 入门OpenGL定义： OpenGL本身并不是一个API，它仅仅是一个由Khronos组织制定并维护的规范(Specification)。 OpenGL规范严格规定了每个函数该如何执行，以及它们的输出值。至于内部具体每个函数是如何实现(Implement)的，将由OpenGL库的开发者自行决定（译注：这里开发者是指编写OpenGL库的人）。因为OpenGL规范并没有规定实现的细节，具体的OpenGL库允许使用不同的实现，只要其功能和结果与规范相匹配（亦即，作为用户不会感受到功能上的差异）。 实际的OpenGL库的开发者通常是显卡的生产商。 核心模式与立即渲染模式早期的OpenGL使用立即渲染模式（Immediate mode，也就是固定渲染管线）核心模式：使用现代的函数 \b扩展OpenGL的一大特性就是对扩展(Extension)的支持，当一个显卡公司提出一个新特性或者渲染上的大优化，通常会以扩展的方式在驱动中实现。如果一个程序在支持这个扩展的显卡上运行，开发者可以使用这个扩展提供的一些更先进更有效的图形功能。 12345678if(GL_ARB_extension_name){ // 使用硬件支持的全新的现代特性}else{ // 不支持此扩展: 用旧的方式去做} 状态机OpenGL自身是一个巨大的状态机(State Machine)：一系列的变量描述OpenGL此刻应当如何运行。OpenGL的状态通常被称为OpenGL上下文(Context)。我们通常使用如下途径去更改OpenGL状态：设置选项，操作缓冲。最后，我们使用当前OpenGL上下文来渲染。 假设当我们想告诉OpenGL去画线段而不是三角形的时候，我们通过改变一些上下文变量来改变OpenGL状态，从而告诉OpenGL如何去绘图。一旦我们改变了OpenGL的状态为绘制线段，下一个绘制命令就会画出线段而不是三角形。 当使用OpenGL的时候，我们会遇到一些状态设置函数(State-changing Function)，这类函数将会改变上下文。以及状态使用函数(State-using Function)，这类函数会根据当前OpenGL的状态执行一些操作。只要你记住OpenGL本质上是个大状态机，就能更容易理解它的大部分特性。 对象OpenGL库是用C语言写的，同时也支持多种语言的派生，但其内核仍是一个C库。由于C的一些语言结构不易被翻译到其它的高级语言，因此OpenGL开发的时候*引入了一些抽象层。“对象(Object)”就是其中一个。 在OpenGL中一个对象是指一些选项的集合，它代表OpenGL状态的一个子集。比如，我们可以用一个对象来代表绘图窗口的设置，之后我们就可以设置它的大小、支持的颜色位数等等。可以把对象看做一个C风格的结构体(Struct)： 12345struct object_name { float option1; int option2; char[] name;}; 当我们使用一个对象时，通常看起来像如下一样（把OpenGL上下文看作一个大的结构体）： 123456// OpenGL的状态struct OpenGL_Context { ... object* object_Window_Target; ... }; 12345678910// 创建对象unsigned int objectId = 0;glGenObject(1, &amp;objectId);// 绑定对象至上下文glBindObject(GL_WINDOW_TARGET, objectId);// 设置当前绑定到 GL_WINDOW_TARGET 的对象的一些选项glSetObjectOption(GL_WINDOW_TARGET, GL_OPTION_WINDOW_WIDTH, 800);glSetObjectOption(GL_WINDOW_TARGET, GL_OPTION_WINDOW_HEIGHT, 600);// 将上下文对象设回默认glBindObject(GL_WINDOW_TARGET, 0); 这一小段代码展现了你以后使用OpenGL时常见的工作流。我们 首先创建一个对象，然后用一个id保存它的引用（实际数据被储存在后台）。 然后我们将对象绑定至上下文的目标位置（例子中窗口对象目标的位置被定义成GL_WINDOW_TARGET）。 接下来我们设置窗口的选项。 最后我们将目标位置的对象id设回0，解绑这个对象。 设置的选项将被保存在objectId所引用的对象中，一旦我们重新绑定这个对象到GL_WINDOW_TARGET位置，这些选项就会重新生效。 使用对象的一个好处是在程序中，我们不止可以定义一个对象，并设置它们的选项，每个对象都可以是不同的设置。在我们执行一个使用OpenGL状态的操作的时候，只需要绑定含有需要的设置的对象即可。 比如说我们有一些作为3D模型数据（一栋房子或一个人物）的容器对象，在我们想绘制其中任何一个模型的时候，只需绑定一个包含对应模型数据的对象就可以了（当然，我们需要先创建并设置对象的选项）。拥有数个这样的对象允许我们指定多个模型，在想画其中任何一个的时候，直接将对应的对象绑定上去，便不需要再重复设置选项了。 创建窗口在我们画出出色的效果之前，首先要做的就是创建一个OpenGL上下文(Context)和一个用于显示的窗口。然而，这些操作在每个系统上都是不一样的，OpenGL有目的地从这些操作抽象(Abstract)出去。这意味着我们不得不自己处理创建窗口，定义OpenGL上下文以及处理用户输入。 有一些库：节省了我们书写操作系统相关代码的时间，提供给我们一个窗口和上下文用来渲染;有GLUT，SDL，SFML和GLFW GLFWGLFW是一个专门针对OpenGL的C语言库，它提供了一些渲染物体所需的最低限度的接口。它允许用户创建OpenGL上下文，定义窗口参数以及处理用户输入。本节和下一节的目标是建立GLFW环境，并保证它恰当地创建OpenGL上下文并显示窗口。这篇教程会一步步从获取、编译、链接GLFW库讲起。 构建GLFWGLFW可以从它官方网站的下载页上获取。GLFW已经有针对Visual Studio 2013/2015的预编译的二进制版本和相应的头文件，为了完整性我们将从编译源代码开始。所以我们需要下载源代码包。 下载源码包之后，将其解压并打开。我们只需要里面的这些内容： 编译生成的库 include文件夹 从源代码编译库可以保证生成的库是兼容你的操作系统和CPU的，而预编译的二进制文件可能会出现兼容问题（甚至有时候没提供支持你系统的文件）。提供源代码所产生的一个问题在于不是每个人都用相同的IDE开发程序，因而提供的工程/解决方案文件可能和一些人的IDE不兼容。所以人们只能从.c/.cpp和.h/.hpp文件来自己建立工程/解决方案，这是一项枯燥的工作。但因此也诞生了一个叫做CMake的工具。 CMakeCMake是一个工程文件生成工具。用户可以使用预定义好的CMake脚本，根据自己的选择（像是Visual Studio, Code::Blocks, Eclipse）生成不同IDE的工程文件。这允许我们从GLFW源码里创建一个Visual Studio 2015工程文件，之后进行编译。首先，我们需要从这里下载安装CMake。当CMake安装成功后，你可以选择从命令行或者GUI启动CMake，由于我不想让事情变得太过复杂，我们选择用GUI。 CMake需要一个源代码目录和一个存放编译结果的目标文件目录。源代码目录我们选择GLFW的源代码的根目录，然后我们新建一个 build 文件夹，选中作为目标目录。 在设置完源代码目录和目标目录之后，点击Configure(设置)按钮，让CMake读取设置和源代码。我们接下来需要选择工程的生成器，由于我们使用的是Visual Studio 2015，我们选择 Visual Studio 14 选项（因为Visual Studio 2015的内部版本号是14）。CMake会显示可选的编译选项用来配置最终生成的库。这里我们使用默认设置，并再次点击Configure(设置)按钮保存设置。保存之后，点击Generate(生成)按钮，生成的工程文件会在你的build文件夹中。 \b编译在build文件夹里可以找到GLFW.sln文件，用Visual Studio 2015打开。因为CMake已经配置好了项目，所以我们直接点击Build Solution(生成解决方案)按钮，然后编译的库glfw3.lib（注意我们用的是第3版）就会出现在src/Debug文件夹内。 库生成完毕之后，我们需要让IDE知道库和头文件的位置。有两种方法： 找到IDE或者编译器的/lib和/include文件夹，添加GLFW的include文件夹里的文件到IDE的/include文件夹里去。用类似的方法，将glfw3.lib添加到/lib文件夹里去。虽然这样能工作，但这不是推荐的方式，因为这样会让你很难去管理库和include文件，而且重新安装IDE或编译器可能会导致这些文件丢失。 推荐的方式是建立一个新的目录包含所有的第三方库文件和头文件，并且在你的IDE或编译器中指定这些文件夹。我个人会使用一个单独的文件夹，里面包含Libs和Include文件夹，在这里存放OpenGL工程用到的所有第三方库和头文件。这样我的所有第三方库都在同一个位置（并且可以共享至多台电脑）。然而这要求你每次新建一个工程时都需要告诉IDE/编译器在哪能找到这些目录。 完成上面步骤后，我们就可以使用GLFW创建我们的第一个OpenGL工程了！ mac 配置环境：https://www.jianshu.com/p/25d5fbf792a2 GLAD因为OpenGL只是一个标准/规范，具体的实现是由驱动开发商针对特定显卡实现的。由于OpenGL驱动版本众多，它大多数函数的位置都无法在编译时确定下来，需要在运行时查询。所以任务就落在了开发者身上，开发者需要在运行时获取函数地址并将其保存在一个函数指针中供以后使用。取得地址的方法因平台而异。 1234567// 定义函数原型typedef void (*GL_GENBUFFERS) (GLsizei, GLuint*);// 找到正确的函数并赋值给函数指针GL_GENBUFFERS glGenBuffers = (GL_GENBUFFERS)wglGetProcAddress(&quot;glGenBuffers&quot;);// 现在函数可以被正常调用了GLuint buffer;glGenBuffers(1, &amp;buffer); 你可以看到代码非常复杂，而且很繁琐，我们需要对每个可能使用的函数都要重复这个过程。幸运的是，有些库能简化此过程，其中GLAD是目前最新，也是最流行的库。 配置GLADGLAD是一个开源的库，它能解决我们上面提到的那个繁琐的问题。GLAD的配置与大多数的开源库有些许的不同，GLAD使用了一个在线服务。在这里我们能够告诉GLAD需要定义的OpenGL版本，并且根据这个版本加载所有相关的OpenGL函数。 使用：打开GLAD的在线服务，将语言(Language)设置为C/C++，在API选项中，选择3.3以上的OpenGL(gl)版本（我们的教程中将使用3.3版本，但更新的版本也能正常工作）。之后将模式(Profile)设置为Core，并且保证生成加载器(Generate a loader)的选项是选中的。现在可以先（暂时）忽略拓展(Extensions)中的内容。都选择完之后，点击生成(Generate)按钮来生成库文件。 GLAD现在应该提供给你了一个zip压缩文件，包含两个头文件目录，和一个glad.c文件。将两个头文件目录（glad和KHR）复制到你的Include文件夹中（或者增加一个额外的项目指向这些目录），并添加glad.c文件到你的工程中。","link":"/2018/12/18/OpenGL快速入门/"},{"title":"Non-IID数据集调研","text":"数据集分析 Paper1: First Analysis of Local GD on Heterogeneous Data异构数据的局部GD分析 Abstract我们提供了局部梯度下降的第一个收敛性分析，以最小化 光滑和凸函数以及其他任意函数的平均值。这种形式的问题和局部梯度下降作为一种解决方法在联邦学习中具有重要意义，其中每个函数都是基于用户在移动设备上存储的私有数据，不同用户的数据可以任意异构。我们表明，在低精度的情况下，该方法具有与梯度下降相同的通信复杂性。 Introduction我们对解决最优化问题感兴趣 $\\min {x \\in \\mathbb{R}^{d}}\\left{f(x) \\stackrel{\\text { def }}{=} \\frac{1}{M} \\sum{m=1}^{M} f_{m}(x)\\right}$ 这是在有监督的机器学习模型的训练中产生的。我们假设$f_{m}: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ 是L-光滑凸函数，我们将$x_{*}$ 作为$f$固定的最小值。 我们的主要兴趣是在这样的情况下，每个功能都基于单个设备上可用的数据，并且跨设备的数据分布可以任意异构。这种情况出现在联合学习中，在这种情况下，机器学习模型是根据消费者设备（如移动电话）上可用的数据进行训练的。在联合学习中，由于隐私的原因，禁止将本地数据传输到单个数据中心进行集中培训，并且频繁的通信是不可取的，因为它昂贵且具有侵入性。因此，最近的几项工作旨在以尽可能少的通信回合的分布式方式构建新的解决方法（1）。 大规模问题通常用一阶方法来解决，因为它们在维度和数据大小上都有很好的伸缩性。一个有吸引力的选择是局部梯度下降，它将优化过程分为几个阶段。每一个阶段的开始都是以一个模型的形式在所有M个设备上平均一步的通信。每个epoch的其余部分不涉及任何通信，致力于执行从平均模型开始的固定数量的梯度下降步骤，并基于所有M设备独立并行执行的本地功能。详见算法一。 这种方法的随机版本是联邦平均算法的核心，联邦平均算法最近被用于联邦学习应用中，见[7，10]。本质上，联邦平均是局部随机梯度下降（SGD）的一个变种，参与设备随机采样。该算法已经在一些机器学习应用中得到应用，如移动键盘预测[5]，并在[7]中探讨了提高其通信效率的策略。尽管这种方法在经验上取得了成功，但人们对它的收敛特性知之甚少，而且观察到当执行过多的局部步骤时，它会发生发散[10]。这并不是很确定，因为大多数常见的假设都没有得到满足；特别是，数据通常非常非i.i.d.[10]，因此局部梯度可以指向不同的方向。数据的这个属性可以写入任何向量x和索引i，j as $\\left|\\nabla f_{i}(x)-\\nabla f_{j}(x)\\right| \\gg 1$ 不幸的是，如果不假设rf i-pxq和rf j-pxq的相异性有界，就很难分析局部方法。由于这个原因，几乎所有先前的工作都假设有界的相异性〔8, 16, 17，18〕，并解决了联合学习的其它较不具挑战性的方面，例如分散的通信、目标的非凸性或不平衡的数据划分。实际上，简化分析的一个常用方法是假设局部函数的Lipschitzness性，$\\left|\\nabla f_{i}(x)\\right| \\leqslant G$ 对于任意的x和i。我们认为这种假设是病态的，在寻求有意义的收敛界时应该避免。首先，在无约束强凸极小化中，这一假设无法得到满足，使得[14]等著作中的分析值得怀疑。其次，存在至少一种方法，其收敛性在有界梯度下得到保证[6 ]，但在实践中该方法发散[3, 12 ]。最后，在有界梯度假设下 $\\left|\\nabla f_{i}(x)-\\nabla f_{j}(x)\\right| \\leqslant\\left|\\nabla f_{i}(x)\\right|+\\left|\\nabla f_{j}(x)\\right| \\leqslant 2 G$ 换言之，我们无法控制函数之间的差异。由于G边界不仅是不同的，而且是梯度本身，这使得这些陈述不那么有见地，甚至是空洞的。例如，如果数据实际上是i.i.d，它就不会很紧，因为在这种情况下G将保持一个正常数。相比之下，我们将表明rate应该取决于一个更有意义的数量， $\\sigma_{f}^{2} \\stackrel{\\text { def }}{=} \\frac{1}{M} \\sum_{m=1}^{M}\\left|\\nabla f_{m}\\left(x_{*}\\right)\\right|^{2}$, where x ˚ is a minimizer of f. 显然，σf总是有限的，它是局部方法方差的自然度量。除此之外，它还允许我们在数据实际上是i.i.d.的情况下获得紧边界。我们注意到，在[13]中已经尝试获得更一般的收敛性声明，但遗憾的是，它们的保证比小批量随机梯度下降（SGD）的保证差得多，使得它们的贡献较小。 我们还注意到，上述工作中的边界[8]不仅使用有界梯度，而且还提供了一个悲观的 $\\mathcal{O}\\left(H^{2} / T\\right)$ rate。其中H是每个epoch中的局部步数，T是该方法的总步数。实际上，这要求H是$\\mathcal{O}(1)$，以使速率与强凸函数的SGD的速率一致。因此，这项工作的主要贡献是将部分参与视为联合平均。 当数据是相同分布的，并且在每个节点上使用随机梯度而不是全梯度时，所得到的方法在文献中以不同的名称进行了广泛的探索，见例如[1，14，15，19]。[11] 提出了一种异步局部算法，该算法在不减小步长的情况下收敛到精确解，但其增加H的好处受到常数因子的限制。[9] 似乎是第一个提出本地方法的工作，但没有显示出该工作的比率。 Convergence of Local GD局部GD的收敛性 假设和符号 Assumptions and notation在介绍我们的主要结果之前，让我们首先明确地阐述我们的假设。 Assumption 1. (1)的极小值集是非空的。更多的是，对于每一个 $m \\in[M] \\stackrel{\\text { def }}{=}{1,2, \\ldots, M}$ , $f_{m}$ 是是凸的和L-光滑的。即，对 $x, y \\in \\mathbb{R}^{d}$ , 满足以下不等式: $0 \\leqslant f_{m}(x)-f_{m}(y)-\\left\\langle\\nabla f_{m}(y), x-y\\right\\rangle \\leqslant \\frac{L}{2}|x-y|^{2}$ 此外，我们假设算法1在有界同步间隔下运行。也就是说，我们假设: $H \\stackrel{\\text { def }}{=} \\max {p \\geqslant 0}\\left|t{p}-t_{p+1}\\right|$ 是受限的。给出向量 $x_{t}^{1}, x_{t}^{2}, \\ldots, x_{t}^{M} \\in \\mathbb{R}^{d}$, 我们定义了平均迭代,t时刻的迭代方差和平均梯度: $\\hat{x}{t} \\stackrel{\\text { def }}{=} \\frac{1}{M} \\sum{m=1}^{M} x_{t}^{m}$, $V_{t} \\stackrel{\\text { def }}{=} \\frac{1}{M} \\sum_{m=1}^{M}\\left|x_{t}^{m}-\\hat{x}{t}\\right|^{2}$, $g{t} \\stackrel{\\text { def }}{=} \\frac{1}{M} \\sum_{m=1}^{M} \\nabla f_{m}\\left(x_{t}^{m}\\right)$ 分别是。关于f的Bregman散度是通过 $D_{f}(x, y) \\stackrel{\\text { def }}{=} f(x)-f(y)-\\langle\\nabla f(y), x-y\\rangle$ 注意，在这种情况下,$y=x_{}$, 我们有 $D_{f}\\left(x, x_{}\\right)=f(x)-f\\left(x_{*}\\right)$ 分析 Analysis第一引理Lemma使我们能够找到局部GD的一个单步最优差的递推： Lemma 1. 在 Assumption 1 和对于 $\\gamma \\geqslant 0$ 我们有： $\\left|r_{t+1}\\right|^{2} \\leqslant\\left|r_{t}\\right|^{2}+\\gamma L(1+2 \\gamma L) V_{t}-2 \\gamma(1-2 \\gamma L) D_{f}\\left(\\hat{x}{t}, x{*}\\right)$ 当$r_{t} \\stackrel{\\text { def }}{=} \\hat{x}{t}-x{}$, 特别是，$\\text { if } \\gamma \\leqslant \\frac{1}{4 L}, \\text { then }\\left|r_{t+1}\\right|^{2} \\leqslant\\left|r_{t}\\right|^{2}+\\frac{3}{2} \\gamma L V_{t}-\\gamma D_{f}\\left(\\hat{x}{t}, x{}\\right)$ 我们现在把方差variances之和$V_{t}$ 限制在一个epoch上。基于epoch的界限直观上是我们想要的，因为我们只对每个epoch结束时产生的$\\hat{x}_{t_{p}}$点感兴趣。 Lemma 2. 假设Assumption 1成立和 $p \\in \\mathbb{N}$, 定义 $v=t_{p+1}-1$ 和 假设 Algorithm 1以同步间隔$H \\geqslant 1$运行, 并且恒定的步长 $ \\gamma &gt; 0$ 如此 $\\gamma \\leqslant \\frac{1}{4 L H}$, 那么下面的不等式成立： $\\begin{aligned}&amp;\\sum_{t=t_{p}}^{v} V_{t} \\leqslant 5 L \\gamma^{2} H^{2} \\sum_{i=t_{p}}^{v} D_{f}\\left(\\hat{x}{i}, x{}\\right)+\\sum_{i=t_{p}}^{v} 8 \\gamma^{2} H^{2} \\sigma_{f}^{2}\\&amp;\\sum_{t=t_{p}}^{v} \\frac{3}{2} L V_{t}-D_{f}\\left(\\hat{x}{t}, x{}\\right) \\leqslant-\\frac{1}{2} \\sum_{t=t_{p}}^{v} D_{f}\\left(\\hat{x}{i}, x{*}\\right)+\\sum_{t=t_{p}}^{v} 12 L \\gamma^{2} H^{2} \\sigma_{f}^{2}\\end{aligned}$ 结合前两个引理，在下一个定理theorem中建立了局部GD的收敛性： Theorem 1: 对于 本地GD运行与恒定的步长 $ \\gamma &gt; 0$ 如此 $\\gamma \\leqslant \\frac{1}{4 L H}$，基于Assumption 1， 我们有： $f\\left(\\bar{x}{T}\\right)-f\\left(x{}\\right) \\leqslant \\frac{2\\left|x_{0}-x_{}\\right|^{2}}{\\gamma T}+24 \\gamma^{2} \\sigma_{f}^{2} H^{2} L$, 其中 $\\bar{x}{T} \\stackrel{\\text { def }}{=} \\frac{1}{T} \\sum{t=0}^{T-1} \\hat{x}_{t}$ Local GD vs GD为了解释上述界限，我们可能会问：有多少沟通回合是足够保证的 $f\\left(\\bar{x}{T}\\right)-f\\left(x{}\\right) \\leqslant \\epsilon ?$, 为了回答这个问题，我们需要最小化$\\frac{T}{H}$ , 受约束于 $0&lt;\\gamma \\leqslant \\frac{1}{4 L}, \\frac{2\\left|x_{0}-x_{}\\right|^{2}}{\\gamma T} \\leqslant \\frac{\\epsilon}{2}$, 和 $24 \\gamma^{2} \\sigma_{f}^{2} H^{2} L \\leqslant \\frac{\\epsilon}{2}$, 在 变量$T, H 和 \\gamma$。 我们可以很容易从约束条件中推断出: $\\frac{T}{H} \\geqslant \\frac{16\\left|x_{0}-x_{*}\\right|^{2}}{\\epsilon} \\max \\left{L, \\sigma_{f} \\sqrt{\\frac{3 L}{\\epsilon}}\\right}$ 另一方面，下界为 $0&lt;\\gamma \\leqslant \\frac{1}{4 L}$, 只要我们选择: $T=T(\\gamma) \\stackrel{\\text { def }}{=} \\frac{4\\left|x_{0}-x_{*}\\right|^{2}}{\\epsilon \\gamma} \\quad \\text { and } \\quad H=H(\\gamma) \\stackrel{\\text { def }}{=} \\frac{1}{4 \\max \\left{L, \\sigma_{f} \\sqrt{\\frac{3 L}{\\epsilon}}\\right} \\gamma}$ 最小的$H$ 达到下界为 $H\\left(\\frac{1}{4 L}\\right)=\\min {1, \\sqrt{\\frac{\\epsilon L}{3 \\sigma_{f}^{2}}}}$ 此外，请注意，只要目标精度不太高, 特别是 $\\epsilon \\geqslant \\frac{3 \\sigma^{2}}{L}$, 然后 $\\max \\left{L, \\sigma_{f} \\sqrt{3 L / \\epsilon}\\right}=L$, 然后[6]说表示本地GD的通信数量（参数设置为 $H=H(\\gamma)$ 和 $T=T(\\gamma)$ Local GD vs Minibatch SGD公式（5）清楚地表明了局部GD的收敛与小批量SGD的收敛速率之间的相似性，根据在最佳σf处的预期2噪声建立了一个1 {T到邻域的收敛。 彼此在最佳x处。 下一个推论表明，SGD和本地GD之间的类比进一步扩展到收敛速度。 Experiments为了验证该理论，我们对逻辑回归进行了实验，其中使用了2正则化和来自LIBSVM库的数据集[2]。我们使用一台具有3.20GHz内核的24个Intel®Xeon®Gold 6146 CPU的机器，并且通过MPI for Python软件包处理通信[4]。 由于我们的体系结构导致了计算和通信之间的非常特殊的折衷，因此我们还提供了通信时间相对于梯度计算时间为1或更低的情况的图表。在所有实验中，我们使用全梯度rf m和恒定步长L。选择2的正则化量1的顺序为n，其中n是数据总量。数据分区不是i.i.d.并根据原始数据集中的索引完成。 我们观察到理论与数值结果之间存在非常紧密的匹配。在通信比梯度计算贵得多的情况下，局部方法对于不精确的收敛要快得多。尽管对于我们的体系结构而言，这并不是一个很大的优势，主要是因为计算整个梯度需要花费大量时间。 Paper2: Measuring the Effects of Non-Identical Data Distribution for Federated Visual Classification联邦视觉分类中非同一数据分布效应的测量 Abstract联合学习使视觉模型能够以保护隐私的方式使用来自移动设备的真实数据进行训练。考虑到它们的分布性质，这些设备上的数据统计可能会有很大的不同。在这项工作中，我们将通过联合学习来研究这些不相同的数据分布对视觉分类的影响。我们提出了一种合成具有连续一致性范围的数据集的方法，并为联邦平均算法提供了性能指标。结果表明，随着分布的不同，性能下降，并提出了一种基于服务器动量的缓解策略。在CIFAR-10上的实验表明，在一系列不可识别的情况下，分类性能得到了改善，在最倾斜的设置下，分类精度从30.1%提高到76.9%。 Introduction联邦学习（FL）[McMahan等人，2017]是一个隐私保护框架，用于从位于边缘设备上的分散用户数据中训练模型。使用联邦平均算法（FedAvg），在每个联邦学习回合中，每个参与设备（也称为客户机）从中央服务器接收初始模型，在其本地数据集上执行随机梯度下降（SGD）并将梯度发回。然后，服务器从参与的客户端聚合所有渐变并更新启动模型。虽然在数据中心培训中，批次通常可以假设为IID（独立且相同分布），但这种假设在联合学习环境中不太可能成立。在这项工作中，我们特别研究了不同的数据分布对每个客户的影响，假设数据是独立于不同的本地分布绘制的。我们考虑了一系列连续的非一致分布，并给出了一系列超参数和优化策略的经验结果。 Related Work几位作者探索了在由图像分类数据集生成的非相同客户端数据分区上的FedAvg算法。McMahan等人。[2017]从MNIST数据集中合成病理性非同一用户分裂，按类标签对训练样本进行排序，并划分成碎片，使每个客户端分配2个碎片。他们证明了非相同客户机上的FedAvg仍然收敛到99%的准确率，尽管比相同客户机进行了更多轮。以类似的分类和划分方式，Zhao等人。[2018]和Sattler等人。[2019]在CIFAR-10数据集上生成极端分区，形成总共10个客户端的总体。这些设置有些不切实际，因为实际的联合学习通常会涉及到比简单分区更大的客户机池和更复杂的分布。 其他作者则关注客户机上更真实的数据分布。例如，Caldas等人。[2018]使用扩展MNIST[Cohen等人，2017]在数字写入器上进行分区，而不是简单地在数字类上进行分区。与我们的工作密切相关，Yurochkin等人。[2019]使用浓度参数为0.5的Dirichlet分布合成非同一数据集。我们扩展了这一概念，探索了浓度α的连续范围，并详细探索了最优超参数和优化设置。 理论方面的前期工作研究了FedAvg变量在不同条件下的收敛性。Sahu等人。〔2018〕引入客户目标的近似项，并证明收敛性保证。Li等人。[2019]分析强凸问题在适当抽样和平均方案下的FedAvg。 Synthetic Non-Identical Client Data合成的不同分布client数据 在我们的视觉分类任务中，我们假设在每个客户机上，训练示例都是独立绘制的，类标签遵循向量q$(q_{i}&gt;= 0, 0&lt;i&lt;N, ||q||_{1} = 1)$参数化的N个类上的分类分布。 为了合成一个不完全相同的客户群，我们从Dirichlet分布中得到q∼Dir（αp），其中p表示N个类上的先验类分布，α&gt;0是控制客户间一致性的浓度参数。我们用8个α值进行实验，以产生覆盖同一光谱的总体。对于α趋向于无穷大，所有客户机的分布都与先前的相同；对于α············，在另一个极端，每个客户机只持有随机选择的一个类的示例。 在这项工作中，我们使用CIFAR-10【Krizhevsky等人，2009年】图像分类数据集，其中包含来自10个类的60000个图像（50000个用于训练，10000个用于测试）。我们产生了由100个客户组成的平衡群体，每个客户持有500张图片。我们将先前的分布设置为在10个类中均匀分布，与我们报告性能的测试集相同。对于每个客户机，给定一个α，我们对q进行采样，并为客户机分配来自10个类的相应数量的图像。图1显示了不同浓度参数下从Dirichlet分布中提取的种群。 图1：具有不同客户的合成种群。用不同的颜色表示class之间的分布。（a） 从排序和分区方案生成的10个客户端，每个客户端分配2个类。（b-e）分别由不同浓度参数α的Dirichlet分布产生的群体，每个群体30个随机客户。 Experiments and Results鉴于以上数据集的准备工作，我们现在开始在从相同到不相同的分布范围内对vanilla FedAvg算法的性能进行基准测试。 我们使用与McMahan等人相同的CNN架构和符号。[2017]除了使用0.004的权重衰减，并且不应用学习率衰减计划。该模型不是CIFAR-10数据集上的最新技术，但足以显示我们调查的相对性能。 FedAvg在客户机批处理大小B=64、本地epoch计数E∈{1，5}、报告分数C∈{0.05，0.1，0.2，0.4}（分别对应每轮参与的5，10，20，40个客户机）下运行，共10000个通信轮。我们在客户学习率的网格上执行超参数搜索 Classiﬁcation Performance with Non-Identical Distributions图2显示了作为Dirichlet浓度参数α函数的分类性能（较大的α意味着更相同的分布）。当客户接近一个class时，测试精度在低α附近发生显著变化。增加报告分数C会产生递减的回报，而对于相同分布的客户机数据集来说，性能的提高尤其微不足道。有趣的是，对于固定优化轮次预算的情况，更频繁地同步权重（E=1）并不总是提高非相同数据的准确性。 除了降低训练结束时的准确度外，我们还观察到在数据不完全相同的情况下，训练误差的波动性更大，见图3。在10000个通信回合内，以小报告率运行难以收敛。 图2：不同α的FedAvg精度。每个细胞在学习率上进行优化，在相同α下，每个学习率在不同种群上平均运行5次以上。 图3：固定学习率的FedAvg学习曲线。集中学习结果（虚线）来自TensorFlow教程[Tensor-Flow]。 图4：超参数搜索中的FedAvg测试精度。（a-b）100名客户中报告分数高（c-d）低。机会准确度用虚线表示。 超参数敏感性。除了影响测试集的整体准确性外，由C和α指定的学习条件对超参数敏感性也有显著影响。在大α的同一端，一系列的学习率（大约两个数量级）可以在测试集上产生良好的精度。然而，当C和α值较小时，需要仔细调整学习率以达到良好的精度。见图4。 Accumulating Model Updates with Momentum用动量累积模型更新 在SGD的基础上利用动量通过梯度历史的累积来抑制振荡，在加速网络训练方面取得了巨大的成功。这似乎与FL特别相关，因为参与方可能拥有稀疏的数据分布，并持有有限的标签子集。在本小节中，我们将测试服务器上的动量对FedAvg性能的影响。 Vanilla FedAvg通过w☆w负极∏w更新权重，其中∏w=k=1 n n∏w k（n k是pk示例数，∏w k是来自第k个客户端的权重更新，n=k=1 n k）。为了在服务器上增加动量，我们计算了v☆βv+☆w，并用w☆w-v更新了模型。我们称这种方法为FedAvgM（服务器动量联合平均） 在实验中，我们使用了具有动量β∈{0，0.7，0.9，0.97，0.99，0.997}的Nesterov加速梯度[Nesterov，2007]。模型架构、客户批处理大小B和学习率η与前一小节中的vanilla FedAvg相同。服务器优化器的学习速率保持不变，为1.0。 服务器动量的影响。图5显示了在有或没有服务器动量的情况下，使用非相同数据进行学习的效果。与FedAvg相比，FedAvgM的测试精度持续提高，在许多情况下，性能接近集中学习基线（86.0%）。例如，当E=1和C=0.05时，FedAvgM的性能保持相对恒定且在75%以上，而FedAvg的精度则迅速下降到35%左右。 图5:FedAvgM和FedAvg在不同非相同性下的性能曲线。右边的数据越来越不一样了。最好用彩色观看。 对C和E的超参数依赖性。对FedAvgM来说，超参数调整更困难，因为它涉及额外的超参数β。在图6中，我们根据有效学习率（定义为ηe ffe=η/（1-β）[Shallue等人，2018年]绘制了准确度曲线，这表明了每一组学习条件的最佳ηeff。值得注意的是，当报告分数C较大时，选择ηe ffe更容易，两个数量级的值范围产生合理的测试精度。相反，当每轮只有少数客户报告时，ηeff的可行窗口可以小到只有一个数量级。为了防止客户端更新出现分歧，我们还必须使用低绝对学习率和高动量的组合。局部历元参数E也影响学习率的选择。广泛的局部优化增加了客户权重更新的方差，因此需要更低的ηeff来抵消噪声。 图6:FedAvgM试验精度灵敏度。绘制α=1。有效学习率定义为ηeff=η/（1-β）。大小与客户学习率η成正比，最有效的点用十字线标记。","link":"/2020/01/04/Non-IID数据集调研/"},{"title":"Python2.7=>3.6","text":"简介目前Python代码主要分为2.7和3.6版本，注意转换 注意点 在2.7中， print xxx; 在3.6中： print(xxx) 在2.7中，使用map(xxx),之后会自动适应list，但在3.6中需要转换 list(map(xxx))；一定要记得转换 学会debug Python3.6 中 /代表 除法，如’1/3 = 0.33333333…’;而’//‘代表地板擦，整除；Python2中相反，所以在np.array记得转换","link":"/2018/05/19/Python2-7-3-6/"},{"title":"Pysyft Prerequisites","text":"密码学密码学是研究密码编制、密码破译和密钥管理的一门综合性应用科学。术语： 明文：被隐蔽的消息称作明文，通常用m表示。 Message，Plaintext 密文：将明文隐蔽后的结果称作密文，通常用c表示。 Ciphertext 加密（ Encryption ）：将明文变换成密文的过程称作加密。 脱密（ Decryption ）：合法用户由密文恢复出明文的过程称作脱密。 密钥( key ) ：控制或参与密码变换的可变参数。密钥又分为加密密钥和脱密密钥。 密码体制：一个密码体制由五部分组成：明文空间（M）；密文空间（C）；密钥空间（K）；加密变换E; 脱密变换 D 。 对密码体制的基本要求: (1)即使达不到理论上是不可破的，也应当是实际上不可破的。 (2)一切秘密蕴涵于密钥之中，即只要敌手不知道密钥，就不能由已知信息推出未知的明文信息。 (3)加密算法和脱密算法适用于密钥空间中的所有元素。弱密钥要尽可能的少！ (4)具有很好的实现性能。 安全计算初探安全计算，中文全称为安全多方计算，英文全称为Secure Multi-party Computation，缩写为SMC或 MPC，指的是在保护数据安全的前提下实现多方计算。 那什么是多方计算呢？其实就是其字面意思：多个参与者将各自的数据凑在一起，并在这个大数据集上进行一定的计算，并得到最后的计算结果。形式化的描述就是：假定有 $n$ 个参与方 $P_1, P_2, \\dots, P_n $，他们各自拥有自己的数据集 $a_1, a_2, \\dots, a_n$ 。那么多方计算的目的就是得到某个函数 f 的输出，该函数的输入就是上面的 $a_i$ 们，即 $f(a_1, a_2, \\dots, a_n)$ 。举个简单的例子：假定五道口某工科学校想统计该校男生的恋爱经历分布情况，那 $a_i$ 取值就是0（母胎单）或1（被甩过）。在这种情况下$f$就是简单的求和，即 $f(a_1, a_2, \\dots, a_n) = \\sum_{i}a_i$。 保护数据安全的多方计算: 就是保护各个参与方所拥有数据的安全;安全计算解决的就是这个问题。有了安全计算，大家都不用把确切数据告诉别人（或曰：真实数据从未离开过自己），最后仍然能得到联合计算的结果。在我们的例子中大家不需要告诉别人自己的感情经历就能让学校得到统计结果。 安全计算的用途:，凡是一个计算任务需要用到来自多个参与者的数据，但各个参与者又不想（或不被允许）交换或公开数据，那安全计算就适用于这样的计算任务。比如，某机构组织了一场拍卖且有多个参与方报价，但大家都不想在自己未中标的情况下让别人知道自己出价多少，也就是说，除了最后的中标者需要让卖方知道自己的出价，其他人都不想透露自己的出价。安全多方计算就适用于此场景。 再如，一家借贷公司A想预测贷款给一个试图借贷的客户的风险，但A公司只知道该客户的很简单的信息（如姓名、性别和年龄等）以及在自家借贷的历史，一些很关键的信息（如社保、健康状况等）却不为A公司所知。同时另一家政府机构B知道这些信息，但这些信息按规定是保密的。这时候双方就可以在不交换原始数据的情况下直接用预测模型预测风险。 安全计算实现方法概览研究计算理论的学者们四十多年当中已经提出了很多种实现安全计算的方法。总的来说，大致可归为两类：一类是基于噪音的，另一类不是基于噪音的。 基于噪音的安全计算方法基于噪音的安全计算方法，最主要代表是目前很火的差分隐私（differential privacy）。这类方法的思想是，对计算过程用噪音干扰，让原始数据淹没在噪音中，使别有用心者无法从得到的结果反推原始数据。这就好像我们拿到一张打了马赛克的图片，虽然可能可以猜出马赛克后面大概长啥样，但很难知道马赛克后面的所有细节。 有一点值得注意：这个干扰既可以是数据源，也可以是模型参数和输出。也就是说，参与者既可以对自己的原始数据加噪音使得原始数据从来没在计算过程中出现过，也可以在模型训练的时候改变通过改变模型参数影响输出结果，也可以直接在输出暴露前在输出上加噪音从而使得从计算结果无法反推输入。 比如我们要计算一个函数 $f(x_1, x_2, \\dots, x_n, \\theta)$ ，那么对输入进行干扰后得到的结果便是 $f(x_1 + r_1, x_2+r_2, \\dots, x_n+r_n, \\theta)$ ，对参数进行干扰后得到的结果为 $f(x_1, x_2, \\dots, x_n, \\theta + r_{\\theta})$ ，对输出进行干扰后的结果是 $f(x_1, x_2, \\dots, x_n, \\theta) + r$ 。 这种方法的优点是效率高（毕竟只需要生成服从特定分布的随机数即可），缺点是最后得到的结果不够准确，而且在复杂的计算任务中结果会和无噪音的结果相差很大导致结果无法使用。 非噪音方法非噪音方法一般是通过密码学方法将数据编码或加密，得到一些奇怪的数字，而且这些奇怪的数字有一些神奇的性质，比如看上去很随机但其实保留了原始数据的线性关系，或者顺序明明被打乱但人们却能从中很容易找到与原始数据的映射关系。如果将计算过程比作炒菜，那数据就是炒菜的原料，输出就是最后做出来的美味佳肴。而实现安全计算方法，就好像是让厨师闭着眼睛炒菜一般。 这一类方法主要包括三种：混淆电路（Garbled Circuit）、同态加密（Homomorphic Encryption）和密钥分享（Secret Sharing）。这些方法一般是在源头上就把数据加密或编码了，计算操作方看到的都是密文，因此只要特定的假设条件满足，这类方法在计算过程中是不会泄露信息的。 比如计算函数 $f(x_1, x_2, \\dots, x_n, \\theta)$ 的时候，实际操作的是 $f(\\hat{x_1}, \\hat{x_2}, \\dots, \\hat{x_n}, \\theta)$ （这里 $\\hat{x}_i$ 是 $x_i$ 的密文）。相比于前一类基于噪音的方法，这种方法的优点是不会对计算过程加干扰，因此我们最终得到的是准确值，且有密码学理论加持，安全性有保障，缺点则是由于使用了很多密码学方法，整个过程中无论是计算量还是通讯量都非常庞大，对于一些复杂的任务（如训练几十上百层的CNN等），短时间内可能无法完成。而且对于密码学基础薄弱的程序猿来说，要实现前一类基于噪音的方法没啥难度，但要实现后一类方法则还是要费不少功夫的。 #差分隐私Differential Privacy介绍差分隐私，英文名为differential privacy，顾名思义，保护的是数据源中一点微小的改动导致的隐私泄露问题。比如有一群人出去聚餐，那么其中某人是否是单身狗就属于差分隐私。 为了更形式化地描述差分隐私，我们需要先定义相邻数据集。现给定两个数据集D和D’, 若它们有且仅有一条数据不一样，那我们就称此二者为相邻数据集。以上面数据集为例：假定有 $n$ 个人，他们是否是单身狗，形成一个集合 ${a_1,a_2, …, a_n}$ （其中 $a_i = 0或1$），那么另一个集合当中只有一个人改变了单身状态，形成另一个集合 ${a_1’, a_2’, …, a_n’}$ ，也就是只存在一个 $i$ 使得 $a_i \\ne a_i’$ ，那么这两个集合便是相邻集合。 那么对于一个随机化算法 A （所谓随机化算法，是指对于特定输入，该算法的输出不是固定值，而是服从某一分布），其分别作用于两个相邻数据集得到的两个输出分布难以区分。差分隐私形式化的定义为：$Pr{A(D) = O} ≤e^\\epsilon \\cdot Pr{A(D’) = O} $也就是说，如果该算法作用于任何相邻数据集，得到一个特定输出 $O$ 的概率应差不多，那么我们就说这个算法能达到差分隐私的效果。也就是说，观察者通过观察输出结果很难察觉出数据集一点微小的变化，从而达到保护隐私的目的。 那如何才能得到差分隐私呢？最简单的方法是加噪音，也就是在输入或输出上加入随机化的噪音，以期将真实数据掩盖掉。比较常用的是加拉普拉斯噪音（Laplace noise）。由于拉普拉斯分布的数学性质正好与差分隐私的定义相契合，因此很多研究和应用都采用了此种噪音。还是以前面那个数据集为例，假设我们想要知道到底有多少人是单身狗，我们只需要计算 $\\sum a_i$ ，那么为了掩盖具体数值，实际输出值应为 $O=\\sum a_i + r_{lap}$ ，相应地，另一个数据集输出的是 $O’= \\sum a_i’+r_{lap}’$ 。这使得观察者分不清最终的输出是由哪个数据集产生的。 前面描述的是差分隐私的严格定义。还有一种稍微放宽一点的定义为：$Pr{A(D) = O} \\le e^\\epsilon \\cdot Pr{A(D’) = O} + \\delta $其中 $\\delta$ 是一个比较小的常数。要获取这种差分隐私，我们可以使用高斯噪音（Gaussian noise）。 当然，对输入或输出加噪音会使得最终的输出结果不准确。而且由于噪音是为了掩盖一条数据，所以很多情况下数据的多少并不影响加的噪音的量。那么在数据量很大的情况下，噪音的影响很小，这时候就可以放心大胆地加噪音了，但数据量很小的情况下，噪音的影响就显得比较大，会使得最终结果偏离准确值较远而变得不可用。也有些算法不需要加噪音就能达到差分隐私的效果，听起来很美好，但这种算法通常要求数据满足一定的分布，这一点在现实中通常很难满足。 混淆电路Garbled Circuit介绍先明确一下混淆电路解决的是什么问题。通俗的说，就是一堆人各自拥有其隐私数据，他们想把这些数据合起来算点什么，但又不想把数据交给别人，混淆电路解决的就是此类问题。我们先来考虑一个经典问题——百万富翁问题。两个身价也就那么百来万的人觉得自己是富翁，他们在某会所里碰见，因为某种原因需要比个高下方能取得某种资格，但又觉得自己那些存款数目是多大秘密似的舍不得告诉别人，于是他俩遮遮掩掩地将数据拆散、打乱、加密，最后算出了结果并只解密结果。那么，我现在要说说混淆电路具体是如何工作的了。注意关键词“电路（circuit）”，我们知道可计算问题都可以转换为一个个电路，于是就有了加法电路、比较电路和乘法电路等等。当然，更复杂的计算过程，如深度学习等等，也是可以转换成电路的。一个电路是由一个个门（gate）组成的，比如与门、非门、或门、与非门等等。TODO如上图所示，封面中的两个人Alice和Bob想要搞点事情，他们搞了个电路（比如比较电路，或者emmmm什么电路），电路里面有一些门，每个门包括输入线（input wire）和输出线（output wire）。混淆电路就是通过加密和扰乱这些电路的值来掩盖信息的。在最经典的混淆电路中，加密和扰乱是以门为单位的。每个门都有一张真值表。比如下图就是与门的真值表和或门的真值表。下面就以与门为例来说明混淆电路的工作原理。Alice和Bob想计算一个与门。该门两个输入线 x 和 y 和一个输出线 z ，每条线有0和1两个可能的值。Alice首先给每条线指定两个随机的key，分别对应0和1。 然后，Alice用这些密钥加密真值表，并将该表打乱后发送给Bob。加密过程就是将真值表中每一行对应的 x 和 y 的密钥加密 z 的密钥。这一加密+打乱的过程，就是混淆电路（garbled circuit）的核心思想。那Bob收到加密表后，如何计算呢？首先Alice把自己的输入对应的key发给Bob，比如Alice的输入是0，那就发 $k_{0x}$ ，输入是1就发 $k_{1x}$ 。同时把和Bob有关的key都发给Bob，也就是 $k_{0y}$ 和 $k_{1y}$ 。然后Bob根据自己的输入挑选相关的key。由于Bob收到的这些key都是随机数，所以其实并没有任何有效信息泄露。Bob根据收到的 $k_x$ 和自己的 $k_y$ ，对上述加密表的每一行尝试解密，最终只有一行能解密成功，并提取出相应的 $k_z$ 。 Bob将kz发给Alice，Alice通过对比是 k_{0z} 还是 k_{1z} 得知计算结果是0还是1。由于整个过程大家收发的都是密文或随机数，所以没有有效信息泄露。 密钥分享Secret Sharing介绍密钥分享的基本思路是将每个数字 x 拆散成多个数 $x_1,x_2,\\dots,x_n$ ，并将这些数分发到多个参与方 $S_1,S_2,\\dots,S_n$ 那里。然后每个参与方拿到的都是原始数据的一部分，一个或少数几个参与方无法还原出原始数据，只有大家把各自的数据凑在一起时才能还原真实数据。计算时，各参与方直接用它自己本地的数据进行计算，并且在适当的时候交换一些数据（交换的数据本身看起来也是随机的，不包含关于原始数据的信息），计算结束后的结果仍以secret sharing的方式分散在各参与方那里，并在最终需要得到结果的时候将某些数据合起来。这样的话，密钥分享便保证了计算过程中各个参与方看到的都是一些随机数，但最后仍然算出了想要的结果。 那密钥分享具体是怎么运作的呢？我们先从一个最简单的方法讲起。假设A这个人有一个秘密数字 $x$ ，他想将其分发到$S_1,S_2,\\dots,S_n$那里。那么A首先要做的便是生成 $n-1$ 个随机数 $r_1, r_2, \\dots, r_n$ ，然后计算第 $n$ 个数 $r_n = x - \\sum_{i=1}^{n-1}r_i$ ，最后A令 $x_1=r_1, x_2 = r_2, \\dots, x_n = r_n$ ，并将它们发给$S_1,S_2,\\dots,S_n$。上面这种简单的方法具有如下几条性质： 各个数字 $x_1, x_2, \\dots, x_n$ 都是随机分布的，单独一个或若干个并不泄露任何信息； 当所有$x_1, x_2, \\dots, x_n$合在一起时，可以还原 $x$ ，因为 $x = \\sum_{i=1}^n x_i$ ; 这种方案具有加法同态的性质，也就是说，各参与方可以在不交换任何数据的情况下直接计算对秘密数据求和。什么意思呢？假设还有另一个人B，他也有一个秘密数字 $y$ ,并且和A一起将数据分发给了$S_1,S_2,\\dots,S_n$，为了做加法， $S_1$ 计算 $z_1=x_1+y_1 ,S_2$ 计算 $z_2=x_2+y_2 , …, S_n$ 计算 $z_n=x_n+y_n$ , 每个参与方都只对本地的随机数进行操作，不交换数据。而且根据secret sharing的性质，我们其实可以看到： $\\sum_{i=1}^nz_i = \\sum_{i=1}^nx_i + \\sum_{i=1}^ny_i$ 。也就是说，我们得到的是 $z= x+y$ 的密钥分享，而这个求和的结果可以不暴露出来，继续用来做其他事情。 上面是一个简单的密钥分享的方法，满足了加法同态，且保证了只有n个参与方全部联合才能把数据解开。但有时候我们并不希望必须凑齐n个人才能解开，一方面是因为数据是分散在多个人手里的，要是有一个人不小心掉线了甚至是故意使坏，那数据就无法恢复了，另一方面很多时候我们不需要这么强的安全性，比如我们可以相信10个人里面至少一半是好人，而好人是不会偷偷把数据解开的，那么我们只需要保证4个或更少的人无法将数据解开就行了，而只有凑齐了5个或更多的人才能将数据解开。这种密钥分享叫做阈值密钥分享（threshold secret sharing）. 更具体地说，我们可以定义一种名为 $(t,n)$ 阈值密钥分享的方案，此类方案允许任意 $t$个参与方将秘密数据解开，但任何不多于 $t-1$ 个参与方的小团体都无法将秘密数据解开。前面提到的那种简单方案其实是 $t=n$ 时的特殊情况。Shamir大神在1979年就提出了阈值密钥分享方案，且该方案支持任意的 $t$ 。该方案运作方式如下：假设A想要使用 $(t,n)$ 阈值密钥分享技术将某秘密数字 $s$ 分享给$S_1,S_2,\\dots,S_n$，那么他首先生成一个 $t-1$ 次多项式多项式 $f(x)=a_0 + a_1 x + a_2 x^2 + \\dots + a_{t-1}x^{t-1}$ ，其中 $a_0$ 就等于要分享的秘密数字 $s$ ，而 $a_1, a_2, \\dots, a_{t-1}$ ，则是A生成的随机数。随后A只需将$s_1 = f(1), s_2 = f(2), \\dots, s_n = f(n)$ 分别发给$S_1,S_2,\\dots,S_n$即可。到了这一步，稍微有点线性代数基础的同学应该很容易看出来， $f(1), f(2), \\dots, f(n)$ 中任意 t 个凑在一起都可以解出，而任意 $t-1$ 个凑在一起都无法得到 $a_0$ （即 $s$ ）的确切解。通过这一点便达到了 $(t,n)$ 阈值的要求。Shamir密钥分享方法也是满足加法同态的（因为多项式本身满足这一性质），有兴趣的同学可以自己验证一下。 说到这里，大家可以看到我们可以很容易地使用密钥分享技术在参与方不交换任何信息的情况下完成保护隐私的加法操作，但本文一开始提到的更重要的乘法操作呢？在完全不交换信息的情况下，要完成乘法是很难实现的，但如果在计算前或计算过程中适当交换信息，要完成乘法操作却有不少解决方案。 我们先考虑最简单的一种情况：一个秘密数字 $x$和一个公开数字 $y$ 相乘，目标是得到一个数字 z 的密钥分享，其中满足 $z=x \\times y$ 。这个做起来其实挺简单的。假设我们使用最开始说的那种简单的密钥分享方法，即，那么我们的目标就是让$S_1,S_2,\\dots,S_n$分别得到 $z_1, z_2, \\dots, z_n$ ，且满足 $z = \\sum_{i=1}^nz_i$ 。要达成此目标，只需让 $S_1 计算 z_1 = x_1 \\times y ,S_2$ 计算 $z_2 = x_2 \\times y ,…, S_n$ 计算 $z_n = x_n \\times y$ ,这个应该很容易理解。好吧这里仍然不需要参与方交换信息。 但如果 y 不是公开数字呢？也就是说$S_1,S_2,\\dots,S_n只拥有 y_1, y_2, \\dots, y_n$ ，而不知道 $y$ 的确切值。这时候上面说的方法就不管用了。在不交换信息的情况下，$S_1,S_2,\\dots,S_n$只能分别算出 $x_1 \\times y_1, x_2 \\times y_2, \\dots, x_n \\times y_n$ ，但无法计算交叉项。欲求交叉项，必有信息交换，不过这个信息的交换，既可以发生在计算前，也可以发生在计算过程中，或者两个阶段都有信息交换。下面介绍一下如何使用预计算生成乘积元组的方法解决乘法问题。 我们假设有某种神奇的方法（具体怎么做就不展开了），使得$S_1,S_2,\\dots,S_n$能在计算发生前预先得到两个随机数 a 和 b 的秘密分享，以及 a 和 b 的乘积 c 的秘密分享，而且它们都不知道 a 和 b 和 c 的真实值，如下图所示：现在有A和B分别分享了两个数字 x 和 y ，参与方需要算出 x 和 y 的乘积 z 的密钥分享。这时候可以借助前面生成的随机乘积元组。我们先令 $s=x-a 以及 t = y - b$ ，然后我们可以看到$x \\times y = (x - a + a) \\times (y - b + b) = (s+a) \\times (t + b) = s \\times t + s \\times b + t \\times a + c$参与方$S_1,S_2,\\dots,S_n$可以联合起来将 s 和 t 的值解开，由于 a 和 b 都是值未知的随机数，因此 s 和 t 的值并不会暴露关于 a 和 b 的信息。上面那个式子中， $s\\times t$ 可以直接用公开的 s 和 t 算出来， $s \\times b 以及 t \\times a$ 的密钥分享则可以用前面的秘密数与公开数的乘法得到，而 c 的密钥分享则是一开始就存在，因此这几项合起来便能得到 $z = x \\times y$ 的密钥分享。 需要注意的是，每个这样的秘密数字的乘法都会消耗一组随机乘积元组，不过由于随机乘积元组的值和计算时的 x 和 y 的值是无关的，因此这样的元组可以由参与方在空闲的时候预先生成一大堆，等需要用上的时候再拿出来消耗掉。 在密钥分享中，由于每次计算后得到的仍然是密钥分享，因此各操作可以串起来，直到算到最终结果，再将其暴露出来。有了加法和乘法，我们理论上可以进行各种计算，比如除法和指数都可以用加法和乘法去拟合，浮点数运算也可以模拟，具体就不展开了。 [1] Shamir, Adi. “How to share a secret.” Communications of the ACM 22.11 (1979): 612-613. [2] Beaver, Donald, and Shaft Goldwasser. “Multiparty computation with faulty majority.” Conference on the Theory and Application of Cryptology. Springer, New York, NY, 1989.","link":"/2018/12/21/Pysyft-Prerequisites/"},{"title":"Pytorch快速入门","text":"简介原文连接： https://zhuanlan.zhihu.com/p/26854386本文为快速学习入门Pytorch。学习之道：明白目的；pytorchd的结构框架；pytorch的使用。 目的：An open source deep learning platform that provides a seamless path from research prototyping to production deployment. 结构：平台，对象，操作，框架 使用：引用，使用 大纲：总体框架；输入是什么？ 怎么执行？ 输出是什么？ 解释论文 the chain structure; =&gt; 源代码hook 解释论文 send the chain structure; =&gt; worker 解释论文 MPC tensor =&gt; dir() 返回参数的属性、方法列表 self在定义时需要定义，但是在调用时会自动传入。 self的名字并不是规定死的，但是最好还是按照约定是用self self总是指调用时的类的实例。 梯度就是函数在某一点的梯度是这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。正向传播: input weights = out; out 与 target对比得到loss函数(误差)；反向传播: 对误差更新权值，重新计算输出；即 计算误差对梯度的求导grad, w = w -学习率grad, 再算输出 10分钟快速入门PyTorch (0)环境配置根据URL： https://pytorch.org/get-started/locally/安装： pip3 install torch torchvision pytorch基础介绍一下pytorch处理的对象以及操作。 Tensor首先介绍里面最基本的操作对象，tensor。使用jupytert notebook: tensor就是张量的英文，表示多维的矩阵，比如一维就是向量，二维就是一般的矩阵等等，pytorch里面处理的单位就是一个一个的tensor.可以显示的得到其大小 这个和numpy很相似，同时tensor和numpy.array之间也可以相互转换.tensor的运算也很简单，一般的四则运算都是支持的 Variablepytorch和numpy不一样的地方就来了，就是其提供了自动求导功能，也就是可以自动给你你要的参数的梯度，这个操作由另外一个基本元素提供，Variable本质上Variable和Tensor没有区别，不过Variable会放入一个计算图，然后进行前向传播，反向传播以及自动求导.一个Variable里面包含着三个属性，data，grad和creator，其中creator表示得到这个Variabel的操作，比如乘法或者加法等等，grad表示方向传播的梯度，data表示取出这个Variabel里面的数据这就是一个简单的计算图的例子 神经网络前面讲了两个操作对象，最后讲一下pytorch里面的模型建立，模型的建立主要依赖于torch.nn，torch.nn包含这个所有神经网络的层的结构。 这就是构建所有神经网络的模板，不管你想构建卷积神经网络还是循环神经网络或者是生成对抗网络都依赖于这个结构.代码网址 10分钟快速入门PyTorch (1)以上基本的介绍了pytorch里面的操作单元，Tensor，以及计算图中的操作单位Variable，相信大家都已经熟悉了，下面这一部分我们就从两个最基本的机器学习，线性回归以及logistic回归来开始建立我们的计算图进行运算。 线性回归对于线性回归，相信大家都很熟悉了，各种机器学习的书第一个要讲的内容必定有线性回归，这里简单的回顾一下什么是简单的一元线性回归。即给出一系列的点，找一条直线，使得这条直线与这些点的距离之和最小。上面这张图就简单地描绘出了线性回归的基本原理，下面我们重点讲讲如何用pytorch写一个简单的线性回归。 code1. data首先我们需要给出一系列的点作为线性回归的数据，使用numpy来存储这些点。 1234567x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], [9.779], [6.182], [7.59], [2.167], [7.042], [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], [3.366], [2.596], [2.53], [1.221], [2.827], [3.465], [1.65], [2.904], [1.3]], dtype=np.float32) 还记得pytorch里面的基本处理单元吗？Tensor，我们需要将numpy转换成Tensor，如果你还记得上一节的内容，那么你就一定记得这个函数，torch.from_numpy() 12x_train = torch.from_numpy(x_train)y_train = torch.from_numpy(y_train) 这样我们的数据就转换成了Tensor。 2. model上一节讲了基本的模型框架，按照这个框架就可以写出一个线性回归模型了. 123456789class LinearRegression(nn.Module): def __init__(self): super(LinearRegression, self).__init__() self.linear = nn.Linear(1, 1) # input and output is 1 dimension def forward(self, x): out = self.linear(x) return outmodel = LinearRegression() 这里的nn.Linear表示的是 y=w*x+b，里面的两个参数都是1，表示的是x是1维，y也是1维。当然这里是可以根据你想要的输入输出维度来更改的，之前使用的别的框架的同学应该很熟悉。 然后需要定义loss和optimizer，就是误差和优化函数 12criterion = nn.MSELoss()optimizer = optim.SGD(model.parameters(), lr=1e-4) 这里使用的是最小二乘loss，之后我们做分类问题更多的使用的是cross entropy loss，交叉熵。优化函数使用的是随机梯度下降，注意需要将model的参数model.parameters()传进去让这个函数知道他要优化的参数是那些。 3. train接着开始训练 12345678910111213141516num_epochs = 1000for epoch in range(num_epochs): inputs = Variable(x_train) target = Variable(y_train) # forward out = model(inputs) # 前向传播 loss = criterion(out, target) # 计算loss # backward optimizer.zero_grad() # 梯度归零 loss.backward() # 方向传播 optimizer.step() # 更新参数 if (epoch+1) % 20 == 0: print(&apos;Epoch[{}/{}], loss: {:.6f}&apos;.format(epoch+1, num_epochs, loss.data[0])) 第一个循环表示每个epoch，接着开始前向传播，然后计算loss，然后反向传播，接着优化参数，特别注意的是在每次反向传播的时候需要将参数的梯度归零，即optimzier.zero_grad() 4. validation训练完成之后我们就可以开始测试模型了 123model.eval()predict = model(Variable(x_train))predict = predict.data.numpy() 特别注意的是需要用 model.eval()，让model变成测试模式，这主要是对dropout和batch normalization的操作在训练和测试的时候是不一样的.最后可以得到这个结果以及loss的结果ok，在这篇文章中我们使用pytorch实现了简单的线性回归模型，掌握了pytorch的一些基本操作，下一节我们将使用logistic回归对MNIST手写字体数据集做识别。 10分钟快速入门PyTorch (2)上一节介绍了简单的线性回归，如何在pytorch里面用最小二乘来拟合一些离散的点，这一节我们将开始简单的logistic回归，介绍图像分类问题，使用的数据是手写字体数据集MNIST。 logistic回归logistic回归简单来说和线性回归是一样的，要做的运算同样是 y = w * x + b，logistic回归简单的是做二分类问题，使用sigmoid函数将所有的正数和负数都变成0-1之间的数，这样就可以用这个数来确定到底属于哪一类，可以简单的认为概率大于0.5即为第二类，小于0.5为第一类。这就是sigmoid的图形而我们这里要做的是多分类问题，对于每一个数据，我们输出的维数是分类的总数，比如10分类，我们输出的就是一个10维的向量，然后我们使用另外一个激活函数，softmax这就是softmax函数作用的机制，其实简单的理解就是确定这10个数每个数对应的概率有多大，因为这10个数有正有负，所以通过指数函数将他们全部变成正数，然后求和，然后这10个数每个数都除以这个和，这样就得到了每个类别的概率。 Codedata首先导入torch里面专门做图形处理的一个库，torchvision，根据官方安装指南，你在安装pytorch的时候torchvision也会安装。 我们需要使用的是torchvision.transforms和torchvision.datasets以及torch.utils.data.DataLoader 首先DataLoader是导入图片的操作，里面有一些参数，比如batch_size和shuffle等，默认load进去的图片类型是PIL.Image.open的类型，如果你不知道PIL，简单来说就是一种读取图片的库. torchvision.transforms里面的操作是对导入的图片做处理，比如可以随机取(50, 50)这样的窗框大小，或者随机翻转，或者去中间的(50, 50)的窗框大小部分等等，但是里面必须要用的是transforms.ToTensor()，这可以将PIL的图片类型转换成tensor，这样pytorch才可以对其做处理. torchvision.datasets里面有很多数据类型，里面有官网处理好的数据，比如我们要使用的MNIST数据集，可以通过torchvision.datasets.MNIST()来得到，还有一个常使用的是torchvision.datasets.ImageFolder()，这个可以让我们按文件夹来取图片，和keras里面的flow_from_directory()类似，具体的可以去看看官方文档的介绍。 123456789101112131415# 定义超参数batch_size = 32learning_rate = 1e-3num_epoches = 100# 下载训练集 MNIST 手写数字训练集train_dataset = datasets.MNIST(root=&apos;./data&apos;, train=True, transform=transforms.ToTensor(), download=True)test_dataset = datasets.MNIST(root=&apos;./data&apos;, train=False, transform=transforms.ToTensor())train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) model之前讲过模型定义的框架，废话不多说，直接上代码 12345678910class Logstic_Regression(nn.Module): def __init__(self, in_dim, n_class): super(Logstic_Regression, self).__init__() self.logstic = nn.Linear(in_dim, n_class) def forward(self, x): out = self.logstic(x) return outmodel = Logstic_Regression(28*28, 10) # 图片大小是28x28 我们需要向这个模型传入参数，第一个参数定义为数据的维度，第二维数是我们分类的数目。 接着我们可以在gpu上跑模型，怎么做呢？首先可以判断一下你是否能在gpu上跑torh.cuda.is_available()如果返回True就说明有gpu支持 接着你只需要一个简单的命令就可以了model = model.cuda()或者model.cuda()都可以 然后需要定义loss和optimizer12criterion = nn.CrossEntropyLoss()optimizer = optim.SGD(model.parameters(), lr=learning_rate) 这里我们使用的loss是交叉熵，是一种处理分类问题的loss，optimizer我们还是使用随机梯度下降 train接着就可以开始训练了 12345678910111213141516171819202122232425for epoch in range(num_epoches): print(&apos;epoch {}&apos;.format(epoch+1)) print(&apos;*&apos;*10) running_loss = 0.0 running_acc = 0.0 for i, data in enumerate(train_loader, 1): img, label = data img = img.view(img.size(0), -1) # 将图片展开成 28x28 if use_gpu: img = Variable(img).cuda() label = Variable(label).cuda() else: img = Variable(img) label = Variable(label) # 向前传播 out = model(img) loss = criterion(out, label) running_loss += loss.data[0] * label.size(0) _, pred = torch.max(out, 1) num_correct = (pred == label).sum() running_acc += num_correct.data[0] # 向后传播 optimizer.zero_grad() loss.backward() optimizer.step() 注意我们如果将模型放到了gpu上，相应的我们的Variable也要放到gpu上，也很简单 12img = Variable(img).cuda()label = Variable(label).cuda() 然后可以测试模型，过程与训练类似，只是注意要将模型改成测试模式model.eval()这是跑完100 epoch的结果 具体的结果多久打印一次，如何打印可以自己在for循环里面去设计 这一部分我们就讲解了如何用logistic回归去做一个简单的图片分类问题，知道了如何在gpu上跑模型，下一节我们将介绍如何写简单的卷积神经网络，不了解卷积网络的同学可以先去我的专栏看看之前卷积网络的介绍。 10分钟快速入门PyTorch (3)前面两节讲了最基本的机器学习算法，线性回归和logistic回归，这一节将介绍传统机器学习里面最后一个算法-神经网络，这也是深度学习的基石，所谓的深度学习，也可以理解为很深层的神经网络。说起这里，有一个小段子，神经网络曾经被打入了冷宫，因为SVM派的崛起，SVM不了解的同学可以去google一下，中文叫支持向量机，因为其有着完备的数学解释，并且之前神经网络运算复杂等问题，导致神经网络停步不前，这个时候任何以神经网络为题目的论文都发不出去，反向传播算法的鼻祖hinton为了解决这个问题，于是就想到了用深度学习为题目。 段子说完，接下来开始我们的简单神经网络。 Neural Network其实简单的神经网络说起来很简单，先放图为敬 通过图片就能很简答的看出来，其实每一层网络所做的就是 y=W*X+b，只不过W的维数由X和输出维数决定，比如X是10维向量，想要输出的维数，也就是中间层的神经元个数为20，那么W的维数就是20x10，b的维数就是20x1，这样输出的y的维数就为20。 中间层的维数可以自己设计，而最后一层输出的维数就是你的分类数目，比如我们等会儿要做的MNIST数据集是10个数字的分类，那么最后输出层的神经元就为10。 Code有了前面两节的经验，这一节的代码就很简单了，数据的导入和之前一样定义模型 12345678910111213141516171819class Neuralnetwork(nn.Module): def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim): super(Neuralnetwork, self).__init__() self.layer1 = nn.Linear(in_dim, n_hidden_1) self.layer2 = nn.Linear(n_hidden_1, n_hidden_2) self.layer3 = nn.Linear(n_hidden_2, out_dim) def forward(self, x): x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) return xmodel = Neuralnetwork(28*28, 300, 100, 10)if torch.cuda.is_available(): model = model.cuda()criterion = nn.CrossEntropyLoss()optimizer = optim.SGD(model.parameters(), lr=learning_rate) 上面定义了三层神经网络，输入是28x28，因为图片大小是28x28，中间两个隐藏层大小分别是300和100，最后是个10分类问题，所以输出层为10.训练过程与之前完全一样这是50次之后的输出结果，可以和上一节logistic回归比较一下. 可以发现准确率大大提高，其实logistic回归可以看成简单的一层网络，从这里我们就可以看出为什么多层网络比单层网络的效果要好，这也是为什么深度学习要叫深度的原因。 下一节我们将正式进入到深度学习，第一个模型将是计算机视觉领域的王牌模型，卷积神经网络。","link":"/2018/12/05/Pytorch快速入门/"},{"title":"Swagger的快速入门","text":"简介Swagger：REST APIs文档生成工具。Swagger 是一个规范和完整的框架，用于生成、描述、调用和可视化 RESTful 风格的 Web 服务。 是一种API设计工具，能更好地实现前后端分离：Swagger - 前后端分离后的契约。 一些环境部署与搭建： 让接口测试成为合格的桥梁——本地搭建 Swagger-UI 环境搭建 Swagger入门教程主要使用swagger-ui 和 swagger-editor。swagger-ui展示swagger-editor生成的api文档，api文档格式可以是yaml或json。（感觉是swagger-ui可以在线测试使用，交互；swagger-editor只是生成文档） 有关Swagger的一些个人理解：Swagger是REST APIs文档生成工具。 主要的工作量应该在于Swagger文档的编写，而文档的编写需要遵循一定的规范，即Swagger API Spec。Swagger API Spec是Swagger用来描述REST API的语言，可以用YAML和JSON表示。 Swagger文档的编写可以采用以下方式： （1）手动编写Swagger文档。要对SwaggerSpec很熟悉。可以利用在线编辑器Swagger editor（http://editor.swagger.io/#/）验证YAML格式的内容是否违反Swagger spec，Swagger editor会标出错误并且给予格式提醒。所以说手动编写要对YAML或者JSON语法非常熟悉。但是这种方法相对好上手。 （2）Swagger文档应该可以从代码注释中自动生成。目前在参考这个教程：http://michal.karzynski.pl/blog/2016/06/19/building-beautiful-restful-apis-using-flask-swagger-ui-flask-restplus/?utm_source=tuicool&amp;utm_medium=referral。因为对API和FLASK等相关内容不太熟悉，还在学习中。 Swagger-UI是一套HTML/CSS/JS框架用于解析遵守Swagger spec的JSON或YML文件，展示swagger-editor生成的API文档，还可以在其中调试API。这是官方demo的地址：http://petstore.swagger.io/。 Swagger-UI界面也可以进行修改，包括进行中文配置等。 下面是一个比较好的Swagger-UI说明：Swagger-UI用户手册 。 FLASK使用python的Flask实现一个RESTful API服务器端 近些年来，REST成为web services和APIs的标准架构。 python的Flask框架可以轻松实现一个RESTful服务。 Flask python web框架。 Flask很简单，micro-framework，所有有很多extensions配合使用，可以实现更多的功能。 Flask-RESTPlus：更方便地搭建REST APIs。核心功能是，可以结合Swagger UI自动生成交互性的API文档。 API的组织方式： API namespaces， RESTful resources 和 HTTP methods。 Namespaces：允许API定义划分为多个文件，每一个是API的一部分，用于不同的 URL prefix（前缀）。 RESTful resources：组织API成不同的endpoints（与不同的数据类型有关） 另外需要搞懂的一些点 1. RESTful API先理解API接口！！！想问一下什么是API接口，具体是什么意思？ – 知乎 API（Application Programming Interface，应用程序编程接口）是一些预先定义的函数，目的是提供应用程序与开发人员基于某软件或硬件得以访问一组例程的能力，而又无需访问源码，或理解内部工作机制的细节。简单的说，就是通过某一预先定义的渠道读/写数据的方式。 下面这篇算是解决了我的困惑== 利用「接口」做产品时我们该如何思考？ 一般接口包含以下几个内容： 接口地址：顾名思义就是接口的地址，以网址的形式展现，你通过发送请求给这个网址来对接口进行交互操作。 请求方法： 请求参数：即传输参数的时候要带的一些参数，一般文档中会用表格的形式清晰的说明。当我向接口发送携带请求参数的请求时，都要携带什么字段，规则是什么。 返回内容：返回内容一般会以json或是XML的形式返回。 错误代码: REST – REpresentational State Transfer 直接翻译：表现层状态转移。 怎样用通俗的语言解释REST，以及RESTful？–知乎 REST描述的是在网络中client和server的一种交互形式；REST本身不实用，实用的是如何设计 RESTful API（REST风格的网络接口）；【其实是要设计一种合适的API，RESTful的】 理解RESTful架构 总结一下什么是RESTful架构： （1）每一个URI代表一种资源； （2）客户端和服务器之间，传递这种资源的某种表现层； （3）客户端通过四个HTTP动词，对服务器端资源进行操作，实现”表现层状态转化”。 总结： URL定位资源，用HTTP动词（GET,POST,DELETE,DETC）描述操作。 看Url就知道要什么 看http method就知道干什么 看http status code就知道结果如何 简洁版 REST不是”rest”这个单词，而是几个单词缩写。但即使那几个单词说出来，也无法理解在说什么 -_-!! （不是要贬低人，是我自己也理解困难）； REST描述的是在网络中client和server的一种交互形式；REST本身不实用，实用的是如何设计 RESTful API（REST风格的网络接口）； Server提供的RESTful API中，URL中只使用名词来指定资源，原则上不使用动词。“资源”是REST架构或者说整个网络处理的核心。 比如：http://api.qc.com/v1/newsfeed: 获取某人的新鲜; http://api.qc.com/v1/friends: 获取某人的好友列表; http://api.qc.com/v1/profile: 获取某人的详细信息; 用HTTP协议里的动词来实现资源的添加，修改，删除等操作。 即通过HTTP动词来实现资源的状态扭转：1.GET-用来获取资源，POST-用来新建资源（也可以用于更新资源），PUT-用来更新资源，DELETE-用来删除资源。 比如：DELETE http://api.qc.com/v1/friends: 删除某人的好友 （在http parameter指定好友id） POST http://api.qc.com/v1/friends: 添加好友 UPDATE http://api.qc.com/v1/profile: 更新个人资料 禁止使用： GET http://api.qc.com/v1/deleteFriend 图例： Server和Client之间传递某资源的一个表现形式，比如用JSON，XML传输文本，或者用JPG，WebP传输图片等。当然还可以压缩HTTP传输时的数据（on-wire data compression）。 用 HTTP Status Code传递Server的状态信息。比如最常用的 200 表示成功，500 表示Server内部错误等。 主要信息就这么点。最后是要解放思想，Web端不再用之前典型的PHP或JSP架构，而是改为前段渲染和附带处理简单的商务逻辑（比如AngularJS或者BackBone的一些样例）。 Web端和Server只使用上述定义的API来传递数据和改变数据状态。格式一般是JSON。iOS和Android同理可得。由此可见，Web，iOS，Android和第三方开发者变为平等的角色通过一套API来共同消费Server提供的服务。 2. Mock server","link":"/2018/11/28/Swagger的快速入门/"},{"title":"TFFIntroVideo","text":"TensorFlow Federated (TFF): Machine Learning on Decentralized Data (TF Dev Summit ‘19)video url IntroTensorFlow Federated (TFF) is an open-source framework for machine learning and other computations on decentralized data. TFF has been developed to facilitate open research and experimentation with Federated Learning (FL), an approach to machine learning where a shared global model is trained across many participating clients that keep their training data locally. server distribute initial model to clients; each client train model on local own data[不用convergence收敛，训练一些会儿] each client produce new model trained and send it to server In practice, we send updates but not models, implementation detail in server, combined model train with many many rounds many extensions to add TFF将代码编译成抽象表示 可运行在不同环境 伪代码，简介 优点：同一书写界面 FL API训练模型，实现联合学习； FC API实现联合学习算法 ，本地环境模拟 分层，从各个方面参与TFF的学习 FL API 训练流程如上； FC API内容如下 FC API实现优点: 1.为分布式计算的语言 2. 类Python的API 3. 部署时的方便 在clients端的数据为一等公民，有类型{float32}@CLIENTS 在server端融合模型，数据类型是float32@SERVER 将联邦操作认为是协议 FC API时，设置 数据类型； 定义联合学习算法内容 较为复杂的例子：两个输入，检测数据，门槛值；如何执行？以下 第一步： 分发threshold值给每一个客户端 使用类似于MapReduce的方式计算以上 使用 tff.federated_average聚合；此为一个完整的例子 代码方面展示，如何创建底层的联合学习的算法； 表达联合学习的算法 未来需要工作方向； 联合学习模型和数据集的增加；发展新的联合学习算法；改进FC API底层；部署环境增加","link":"/2019/05/23/TFFIntroVideo/"},{"title":"Numpy使用","text":"Numpy教程Python中用于科学计算的核心库。numpy（Numerical Python）提供了python对多维数组对象的支持：ndarray，具有矢量运算能力，快速、节省空间。numpy支持高级大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库。 基础篇NumPy的主要对象是同种元素的多维数组。这是一个所有的元素都是一种类型、通过一个正整数元组索引的元素表格(通常是元素是数字)。在NumPy中维度(dimensions)叫做轴(axes)，轴的个数叫做秩(rank)。NumPy的数组类被称作 ndarray 。通常被称作数组。注意numpy.array和标准Python库类array.array并不相同，后者只处理一维数组和提供少量功能。更多重要ndarray对象属性有： ndarray.ndim数组轴的个数，在python的世界中，轴的个数被称作秩 ndarray.shape数组的维度。这是一个指示数组在每个维度上大小的整数元组。例如一个n排m列的矩阵，它的shape属性将是(n,m),这个元组的长度显然是秩，即维度或者ndim属性 ndarray.size数组元素的总个数，等于shape属性中元组元素的乘积。 ndarray.dtype一个用来描述数组中元素类型的对象，可以通过创造或指定dtype使用标准Python类型。另外NumPy提供它自己的数据类型。 ndarray.itemsize数组中每个元素的字节大小。例如，一个元素类型为float64的数组itemsiz属性值为8(=64/8),又如，一个元素类型为complex32的数组item属性为4(=32/8). ndarray.data包含实际数组元素的缓冲区，通常我们不需要使用这个属性，因为我们总是通过索引来使用数组中的元素。 一个例子：12345678910from numpy import *a = arange(15).reshape(3, 5)print(a)print(type(a))print('a.ndim',a.ndim)print('a.shape', a.shape)print('a.size', a.size)print('a.dtype', a.dtype)print('a.itemsize', a.itemsize)print('a.data',a.data) [[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14]] &lt;class &apos;numpy.ndarray&apos;&gt; a.ndim 2 a.shape (3, 5) a.size 15 a.dtype int64 a.itemsize 8 a.data &lt;memory at 0x7f1a641e17e0&gt; 创建数组使用 array 函数使用 array 函数从常规的Python列表和元组创造数组。所创建的数组类型由原序列中的元素类型推导而来。 12345678910111213#from numpy import * # 申明为*，不需要前缀import numpy as npa = np.array([[1, 2, 3, 4],[5, 6,7,8],[10, 2, 3,4]])b = array([1.2, 3.5, 5.1])print('a',a)print('b',b)# 一个常见的错误包括用多个数值参数调用array而不是提供一个由数值组成的列表作为一个参数。 c = array(1, 2, 3, 4) wrong# 数组将序列包含序列转化成二维的数组，序列包含序列包含序列转化成三维数组等等。c = array([(1, 2, 3, 4), (4, 5, 6, 7)])print('c:',c)# 数组类型可以在创建时显示指定d = array([1, 2, 3, 4], dtype=complex)print('d', d) a [[ 1 2 3 4] [ 5 6 7 8] [10 2 3 4]] b [ 1.2 3.5 5.1] c: [[1 2 3 4] [4 5 6 7]] d [ 1.+0.j 2.+0.j 3.+0.j 4.+0.j] 使用占位符创建 通常，数组的元素开始都是未知的，但是它的大小已知。因此，NumPy提供了一些使用占位符创建数组的函数。这最小化了扩展数组的需要和高昂的运算代价。 函数zero创建一个全是0的数组，函数ones创建一个全1的数组，函数empty创建一个内容随机并且依赖与内存状态的数组。默认创建的数组类型(dtype)都是float64。 123print(np.zeros((3, 4)))print(np.ones((2, 4)))print(np.empty((1, 2))) [[ 0. 0. 0. 0.] [ 0. 0. 0. 0.] [ 0. 0. 0. 0.]] [[ 1. 1. 1. 1.] [ 1. 1. 1. 1.]] [[ 6.90466352e-310 6.90466352e-310]] 使用arange 函数为了创建一个数列，NumPy提供一个类似arange的函数返回数组而不是列表: 12print(np.arange(10, 30, 5))print(np.arange(0, 3, 0.3)) [10 15 20 25] [ 0. 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7] 使用 lineapce 函数当arange使用浮点数参数时，由于有限的浮点数精度，通常无法预测获得的元素个数。因此，最好使用函数linspace去接收我们想要的元素个数来代替用range来指定步长。 12345from numpy import piprint(np.linspace(0, 2, 9)) # 9 numbers 0 to 2x = np.linspace(0, 2*pi, 10)f = np.sin(x)print(f) [ 0. 0.25 0.5 0.75 1. 1.25 1.5 1.75 2. ] [ 0.00000000e+00 6.42787610e-01 9.84807753e-01 8.66025404e-01 3.42020143e-01 -3.42020143e-01 -8.66025404e-01 -9.84807753e-01 -6.42787610e-01 -2.44929360e-16] 其它函数array, zeros, zeros_like, ones, ones_like, empty, empty_like, arange, linspace, rand, randn, fromfunction, fromfile, 赋值拷贝。 打印数组当你打印一个数组，NumPy以类似嵌套列表的形式显示它，但是呈以下布局： 最后的轴从左到右打印 次后的轴从顶向下打印 剩下的轴从顶向下打印，每个切片通过一个空行与下一个隔开 一维数组被打印成行，二维数组成矩阵，三维数组成矩阵列表。 12345678910a = np.arange(6)# 1d arrayprint(a)b = np.arange(12).reshape(4,3)# 2d arrayprint(b)c = np.arange(24).reshape(2,3,4)# 3d arrayprint(c)# 如果一个数组用来打印太大了，NumPy自动省略中间部分而只打印角落print(np.arange(10000))# 禁用NumPy的这种行为并强制打印整个数组，你可以设置printoptions参数来更改打印选项。print(np.set_printoptions(threshold=np.nan)) [0 1 2 3 4 5] [[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]] [[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] [ 0 1 2 ..., 9997 9998 9999] 基本运算数组的算术运算是按元素的。新的数组被创建并且被结果填充。NumPy中的乘法运算符 * 指示按元素计算，矩阵乘法可以使用 dot 函数或创建矩阵对象实现。 12345678a = np.array([1, 2, 3, 4])b = np.arange(4)print('b',b)c = a-bprint('c',c)print('b**2',b**2)print('10*np.sin(a)',10*np.sin(a))print('a&lt;35',a&lt;35) b [0 1 2 3] c [1 1 1 1] b**2 [0 1 4 9] 10*np.sin(a) [ 8.41470985 9.09297427 1.41120008 -7.56802495] a&lt;35 [ True True True True] 12345678910# NumPy中的乘法运算符*指示按元素计算，矩阵乘法可以使用dot函数或创建矩阵对象实现import numpyprint(numpy.arange(6))A = np.array([[1, 1,], [0, 1]])B = np.array([[2, 0], [3, 4]])print('A*B',A*B)print('A.dot(B)',A.dot(B))print('np.dot(A, B)',np.dot(A, B)) [0 1 2 3 4 5] A*B [[2 0] [0 4]] A.dot(B) [[5 4] [3 4]] np.dot(A, B) [[5 4] [3 4]] 12345678# 有些操作符像+=和*=被用来更改已存在数组而不创建一个新的数组。a = np.ones((2, 3), dtype=int)b = np.random.random((2, 3))a *= 3print(a)b += aprint(b)# print(a += b) [[3 3 3] [3 3 3]] [[ 3.77601215 3.67874255 3.09996677] [ 3.33283587 3.62279173 3.90198359]] 1234567891011121314# 当运算的是不同类型的数组时，结果数组靠近更普遍和精确的已知数组(这种行为叫做upcast)。a=np.ones(3, dtype=np.int32)print(a)b = np.linspace(0,pi,3)print(b.dtype.name)c = a+bprint('c',c)print('c dtype name',c.dtype.name)d = np.exp(c*1j)print(d)print('d dtype name', d.dtype.name) # 许多非数组运算，如计算数组所有元素之和，被作为ndarray类的方法实现e = np.random.random((2, 3))print('e:',e)print('e.sum:%s e.max:%s e.min:%s'%(e.sum(),e.max(),e.min())) [1 1 1] float64 c [ 1. 2.57079633 4.14159265] c dtype name float64 [ 0.54030231+0.84147098j -0.84147098+0.54030231j -0.54030231-0.84147098j] d dtype name complex128 e: [[ 0.48800489 0.55796234 0.16459517] [ 0.40017365 0.41096132 0.01136693]] e.sum:2.03306428928 e.max:0.557962337045 e.min:0.0113669291358 1234567891011# 这些运算默认应用到数组好像它就是一个数字组成的列表，无关数组的形状。然而，指定axis参数你可以吧运算应用到数组指定的轴上：b = np.arange(12).reshape(3, 4)print('b',b)print('b.sum(axis=0):',b.sum(axis=0)) # sum of each columnprint('b.sum(axis=1):',b.sum(axis=1))# sum of each rowprint('b.min(axis=0):',b.min(axis=0))print('b.min(axis=1):',b.min(axis=1))print('b.max(axis=0):',b.max(axis=0))print('b.max(axis=1):',b.max(axis=1))print('b.cumsum(axis=0):',b.cumsum(axis=0))print('b.cumsum(axis=1):',b.cumsum(axis=1)) b [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] b.sum(axis=0): [12 15 18 21] b.sum(axis=1): [ 6 22 38] b.min(axis=0): [0 1 2 3] b.min(axis=1): [0 4 8] b.max(axis=0): [ 8 9 10 11] b.max(axis=1): [ 3 7 11] b.cumsum(axis=0): [[ 0 1 2 3] [ 4 6 8 10] [12 15 18 21]] b.cumsum(axis=1): [[ 0 1 3 6] [ 4 9 15 22] [ 8 17 27 38]] 通用函数(ufunc)NumPy提供常见的数学函数如sin,cos和exp。在NumPy中，这些叫作“通用函数”(ufunc)。在NumPy里这些函数作用按数组的元素运算，产生一个数组作为输出。 1234567B = np.arange(3)print(B)print(np.exp(B))print(np.sqrt(B))C = np.array([2., -1., 4.])d = np.add(B, C)print(d) [0 1 2] [ 1. 2.71828183 7.3890561 ] [ 0. 1. 1.41421356] [ 2. 0. 6.] 更多函数all, alltrue, any, apply along axis, argmax, argmin, argsort, average, bincount, ceil, clip, conj, conjugate, corrcoef, cov, cross, cumprod, cumsum, diff, dot, floor, inner, inv, lexsort, max, maximum, mean, median, min, minimum, nonzero, outer, prod, re, round, sometrue, sort, std, sum, trace, transpose, var, vdot, vectorize, where 索引，切片和迭代(Indexing, Slicing and Iterating)一维 数组可以被索引、切片和迭代，就像 列表 和其它Python序列。一维数组的索引：与Python的列表索引功能相似 123456789a = np.arange(10)**2print(a)print(a[2])print(a[2:5])a[:6:2]=-1000 # equivalent to a[0:6:2] = -1000; from start to position 6, exclusive, set every 2nd element to -1000print(a)print(a[::-1])#reverse afor i in a: print(i**(1/3.)) [ 0 1 4 9 16 25 36 49 64 81] 4 [ 4 9 16] [-1000 1 -1000 9 -1000 25 36 49 64 81] [ 81 64 49 36 25 -1000 9 -1000 1 -1000] nan 1.0 nan 2.08008382305 nan 2.92401773821 3.30192724889 3.65930571002 4.0 4.32674871092 /opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in power if __name__ == &apos;__main__&apos;: 多维 数组可以每个轴有一个索引。这些索引由一个逗号分割的元组给出。(二维数组: 纵轴,横轴)多维数组的索引：123arr[r1:r2, c1:c2]arr[1,1] 等价 arr[1][1][:] 代表某个维度的数据 123456789def f(x, y): return 10*x+yb = fromfunction(f, (5, 4), dtype=int32)print(b)print('b[2,3]:',b[2,3])print('b[0:5, 1]:',b[0:5, 1])# each row in the second column of bprint('b[:, 1]:',b[:, 1])# equivalent to the previous exampleprint('b[1:3, : ]:',b[1:3, :])# each column in the second and third row of bprint('b[-1]:',b[-1])# 当少于轴数的索引被提供时，确失的索引被认为是整个切片：b[-1,:] [[ 0 1 2 3] [10 11 12 13] [20 21 22 23] [30 31 32 33] [40 41 42 43]] b[2,3]: 23 b[0:5, 1]: [ 1 11 21 31 41] b[:, 1]: [ 1 11 21 31 41] b[1:3, : ]: [[10 11 12 13] [20 21 22 23]] b[-1]: [40 41 42 43] b[i]中括号中的表达式被当作i和一系列:，来代表剩下的轴。NumPy也允许你使用“点”像b[i,...]。 dot(…)代表许多产生一个完整的索引元组必要的分号。如果x是秩为5的数组(即它有5个轴)，那么: x[1,2,…] 等同于 x[1,2,:,:,:], x[…,3] 等同于 x[:,:,:,:,3] x[4,…,5,:] 等同 x[4,:,:,5,:]. 1234567c = np.array([[[0, 1, 2], [10, 12, 13]], [[100, 101, 102], [110, 111, 112]]])# a 3d array (two stacked 2D array)print(c.shape)print(c[1,...])# same as c[1,:,:] or c[1]print(c[...,2])# same as c[:,:,2] (2, 2, 3) [[100 101 102] [110 111 112]] [[ 2 13] [102 112]] 12345678910# 迭代多维数组是就第一个轴而言的:for i in b: print(i)for i in c: print(i)# 然而，如果一个人想对每个数组中元素进行运算，我们可以使用flat属性，该属性是数组元素的一个迭代器:for element in b.flat: print(element)for element in c.flat: print(element) [0 1 2 3] [10 11 12 13] [20 21 22 23] [30 31 32 33] [40 41 42 43] [[ 0 1 2] [10 12 13]] [[100 101 102] [110 111 112]] 0 1 2 3 10 11 12 13 20 21 22 23 30 31 32 33 40 41 42 43 0 1 2 10 12 13 100 101 102 110 111 112 更多操作 Indexing, Indexing (reference), newaxis, ndenumerate, indices 形状操作（shape manipulation）改变数组的形状一个数组的形状由它每个轴上的元素个数给出： 12345678910111213a = np.floor(10*np.random.random((3,4)))print(a)print(a.shape)# 一个数组的形状可以被多种命令修改, 下面三个命令返回的修改后的array,并不是修改原来的arrayprint('a.ravel():',a.ravel()) # return the flatten arrayprint('orginal a:',a)print('a.reshape(6, 2):',a.reshape(6, 2))# returns the array with a modified shapeprint('orginal a:',a)print('a.T:',a.T)# return the array, transposedprint('orginal a:',a)print('a.T.shape:',a.T.shape)print('a.shape:',a.shape)print('a.transpose():',a.transpose()) [[ 2. 6. 1. 9.] [ 3. 9. 1. 5.] [ 2. 4. 0. 2.]] (3, 4) a.ravel(): [ 2. 6. 1. 9. 3. 9. 1. 5. 2. 4. 0. 2.] orginal a: [[ 2. 6. 1. 9.] [ 3. 9. 1. 5.] [ 2. 4. 0. 2.]] a.reshape(6, 2): [[ 2. 6.] [ 1. 9.] [ 3. 9.] [ 1. 5.] [ 2. 4.] [ 0. 2.]] orginal a: [[ 2. 6. 1. 9.] [ 3. 9. 1. 5.] [ 2. 4. 0. 2.]] a.T: [[ 2. 3. 2.] [ 6. 9. 4.] [ 1. 1. 0.] [ 9. 5. 2.]] orginal a: [[ 2. 6. 1. 9.] [ 3. 9. 1. 5.] [ 2. 4. 0. 2.]] a.T.shape: (4, 3) a.shape: (3, 4) a.transpose(): [[ 2. 3. 2.] [ 6. 9. 4.] [ 1. 1. 0.] [ 9. 5. 2.]] 由ravel()展平的数组元素的顺序通常是“C风格”的，就是说，最右边的索引变化得最快，所以元素a[0,0]之后是a[0,1]。如果数组被改变形状(reshape)成其它形状，数组仍然是“C风格”的。NumPy通常创建一个以这个顺序保存数据的数组，所以ravel()将总是不需要复制它的参数。但是如果数组是通过切片其它数组或有不同寻常的选项时，它可能需要被复制。函数reshape()和ravel()还可以被同过一些可选参数构建成FORTRAN风格的数组，即最左边的索引变化最快。 reshape函数改变参数形状并返回它，而resize函数改变数组自身,原本函数。如果在改变形状操作中一个维度被给做-1，其维度将自动被计算 1234567print('original a', a)b = a.reshape(2, 6)print('b:',b)print('original a:',a)a.resize((6,2))print(a)print(a.reshape(4, -1)) original a [[ 2. 6.] [ 1. 9.] [ 3. 9.] [ 1. 5.] [ 2. 4.] [ 0. 2.]] b: [[ 2. 6. 1. 9. 3. 9.] [ 1. 5. 2. 4. 0. 2.]] original a: [[ 2. 6.] [ 1. 9.] [ 3. 9.] [ 1. 5.] [ 2. 4.] [ 0. 2.]] [[ 2. 6.] [ 1. 9.] [ 3. 9.] [ 1. 5.] [ 2. 4.] [ 0. 2.]] [[ 2. 6. 1.] [ 9. 3. 9.] [ 1. 5. 2.] [ 4. 0. 2.]] See alsondarray.shape, reshape, resize, ravel 组合(stack)不同的数组几种方法可以沿不同轴将数组堆叠在一起 12345678910a = np.floor(10*np.random.random((2, 2)))print('original a:',a)b = np.floor(1*np.random.random((2,2)))print('original b:', b)# stack two arraies on vertical row_stack函数，另一方面，将一维数组以行组合成二维数组。print(np.vstack((a, b)))#stack two arraies on horizon # 对那些维度比二维更高的数组，hstack沿着第二个轴组合，vstack沿着第一个轴组合,concatenate允许可选参数给出组合时沿着的轴。print(np.hstack((a, b))) original a: [[ 4. 3.] [ 4. 5.]] original b: [[ 0. 0.] [ 0. 0.]] [[ 4. 3.] [ 4. 5.] [ 0. 0.] [ 0. 0.]] [[ 4. 3. 0. 0.] [ 4. 5. 0. 0.]] 12345678910# 函数column_stack以列将一维数组合成二维数组，它等同与vstack对一维数组。#print(np.column_stack((a, b))) # with 2D arraysa = np.array([2., 4.])b = np.array([3., 8.])print('np.column_stack((a, b)):',np.column_stack((a, b))) # return with 2D arrays 二维数组 一列列合并 旧列作为新行print('np.hstack((a, b)):',np.hstack((a, b))) # return a different result 行合并print('np.vstack((a, b)):',np.vstack((a, b)))# return a different array 列合并 print('a[:, newaxis]:',a[:,newaxis]) # 允许生成2维数组print('np.column_stack((a[:,newaxis],b[:,newaxis])):',np.column_stack((a[:,newaxis],b[:,newaxis])))print(np.hstack((a[:,newaxis], b[:,newaxis])))# result is same np.column_stack((a, b)): [[ 2. 3.] [ 4. 8.]] np.hstack((a, b)): [ 2. 4. 3. 8.] np.vstack((a, b)): [[ 2. 4.] [ 3. 8.]] a[:, newaxis]: [[ 2.] [ 4.]] np.column_stack((a[:,newaxis],b[:,newaxis])): [[ 2. 3.] [ 4. 8.]] [[ 2. 3.] [ 4. 8.]] 1234# Note# 在复杂情况下，r_[]和c_[]对创建沿着一个方向组合的数很有用，它们允许范围符号(“:”):# 当使用数组作为参数时，r_和c_的默认行为和vstack和hstack很像，但是允许可选的参数给出组合所沿着的轴的代号。print(np.r_[1:4,0,4]) [1 2 3 0 4] 将一个数组分割(split)成几个小数组使用hsplit你能将数组沿着它的水平轴分割，或者指定返回相同形状数组的个数，或者指定在哪些列后发生分割: 12345a=np.floor(10*np.random.random((2, 12)))print(a)print(np.hsplit(a, 3)) # Split a into 3print(np.hsplit(a, (3, 4)))# Split a after the third and the fourth column# vsplit沿着纵向的轴分割，array split允许指定沿哪个轴分割。 [[ 4. 9. 3. 1. 3. 6. 7. 6. 4. 3. 2. 8.] [ 4. 0. 2. 1. 0. 1. 1. 7. 8. 1. 8. 4.]] [array([[ 4., 9., 3., 1.], [ 4., 0., 2., 1.]]), array([[ 3., 6., 7., 6.], [ 0., 1., 1., 7.]]), array([[ 4., 3., 2., 8.], [ 8., 1., 8., 4.]])] [array([[ 4., 9., 3.], [ 4., 0., 2.]]), array([[ 1.], [ 1.]]), array([[ 3., 6., 7., 6., 4., 3., 2., 8.], [ 0., 1., 1., 7., 8., 1., 8., 4.]])] 复制与视图（Copies and Views）当运算和处理数组时，它们的数据有时被拷贝到新的数组有时不是。这通常是新手的困惑之源。这有三种情况: 完全不拷贝简单的赋值不拷贝数组对象或它们的数据。 12345678910a = np.arange(12)b = a # no new object is createdprint(b is a) # a and b are two names for the same ndarray objectb.shape = 3,4print(a.shape)# Python 传递不定对象作为参考，所以函数调用不拷贝数组。def f(x): print(id(x))print(id(a)) # id is a unique identifier of an objectprint(f(a)) True (3, 4) 139751323982384 139751323982384 None 视图(view)和浅复制不同的数组对象分享同一个数据。视图方法view()创造一个新的数组对象指向同一数据。 12345678910111213print(a)c = a.view()print('c:',c)print('c is a:',c is a)print('c.base is a:',c.base is a) # c is a view of the data owned by aprint('c.flags.owndata:',c.flags.owndata)print('a.flags.owndata:',c.flags.owndata)c.shape=2,6print(c.shape)print(a.shape)c[0,4] = 1234print('c:',c)print('a:',a) [[ 0 1 2 3] [1234 5 6 7] [ 8 9 10 11]] c: [[ 0 1 2 3] [1234 5 6 7] [ 8 9 10 11]] c is a: False c.base is a: True c.flags.owndata: False a.flags.owndata: False (2, 6) (3, 4) c: [[ 0 1 2 3 1234 5] [ 6 7 8 9 10 11]] a: [[ 0 1 2 3] [1234 5 6 7] [ 8 9 10 11]] 123456# 切片数组返回它的一个视图：s = a[:, 1:3]# spaces add for clarity; could be also wirtten as \"s=[:,1:3]\"print(s)s[:]=10 # s[:] is a view of s. Note the difference between s=10 and s[:]=10print(s)print(a) [[10 10] [10 10] [10 10]] [[10 10] [10 10] [10 10]] [[10 10 10 3] [10 10 10 7] [10 10 10 11]] 深复制(deep copy)这个复制方法完全复制数组和它的数据。 12345678print(a)d = a.copy() # a new array object createdprint(d)print(d is a)print(d.base is a)d[0, 0] = 999print(d)print(a) [[10 10 10 3] [10 10 10 7] [10 10 10 11]] [[10 10 10 3] [10 10 10 7] [10 10 10 11]] False False [[999 10 10 3] [ 10 10 10 7] [ 10 10 10 11]] [[10 10 10 3] [10 10 10 7] [10 10 10 11]] 函数和方法(method)总览这是个NumPy函数和方法分类排列目录。这些名字链接到NumPy示例,你可以看到这些函数起作用 创建数组 arange, array, copy, empty, empty_like, eye, fromfile, fromfunction, identity, linspace, logspace, mgrid, ogrid, ones, ones_like, r, zeros, zeros_like 转化 ndarray.astype, atleast_1d, atleast_2d, atleast_3d, mat 操作 array_split, column_stack, concatenate, diagonal, dsplit, dstack, hsplit, hstack, ndarray.item, newaxis, ravel, repeat, reshape, resize, squeeze, swapaxes, take, transpose, vsplit, vstack 询问 all, any, nonzero, where 排序 argmax, argmin, argsort, max, min, ptp, searchsorted, sort 运算 choose, compress, cumprod, cumsum, inner, fill, imag, prod, put, putmask, real, sum 基本统计 cov, mean, std, var 基本线性代数 cross, dot, outer, svd, vdot 进阶广播法则(rule) 广播法则能使通用函数有意义地处理不具有相同形状的输入。 广播第一法则是，如果所有的输入数组维度不都相同，一个“1”将被重复地添加在维度较小的数组上直至所有的数组拥有一样的维度。 广播第二法则确定长度为1的数组沿着特殊的方向表现地好像它有沿着那个方向最大形状的大小。对数组来说，沿着那个维度的数组元素的值理应相同。 应用广播法则之后，所有数组的大小必须匹配。更多网址 We will add the vector v to each row of the matrix x,storing the result in the matrix y 对两个数组使用广播机制要遵守下列规则： 如果数组的秩不同，使用1来将秩较小的数组进行扩展，直到两个数组的尺寸的长度都一样。 如果两个数组在某个维度上的长度是一样的，或者其中一个数组在该维度上长度为1，那么我们就说这两个数组在该维度上是相容的。 如果两个数组在所有维度上都是相容的，他们就能使用广播。 如果两个输入数组的尺寸不同，那么注意其中较大的那个尺寸。因为广播之后，两个数组的尺寸将和那个较大的尺寸一样。 在任何一个维度上，如果一个数组的长度为1，另一个数组长度大于1，那么在该维度上，就好像是对第一个数组进行了复制 花哨的索引和索引技巧NumPy比普通Python序列提供更多的索引功能。除了索引整数和切片，正如我们之前看到的，数组可以被整数数组和布尔数组索引。 通过数组索引123456a = np.arange(12)**2print('a:',a)i = np.array([1, 1, 3, 7, 8])print(a[i]) # the elements of a at the positions ij = np.array([[3, 5],[7, 9]]) # a bidimensional array of indicesprint(a[j]) # the same shape as j a: [ 0 1 4 9 16 25 36 49 64 81 100 121] [ 1 1 9 49 64] [[ 9 25] [49 81]] 12345678910# 当被索引数组a是多维的时，每一个唯一的索引数列指向a的第一维。以下示例通过将图片标签用调色版转换成色彩图像展示了这种行为。palette = np.array([[0,0,0], #black [255,0,0],#red [0, 255, 0],#green [0,0,255], # blue [255,255, 2555]#white ])image = np.array([[0, 1, 2, 0], [0, 3, 4, 0]]) # each value corresponds to a color in the paletteprint(palette[image]) # the color(2, 4, 3) color image array([[[ 0, 0, 0], [ 255, 0, 0], [ 0, 255, 0], [ 0, 0, 0]], [[ 0, 0, 0], [ 0, 0, 255], [ 255, 255, 2555], [ 0, 0, 0]]]) 12345678910# 我们也可以给出不不止一维的索引，每一维的索引数组必须有相同的形状。a = np.arange(12).reshape(3, 4)print(a)i = np.array([[0, 1], [1, 2]])j = np.array([[2, 1], [3, 3]])print('a[i,j]:',a[i,j])print('a[i,2]:',a[i,2])print('a[:,j]:',a[:,j]) [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] a[i,j]: [[ 2 5] [ 7 11]] a[i,2]: [[ 2 6] [ 6 10]] a[:,j]: [[[ 2 1] [ 3 3]] [[ 6 5] [ 7 7]] [[10 9] [11 11]]] 123456# 自然，我们可以把i和j放到序列中(比如说列表)然后通过list索引。l = [i,j]print(l)# 然而，我们不能把i和j放在一个数组中，因为这个数组将被解释成索引a的第一维。s = array( [i,j] ) # not we wanta[s] [array([[0, 1], [1, 2]]), array([[2, 1], [3, 3]])] 123456789101112# 另一个常用的数组索引用法是搜索时间序列最大值time = np.linspace(20, 145, 5) # time scaledata = np.sin(arange(20)).reshape(5,4) # 4 time-dependent seriesprint(time)print(data)ind = data.argmax(axis=0) # index of the maxima for each seriesprint(ind)time_max = time[ind] # times corresponding to the maximaprint(time_max)data_max = data[ind, range(data.shape[1])] # =&gt; data[ind[0],0], data[ind[1],1]...print(data_max)print(np.all(data_max == data.max(axis=0))) [ 20. 51.25 82.5 113.75 145. ] [[ 0. 0.84147098 0.90929743 0.14112001] [-0.7568025 -0.95892427 -0.2794155 0.6569866 ] [ 0.98935825 0.41211849 -0.54402111 -0.99999021] [-0.53657292 0.42016704 0.99060736 0.65028784] [-0.28790332 -0.96139749 -0.75098725 0.14987721]] [2 0 3 1] [ 82.5 20. 113.75 51.25] [ 0.98935825 0.84147098 0.99060736 0.6569866 ] True 1234567891011121314# 你也可以使用数组索引作为目标来赋值：a = np.arange(5)print(a)a[[1,3,4]] = 0print(a)# 然而，当一个索引列表包含重复时，赋值被多次完成，保留最后的值：a = np.arange(5)a[[0,0,2]]=[1,2,3]print(a)# 这足够合理，但是小心如果你想用Python的+=结构，可能结果并非你所期望：a = np.arange(5)a[[0,0,2]]+=1print(a)# 即使0在索引列表中出现两次，索引为0的元素仅仅增加一次。这是因为Python要求a+=1和a=a+1等同。 [0 1 2 3 4] [0 0 2 0 0] [2 1 3 3 4] [1 1 3 3 4] 通过布尔数组索引当我们使用整数数组索引数组时，我们提供一个索引列表去选择。通过布尔数组索引的方法是不同的我们显式地选择数组中我们想要和不想要的元素。 我们能想到的使用布尔数组的索引最自然方式就是使用和原数组一样形状的布尔数组。 1234567a = np.arange(12).reshape(3, 4)b = a &gt; 4print(b) # b is a boolean with a's shapeprint(a[b]) # 1d array with the selected elements# 这个属性在赋值时非常有用：a[b] = 0 # All elements of 'a' higher than 4 become 0print(a) [[False False False False] [False True True True] [ True True True True]] [ 5 6 7 8 9 10 11] [[0 1 2 3] [4 0 0 0] [0 0 0 0]] 12345678910111213141516171819# 例子import numpy as npimport matplotlib.pyplot as pltdef mandelbrot(h,w, maxit=20): \"\"\"Returns an image of the Mandelbrot fractal of size (h,w).\"\"\" y,x = np.ogrid[ -1.4:1.4:h*1j, -2:0.8:w*1j ] c = x+y*1j z = c divtime = maxit + np.zeros(z.shape, dtype=int) for i in range(maxit): z = z**2 + c diverge = z*np.conj(z) &gt; 2**2 # who is diverging div_now = diverge &amp; (divtime==maxit) # who is diverging now divtime[div_now] = i # note when z[diverge] = 2 # avoid diverging too much return divtimeplt.imshow(mandelbrot(400,400))plt.show() 第二种通过布尔来索引的方法更近似于整数索引；对数组的每个维度我们给一个一维布尔数组来选择我们想要的切片。 12345678a = np.arange(12).reshape(3, 4)b1 = np.array([False,True,True]) # first dim selectionb2 = np.array([True,False,True,False]) # second dim selectionprint(a)print('a[b1,:]:',a[b1,:])print('a[b1]:',a[b1])print('a[:,b2]:',a[:,b2]) # selecting columnsprint('a[b1,b2]:',a[b1,b2]) [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] a[b1,:]: [[ 4 5 6 7] [ 8 9 10 11]] a[b1]: [[ 4 5 6 7] [ 8 9 10 11]] a[:,b2]: [[ 0 2] [ 4 6] [ 8 10]] a[b1,b2]: [ 4 10] 12345import numpy as npx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])v = np.array([1, 0, 1])y = x + v # Add v to each row of x using broadcastingprint(y) [[ 2 2 4] [ 5 5 7] [ 8 8 10] [11 11 13]] ix_()函数ix_函数可以为了获得多元组的结果而用来结合不同向量。例如，如果你想要用所有向量a、b和c元素组成的三元组来计算a+b*c： 123456789101112a = np.array([2,3,4,5])b = np.array([8,5,4])c = np.array([5,4,6,8,3])ax,bx,cx = np.ix_(a,b,c)print('ax:',ax)print('bx:',bx)print('cx:',cx)print(ax.shape, bx.shape, cx.shape)result = ax+bx*cxprint('result:',result)print(result[3,2,4])print(a[3]+b[2]*c[4]) ax: [[[2]] [[3]] [[4]] [[5]]] bx: [[[8] [5] [4]]] cx: [[[5 4 6 8 3]]] (4, 1, 1) (1, 3, 1) (1, 1, 5) result: [[[42 34 50 66 26] [27 22 32 42 17] [22 18 26 34 14]] [[43 35 51 67 27] [28 23 33 43 18] [23 19 27 35 15]] [[44 36 52 68 28] [29 24 34 44 19] [24 20 28 36 16]] [[45 37 53 69 29] [30 25 35 45 20] [25 21 29 37 17]]] 17 17 12345678910# 你也可以实行如下简化：def ufunc_reduce(ufct, *vectors): vs = ix_(*vectors) r = ufct.identity for v in vs: r = ufct(r,v) return r# 然后这样使用它：ufunc_reduce(add,a,b,c)# 这个reduce与ufunc.reduce(比如说add.reduce)相比的优势在于它利用了广播法则，避免了创建一个输出大小乘以向量个数的参数数组。 array([[[15, 14, 16, 18, 13], [12, 11, 13, 15, 10], [11, 10, 12, 14, 9]], [[16, 15, 17, 19, 14], [13, 12, 14, 16, 11], [12, 11, 13, 15, 10]], [[17, 16, 18, 20, 15], [14, 13, 15, 17, 12], [13, 12, 14, 16, 11]], [[18, 17, 19, 21, 16], [15, 14, 16, 18, 13], [14, 13, 15, 17, 12]]]) 用字符串索引See Structured arrays. 线性代数简单数组运算参考numpy文件夹中的linalg.py获得更多信息 12345678910111213import numpy as npa = np.array([[1.0, 2.0], [3.0, 4.0]])print(a)print(a.T)print(np.linalg.inv(a))u = np.eye(2) # unit 2x2 matrix; \"eye\" represents \"I\"print(u)j = np.array([[0.0, -1.0], [1.0, 0.0]])print(np.dot (j, j)) # matrix productprint(np.trace(u))y = np.array([[5.], [7.]])print(np.linalg.solve(a, y))print(np.linalg.eig(j)) [[ 1. 2.] [ 3. 4.]] [[ 1. 3.] [ 2. 4.]] [[-2. 1. ] [ 1.5 -0.5]] [[ 1. 0.] [ 0. 1.]] [[-1. 0.] [ 0. -1.]] 2.0 矩阵类这是一个关于矩阵类的简短介绍。 123456789A = np.matrix('1.0 2.0;3.0 4.0')print(A)print(type(A))print(A.T) # transposeX = np.matrix('5.0 7.0')Y = X.Tprint(Y)print(A*Y) # matrix multiplicationprint(A.I) # inverse [[ 1. 2.] [ 3. 4.]] &lt;class &apos;numpy.matrixlib.defmatrix.matrix&apos;&gt; [[ 1. 3.] [ 2. 4.]] [[ 5.] [ 7.]] [[ 19.] [ 43.]] [[-2. 1. ] [ 1.5 -0.5]] 索引：比较矩阵和二维数组注意NumPy中数组和矩阵有些重要的区别。NumPy提供了两个基本的对象：一个N维数组对象和一个通用函数对象。其它对象都是建构在它们之上的。特别的，矩阵是继承自NumPy数组对象的二维数组对象。对数组和矩阵，索引都必须包含合适的一个或多个这些组合：整数标量、省略号(ellipses)、整数列表;布尔值，整数或布尔值构成的元组，和一个一维整数或布尔值数组。矩阵可以被用作矩阵的索引，但是通常需要数组、列表或者其它形式来完成这个任务。 像平常在Python中一样，索引是从0开始的。传统上我们用矩形的行和列表示一个二维数组或矩阵，其中沿着0轴的方向被穿过的称作行，沿着1轴的方向被穿过的是列。 让我们创建数组和矩阵用来切片： 1234567A=np.arange(12)print(A)A.shape=(3, 4)print(A)M = np.mat(A.copy())print(M)print(type(A), type(M)) [ 0 1 2 3 4 5 6 7 8 9 10 11] [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] &lt;class &apos;numpy.ndarray&apos;&gt; &lt;class &apos;numpy.matrixlib.defmatrix.matrix&apos;&gt; 现在，让我们简单的切几片。基本的切片使用切片对象或整数。例如，A[:]和M[:]的求值将表现得和Python索引很相似。然而要注意很重要的一点就是NumPy切片数组不创建数据的副本;切片提供统一数据的视图。 12345print(A[:],A[:].shape)print(M[:],M[:].shape)# 现在有些和Python索引不同的了：你可以同时使用 逗号 分割索引来沿着多个轴索引。print(A[:,1]); print(A[:,1].shape)print(M[:,1]); print(M[:,1].shape) [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] (3, 4) [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] (3, 4) [1 5 9] (3,) [[1] [5] [9]] (3, 1) 注意最后两个结果的不同。对二维数组使用一个冒号产生一个一维数组，然而矩阵产生了一个二维矩阵。例如，一个M[2,:]切片产生了一个形状为(1,4)的矩阵，相比之下，一个数组的切片总是产生一个最低可能维度11的数组。例如，如果C是一个三维数组，C[...,1]产生一个二维的数组而C[1,:,1]产生一个一维数组。从这时开始，如果相应的矩阵切片结果是相同的话，我们将只展示数组切片的结果。 假如我们想要一个数组的第一列和第三列，一种方法是使用列表切片： 12345678910print(A[:,[1,3]])#稍微复杂点的方法是使用take()方法(method):print(A[:,].take([1,3],axis=1))# 如果我们想跳过第一行，我们可以这样：print(A[1:,].take([1,3],axis=1))# 或者我们仅仅使用A[1:,[1,3]]。还有一种方法是通过矩阵向量积(叉积)。print(A[ix_((1,2),(1,3))])# 现在让我们做些更复杂的。比如说我们想要保留第一行大于1的列。一种方法是创建布尔索引：print(A[0,:]&gt;1)print(A[:,A[0,:]&gt;1]) [[ 1 3] [ 5 7] [ 9 11]] [[ 1 3] [ 5 7] [ 9 11]] [[ 5 7] [ 9 11]] [[ 5 7] [ 9 11]] [False False True True] [[ 2 3] [ 6 7] [10 11]] 12345678910111213# 就是我们想要的！但是索引矩阵没这么方便。M = np.mat(A.copy())print(M[0,:]&gt;1)# print(M[:,M[0,:]&gt;1])# 这个过程的问题是用“矩阵切片”来切片产生一个矩阵12，但是矩阵有个方便的A属性，它的值是数组呈现的。所以我们仅仅做以下替代：print(M[:,M.A[0,1]&gt;1])print(M[:,M.A[0,:]&gt;1])# 如果我们想要在矩阵两个方向有条件地切片，我们必须稍微调整策略，代之以：print(A[A[:,0]&gt;2,A[0,:]&gt;1])print(M[M.A[:,0]&gt;2,M.A[0,:]&gt;1])# 我们需要使用向量积ix_:print(A[ix_(A[:,0]&gt;2,A[0,:]&gt;1)])print(M[ix_(M.A[:,0]&gt;2,M.A[0,:]&gt;1)]) [[False False True True]] [] [[ 2 3] [ 6 7] [10 11]] [ 6 11] [[ 6 11]] [[ 6 7] [10 11]] [[ 6 7] [10 11]] 技巧和提示“自动”改变形状更改数组的维度，你可以省略一个尺寸，它将被自动推导出来。 1234a = np.arange(30)a.shape = 2,-1,3 # -1 means \"whatever is needed\"print(a.shape)print(a) (2, 5, 3) [[[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11] [12 13 14]] [[15 16 17] [18 19 20] [21 22 23] [24 25 26] [27 28 29]]] 向量组合(stacking)我们如何用两个相同尺寸的行向量列表构建一个二维数组？在MATLAB中这非常简单：如果x和y是两个相同长度的向量，你仅仅需要做m=[x;y]。在NumPy中这个过程通过函数column_stack、dstack、hstack和vstack来完成，取决于你想要在那个维度上组合。例如： 123456x=np.arange(0, 10, 2)y = np.arange(5)m = vstack([x,y])print(m)xy = hstack([x,y])print(xy) [[0 2 4 6 8] [0 1 2 3 4]] [0 2 4 6 8 0 1 2 3 4] 直方图(histogram)NumPy中histogram函数应用到一个数组返回一对变量：直方图数组和箱式向量。注意：matplotlib也有一个用来建立直方图的函数(叫作hist,正如matlab中一样)与NumPy中的不同。主要的差别是pylab.hist自动绘制直方图，而numpy.histogram仅仅产生数据。 123456789101112import numpyimport pylab# Build a vector of 10000 normal deviates with variance 0.5^2 and mean 2mu, sigma = 2, 0.5v = numpy.random.normal(mu,sigma,10000)# Plot a normalized histogram with 50 binspylab.hist(v, bins=50, normed=1) # matplotlib version (plot)pylab.show()# Compute the histogram with numpy and then plot it(n, bins) = numpy.histogram(v, bins=50, normed=True) # NumPy version (no plot)pylab.plot(.5*(bins[1:]+bins[:-1]), n)pylab.show()","link":"/2018/09/28/Numpy使用/"},{"title":"goa快速入门","text":"简介配置环境，学习新东西请看官网教程，官网教程，官网教程！重要的事情强调三遍！！！！国内的博客坑实在太多了，耐心点，看英文官方教程！！ 代理go get命令在提取一些工程或依赖时（如golang.org域名）被墙掉。通过使用vpn或代理的方法可以解决。 买VPN 这个就不多说了，买vpn，相当于直接连接。 使用代理 通过shell环境变量 export http_proxy=http://ip:port 通过设置git代理 git config –global http.proxy 'http://127.0.0.1:port' 如果是ssh代理上网，127.0.0.1+ 代理端口号。 go命令go build 加上可以编译的go源文件可以得到一个可执行文件。go install 在编译源代码之后还安装到指定的目录go get 从指定源上面下载或者更新指定的代码和依赖，并对他们进行编译和安装 go build : 编译出可执行文件go install : go build + 把编译后的可执行文件放到GOPATH/bin目录下go get : git clone + go install 以上是一些准备工作。介绍goa, 目的 + 结构 + 使用gao是基于微服务的go语言框架，能够有效帮助开发人员快速开发基于微服务的系统。它通过DSL和代码生成器来生成样板代码和辅助套件(如文档，客户端模块，客户端工具等)。这些生成数据均基于服务的设计描述，goa遵循单一数据源(Single Source of Truth, SSOT)原则，任何对设计的改变，都将自动反映到系统各处。 goa可以分为三个部分： goa的设计语言是内置DSL，用于描述微服务的设计 goa代码生成器，用于根据DSL描述生成代码模块，辅助工具，和文档等 goa利用生成代码和用户代码来实现一个服务，并提供一个完全可插拨的框架 goa的特点： 重视框架设计(Design-Based)，将框架，文档，胶水代码和辅助工具作为一个整体来设计和描述 为用户生成了大量的代码(框架代码，胶水代码，测试代码，客户端工具等等)，上手快速 DSL，代码生成器，用户代码均使用Go语言编写，并且前两者使用plugin实现，可以替换 基于微服务，对RESTful API有非常好的支持，方便构建更高效，易于扩展的HTTP服务器 二. 使用1. 安装 安装 golang 按照golang官网配置go的环境变量；！！ Install goa and goagen: go get -u github.com/goadesign/goa/... 2. DSL服务设计 DesignThe first thing to do when writing a goa service is to describe the API using the goa design language. Create a new directory under $GOPATH/src for the new goa service, for example $GOPATH/src/cellar. In that directory create a design sub directory and the file design/design.go with the following content: 1234567891011121314151617181920212223242526272829303132333435363738394041424344package design // The convention consists of naming the design // package \"design\"import ( . \"github.com/goadesign/goa/design\" // Use . imports to enable the DSL . \"github.com/goadesign/goa/design/apidsl\")var _ = API(\"cellar\", func() { // API defines the microservice endpoint and Title(\"The virtual wine cellar\") // other global properties. There should be one Description(\"A simple goa service\") // and exactly one API definition appearing in Scheme(\"http\") // the design. Host(\"localhost:8080\")})var _ = Resource(\"bottle\", func() { // Resources group related API endpoints BasePath(\"/bottles\") // together. They map to REST resources for REST DefaultMedia(BottleMedia) // services. Action(\"show\", func() { // Actions define a single API endpoint together Description(\"Get bottle by id\") // with its path, parameters (both path Routing(GET(\"/:bottleID\")) // parameters and querystring values) and payload Params(func() { // (shape of the request body). Param(\"bottleID\", Integer, \"Bottle ID\") }) Response(OK) // Responses define the shape and status code Response(NotFound) // of HTTP responses. })})// BottleMedia defines the media type used to render bottles.var BottleMedia = MediaType(\"application/vnd.goa.example.bottle+json\", func() { Description(\"A bottle of wine\") Attributes(func() { // Attributes define the media type shape. Attribute(\"id\", Integer, \"Unique bottle ID\") Attribute(\"href\", String, \"API href for making requests on the bottle\") Attribute(\"name\", String, \"Name of wine\") Required(\"id\", \"href\", \"name\") }) View(\"default\", func() { // View defines a rendering of the media type. Attribute(\"id\") // Media types may have multiple views and must Attribute(\"href\") // have a \"default\" view. Attribute(\"name\") })}) 上面的DSL主要用到的接口： API: 描述一个Service及其地址，协议规范等 Resource: 定义一个资源及其一系列相关的操作(Action)，以及这些操作所共用的一些属性 Action: 定义针对于某个资源的操作，包括方法(GET,POST等)，URL(可有多个)，参数(goa自动做类型检查，值检查等)等 Response: 定义一个响应，包括响应模板和承载内容(payload)，在代码中决定调用那个响应模板 MediaType: 定义Response返回的数据结构，一个Media可以有多个View，可在Response中指定返回的View goa本身DSL设计是RESTful的，通过Go的匿名函数，提供了非常强大的描述能力，如参数定义，参数检查，传输媒体，响应模板等。goa基于服务提供功能，每个API定义一个服务(Service)，每个服务有若干资源(Resource)，每个资源对应若干操作(Action)，每个操作(Action)有多种响应(Response)，每个响应可能返回不同媒介(Media)的不同视图(View)。当然goa提供了更好的层级控制和继承关系(如上例，Response返回的视图继承于Resource中定义的默认媒介(BottleMedia)的默认视图(default))。更详细的DSL设计文档参考goa dsl design和goa dsl api。 3. 生成代码通过goa根据单个DSL文件，即可生成一整套框架代码： 12cd src/cellargoagen bootstrap -d cellar/design goa会生成一堆代码，主要包括四个目录两个文件： app目录: 根据DSL，生成若干类，并将底层的HTTP服务器和DSL中的资源，路由结合起来 client目录: 配套的client包，包含对媒介类型的定义，和对请求响应的编解码 tool目录：根据client包生成的控制台工具，用于模拟客户端发送请求 swagger目录：包含对整个服务(API)的总体描述(Json和Yaml格式) main.go文件：主文件，挂载资源路由(BottleController)，启动服务 bottle.go文件：bottle资源的逻辑处理，即BottleController的Action实现 当改变DSL文件并再次用goagen生成代码时，goagen只会重新生成框架代码(app,client,tool,swagger)，而不会覆盖逻辑代码(main.go和bottle.go以及其它自定义文件)，做到框架与逻辑分离。 得到这些文件之后，我们直接编辑bottle.go，完善bottle资源的Action逻辑即可： 123456789101112131415161718// Show runs the show action.func (c *BottleController) Show(ctx *app.ShowBottleContext) error { // BottleController_Show: start_implement // Put your logic here if ctx.BottleID == 0 { return ctx.NotFound() } bottle := app.GoaExampleBottle{ ID : ctx.BottleID, Name : fmt.Sprintf(\"Bottle #%d\", ctx.BottleID), Href : app.BottleHref(ctx.BottleID), } // BottleController_Show: end_implement return ctx.OK(&amp;bottle)} 至此，服务器就已经设计好了，剩下的HTTP Server，消息编解码，参数检查，路由，响应模板，甚至测试工具，gagen都已经为你做好了。 4. 运行和测试运行服务器： 12345cd src/cellargo build -o cellar./cellar2016/09/20 00:26:41 [INFO] mount ctrl=Bottle action=Show route=GET /bottles/:bottleID2016/09/20 00:26:41 [INFO] listen transport=http addr=:8080 通过curl测试： 123456# 404 NOT FOUNDcurl -i localhost:8080/bottles/0# 200 一个有效的BottleMedia Viewcurl -i localhost:8080/bottles/1# 400 无效参数 得到参数检查错误提示curl -i localhost:8080/bottles/n 通过celler-cli工具测试： 12345678cd src/cellar/tool/cellar-cligo build -o cellar-cli# 使用帮助./cellar-cli # show bottle 命令的用法./cellar-cli show bottle# 发送HTTP请求 cellar-cli中集成了服务的地址信息./cellar-cli show bottle /bottles/1 最终我们只写了几十行的DSL和几行逻辑代码，就得到了一个基于微服务，RESTful风格的HTTP服务器，附以完整的客户端代码，测试工具，甚至服务API描述。更关键的是，这一套环境是SSOT(Single Source of Truth)的，更改一份DSL服务描述文件，整个服务器底层代码，胶水代码，测试环境，甚至API描述都会重新生成(不会影响到已有的逻辑代码)，这让整个服务保持高度一致性和可控性。 最后，以一段goa github上的描述收尾： There are a number of good Go packages for writing modular web services out there so why build another one? Glad you asked! The existing packages tend to focus on providing small and highly modular frameworks that are purposefully narrowly focused. The intent is to keep things simple and to avoid mixing concerns. This is great when writing simple APIs that tend to change rarely. However there are a number of problems that any non trivial API implementation must address. Things like request validation, response media type definitions or documentation are hard to do in a way that stays consistent and flexible as the API surface evolves. goa takes a different approach to building these applications: instead of focusing solely on helping with implementation, goa makes it possible to describe the design of an API in an holistic way. goa then uses that description to provide specialized helper code to the implementation and to generate documentation, API clients, tests, even custom artifacts. 完整示例参考goa learn guide和goa github. 三. 总结goa的优点： 先进的理念：Design-Based, DSL, Micro-Service, RESTful API，Plugins等 DSL，代码生成器，用户代码，辅助工具等一整套环境都用Go实现 一份服务设计(DSL文件)，生成了包括框架代码，辅助(胶水)代码，测试代码，客户端工具等一整套环境(SSOT) 文档齐全，社区活跃度高","link":"/2018/11/30/goa快速入门/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2018/04/16/hello-world/"},{"title":"iOS之LighterviewController","text":"被误解的 MVC 和被神化的 MVVM被误解的 MVCMVC，全称是 Model View Controller，是模型 (model)－视图 (view)－控制器 (controller) 的缩写。它表示的是一种常见的客户端软件开发框架。 Controller 的臃肿问题何解？ 「什么样的内容才应该放到 Controller 中？」 我们来看看 MVC 这种架构的特点。其实设计模式很多时候是为了 Don’t repeat yourself 原则来做的，该原则要求能够复用的代码要尽量复用，来保证重用。在 MVC 这种设计模式中，我们发现 View 和 Model 都是符合这种原则的。 对于 View 来说，你如果抽象得好，那么一个 App 的动画效果可以很方便地移植到别的 App 上，而 Github 上也有很多 UI 控件，这些控件都是在 View 层做了很好的封装设计，使得它能够方便地开源给大家复用。 对于 Model 来说，它其实是用来存储业务的数据的，如果做得好，它也可以方便地复用。比如我当时在做有道云笔记 iPad 版的时候，我们就直接和 iOS 版复用了所有的 Model 层的代码。在创业做猿题库客户端时，iOS 和 iPad 版的 Model 层代码再次被复用上了。当然，因为和业务本身的数据意义相关，Model 层的复用大多数是在一个产品内部，不太可能像 View 层那样开源给社区。 说完 View 和 Model 了，那我们想想 Controller，Controller 有多少可以复用的？我们写完了一个 Controller 之后，可以很方便地复用它吗？结论是：非常难复用。在某些场景下，我们可能可以用 addSubViewController 之类的方式复用 Controller，但它的复用场景还是非常非常少的。 如果我们能够意识到 Controller 里面的代码不便于复用，我们就能知道什么代码应该写在 Controller 里面了，那就是那些不能复用的代码。在我看来，Controller 里面就只应该存放这些不能复用的代码，这些代码包括： 在初始化时，构造相应的 View 和 Model。 监听 Model 层的事件，将 Model 层的数据传递到 View 层。 监听 View 层的事件，并且将 View 层的事件转发到 Model 层。如果 Controller 只有以上的这些代码，那么它的逻辑将非常简单，而且也会非常短。 如何对 ViewController 瘦身？objc.io 是一个非常有名的 iOS 开发博客，它上面的第一课 《Lighter View Controllers》 上就讲了很多这样的技巧，我们先总结一下它里面的观点： 将 UITableView 的 Data Source 分离到另外一个类中。 将数据获取和转换的逻辑分别到另外一个类中。 将拼装控件的逻辑，分离到另外一个类中。 其实 MVC 虽然只有三层，但是它并没有限制你只能有三层。所以，我们可以将 Controller 里面过于臃肿的逻辑抽取出来，形成新的可复用模块或架构层次。 将网络请求抽象到单独的类中新手写代码，直接就在 Controller 里面用 AFNetworking 发一个请求，请求的完数据直接就传递给 View。入门一些的同学，知道把这些请求代码移到另外一个静态类里面。但是我觉得还不够，所以我建议将每一个网络请求直接封装成类。 把每一个网络请求封装成对象其实是使用了设计模式中的 Command 模式，它有以下好处： 将网络请求与具体的第三方库依赖隔离，方便以后更换底层的网络库。实际上我们公司的 iOS 客户端最初是基于 ASIHttpRequest 的，我们只花了两天，就很轻松地切换到了 AFNetworking。 方便在基类中处理公共逻辑，例如猿题库的数据版本号信息就统一在基类中处理。 方便在基类中处理缓存逻辑，以及其它一些公共逻辑。 方便做对象的持久化。 大家如果感兴趣，可以看我们公司开源的 iOS 网络库：YTKNetwork。它在这种思考的指导下，不但将 Controller 中的代码瘦身，而且进一步演化和加强，现在它还支持诸如复杂网络请求管理，断点续传，插件机制，JSON 合法性检查等功能。这部分代码从 Controller 中剥离出来后，不但简化了 Controller 中的逻辑，也达到了网络层的代码复用的效果。 将界面的拼装抽象到专门的类中新手写代码，喜欢在 Controller 中把一个个 UILabel ，UIButton，UITextField 往 self.view 上用 addSubView 方法放。我建议大家可以用两种办法把这些代码从 Controller 中剥离。 方法一：构造专门的 UIView 的子类，来负责这些控件的拼装。这是最彻底和优雅的方式，不过稍微麻烦一些的是，你需要把这些控件的事件回调先接管，再都一一暴露回 Controller。 方法二：用一个静态的 Util 类，帮助你做 UIView 的拼装工作。这种方式稍微做得不太彻底，但是比较简单。 对于一些能复用的 UI 控件，我建议用方法一。如果项目工程比较复杂，我也建议用方法一。如果项目太紧，另外相关项目的代码量也不多，可以尝试方法二。 构造 ViewModel谁说 MVC 就不能用 ViewModel 的？MVVM 的优点我们一样可以借鉴。具体做法就是将 ViewController 给 View 传递数据这个过程，抽象成构造 ViewModel 的过程。 这样抽象之后，View 只接受 ViewModel，而 Controller 只需要传递 ViewModel 这么一行代码。而另外构造 ViewModel 的过程，我们就可以移动到另外的类中了。 在具体实践中，我建议大家专门创建构造 ViewModel 工厂类，参见 工厂模式。另外，也可以专门将数据存取都抽将到一个 Service 层，由这层来提供 ViewModel 的获取。 专门构造存储类刚刚说到 ViewModel 的构造可以抽奖到一个 Service 层。与此相应的，数据的存储也应该由专门的对象来做。在小猿搜题项目中，我们由一个叫 UserAgent 的类，专门来处理本地数据的存取。 数据存取放在专门的类中，就可以针对存取做额外的事情了。比如： 对一些热点数据增加缓存 处理数据迁移相关的逻辑如果要做得更细，可以把存储引擎再抽象出一层。这样你就可以方便地切换存储的底层，例如从 sqlite 切换到 key-value 的存储引擎等。 小结通过代码的抽取，我们可以将原本的 MVC 设计模式中的 ViewController 进一步拆分，构造出 网络请求层、ViewModel 层、Service 层、Storage 层等其它类，来配合 Controller 工作，从而使 Controller 更加简单，我们的 App 更容易维护。 被神化的 MVVMMVVM 是 Model-View-ViewModel 的简写。 MVVM 在使用当中，通常还会利用双向绑定技术，使得 Model 变化时，ViewModel 会自动更新，而 ViewModel 变化时，View 也会自动变化。所以，MVVM 模式有些时候又被称作：model-view-binder 模式。具体在 iOS 中，可以使用 KVO 或 Notification 技术达到这种效果。 第一点：数据绑定使得 Bug 很难被调试。你看到界面异常了，有可能是你 View 的代码有 Bug，也可能是 Model 的代码有问题。数据绑定使得一个位置的 Bug 被快速传递到别的位置，要定位原始出问题的地方就变得不那么容易了。 第二点：对于过大的项目，数据绑定需要花费更多的内存。 某种意义上来说，我认为就是数据绑定使得 MVVM 变得复杂和难用了。但是，这个缺点同时也被很多人认为是优点。 ReactiveCocoa函数式编程（Functional Programming）和响应式编程（React Programming）也是当前很火的两个概念，它们的结合可以很方便地实现数据的绑定。 函数式编程（Functional Programming），函数也变成一等公民了，可以拥有和对象同样的功能，例如当成参数传递，当作返回值等。看看 Swift 语言带来的众多函数式编程的特性，就你知道这多 Cool 了。 响应式编程（React Programming），原来我们基于事件（Event）的处理方式都弱了，现在是基于输入（在 ReactiveCocoa 里叫 Signal）的处理方式。输入还可以通过函数式编程进行各种 Combine 或 Filter，尽显各种灵活的处理。 无状态（Stateless），状态是函数的魔鬼，无状态使得函数能更好地测试。 不可修改（Immutable），数据都是不可修改的，使得软件逻辑简单，也可以更好地测试。 而我想说，我们需要保持的是一个拥抱变化的心，以及理性分析的态度。在新技术的面前，不盲从，也不守旧，一切的决策都应该建立在认真分析的基础上，这样才能应对技术的变化。","link":"/2019/05/07/iOS之LighterviewController/"},{"title":"TensorflowFederatedGuide","text":"OverviewTensorFlow联合（TFF）平台由两层组成： 联合学习（FL），将现有Keras或非Keras机器学习模型插入TFF框架的高级接口。您可以执行基本任务，例如联合训练或评估，而无需研究联合学习算法的详细节。 联合核心（FC），通过在强类型函数编程环境中将TensorFlow与分布式通信运算符相结合，简洁地表达自定义联合算法的低级接口。 首先阅读以下教程，使用实际示例向您介绍主要的TFF概念和API。确保按照安装说明配置环境以与TFF一起使用。 用图像分类的联合学习介绍了联合学习（FL）API的关键部分，并演示了如何使用TFF模拟联合类MNIST数据的联合学习。 用于文本生成的联合学习进一步演示了如何使用TFF的FL API来优化语言建模任务的序列化预训练模型。 自定义联合算法，第1部分：联合核心简介和第2部分：实现联合平均，介绍了联合核心API（FC API）提供的关键概念和接口，并演示了如何实现简单的联合平均训练算法以及如何执行联合评估。 Installation1234virtualenv --python python3 \"venv\"source \"venv/bin/activate\"pip install --upgrade pippip install --upgrade tensorflow_federated Federated LearningOverview本文档介绍了促进联合学习任务的接口，例如联合训练或使用TensorFlow中实现的现有机器学习模型进行评估。在设计这些接口时，我们的主要目标是使其能够在不需要了解其工作原理的情况下进行联邦学习的实验，并评估在各种现有模型和数据上实施的联合学习算法。我们鼓励您回馈该平台。 TFF的设计考虑了可扩展性和可组合性，我们欢迎贡献;我们很高兴看到你想出了什么！ 该层提供的接口包括以下三个关键部分： Models: Classes and helper functions that allow you to wrap your existing models for use with TFF。包装模型可以像调用单个包装函数（例如，tff.learning.from_keras_model）一样简单，也可以定义tff.learning.Model接口的子类以实现完全可定制性。 Federated Computation Builders. 辅助函数，使用现有模型构建用于训练或评估的联合计算。 Datasets: 可以在Python中下载和访问的罐装数据集，用于模拟联合学习方案。虽然联邦学习设计用于分散数据，无法简单地在集中位置下载，但在研究和开发阶段，使用可在本地下载和操作的数据进行初始实验通常很方便，特别是对于可能是方法的新手。 这些接口主要在tff.learning命名空间中定义，但除研究数据集和已在tff.simulation中分组的其他模拟相关功能外。该层FL使用Federated Core（FC）提供的低级接口实现，该接口还提供运行时环境。 在继续之前，我们建议您首先查看有关图像分类和文本生成的教程，因为它们使用具体示例介绍了此处描述的大多数概念。如果您有兴趣了解有关TFF如何工作的更多信息，您可能希望略过自定义算法教程and作为我们用来表达联邦计算逻辑的低级接口的介绍，并研究现有的实现。 tff.learning接口。 ModelsArchitectural assumptionsSerialization[序列化]TFF旨在支持各种分布式学习场景，其中您编写的机器学习模型代码可能在具有不同功能的大量异构客户端上执行。虽然在一端，但在某些应用中，这些客户端可能是功能强大的数据库服务器，我们平台打算支持的许多重要用途涉及资源有限的移动和嵌入式设备。我们不能假设这些设备能够托管Python运行时;我们现在唯一可以假设的是它们能够托管本地TensorFlow运行时。因此，我们在TFF中做出的基本架构假设是您的模型代码必须可序列化为TensorFlow图。 您可以（并且应该）按照最新的最佳实践（如使用预先模式）开发TF代码。但是，最终代码必须是可序列化的（例如，可以包装为eager-mode代码的tf.function）。这可以确保执行时所需的任何Python状态或控制流都可以序列化（可能在Autograph的帮助下）。 Currently, TensorFlow does not fully support serializing and deserializing eager-mode TensorFlow. 因此，TFF中的序列化目前遵循TF 1.0模式，其中所有代码必须在TFF控制的tf.Graph内构建。这意味着目前TFF不能使用已经构建的模型;相反，模型定义逻辑打包在一个返回tff.learning.Model的无参数函数中。然后，TFF调用此函数以确保模型的所有组件都已序列化。此外，作为强类型环境，TFF将需要一些额外的元数据，例如模型输入类型的规范。 使用 tff.learning.Model来包装模型 Aggregation[聚合]我们强烈建议大多数用户使用Keras构建模型，请参阅下面的Converters for Keras部分。这些包装器自动化处理模型更新的聚合以及模型定义的任何度量。但是，了解如何处理常规tff.learning.Model的聚合可能仍然有用。 联合学习中始终至少有两层聚合：本地设备上聚合和跨设备（或联合）聚合： 本地聚合。此聚合级别是指由单个客户端拥有的多批示例之间的聚合。它适用于模型参数（变量），它们随着模型的本地训练而继续顺序演变，以及您计算的统计数据（例如平均损失，准确度和其他指标），您的模型将再次在本地更新因为它遍历每个客户端的本地数据流。 在此级别执行聚合是模型代码的责任，并使用标准TensorFlow构造完成 处理的一般结构如下： 该模型首先构造tf.Variables以保存聚合，例如batches次数或处理的示例数，每批或每例损失的总和等。 TFF多次在TF模型上调用forward_pass方法，依次在后续批次的客户端数据上调用，这允许您更新包含各种聚合的变量作为副作用。 最后，TFF在您的模型上调用report_local_outputs方法，以允许您的模型将其收集的所有摘要统计的信息编译为一组紧凑的度量标准，以供客户端导出。例如，您的模型代码可以将损失总和除以处理的示例数量，以输出平均损失等。 联合聚合。此聚合级别是指系统中多个客户端（设备）之间的聚合。同样，它适用于在客户端之间上进行平均的模型参数（变量）和模型作为本地聚合的结果导出的度量。 在此级别执行聚合是TFF的责任。但是，作为模型创建者，您可以控制此过程（以下更多内容）。 处理的一般结构如下： 初始模型和培训所需的任何参数由服务器分发给将参与一轮培训或评估的客户端子集。 在每个客户端上，独立地并行地，在本地数据批次流上重复调用模型代码以生成一组新的模型参数（在培训时），以及一组新的本地度量标准，如上所述（这是本地的聚合）。 TFF运行分布式聚合协议，以在整个系统中累积和聚合模型参数和本地导出的指标。在Model的federated_output_computation中，使用TFF自己的联合计算语言（不在TensorFlow中）以声明方式表示此逻辑。有关聚合API的更多信息，请参阅custom algorithms 。 Abstract interfaces[接口tff.learning.Model]此基本构造函数+元数据接口由接口tff.learning.Model表示实现，如下所示： The constructor，forward_pass和report_local_outputs方法应相应地构造和导出的模型变量，正向传递和统计信息。如上所述，由这些方法构造的TensorFlow必须是可序列化的。 input_spec属性以及返回可训练，不可训练和本地变量子集的3个属性表示元数据。 TFF使用此信息来确定如何将模型的各个部分连接到联合优化算法，以及定义内部类型签名以帮助验证构造系统的正确性（以便您的模型不能通过与不匹配的数据相匹配的数据进行实例化该模型旨在消费）。 此外，抽象接口tff.learning.Model公开了一个属性federated_output_computation，它与前面提到的report_local_outputs属性一起去控制聚合摘要统计信息的过程。 最后，派生的抽象接口tff.learning.TrainableModel允许您自定义TFF执行各个训练步骤的方式，例如通过指定自己的优化器，或定义在训练之前和之后计算的单独度量。您可以通过覆盖新引入的抽象方法train_on_batch来自定义它。但是，如果您只打算使用模型进行评估，或者您满意TFF为您选择标准优化器，则不必这样做。 您可以在我们的image classification教程的第二部分以及我们用于在model_examples.py中进行测试的示例模型中找到如何定义自己的自定义tf.learning.Model的示例。 Converters for Keras几乎所有TFF所需的信息都可以通过调用tf.keras接口来获得，所以如果你有一个Keras模型，你可以依赖以下两种方法之一为你构建一个tff.learning.TrainableModel实例： tff.learning.from_keras_model tff.learning.from_compiled_keras_model 请注意，TFF仍然希望您提供构造函数 - 无参数模型函数，如下所示： 1234def model_fn(): keras_model = ... keras_model.compile(...) return tff.learning.from_compiled_keras_model(keras_model, sample_batch) 除了模型本身，您还需要提供一组样本数据，TFF使用这些数据来确定模型输入的类型和形状。这确保了TFF可以正确地实例化实际存在于客户端设备上的数据的模型（因为我们假设在构建要序列化的TensorFlow时，这些数据通常不可用）。 The use of Keras wrappers is illustrated in our image classification and text generation tutorials. Federated Computation Builders[tff.Computations]tff.learning包为执行学习相关任务的tff.Computations提供了几个builder。我们希望未来可以扩展这类计算。 Architectural assumptionsExecution[运行计算]运行联合计算有两个不同的阶段。 编译：TFF首先将联合学习算法编译成整个分布式计算的抽象序列化表示。这是TensorFlow序列化发生的时候，但是可以进行其他转换以支持更高效的执行。我们将编译器发出的序列化表示称为federated computation。 Execution ：TFF提供了执行这些computations的方法。目前，仅通过本地模拟（例如，在使用模拟分散数据的Jupyter笔记本中）支持执行。 A federated computation generated by TFF’s Federated Learning API, such as a training algorithm that uses federated model averaging, or a federated evaluation, includes a number of elements, most notably: A serialized form of your model code as well as additional TensorFlow code constructed by the Federated Learning framework to drive your model’s training/evaluation loop (such as constructing optimizers, applying model updates, iterating over tf.data.Datasets, and computing metrics, and applying the aggregated update on the server, to name a few).模型的序列化形式以及联邦学习框架构建的其他TensorFlow代码，用于驱动模型的训练/评估循环(例如构建优化器，应用模型更新，迭代tf.data.Datasets和计算度量，以及在服务器上应用聚合更新，仅举几例）。 A declarative specification of the communication between the clients and a server (typically various forms of aggregation across the client devices, and broadcasting from the server to all clients), and how this distributed communication is interleaved with the client-local or server-local execution of TensorFlow code.客户端与服务器之间通信的声明性规范（通常是客户端设备之间的各种形式的聚合，以及从服务器向所有客户端广播），以及此分布式通信如何与客户端本地或服务器本地执行交错TensorFlow代码。 以序列化形式表示的federated computations 以与Python不同的独立于平台的内部语言表示，但是要使用联合学习API，您无需关心此表示的详细信息。计算在您的Python代码中表示为tff.Computation类型的对象，在大多数情况下，您可以将其视为不透明的Python可调用对象。 在教程中，您将调用那些federated computations，就像它们是常规Python函数一样，在本地执行。然而，TFF被设计为以对执行环境的大多数方面不可知的方式表达联合计算，使得它们可以潜在地部署到例如运行Android的设备组或数据中心中的集群。同样，其主要结果是对序列化的强烈假设。特别是，当您调用下面描述的某个build _…方法时，计算将完全序列化(使用tff.learning.Model)。 Modeling state[建模状态]TFF是一个功能性编程环境，但联邦学习中许多有关的过程都是有状态的。例如，涉及多轮联合模型平均的训练迭代过程是我们可以归类为有状态过程的一个示例。在该过程中，从一轮到另一轮发展的状态包括正在训练的一组模型参数，以及可能与优化器相关联的附加状态（例如，动量矢量）。 由于TFF是有功能性的，因此有状态进程在TFF中被建模为接受当前状态作为输入然后提供更新状态作为输出的计算。为了完全定义有状态进程，还需要指定初始状态的来源（否则我们无法引导进程）。这是在辅助类tff.utils.IterativeProcess的定义中捕获的，其中2个属性initialize，next分别对应于初始化和迭代。 Available builders目前，TFF提供了两个构建器函数，用于生成联合训练和评估的联合计算： tff.learning.build_federated_averaging_process takes a model function, and returns a stateful tff.utils.IterativeProcess. tff.learning.build_federated_evaluation takes a model function and returns a single federated computation for federated evaluation of models, since evaluation is not stateful. DatasetsArchitectural assumptionsClient selection在典型的联合学习场景中，我们拥有大量潜在的数亿个客户端设备，其中只有一小部分可能处于活动状态且可在任何给定时刻进行培训（例如，这可能仅限于客户端插入电源，而不是计量网络，否则空闲）。通常，可用于参与培训或评估的客户端集合不受开发人员的控制。此外，由于协调数百万客户是不切实际的，因此典型的一轮培训或评估将仅包括可用客户端的一小部分，其可以随机采样。 这样做的关键结果是，联邦计算(federated computations,)在设计上以对参与者的确切集合无关的方式表达;所有处理都表示为一组抽象的匿名客户端上的聚合操作，并且该组可能会因一轮培训而异。因此，计算与具体参与者的实际绑定，以及因此它们输入计算的具体数据，在计算本身之外被建模。 为了模拟联邦学习代码的实际部署，您通常会编写一个如下所示的训练循环： 12345678910trainer = tff.learning.build_federated_averaging_process(...)state = trainer.initialize()federated_training_data = ...def sample(federate_data): return ...while True: data_for_this_round = sample(federated_training_data) state, metrics = trainer.next(state, data_for_this_round) 为了实现这一点，在模拟中使用TFF时，联邦数据被接受为Python列表，每个参与的客户端设备有一个元素来表示该设备的本地tf.data.Dataset。 Abstract interfaces为了标准化处理模拟联邦数据集，TFF提供了一个抽象接口tff.simulation.ClientData，它允许人们枚举客户端集，并构造一个包含特定客户端数据的tf.data.Dataset。那些tf.data.Datasets可以作为输入直接馈送到急切模式下生成的联合计算。 应该注意的是，访问客户端身份的能力是仅由用于模拟的数据集提供的特征，其中可能需要训练来自客户的特定子集的数据的能力（例如，模拟不同的昼夜可用性）。客户类型）。编译的计算和底层运行时不涉及客户端身份的任何概念。一旦选择了来自特定客户端子集的数据作为输入，例如，在对tff.utils.IterativeProcess.next的调用中，客户端身份不再出现在其中。 Available data sets我们为实现tff.simulation.ClientData接口的数据集专门设置了名称空间tff.simulation.datasets，用于模拟，并将其与2个数据集一起播种，以支持图像分类和文本生成教程。我们建议您鼓励您将自己的数据集提供给平台。 Federated Core本文档介绍了TFF的核心层，它作为联合学习的基础，以及未来可能的非学习联合算法。 有关Federated Core的简要介绍，请阅读以下教程，因为它们通过示例介绍了一些基本概念，并逐步演示了简单联合平均算法的构建。 Custom Federated Algorithms, Part 1: Introduction to the Federated Core. Custom Federated Algorithms, Part 2: Implementing Federated Averaging. 我们还鼓励您熟悉联合学习以及关于图像分类和文本生成的相关教程，因为联邦核心学习API（FC API）用于联合学习为我们在以下方面做出的一些选择提供了重要的背景。设计这一层。 OverviewGoals, Intended Uses, and Scope[目标，预期用途和范围]联邦核心（FC）最好被理解为用于实现分布式计算的编程环境，即涉及可以各自执行非计算机的多个计算机（移动电话，平板电脑，嵌入式设备，台式计算机，传感器，数据库服务器等）的计算。在本地进行简单处理，并通过网络进行通信以协调其工作。 分布式术语非常通用，而TFF并未针对所有可能类型的分布式算法，因此我们更倾向于使用较不通用的术语:联合计算(federated computation)来描述可在此框架中表达的算法类型。 虽然以完全正式的方式定义术语联合计算超出了本文档的范围，但请考虑在描述新的分布式学习算法的research publication中您可能在伪代码中看到的算法类型。 FC的目标，就是在类似 伪代码的抽象级别上实现类似的紧凑表示，不是伪代码的程序逻辑，而是在各种目标环境中可执行的。 FC旨在表达的各种算法的关键定义特征是系统参与者的动作以集体方式描述。因此，我们倾向于讨论每个设备在本地转换数据，并且多个设备通过集中协调器协调工作，广播，收集或聚合其结果。 虽然TFF的设计能够超越简单的客户端 - 服务器架构，但集体处理的概念才是最基本的。这是由于联邦学习中TFF的起源，联合学习是一种最初设计用于支持对仍然受客户端设备控制的潜在敏感数据的计算的技术，并且出于隐私原因可能不会简单地下载到集中式位置。虽然此类系统中的每个客户端都有助于计算系统结果的数据和处理能力（我们通常期望这对所有参与者都有价值），但我们也努力保护每个客户的隐私和匿名性。 因此，尽管分布式计算的大多数框架被设计为从个体参与者的角度表达处理 - 即，在单个点对点消息交换的层面上，以及参与者的本地状态转换与传入和传出消息的相互依赖性。 ，TFF的Federated Core旨在从全球系统的角度描述系统的行为（类似于MapReduce）。 因此，虽然用于一般目的的分布式框架可以提供诸如发送和接收之类的操作作为构建块，但FC提供诸如tff.federated_sum，tff.federated_reduce或tff.federated_broadcast的构建块，其封装简单的分布式协议。 LanguagePython InterfaceTFF使用内部语言来表示联合计算，其语法由computation.proto中的可序列化表示定义。但是，FC API的用户通常不需要直接与该语言交互。相反，我们提供了一个Python API（tff命名空间），它围绕它作为定义计算的方式。 具体来说，TFF提供Python函数装饰器，如tff.federated_computation，它跟踪修饰函数的主体，并以TFF语言生成联合计算逻辑的序列化表示。用tff.federated_computation修饰的函数充当这种序列化表示的载体，并且可以将其作为构建块嵌入另一个计算的主体中，或者在调用时按需执行。 Here’s just one example; more examples can be found in the custom algorithms tutorials. 123@tff.federated_computation(tff.FederatedType(tf.float32, tff.CLIENTS))def get_average_temperature(sensor_readings): return tff.federated_mean(sensor_readings) 装饰器注：装饰器本质上是一个 Python 函数或类，它可以让其他函数或类在不需要做任何代码修改的前提下增加额外功能，装饰器的返回值也是一个函数/类对象。它经常用于有切面需求的场景，比如：插入日志、性能测试、事务处理、缓存、权限校验等场景，装饰器是解决这类问题的绝佳设计。有了装饰器，我们就可以抽离出大量与函数功能本身无关的雷同代码到装饰器中并继续重用。概括的讲，装饰器的作用就是为已经存在的对象添加额外的功能。 谈装饰器前，还要先要明白一件事，Python 中的函数和 Java、C++不太一样，Python 中的函数可以像普通变量一样当做参数传递给另外一个函数，例如： 1234567def foo(): print(&quot;foo&quot;)def bar(func): func()bar(foo) 简单装饰器123456789101112def use_logging(func): def wrapper(): logging.warn(\"%s is running\" % func.__name__) return func() # 把 foo 当做参数传递进来时，执行func()就相当于执行foo() return wrapperdef foo(): print('i am foo')foo = use_logging(foo) # 因为装饰器 use_logging(foo) 返回的时函数对象 wrapper，这条语句相当于 foo = wrapperfoo() use_logging 就是一个装饰器，它一个普通的函数，它把执行真正业务逻辑的函数 func 包裹在其中，看起来像 foo 被 use_logging 装饰了一样，use_logging 返回的也是一个函数，这个函数的名字叫 wrapper。在这个例子中，函数进入和退出时 ，被称为一个横切面，这种编程方式被称为面向切面的编程。 @ 语法糖如果你接触 Python 有一段时间了的话，想必你对 @ 符号一定不陌生了，没错 @ 符号就是装饰器的语法糖，它放在函数开始定义的地方，这样就可以省略最后一步再次赋值的操作。 1234567891011def use_logging(func): def wrapper(): logging.warn(&quot;%s is running&quot; % func.__name__) return func() return wrapper@use_loggingdef foo(): print(&quot;i am foo&quot;)foo() 带参数的装饰器1234567891011121314151617def use_logging(level): def decorator(func): def wrapper(*args, **kwargs): if level == \"warn\": logging.warn(\"%s is running\" % func.__name__) elif level == \"info\": logging.info(\"%s is running\" % func.__name__) return func(*args) return wrapper return decorator@use_logging(level=\"warn\")def foo(name='foo'): print(\"i am %s\" % name)foo() 上面的 use_logging 是允许带参数的装饰器。它实际上是对原有装饰器的一个函数封装，并返回一个装饰器。我们可以将它理解为一个含有参数的闭包。当我 们使用@use_logging(level=&quot;warn&quot;)调用的时候，Python 能够发现这一层的封装，并把参数传递到装饰器的环境中。 1@use_logging(level=&quot;warn&quot;)`等价于`@decorator 熟悉 non-eager TensorFlow的读者会发现这种方法类似于编写Python代码，该代码在定义TensorFlow图的Python代码部分中使用tf.add或tf.reduce_sum等函数。虽然代码在技术上用Python表示，但其目的是在下面构造一个tf.Graph的可序列化表示，它是由TensorFlow运行时内部执行的图形，而不是Python代码。同样，可以将tff.federated_mean视为将联合op插入由get_average_temperature表示的联合计算中。 FC定义语言的部分原因与如下事实有关：如上所述，联合计算指定分布式集体行为，因此，它们的逻辑是非本地的。例如，TFF提供的operators，输入和输出可能存在于网络中的不同位置。 这需要一种捕获分布式概念的语言和类型系统。 Type System[类型系统]Federated Core提供以下类别的类型。在描述这些类型时，我们指向类型构造函数以及引入紧凑符号，因为它是一种方便的方式或描述计算和运算符的类型。 首先，以下是概念上与现有主流语言类似的类型类别： Tensor types（tff.TensorType）。就像在TensorFlow中一样，它们具有dtype和shape。唯一的区别是这种类型的对象不限于是Python中的tf.Tensor实例， which表示TensorFlow图中TensorFlow操作的输出，但也可以包括可以生成的数据单元，例如，作为分布式的输出聚合协议。因此，TFF张量类型只是Python或TensorFlow中此类型的具体物理表示的抽象版本。 张量类型的紧凑符号是dtype或dtype [shape]。例如，int32和int32 [10]分别是整数和int向量的类型。 Sequence types（tff.SequenceType）。这些是TFF的抽象等同于TensorFlow的tf.data.Datasets的具体概念。序列的元素可以以顺序方式消费，并且可以包括复杂类型。序列类型的紧凑表示是T *，其中T是元素的类型。例如，int32 *表示整数序列。 Named tuple types命名元组类型（tff.NamedTupleType）。这些是TFFNamed tuple types的方式，这些结构具有预定义数量的具有特定类型（命名或未命名）的元素。重要的是，TFF的命名元组概念包含Python的参数元组的抽象等价物，即元素的集合，其中一些（但不是全部）被命名，一些是位置元素。 命名元组的紧凑表示法是&lt;n_1 = T_1，…，n_k = T_k&gt;，其中n_k是可选元素名称，T_k是元素类型。例如，&lt;int32，int32&gt;是一对未命名整数的紧凑表示法，是一对名为X和Y的浮点的紧凑表示法，可以表示平面上的一个点。元组可以嵌套以及与其他类型混合，例如， *将是一系列点的紧凑符号。 Function types函数类型（tff.FunctionType）。 TFF是一个函数式编程框架，其函数被视为first-class values。函数最多只有一个参数，而且只有一个结果。 函数的紧凑表示法是（T - &gt; U），其中T是参数的类型，U是结果的类型，或者（ - &gt; U）如果没有参数（尽管无参数函数是退化的）主要存在于Python级别的概念）。例如（int32 * - &gt; int32）是一种将整数序列减少为单个整数值的函数的表示法。 以下类型解决了TFF计算的分布式系统方面。由于这些概念在某种程度上与TFF无关，因此我们建议您参考custom algorithms自定义算法教程以获取其他评论和示例。 Placement type展示位置类型。此类型尚未在公共API中公开，而不是以2个文字tff.SERVER和tff.CLIENTS的形式公开，您可以将其视为此类型的常量。但是，它在内部使用，并将在将来的版本中引入公共API。这种类型的紧凑表示是放置。 展示位置代表一组扮演特定角色的系统参与者。初始版本针对客户端 - 服务器计算，其中有2组参与者：客户端和服务器（您可以将后者视为单例组）。但是，在更复杂的体系结构中，可能还有其他角色，例如多层系统中的中间聚合器，可能正在执行不同类型的聚合，或者使用与服务器或服务器使用的不同类型的数据压缩/解压缩。客户。 定义展示位置概念的主要目的是作为定义联合类型的基础。 Federated types联合类型（tff.FederatedType）。联合类型的值是由特定放置（例如tff.SERVER或tff.CLIENTS）定义的一组系统参与者托管的值。联合类型由放置值（因此，它是依赖类型），成员组成部分的类型（每个参与者在本地托管的内容类型）以及指定所有参与者是否在本地的附加位all_equal定义托管相同的项目。 包含类型T的项（成员成分）的联合类型值的紧凑表示法，每个由组（放置）G托管，分别是设置或未设置all_equal位的T @ G或{T} @G。 For example: {int32} @CLIENTS表示一个联合值，它由一组可能不同的整数组成，每个客户端设备一个整数。请注意，我们所讨论的是单个联合值，它包含出现在网络中多个位置的多个数据项。考虑它的一种方式是作为一种具有“网络”维度的张量，尽管这种类比并不完美，因为TFF不允许随机访问联合值的成员组成部分。 { *} @ CLIENTS表示联合数据集，该值由多个XY坐标序列组成，每个客户端设备一个序列。 @ SERVER表示服务器上的权重和偏差张量的命名元组。由于我们删除了花括号，这表示all_equal位已设置，即只有一个元组（无论托管此值的群集中可能有多少个服务器副本）。 Building BlocksFederated Core的语言是lambda演算lambda-calculus的一种形式，还有一些额外的元素。 它提供了当前在公共API中公开的以下编程抽象： TensorFlow计算（tff.tf_computation）。这些是TensorFlow代码的部分，使用tff.tf_computation装饰器包装为TFF中的可重用组件。它们总是具有函数类型，与TensorFlow中的函数不同，它们可以采用结构化参数或返回序列类型的结构化结果。 这是一个例子，类型（int32 * - &gt; int）的TF计算，它使用tf.data.Dataset.reduce运算符来计算整数之和： 123@tff.tf_computation(tff.SequenceType(tf.int32))def add_up_integeres(x): return x.reduce(np.int32(0), lambda x, y: x + y) 内在函数或联合运算符（tff.federated _…）。这是一个函数库，如tff.federated_sum或tff.federated_broadcast，它们构成了FC API的大部分，其中大部分代表了与TFF一起使用的分布式通信运算符。 我们将这些称为内在函数，因为它们有点像内部函数，它们是TFF理解的开放式，可扩展的运算符集，并且编译成较低级别的代码。 这些运算符中的大多数都具有联合类型的参数和结果，并且大多数是可以应用于各种数据的模板。 例如，tff.federated_broadcast可以被认为是函数类型T @ SERVER - &gt; T @ CLIENTS的模板运算符。 Lambda表达式（tff.federated_computation）。 TFF中的lambda表达式相当于Python中的lambda或def;它由参数名称和包含对此参数的引用的body（表达式）组成。 在Python代码中，可以通过使用tff.federated_computation修饰Python函数并定义参数来创建这些代码。 这是我们之前已经提到过的lambda表达式的一个例子： 123@tff.federated_computation(tff.FederatedType(tf.float32, tff.CLIENTS))def get_average_temperature(sensor_readings): return tff.federated_mean(sensor_readings) 放置文字Placement literals.。目前，只有tff.SERVER和tff.CLIENTS允许定义简单的客户端 - 服务器计算。 函数调用Function invocations（__call__）。可以使用标准Python __call__语法调用任何具有函数类型的函数。调用是一个表达式，其类型与被调用函数的结果类型相同。 For example: add_up_integeres（x）表示先前在参数x上定义的TensorFlow计算的调用。这个表达式的类型是int32。 tff.federated_mean（sensor_readings）表示对sensor_readings的联合平均运算符的调用。此表达式的类型是float32 @ SERVER（假设上面示例中的上下文）。 Forming tuples and selecting their elements 形成元组并选择它们的元素。形式为[x，y]，x [y]或x.y的Python表达式，出现在用tff.federated_computation修饰的函数体中。","link":"/2019/05/09/TensorflowFederatedGuide/"},{"title":"iOS学习入门","text":"Objective-C入门教程Objective-C 通过提供类定义，方法以及属性的语法，还有其他可以提高类的动态扩展能力的结构等，扩展了标准的 ANSI C 语言。类的语法和设计主要是基于 Smalltalk。 ##Smalltalk 基本规则非常简洁优雅。 所有一切都是物体,所有的计算都通过消息（message）来触发。你向某个物体发送消息，然后就会发生某些事情。 几乎所有Smalltalk中的可执行表达式都是这样的形式：&lt;接受消息的物体&gt; &lt;消息&gt;。 消息激发方法（method），一个消息会对应什么方法是由接受消息的物体决定的。 方法是Smalltalk的代码单元。你可以认为方法就是你常用的编程语言中的函数或者过程。方法是计算发生的地方。 每一个物体都是某类（class）事物的一个实例（instance）。例如，物体12属于SmallInteger（小整数）类，物体’abc’属于String(字串)类。一个物体的数据和行为将由它所属的类决定。 Smalltalk中，类是真实的对象。创建实例，只需向class发送new。针对类的方法称为类方法（类似于Java的staticmethods）。 Smalltalk没有构造函数。如果你想执行实例初始化,你可以重定义“new”类方法来初始化实例 Objective-C被描述为盖在C语言上的薄薄一层，因为Objective-C的原意就是在C语言主体上加入面向对象的特性。 Objective-C代码的文件扩展名 扩展名 内容类型 .h 头文件。头文件包含类，类型，函数和常数的声明。 .m 源代码文件。这是典型的源代码文件扩展名，可以包含 Objective-C 和 C 代码。 .mm 源代码文件。带有这种扩展名的源代码文件，除了可以包含Objective-C和C代码以外还可以包含C++代码。仅在你的Objective-C代码中确实需要使用C++类或者特性的时候才用这种扩展名。 #import 选项和 #include 选项完全相同，只是它可以确保相同的文件只会被包含一次。Objective-C 的例子和文档都倾向于使用 #import，你的代码也应该是这样的。 语法 Objective-C的面向对象语法源于Smalltalk消息传递风格。 消息传递Objective-C最大的特色是承自Smalltalk的消息传递模型（message passing），此机制与今日C++式之主流风格差异甚大。Objective-C里，与其说对象互相调用方法，不如说对象之间互相传递消息更为精确。此二种风格的主要差异在于调用方法/消息传递这个动作。C++里类别与方法的关系严格清楚，一个方法必定属于一个类别，而且在编译时（compile time）就已经紧密绑定，不可能调用一个不存在类别里的方法。但在Objective-C，类别与消息的关系比较松散，调用方法视为对对象发送消息，所有方法都被视为对消息的回应。所有消息处理直到运行时（runtime）才会动态决定，并交由类别自行决定如何处理收到的消息。也就是说，一个类别不保证一定会回应收到的消息，如果类别收到了一个无法处理的消息，程序只会抛出异常，不会出错或崩溃。 C++里，送一个消息给对象（或者说调用一个方法）的语法如下：obj.method(argument); Objective-C则写成：[obj method: argument]; 这里以一个汽车类（car class）的简单例子来解释Objective-C的消息传递特性：[car fly]; 典型的C++意义解读是”调用car类别的fly方法“。若car类别里头没有定义fly方法，那编译肯定不会通过。但是Objective-C里，我们应当解读为”发提交一个fly的消息给car对象“，fly是消息，而car是消息的接收者。car收到消息后会决定如何回应这个消息，若car类别内定义有fly方法就运行方法内之代码，若car内不存在fly方法，则程序依旧可以通过编译，运行期则抛出异常。 此二种风格各有优劣。C++强制要求所有的方法都必须有对应的动作，且编译期绑定使得函数调用非常快速。缺点是仅能借由virtual关键字提供有限的动态绑定能力。Objective-C天生即具备鸭子类型之动态绑定能力，因为运行期才处理消息，允许发送未知消息给对象。可以送消息给整个对象集合而不需要一一检查每个对象的类型，也具备消息转送机制。同时空对象nil接受消息后默认为不做事，所以送消息给nil也不用担心程序崩溃。 字符串 大多数框架把字符串传递给NSString对象。NSString类提供了字符串的类包装. 因为这种字符串使用的非常频繁，Objective-C提供了一个助记符可以方便地从常量值创建NSString对象。要使用这个助记符，你需要做的全部事情，是在普通的双引号字符串前放置一个@符号。 123456NSString* myString = @&quot;My String\\n&quot;;NSString* anotherString = [NSString stringWithFormat:@&quot;%d %s&quot;, 1, @&quot;String&quot;];// 从一个C语言字符串创建Objective-C字符串NSString* fromCString = [NSString stringWithCString:&quot;A C string&quot; encoding:NSASCIIStringEncoding]; 类如同所有其他的面向对象语言，类是 Objective-C 用来封装数据，以及操作数据的行为的基础结构。对象就是类的运行期间实例，它包含了类声明的实例变量自己的内存拷贝，以及类成员的指针。Objective-C 的类规格说明包含了两个部分：定义（interface）与实现（implementation）。定义（interface）部分包含了类声明和实例变量的定义，以及类相关的方法。实现（implementation）部分包含了类方法的实际代码。 下图展现了声明一个叫做 MyClass 的类的语法，这个类继承自 NSObject 基础类。类声明总是由 @interface 编译选项开始，由 @end 编译选项结束。类名之后的（用冒号分隔的）是父类的名字。类的实例（或者成员）变量声明在被大括号包含的代码块中。实例变量块后面就是类声明的方法的列表。每个实例变量和方法声明都以分号结尾。 类的定义文件遵循C语言之惯例以.h为后缀，实现文件以.m为后缀。 Interface定义部分，清楚定义了类的名称、数据成员和方法。 以关键字@interface作为开始，@end作为结束。 1234567891011@interface MyObject: NSObject { int memberVar1; //实例变量 int memberVar2;}+(return_type) class_method; 类方法-(return_type) instance_method1; //实例方法-(return_type) instance_method2: (int) p1;-(return_type) instance_method3: (int) p1 andPar: (int) p2;@end 方法前面的 +/- 号代表函数的类型：加号（+）代表类方法（class method），不需要实例就可以调用，与C++ 的静态函数（static member function）相似。减号（-）即是一般的实例方法（instance method）。 Objective-C定义一个新的方法时，名称内的冒号（:）代表参数传递，不同于C语言以数学函数的括号来传递参数。Objective-C方法使得参数可以夹杂于名称中间，不必全部附缀于方法名称的尾端，可以提高程序可读性。设定颜色RGB值的方法为例： 12- (void) setColorToRed: (float)red Green: (float)green Blue:(float)blue; /* 宣告方法*/[myColor setColorToRed: 1.0 Green: 0.8 Blue: 0.2]; /* 呼叫方法*/ 这个方法的签名是setColorToRed:Green:Blue:。每个冒号后面都带着一个float类别的参数，分别代表红，绿，蓝三色。 Implementation实现区块则包含了公开方法的实现，以及定义私有（private）变量及方法。 以关键字@implementation作为区块起头，@end结尾。 12345678910111213141516@Implementation MyObject { int memberVar3; //私有变量}+(return_type) class_method { .... //method implementation}-(return_type) instance_method1 { ....}-(return_type) instance_method2: (int) p1 { ....}-(return_type) instance_method3: (int) p1 andPar: (int) p2 { ....}@end 值得一提的是不只Interface区块可定义实体变量，Implementation区块也可以定义实体变量，两者的差别在于访问权限的不同，Interface区块内的实体变量默认权限为protected，宣告于implementation区块的实体变量则默认为private，故在Implementation区块定义私有成员更匹配面向对象之封装原则，因为如此类别之私有信息就不需曝露于公开interface（.h文件）中。 创建对象Objective-C创建对象需通过alloc以及init两个消息。alloc的作用是分配内存，init则是初始化对象。 init与alloc都是定义在NSObject里的方法，父对象收到这两个信息并做出正确回应后，新对象才创建完毕。以下为范例： MyObject * my = [[MyObject alloc] init]; 在Objective-C 2.0里，若创建对象不需要参数，则可直接使用newMyObject * my = [MyObject new];仅仅是语法上的精简，效果完全相同。 若要自己定义初始化的过程，可以重写init方法，来添加额外的工作。（用途类似C++ 的构造函数constructor） 方法Objective-C 中的类可以声明两种类型的方法：实例方法和类方法。实例方法就是一个方法，它在类的一个具体实例的范围内执行。也就是说，在你调用一个实例方法前，你必须首先创建类的一个实例。而类方法，比较起来，也就是说，不需要你创建一个实例。 方法声明包括方法类型标识符，返回值类型，一个或多个方法标识关键字，参数类型和名信息。下图展示 insertObject:atIndex: 实例方法的声明。声明由一个减号(-)开始，这表明这是一个实例方法。方法实际的名字(insertObject:atIndex:)是所有方法标识关键的级联，包含了冒号。冒号表明了参数的出现。如果方法没有参数，你可以省略第一个(也是唯一的)方法标识关键字后面的冒号。本例中，这个方法有两个参数。 当你想调用一个方法，你传递消息到对应的对象。这里消息就是方法标识符，以及传递给方法的参数信息。发送给对象的所有消息都会动态分发，这样有利于实现Objective-C类的多态行为。也就是说，如果子类定义了跟父类的具有相同标识符的方法，那么子类首先收到消息，然后可以有选择的把消息转发（也可以不转发）给他的父类。 消息被中括号( [ 和 ] )包括。中括号中间，接收消息的对象在左边，消息（包括消息需要的任何参数）在右边。例如，给myArray变量传递消息insertObject:atIndex:消息，你需要使用如下的语法： [myArray insertObject:anObj atIndex:0]; 为了避免声明过多的本地变量保存临时结果，Objective-C允许你使用嵌套消息。每个嵌套消息的返回值可以作为其他消息的参数或者目标。例如，你可以用任何获取这种值的消息来代替前面例子里面的任何变量。所以，如果你有另外一个对象叫做myAppObject拥有方法，可以访问数组对象，以及插入对象到一个数组，你可以把前面的例子写成如下的样子：[[myAppObject getArray] insertObject:[myAppObject getObjectToInsert] atIndex:0]; 虽然前面的例子都是传递消息给某个类的实例，但是你也可以传递消息给类本身。当给类发消息，你指定的方法必须被定义为类方法，而不是实例方法。你可以认为类方法跟C++类里面的静态成员有点像（但是不是完全相同的）。 类方法的典型用途是用做创建新的类实例的工厂方法，或者是访问类相关的共享信息的途径。类方法声明的语法跟实例方法的几乎完全一样，只有一点小差别。与实例方法使用减号作为方法类型标识符不同，类方法使用加号( + )。 下面的例子演示了一个类方法如何作为类的工厂方法。在这里，arrayWithCapacity是NSMutableArray类的类方法，为类的新实例分配内容并初始化，然后返回给你。 1234NSMutableArray* myArray = nil; // nil 基本上等同于 NULL// 创建一个新的数组，并把它赋值给 myArray 变量myArray = [NSMutableArray arrayWithCapacity:0]; 属性属性是用来代替声明存取方法的便捷方式。属性不会在你的类声明中创建一个新的实例变量。他们仅仅是定义方法访问已有的实例变量的速记方式而已。暴露实例变量的类，可以使用属性记号代替getter和setter语法。类还可以使用属性暴露一些“虚拟”的实例变量，他们是部分数据动态计算的结果，而不是确实保存在实例变量内的。 实际上可以说，属性节约了你必须要写的大量多余的代码。因为大多数存取方法都是用类似的方式实现的，属性避免了为类暴露的每个实例变量提供不同的getter和setter的需求。取而代之的是，你用属性声明指定你希望的行为，然后在编译期间合成基于声明的实际的getter和setter方法。 属性声明应该放在类接口interface的方法声明那里。基本的定义使用@property编译选项，紧跟着类型信息和属性的名字。你还可以用定制选项对属性进行配置，这决定了存取方法的行为。下面的例子展示了一些简单的属性声明： 123456789101112@interface Person : NSObject { @public NSString *name; @private int age;}@property(copy) NSString *name;@property(readonly) int age;-(id)initWithAge:(int)age;@end 属性的访问方法由@synthesize关键字来实现，它由属性的声明自动的产生一对访问方法。另外，也可以选择使用@dynamic关键字表明访问方法会由程序员手工提供。 123456789101112131415@implementation Person@synthesize name;@dynamic age;-(id)initWithAge:(int)initAge{ age = initAge; // 注意：直接赋给成员变量，而非属性 return self;}-(int)age{ return 29; // 注意：并非返回真正的年龄}@end 属性可以利用传统的消息表达式、点表达式或“valueForKey:”/“setValue:forKey:”方法对来访问。 1234Person *aPerson = [[Person alloc] initWithAge: 53];aPerson.name = @&quot;Steve&quot;; // 注意：点表达式，等于[aPerson setName: @&quot;Steve&quot;];NSLog(@&quot;Access by message (%@), dot notation(%@), property name(%@) and direct instance variable access (%@)&quot;, [aPerson name], aPerson.name, [aPerson valueForKey:@&quot;name&quot;], aPerson-&gt;name); 为了利用点表达式来访问实例的属性，需要使用”self”关键字： 1234-(void) introduceMyselfWithProperties:(BOOL)useGetter{ NSLog(@&quot;Hi, my name is %@.&quot;, (useGetter ? self.name : name)); // NOTE: getter vs. ivar access} 类或协议的属性可以被动态的读取。 123456789int i;int propertyCount = 0;objc_property_t *propertyList = class_copyPropertyList([aPerson class], &amp;propertyCount);for ( i=0; i &lt; propertyCount; i++ ) { objc_property_t *thisProperty = propertyList + i; const char* propertyName = property_getName(*thisProperty); NSLog(@&quot;Person has a property: &apos;%s&apos;&quot;, propertyName);} 快速枚举比起利用NSEnumerator对象或在集合中依次枚举，Objective-C 2.0提供了快速枚举的语法。在Objective-C 2.0中，以下循环的功能是相等的，但性能特性不同。 123456789101112131415161718// 使用NSEnumeratorNSEnumerator *enumerator = [thePeople objectEnumerator];Person *p;while ( (p = [enumerator nextObject]) != nil ) { NSLog(@&quot;%@ is %i years old.&quot;, [p name], [p age]);}// 使用依次枚举for ( int i = 0; i &lt; [thePeople count]; i++ ) { Person *p = [thePeople objectAtIndex:i]; NSLog(@&quot;%@ is %i years old.&quot;, [p name], [p age]);}// 使用快速枚举for (Person *p in thePeople) { NSLog(@&quot;%@ is %i years old.&quot;, [p name], [p age]);} 快速枚举可以比标准枚举产生更有效的代码，由于枚举所调用的方法被使用NSFastEnumeration协议提供的指针算术运算所代替了。 协议（Protocol）协议是一组没有实现的方法列表，任何的类均可采纳协议并具体实现这组方法。 协议类似于Java与C#语言中的”接口”。在Objective-C中，有两种定义协议的方式：由编译器保证的”正式协议”，以及为特定目的设定的”非正式协议”。 非正式协议为一个可以选择性实现的一系列方法列表。optional 非正式协议已经被废弃不再使用。 正式协议类似于Java中的”接口”，它是一系列方法的列表，任何类都可以声明自身实现了某个协议。在Objective-C 2.0之前，一个类必须实现它声明匹配的协议中的所有方法，否则编译器会报告错误，表明这个类没有实现它声明匹配的协议中的全部方法。Objective-C 2.0版本允许标记协议中某些方法为可选的（Optional），这样编译器就不会强制实现这些可选的方法。协议经常应用于Cocoa中的委托及事件触发。例如文本框类通常会包括一个委托（delegate）对象，该对象可以实现一个协议，该协议中可能包含一个实现文字输入的自动完成方法。若这个委托对象实现了这个方法，那么文本框类就会在适当的时候触发自动完成事件，并调用这个方法用于自动完成功能。 Objective-C中协议的概念与Java中接口的概念并不完全相同，即一个类可以在不声明它匹配某个协议的情况下，实现这个协议所包含的方法，也即实质上匹配这个协议，而这种差别对外部代码而言是不可见的。正式协议的声明不提供实现，它只是简单地表明匹配该协议的类实现了该协议的方法，保证调用端可以安全调用方法。 语法协议以关键字@protocol作为区块起始，@end结束，中间为方法列表。 1234@protocol Locking- (void)lock;- (void)unlock;@end 这是一个协议的例子，多线程编程中经常要确保一份共享资源同时只有一个线程可以使用，会在使用前给该资源挂上锁 ，以上即为一个表明有”锁”的概念的协议，协议中有两个方法，只有名称但尚未实现。下面的SomeClass宣称他采纳了Locking协议： 12@interface SomeClass : SomeSuperClass &lt;Locking&gt;@end 一旦SomeClass表明他采纳了Locking协议，SomeClass就有义务实现Locking协议中的两个方法。 12345678@implementation SomeClass- (void)lock { // 實現lock方法...}- (void)unlock { // 實現unlock方法...}@end 由于SomeClass已经确实遵从了Locking协议，故调用端可以安全的发送lock或unlock消息给SomeClass实体变量，不需担心他没有办法回应消息。 插件是另一个使用抽象定义的例子，可以在不关心插件的实现的情况下定义其希望的行为。 动态类型类似于Smalltalk，Objective-C具备动态类型：即消息可以发送给任何对象实体，无论该对象实体的公开接口中有没有对应的方法。对比于C++这种静态类型的语言，编译器会挡下对（void）指针调用方法的行为。但在Objective-C中，你可以对id发送任何消息（id很像void，但是被严格限制只能使用在对象上），编译器仅会发出”该对象可能无法回应消息”的警告，程序可以通过编译，而实际发生的事则取决于运行期该对象的真正形态，若该对象的确可以回应消息，则依旧运行对应的方法。 一个对象收到消息之后，他有三种处理消息的可能手段，第一是回应该消息并运行方法，若无法回应，则可以转发消息给其他对象，若以上两者均无，就要处理无法回应而抛出的例外。只要进行三者之其一，该消息就算完成任务而被丢弃。若对”nil”（空对象指针）发送消息，该消息通常会被忽略，取决于编译器选项可能会抛出例外。 虽然Objective-C具备动态类型的能力，但编译期的静态类型检查依旧可以应用到变量上。以下三种声明在运行时效果是完全相同的，但是三种声明提供了一个比一个更明显的类型信息，附加的类型信息让编译器在编译时可以检查变量类型，并对类型不符的变量提出警告。 下面三个方法，差异仅在于参数的形态：- setMyValue:(id) foo;id形态表示参数”foo”可以是任何类的实例。- setMyValue:(id &lt;aProtocol&gt;) foo;id表示”foo”可以是任何类的实例，但必须采纳”aProtocol”协议。- setMyValue:(NSNumber*) foo;该声明表示”foo”必须是”NSNumber”的实例。 动态类型是一种强大的特性。在缺少泛型的静态类型语言（如Java 5以前的版本）中实现容器类时，程序员需要写一种针对通用类型对象的容器类，然后在通用类型和实际类型中不停的强制类型转换。无论如何，类型转换会破坏静态类型，例如写入一个”整数”而将其读取为”字符串”会产生运行时错误。这样的问题被泛型解决，但容器类需要其内容对象的类型一致，而对于动态类型语言则完全没有这方面的问题。 转发Objective-C允许对一个对象发送消息，不管它是否能够响应之。除了响应或丢弃消息以外，对象也可以将消息转发到可以响应该消息的对象。转发可以用于简化特定的设计模式，例如观测器模式或代理模式。Objective-C运行时在Object中定义了一对方法： 转发方法： 12- (retval_t) forward:(SEL) sel :(arglist_t) args; // with GCC- (id) forward:(SEL) sel :(marg_list) args; // with NeXT/Apple systems 响应方法： 12- (retval_t) performv:(SEL) sel :(arglist_t) args; // with GCC- (id) performv:(SEL) sel :(marg_list) args; // with NeXT/Apple systems 希望实现转发的对象只需用新的方法覆盖以上方法来定义其转发行为。无需重写响应方法performv::，由于该方法只是单纯的对响应对象发送消息并传递参数。其中，SEL类型是Objective-C中消息的类型。 以下代码演示了转发的基本概念：Forwarder.h 文件代码： 12345678910#import &lt;objc/Object.h&gt;@interface Forwarder : Object{ id recipient; //该对象是我们希望转发到的对象。}@property (assign, nonatomic) id recipient;@end Forwarder.m 文件代码： 1234567891011121314151617#import &quot;Forwarder.h&quot;@implementation Forwarder@synthesize recipient;- (retval_t) forward: (SEL) sel : (arglist_t) args{ /* *检查转发对象是否响应该消息。 *若转发对象不响应该消息，则不会转发，而产生一个错误。 */ if([recipient respondsTo:sel]) return [recipient performv: sel : args]; else return [self error:&quot;Recipient does not respond&quot;];} Recipient.h 文件代码： 123456#import &lt;objc/Object.h&gt;// A simple Recipient object.@interface Recipient : Object- (id) hello;@end Recipient.m 文件代码： 123456789101112#import &quot;Recipient.h&quot;@implementation Recipient- (id) hello{ printf(&quot;Recipient says hello!\\n&quot;); return self;}@end main.m 文件代码： 1234567891011121314151617#import &quot;Forwarder.h&quot;#import &quot;Recipient.h&quot;int main(void){ Forwarder *forwarder = [Forwarder new]; Recipient *recipient = [Recipient new]; forwarder.recipient = recipient; //Set the recipient. /* *转发者不响应hello消息！该消息将被转发到转发对象。 *（若转发对象响应该消息） */ [forwarder hello]; return 0;} 利用GCC编译时，编译器报告： 123$ gcc -x objective-c -Wno-import Forwarder.m Recipient.m main.m -lobjcmain.m: In function `main&apos;:main.m:12: warning: `Forwarder&apos; does not respond to `hello&apos; 如前文所提到的，编译器报告Forwarder类不响应hello消息。在这种情况下，由于实现了转发，可以忽略这个警告。 运行该程序产生如下输出： 12$ ./a.outRecipient says hello! 类别 (Category)在Objective-C的设计中，一个主要的考虑即为大型代码框架的维护。结构化编程的经验显示，改进代码的一种主要方法即为将其分解为更小的片段。Objective-C借用并扩展了Smalltalk并扩展了Smalltalk实现中的”分类”的目的。 一个分类可以将方法的实现分解进一系列分离的文件。程序员可以将一组相关的方法放进一个分类，使程序更具可读性。举例来讲，可以在字符串类中增加一个名为”拼写检查”的分类，并将拼写检查的相关代码放进这个分类中。 进一步的，分类中的方法是在运行时被加入类中的，这一特性允许程序员向现存的类中增加方法，而无需持有原有的代码，或是重新编译原有的类。例如若系统提供的字符串类的实现中不包含拼写检查的功能，可以增加这样的功能而无需更改原有的字符串类的代码。 在运行时，分类中的方法与类原有的方法并无区别，其代码可以访问包括私有类成员变量在内的所有成员变量。 分类声明了与类中原有方法同名的函数，则分类中的方法会被调用。因此分类不仅可以增加类的方法，也可以代替原有的方法。这个特性可以用于修正原有代码中的错误，更可以从根本上改变程序中原有类的行为。若两个分类中的方法同名，则被调用的方法是不可预测的。 使用分类的例子这个例子创建了Integer类，其本身只定义了integer属性，然后增加了两个分类Arithmetic与Display以扩展类的功能。虽然分类可以访问类的私有成员，但通常利用属性的访问方法来访问是一种更好的做法，可以使得分类与原有类更加独立。这是分类的一种典型应用—另外的应用是利用分类来替换原有类中的方法，虽然用分类而不是继承来替换方法不被认为是一种好的做法。 Integer.h 文件代码： 1234567891011#import &lt;objc/Object.h&gt;@interface Integer : Object{@private int integer;}@property (assign, nonatomic) integer;@end Integer.m 文件代码： 1234567#import &quot;Integer.h&quot;@implementation Integer@synthesize integer;@end Arithmetic.h 文件代码： 123456#import &quot;Integer.h&quot;@interface Integer(Arithmetic)- (id) add: (Integer *) addend;- (id) sub: (Integer *) subtrahend;@end Arithmetic.m 文件代码： 123456789101112131415#import &quot;Arithmetic.h&quot;@implementation Integer(Arithmetic)- (id) add: (Integer *) addend{ self.integer = self.integer + addend.integer; return self;}- (id) sub: (Integer *) subtrahend{ self.integer = self.integer - subtrahend.integer; return self;}@end Display.h 文件代码： 123456#import &quot;Integer.h&quot;@interface Integer(Display)- (id) showstars;- (id) showint;@end Display.m 文件代码： 1234567891011121314151617181920#import &quot;Display.h&quot;@implementation Integer(Display)- (id) showstars{ int i, x = self.integer; for(i=0; i &lt; x; i++) printf(&quot;*&quot;); printf(&quot;\\n&quot;); return self;}- (id) showint{ printf(&quot;%d\\n&quot;, self.integer); return self;}@end main.m 文件代码： 123456789101112131415161718192021222324252627#import &quot;Integer.h&quot;#import &quot;Arithmetic.h&quot;#import &quot;Display.h&quot;intmain(void){ Integer *num1 = [Integer new], *num2 = [Integer new]; int x; printf(&quot;Enter an integer: &quot;); scanf(&quot;%d&quot;, &amp;x); num1.integer = x; [num1 showstars]; printf(&quot;Enter an integer: &quot;); scanf(&quot;%d&quot;, &amp;x); num2.integer = x; [num2 showstars]; [num1 add:num2]; [num1 showint]; return 0;} 利用以下命令来编译： gcc -x objective-c main.m Integer.m Arithmetic.m Display.m -lobjc在编译时间，可以利用省略#import “Arithmetic.h” 与[num1 add:num2]命令，以及Arithmetic.m文件来实验。程序仍然可以运行，这表明了允许动态的、按需的加载分类；若不需要某一分类提供的功能，可以简单的不编译之。 垃圾收集Objective-C运行时会将引用计数操作.当垃圾收集启用时，所有的对象都是收集器的工作对象。普通的C指针可以以”strong”修饰，标记指针指向的对象仍在使用中。被标记为”weak”的指针不被计入收集器的计数中，并在对象被回收时改写为”nil”。 OC是支持垃圾回收机制的(Garbage collection简称GC), iOS开发只支持手动内存管理和ARC；如果两个对象互相强引用（strong references）将导致它们永远不会被释放，甚至没有任何对象引用它们。 引用计数引用计数（Reference Count）是一个简单而有效的管理对象生命周期的方式。当我们创建一个新对象的时候，它的引用计数为 1，当有一个新的指针指向这个对象时，我们将其引用计数加 1，当某个指针不再指向这个对象是，我们将其引用计数减 1，当对象的引用计数变为 0 时，说明这个对象不再被任何指针指向了，这个时候我们就可以将对象销毁，回收内存。由于引用计数简单有效。 引用计数真正派上用场的场景是在面向对象的程序设计架构中，用于对象之间传递和共享数据。 ARC 下的内存管理问题对于 ARC 盲目依赖的 iOS 新人们，由于不知道引用计数，他们的问题主要体现在： 过度使用 block 之后，无法解决循环引用问题。 遇到底层 Core Foundation 对象，需要自己手工管理它们的引用计数时，显得一筹莫展。 循环引用（Reference Cycle）问题 引用计数这种管理内存的方式虽然很简单，但是有一个比较大的瑕疵，即它不能很好的解决循环引用问题。如下图所示：对象 A 和对象 B，相互引用了对方作为自己的成员变量，只有当自己销毁时，才会将成员变量的引用计数减 1。因为对象 A 的销毁依赖于对象 B 销毁，而对象 B 的销毁与依赖于对象 A 的销毁，这样就造成了我们称之为循环引用（Reference Cycle）的问题，这两个对象即使在外界已经没有任何指针能够访问到它们了，它们也无法被释放。 解决循环引用问题主要有两个办法，第一个办法是我明确知道这里会存在循环引用，在合理的位置主动断开环中的一个引用，使得对象得以回收。主动断开循环引用这种方式常见于各种与 block 相关的代码逻辑中。例如在我开源的 YTKNetwork 网络库中，网络请求的回调 block 是被持有的，但是如果这个 block 中又存在对于 View Controller 的引用，就很容易产生从循环引用，因为： Controller 持有了网络请求对象网络请求对象持有了回调的 block回调的 block 里面使用了 self，所以持有了 Controller 解决办法就是，在网络请求结束后，网络请求对象执行完 block 之后，主动释放对于 block 的持有，以便打破循环引用。相关的代码见： 1234567// https://github.com/yuantiku/YTKNetwork/blob/master/YTKNetwork/YTKBaseRequest.m// 第 147 行：- (void)clearCompletionBlock { // 主动释放掉对于 block 的引用 self.successCompletionBlock = nil; self.failureCompletionBlock = nil;} 不过，主动断开循环引用这种操作依赖于程序员自己手工显式地控制，相当于回到了以前 “谁申请谁释放” 的内存管理年代，它依赖于程序员自己有能力发现循环引用并且知道在什么时机断开循环引用回收内存（这通常与具体的业务逻辑相关），所以这种解决方法并不常用，更常见的办法是使用弱引用 (weak reference) 的办法。 使用弱引用弱引用虽然持有对象，但是并不增加引用计数，这样就避免了循环引用的产生。在 iOS 开发中，弱引用通常在 delegate 模式中使用。举个例子来说，两个 ViewController A 和 B，ViewController A 需要弹出 ViewController B，让用户输入一些内容，当用户输入完成后，ViewController B 需要将内容返回给 ViewController A。这个时候，View Controller 的 delegate 成员变量通常是一个弱引用，以避免两个 ViewController 相互引用对方造成循环引用问题， 弱引用的实现原理 弱引用的实现原理是这样，系统对于每一个有弱引用的对象，都维护一个表来记录它所有的弱引用的指针地址。这样，当一个对象的引用计数为 0 时，系统就通过这张表，找到所有的弱引用指针，继而把它们都置成 nil。 从这个原理中，我们可以看出，弱引用的使用是有额外的开销的。虽然这个开销很小，但是如果一个地方我们肯定它不需要弱引用的特性，就不应该盲目使用弱引用。举个例子，有人喜欢在手写界面的时候，将所有界面元素都设置成 weak 的，这某种程度上与 Xcode 通过 Storyboard 拖拽生成的新变量是一致的。但是我个人认为这样做并不太合适。因为： 我们在创建这个对象时，需要注意临时使用一个强引用持有它，否则因为 weak 变量并不持有对象，就会造成一个对象刚被创建就销毁掉。 大部分 ViewController 的视图对象的生命周期与 ViewController 本身是一致的，没有必要额外做这个事情。 Core Foundation 对象的内存管理下面我们就来简单介绍一下对底层 Core Foundation 对象的内存管理。底层的 Core Foundation 对象，在创建时大多以 XxxCreateWithXxx 这样的方式创建，例如： 1234// 创建一个 CFStringRef 对象CFStringRef str= CFStringCreateWithCString(kCFAllocatorDefault, “hello world&quot;, kCFStringEncodingUTF8);// 创建一个 CTFontRef 对象CTFontRef fontRef = CTFontCreateWithName((CFStringRef)@&quot;ArialMT&quot;, fontSize, NULL); 对于这些对象的引用计数的修改，要相应的使用 CFRetain 和 CFRelease 方法。如下所示： 123456// 创建一个 CTFontRef 对象CTFontRef fontRef = CTFontCreateWithName((CFStringRef)@&quot;ArialMT&quot;, fontSize, NULL);// 引用计数加 1CFRetain(fontRef);// 引用计数减 1CFRelease(fontRef); 对于 CFRetain 和 CFRelease 两个方法，读者可以直观地认为，这与 Objective-C 对象的 retain 和 release 方法等价。 所以对于底层 Core Foundation 对象，我们只需要延续以前手工管理引用计数的办法即可。 除此之外，还有另外一个问题需要解决。在 ARC 下，我们有时需要将一个 Core Foundation 对象转换成一个 Objective-C 对象，这个时候我们需要告诉编译器，转换过程中的引用计数需要做如何的调整。这就引入了bridge相关的关键字，以下是这些关键字的说明： __bridge: 只做类型转换，不修改相关对象的引用计数，原来的 Core Foundation 对象在不用时，需要调用 CFRelease 方法。 __bridge_retained：类型转换后，将相关对象的引用计数加 1，原来的 Core Found-ation 对象在不用时，需要调用 CFRelease 方法。 __bridge_transfer：类型转换后，将该对象的引用计数交给 ARC 管理，Core Foundation 对象在不用时，不再需要调用 CFRelease 方法。 OC @property 指示符常用的指示符有assign、atomic、copy、retain、strong、week、等。下面对它们的用途和常常对应的属性讲解一下。 assign：该指示符号对属性只是简单的赋值，不更改引用计数。常用于NSInteger等OC基础类型，以及short、int、double、结构体等C数据类型，因为这些类型不存在被内存回收的问题。 atomic、nonatomic：指定setter和getter是否是原子操作，即是否线程安全。如果是atomic，那么存取方法都是线程安全的，即某一线程访问存或者取方法，其他线程不可以进入该存、取方法。nonatomic则不具备线程安全的功能。需要指出的是atomic是默认值，可以保证数据的完整性，但是相应的降低了性能，所以在单线程环境中建议使用nonatomic来提升性能。 copy：如果使用copy指示符，当调用setter方法对成员变量赋值时，会将被赋值的对象复制的一个副本，再将该副本给成员变量，相应的原先的被赋值的对象的引用计数加1。当成员变量的类型是可变类型，或其子类是可变类型，被赋值的对象在赋值后有可能再被修改，如果不需要这种修改，则可以考虑copy指示符。 getter、setter：用于为getter方法、setter方法指定自定义方法名。比如getter＝myName,setter=setName:，我们可以看到setter方法后面有一个（:）,这是因为我们需要在后面添加参数。 readonly、readwrite：readonly指示系统只合成getter方法，不合成setter方法；readwrite是默认值，指示系统需要合成setter方法和getter方法。 retain：当把某个对象赋值给该属性时，该属性原来所引用的对象的引用计数减1，被赋值对象的引用计数加1。在未启用ARC机制的的情况下，retain可以保证一个对象的引用计数大于1时，该对象不会被回收。启用ARC后一般较少使用retain strong、weak：strong指示符该属性对被赋值对象持有强引用，而weak指示符指定该属性对被赋值对象持有弱引用。强引用的意思是：只要该强引用指向被赋值的对象，那么该对象就不会自动回收。弱引用的意思是：即使该弱引用指向被赋值的对象，该对象也可能被回收。如果不希望对象被回收，可以使用strong指示符。如果需要保证程序性能，避免内存溢出，可以使用weak，内存一旦被回收，指针会被赋值为nil。 1、atomic是默认行为，assign是默认行为，readwrite是默认行为2、推荐做法是NSString用copy3、delegate用assign（且一定要用assign）4、非objc数据类型，比如int，float等基本数据类型用assign（默认就是assign）5、其它objc类型，比如NSArray，NSDate用retain。retain就如2中所述，使用了引用计数，retain引起引用计数加1, release引起引用计数减1，当引用计数为0时，dealloc函数被调用，内存被回收。 assign： 简单赋值，不更改索引计数copy： 建立一个索引计数为1的对象，然后释放旧对象retain：释放旧的对象，将旧对象的值赋予输入对象，再提高输入对象的索引计数为1 Block 详解 介绍 截获变量 __block修饰符 Block的内存管理 Block的循环引用 Block介绍 Block是将函数及其执行上下文封装起来的对象 1234567{ int multiplier = 6; int(^Block)(int) = ^int(int num){ return num*multiplier; }; Block(2);} 本质 使用先从一个简单的需求来说：传入两个数，并且计算这两个数的和，为此创建了这样一个block： 123int (^sumOfNumbers)(int a, int b) = ^(int a, int b) { return a + b;}; 这段代码等号左侧声明一个名为sumOfNumbers的代码块，名称前用^符号表示后面的字符串是block的名称。最左侧的int表示这个block的返回值，括号中间表示这个block的参数列表，这里接收两个int类型的参数。 而在等号右侧表示这个block的定义，其中返回值是可以省略的，编译器会根据上下文自动补充返回值类型。使用^符号衔接着一个参数列表，使用括号包起来，告诉编译器这是一个block，然后使用大括号将block的代码封装起来。 这只局限于只读操作。如果我们在block中修改外部变量，编译器将会报错; 对于希望在block中修改的外界局部对象，我们可以给这些变量加上__block关键字修饰，这样就能在block中修改这些变量。在捕获变量特性中，还有一个有趣的小机制，我们把上面的代码改成这样： 123456CGPoint center = CGPointZero;CGPoint (^pointAddHandler)(CGPoint addPoint) = ^(CGPoint addPoint) { return CGPointMake(center.x + addPoint.x, center.y + addPoint.y);}center = CGPointMake(100, 100);NSLog(@&quot;%@&quot;, pointAddHandler(CGPointMake(10, 10))); //输出{10,10} block在捕获变量的时候只会保存变量被捕获时的状态（对象变量除外），之后即便变量再次改变，block中的值也不会发生改变。所以上面的代码在计算新的坐标值时center的值依旧等于CGPointZero 循环引用 开头说过，block在iOS开发中被视作是对象，因此其生命周期会一直等到持有者的生命周期结束了才会结束。另一方面，由于block捕获变量的机制，使得持有block的对象也可能被block持有，从而形成循环引用，导致两者都不能被释放: 1234567891011121314@implementation LXDObject{ void (^_cycleReferenceBlock)(void);}- (void)viewDidLoad{ [super viewDidLoad]; _cycleReferenceBlock = ^{ NSLog(@&quot;%@&quot;, self); //引发循环引用 };}@end 遇到这种代码编译器只会告诉你存在警告，很多时候我们都是忽略警告的，这最后会导致内存泄露，两者都无法释放。跟普通变量存在__block关键字一样的，系统提供给我们__weak的关键字用来修饰对象变量，声明这是一个弱引用的对象，从而解决了循环引用的问题： 1234__weak typeof(*&amp;self) weakSelf = self;_cycleReferenceBlock = ^{ NSLog(@&quot;%@&quot;, weakSelf); //弱指针引用，不会造成循环引用}; 对于block这种有趣的特性，在唐巧的谈Objective-C block的实现有详细介绍block的底层实现代码，我在这里就不多说了 申明一个block变量 通过^符号来声明block类型；形式如： void (^myBlock)(); 其中第一个void是返回值，可以是任意类型，中间括号中^后面的是这个block变量的名字，我把它命名为myBlock，最后一个括号中是参数，如果多参数，可以写成如下样式： 1int (^myBlock)(int,int); 同样，你也可以给参数起名字： 1int (^myBlock)(int a,int b); 很多时候，我们需要将我们声明的block类型作为函数的参数，也有一下几种方式： 1-(void)func:(int (^)(int a,int b))block； 第二种方式是通过typedef定义一种新的类型，这也是大多数情况下采用的方式： 12typedef int (^myBlock)(int a,int b) ;-(void)func:(myBlock)block ; 第三种方式也是通过typedef定义，但将blcok包装重命名成为属性再调用，这种思路也更清晰明了 123typedef void (^myBlock)(int a ,int b);@property (nonatomic, strong) myBlcok block;-(void)func:(myBlcok)block; 如何实现一个block 既然block可以被声明为变量，那么就一定可以实现它，就像其他类型变量的赋值。我自己对block的理解为它是一段代码块，所以给它赋值赋便是一段代码段： 123456789101112131415typedef int (^myBlock)(int,int) ;@interface ViewController (){ myBlock block1;}@end@implementation ViewController- (void)viewDidLoad { [super viewDidLoad]; block1 =^(int a, int b) { return a+b; }; NSLog(@&quot;%d&quot;,block1(1,1));} 这里打印的结果是2，从这里可以发现block和函数的功能很像。 注意：1、在上面的代码里 block1是一个对象，如果直接打印将打印对象地址 ​ 2、block()，加上后面的括号才是执行block语句块 block中访问对象的微妙关系 1、如果你在一个block块中仅仅访问对象，而不是对他进行修改操作，是没有任何问题的： 123456789- (void)viewDidLoad { [super viewDidLoad]; int tem=2; block1 = ^(int a,int b){ int count= tem+1; return count; }; NSLog(@&quot;%d&quot;,block1(1,1));} 而如果我在block块中直接修改，编译器会报错： 1234 block1 = ^(int a,int b){ tem+=1; return tem+1; }; 为什么会出现这样的情况，根据猜测，可能是block内部将访问的变量都备份了一份，如果我们在内部修改，外部的变量并不会被修改，我们可以通过打印变量的地址来证明这一点： 123456789- (void)viewDidLoad { [super viewDidLoad]; int tem=2; NSLog(@&quot;%p&quot;,&amp;tem); block1 = ^(int a,int b){ NSLog(@&quot;%p&quot;,&amp;tem); return tem+1; }; NSLog(@&quot;%d&quot;,block1(1,1)); } 打印结果如下：","link":"/2019/05/07/iOS学习入门/"},{"title":"jupyter学习笔记","text":"jupyter-交互笔记本 简介Jupyter Notebook（此前被称为 IPython notebook）是一个交互式笔记本，支持运行 40 多种编程语言。在本文中，我们将介绍 Jupyter notebook 的主要特性，以及为什么对于希望编写漂亮的交互式文档的人来说是一个强大工具。 安转与运行 在开始使用 notebook 之前，我们先需要安装该库。 方式1: Jupyter 官网 方式二: 使用pip install jupyter 然后在命令行运行： jupyter notebook，运行上面的命令之后，你将看到类似下面这样的输出： 12345[I 20:06:36.367 NotebookApp] Writing notebook server cookie secret to /run/user/1000/jupyter/notebook_cookie_secret[I 20:06:36.813 NotebookApp] Serving notebooks from local directory: /home/your_username[I 20:06:36.813 NotebookApp] 0 active kernels[I 20:06:36.813 NotebookApp] The IPython Notebook is running at: http://localhost:8888/[I 20:06:36.813 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). 同时，会在你开启 notebook 的文件夹中启动 Jupyter 主界面，如下所示 接着在该界面可以进行编程，markdown等等 LaTexUse double US dollars sign pair for Block level Math formula, and one US dollar sign pair for Inline Level. 12345For example this is a Block level $$x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$$ formula, and this is an inline Level $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ formula.\\\\[ \\frac{1}{\\Bigl(\\sqrt{\\phi \\sqrt{5}}-\\phi\\Bigr) e^{\\frac25 \\pi}} =1+\\frac{e^{-2\\pi}} {1+\\frac{e^{-4\\pi}} {1+\\frac{e^{-6\\pi}}{1+\\frac{e^{-8\\pi}} {1+\\ldots} } } } \\\\] Result: For example this is a Block level $$x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$$ formula, and this is an inline Level $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ formula. \\[ \\frac{1}{\\Bigl(\\sqrt{\\phi \\sqrt{5}}-\\phi\\Bigr) e^{\\frac25 \\pi}} =1+\\frac{e^{-2\\pi}} {1+\\frac{e^{-4\\pi}} {1+\\frac{e^{-6\\pi}}{1+\\frac{e^{-8\\pi}} {1+\\ldots} } } } \\]","link":"/2018/05/02/jupyter学习笔记/"},{"title":"iOS路由设计","text":"路由设计移动端路由层设计 什么是移动端路由层： 路由层的概念在服务端是指url请求的分层解析，将一个请求分发到对应的应用处理程序。移动端的路由层指的是将诸如App内页面访问、H5与App访问的访问请求和App间的访问请求，进行分发处理的逻辑层。 移动端路由层需要解决的问题： 对外部提供远程访问的功能，实现跨应用调用响应，包括H5应用调用、其他App应用调用、系统访问调用等 原生页面、模块、组件等定义，统称为资源(Resource)，在跨应用调用和路由层在不同端实现的业务表现需要一致的前提下，需要对资源进行定义，在路由提供内部请求分发的时候则可以提供不依赖对外进行资源定义的功能 外部调用如何使用统一标示(Uniform)进行表示资源 如何在移动端统一定义访问请求的过程，从而达成移动端与web端的统一性 如何更好的兼容iOS、Android的系统访问机制、App链接协议、web端路由机制与前端开发规范等 如何兼容各平台(Android、iOS)App页面导航机制 如何解决安全访问问题 移动端在客户端进行动态配置 移动端路由所应用的场景： H5页面与App原生页面、模块与组件的交互 App与App之间的相互访问 App内部页面跳转、模块调度与组件加载等 推送与通知系统解除硬编码的逻辑，动态访问原生资源，更好的支持通过通知和推送完成动态页面访问和逻辑执行 Extension等动态调用主App的资源 App实现更复杂的架构MVVM或者是VIPER架构，提供解除业务相互依赖的能力 以组件化为目的的工程改造，隔离各个业务，以制作单独的组件 对外如何定义资源 在路由提供对外的资源请求转发的时候，因为要照顾到其他应用的请求表达方式，比如H5应用或者是其他App的应用的访问请求，定义单纯依赖业务的资源定义就显得有些必要了。 举个例子，一个H5的商品详情页，被用户分享，当其他用户看到这个H5应用的页面的时候，点击，如果该用户装了有对应这个H5商品详情页的App的时候，应该跳转到该App的原生商品详情页，如果没有安装则加载这个H5页面，在这个过程中，H5的页面是通过URL进行标识的，那这个URL的标识也应该对照到App的原生页面，但是要只依赖业务标识而不能依赖App的代码实现，比如说iOS端的App的商品详情页叫做DetailViewController，那这个URL是不能包含这个名字的，Android端可能叫DetailActivity，如果不单纯依赖业务，那H5应用就要根据平台来重新发送不同的资源定义的URL，就造成了硬编码问题，H5应用要依赖App的实现逻辑，如果有一天，原生App的页面代码实现变成了GoodDetailViewController，所有依赖DetailViewController这个资源标示的H5应用都要进行更改，就会出现问题。所以路由层的设计应该具备根据业务定义来映射App内的资源定义。常常在设计路由层的时候，我们会更加关注通信行为的细节、如何改进特定通信机制的表现，常常忽略了一个事实，那就是改变应用程序的互动风格比改变协议对整体的表现有更大的影响。 所谓资源，就是一个应用程序提供的不可分割的服务，从这个层面上看，App的资源即是一种实体的存在，可以进行获取和访问，必须进行良好的表示，在有些必要的情况下，必须是独一无二的识别符来表示一个应用程序所提供的服务是什么。表示资源我们更倾向于使用URI进行标示，因为移动端没有一个横跨iOS、Android、Web后端与H5应用的资源标示方式，而URI是web service模式的资源通用表示方式，包括后面将要提到的Android与iOS统一支持的universal link(通用链接)也是借用URI的概念，App路由层所涉及到的资源表示方法还是建议使用URI的标示方式，同时更应该借鉴RESTful风格来架构这一层，原因是App的页面、组件或者说一整套功能性的服务是非常复杂的，相比于H5有更加多与复杂的交互，相比于后端存在更加苛刻的网络环境与多设备多平台的技术考量，所以URI在标示横跨多平台多版本的资源的情况下，能够更好的表示某一个资源实体而不是资源的表现形式。 在Android与iOS系统中，均支持URL Scheme，所以资源的标示通常会是这个样子： 12345AppScheme://path//例如qq app:mqq:// //支付宝:支付宝alipay:// 如果协议是Http或者是Https标示的是Web应用或者是H5应用，你的App也是一个与WebService相同级别的应用，那么URL的协议部分应该是App的唯一标示符，这个主机部分和路径部分则需要我们使用RESTful的风格进行重新设计。重点是如何标示资源，例如表示App中的登录服务，那可以表示为： 1AppScheme://host/login host为主机部分，在一般的WebService上，在业务表现形式上一般是比较大的业务条线的标示，比方说 https://news.sina.com.cn ，主机部分是news.sina.com.cn，则标示新浪新闻这条业务线，在App内你的业务条线也应该是清晰的，假如移动App的主UI框架是Tab分栏，那么每个Tab分栏就是你的业务条线的分割，这点跟WebService应用的导航栏类似，App的资源大多是页面或者是可交互的组件，与UI关系比较大，假如你的Tab有四个：分别叫首页、商品、发现、我的，那么我们可以这样定义： 1234AppScheme://index/AppScheme://goods/AppScheme://discover/AppScheme://user/ 当然，也可以有额外的定义，比方说App有Api服务，Api提供实现一个纯数据同步的服务标示，那么这个URL可以设计为： AppScheme://api-asycn/collections?action='insert'&amp;value='***'&amp;&amp;userUoken='*******'&amp;&amp;source=&quot;https//***.***.com/collection.html&quot;由于RESTful风格强调URL的资源标示而不是行为表示，所以”AppScheme://api-asycn/collections” 是一个良好的资源标示，表示了一个收藏功能的实体，而”?”后面的GET方式的参数实际上是不得已为之，因为实际上没有Web的http request的实体，所以只能勉强借助GET参数来替代RESTful风格中强调的Accept和Content-Type字段来标示表现层的行为描述。当然action与value这样的描述可以根据业务划分，但是重点是要用参数表现形式。 iOS与Android的系统访问机制、统一的链接协议苹果的URL Scheme由来已久： Apple URLScheme，Android平台同样也实现了该功能，使得App能够在沙盒机制的前提下，能够相互调用声明过的服务。由于URL Scheme天生没有返回的callBack机制，著名的App Drafts的作者联合Marco Arment、Justin Williams 等人开发了x-callback-URL来做出统一跳转的协议: x-callback-url，在此不过多表述。利用URL-Scheme的机制，可以定义如下的统一链接协议： 协议部分来标示App应用 主机Host部分用于标示业务线或者是应用提供的划分好的服务实体，比方说index、discover是业务条线，api-asycn是对外提供的api，pushService是App内部的推送服务等。 路径部分则可以是细分的页面、组件或者服务的标示 参数定义有一些是必要的，比如说action来标示动作，比方说可以使用get标示获取、insert增加，userToken表示安全的用户令牌，source表示来源，当然像是userToken与source这些都是路由层需要进行解析和验证的，而action则是业务相关的参数，这一点在路由曾设计的时候需要进行详细区分 统一访问请求过程整个统一的访问请求过程如图，关于最后的response返回有一些说明：在WebService的工作栈中，http的request与response是有标准协议规范的，而App的路由层只是套用的URI的资源标示和RESTFul风格的交互，没有标准的request和response结构，这部分实现在App内部，response对外部调用系统而言关心的有三个重要元素，资源状态码、返回值与错误，在路由层在响应外部调用的时候需要返回这三种元素 路由层逻辑结构 路由层安全 路由层的安全包含两个方面： 跨应用时，需要注意注入攻击，做到敏感参数加密防篡改，同时需要注意路由层应提供能够实现风控的机制 跨业务系统的时候，需要开启会话访问机制，通过令牌或者是session会话等来实现路由层身份认证 一步步构建iOS路由 实战篇 这个路由究竟是什么鬼？能解决什么问题？ 场景1：一个App项目中团队人员比较多，不同的人负责不同的模块开发，有的人直接使用资源文件设计的，有的人用代码直接写的，有的人负责登录，有的人负责订单，突然有一天搞订单的开发A找搞登录的开发B说要调一下登录，登录成功以后你要再回调下我写的模块的方法告诉我成功登录，我要刷新一下订单页面，B傻傻的就答应了，找B的人C、D、F….越来越多，B负责的代码越写越多，同时A也不怎么开心，因为A发现调B写的登录要通过类实例化函数获取模块，调C写的支付使用工厂方法，调D写的计算器组件又是另外一种写法，结果A自己的代码也越来越丑。 我想上面的两个场景出现的问题大家或多或少都会遇见，总结一下就是： 因为不同人负责不同模块，调用他人必须了解他人编写的模块如何调用，对象是啥，初始化方式是啥，这违背了面向对象的封装原则 引入不同的模块头文件，多了以后，所依赖的外部发生一丁点变化你就要跟着变，逻辑变得越来越耦合，不利于维护 调用不同模块要反复与他人沟通传参、回调流程、接口定义等等，沟通效率低下 产品提出各种需求，但是我写的代码都是差不多的，来一个页面我需要写一些相同逻辑的代码，而且产品还抱怨每次加相同的东西就要改代码发版，这显然不能满足复用的要求。 总结:依赖多、耦合高、复用低。可我们都知道有这么句话啊：高内聚、低耦合，职责单一逻辑清晰。 路由就是解决上面的问题 我们已经发现依赖比较大是因为要导入其他模块的头文件，了解其他模块的逻辑和定义，如果多了，你的代码中引入的头文件或者导入的包名越来越多，改一下牵一发而动全身啊。大概是这个样子： 依赖的问题很严重，要想破除这样的依赖，我们能想到的办法就是找个调度中心去做这件事，其实各个业务模块并不关心其他模块具体的业务逻辑是什么，也不需要知道这个模块如何获取，我只关心怎么调用和反馈的结果，而这个有了调度中心这个东西，每个模块不需要依赖其他模块，只需要调度中心关心每个模块的调度。 有了Route这个调度中心，每个模块就不用写那么多重复的耦合代码了，也不需要在导入那么多头文件了和引入那么多包名了，这些蓝色的箭头代表着调用方式，如果调用方式再统一一下，沟通效率就提升上去了，因为我们可以用一套约定好的数据协议来代替重复沟通，有时候我们需要靠约定和协议来提高我们的工作效率。 Tips:发现问题这个环节很重要，你在工作中经常要反复做的，浪费时间的都是需要你去优化和花大力气去解决的，作为一个专业人士，不断改进你的代码，优化你的工作流程，带动团队向好的协作方式去转型，这是专业人士的习惯，更应该成为你的习惯。同时针对代码存在的问题，也许你经常会隐隐约约感到有问题，就是不知道问题在什么地方，那么需要问问自己有没有以下情况：哪些代码是经常写且重复度很高的，是不是可以抽象出来？哪些代码需要反复的变动，是不是可以做成配置或者是定义一套数据格式来满足动态兼容？有没有一些现成的设计模式可以解决这些问题？比方说，调度中心则使用的是中介者模式。 为啥要说iOS路由呢？路由层其实在逻辑功能上的设计都是一样的，很多人把App中的视图切换当做是路由组件的功能职责，这点我持否定态度，从单一职责角度和MVC框架分析来看，视图切换属于View中的交互逻辑并不属于消息传递或者是事件分发的范畴，但路由请求、视图转场的实现部分与Android平台和iOS平台上的导航机制有着非常紧密的关系，Android操作系统有着天然的架构优势，Intent机制可以协助应用间的交互与通讯，是对调用组件和数据传递的描述，本身这种机制就解除了代码逻辑和界面之间的依赖关系，只有数据依赖。而iOS的界面导航和转场机制则大部分依赖UI组件各自的实现，所以如何解决这个问题，iOS端路由的实现则比较有代表性。 其实说白一点，路由层解决的核心问题就是原来界面或者组件之间相互调用都必须相互依赖，需要导入目标的头文件、需要清楚目标对象的逻辑，而现在全部都通过路由中转，只依赖路由或者某种通讯协议，或者依靠一些消息传递机制连路由都不依赖。其次，路由的核心逻辑就是目标匹配，对于外部调用的情况来说，URL如何匹配Handler是最为重要的，匹配就必然用到正则表达式。了解这些关键点以后就有了设计的目的性，let‘s do it~ 总结一下这个路由都要有什么？(需求分析)我们先根据上面的模糊的总结梳理一下： 路由需要能够实现被其他模块调度，从而调度另外一个模块 接入路由的模块不需要知道目标模块的实现 调度发起方需要有目标的响应回调，类似于http请求，有一个request就要有一个response，才能实现双向的调用 调用方式需要统一，统一而松散的调用协议和数据协议可以减少大量接入成本和沟通成本那一个完整的调度流程应该是这样的： 看到这个流程以后，可以确定以下几件事： A模块调用路由，为表达自己需要调用的是B模块，考虑到H5、推送以及其他App的外部调用，可以使用URL这种方式来定义目标，也就是说用URL来表示目标B 对一个URL的请求来说，路由需要有统一的回调处理，当然，如果不需要回调也是可以的，回调是需要目标去触发的 路由要有处理URL的功能，并调用其他模块的能力 根据以上粗略的定义一下路由的框架：这里面以供有4部分： WLRRouter就是一个实体对象，用来提供给其他模块调用。 WLRRouteRequest是一个以URL为基础的实体对象，为什么不直接用URL字符串？因为考虑到如果路由在内部调用其他模块的时候需要传入一些原生对象，而URL上只能携带类型单一的字符串键值对表示参数，所以需要使用这么一个对象进行包装。 WLRRouteHandler是一个处理某一个WLRRouteRequest请求的对象，当路由接收一个WLRRouteRequest请求，转发给一个WLRRouteHandler处理，处理完毕以后如果有回调，则回调给调用者。 URL的请求与Handler的对应关系肯定需要匹配的逻辑，为了使得路由内部逻辑更加清晰单独使用WLRRouteMatcher来处理匹配的逻辑。 深入具体需求，细化功能实现(详细设计)有了粗略的需求分析接下来就是细化需求并给出详细设计的阶段了，其实编写一个模块要有系统性思维，粗略的需求里面包含了整个模块要实现的主要核心功能，核心流程是什么，要有哪几个类才能实现这样的流程，不要妄图一下子深入到细枝末节上，让细节左右宏观上的逻辑架构，大脑不适合同时考虑宏观和微观的事情，尤其是对经验不太足的开发者来说，要逐渐学会大脑在不同的时期进行宏观和微观的无缝切换，这样才能专注目标和结果，在实现过程中再投入全部精力考虑细节，才能保证具体的实现是不偏离总体目标的。 WLRRouteRequest设计路由层的请求，无论是跨应用的外部调用(H5调用、其他App调用)还是内部调用(内部模块相互调用)，最后都要形成一个路由请求，一个以URL为基础的request对象，首先需要有携带URL，再一个要携带请求所需要的参数，参数有三种，一种是Url上的键值对参数，一种是RESTFul风格的Url上的路径参数，一种是内部调用适用的原生参数，具体是: 这里说一下路径参数，很多有后端开发经验的人都知道，一个url上传递参数，或者是匹配后端服务的service，Url的路径对于表达转发语义十分重要，比方说 :http://aaaa.com/loginhttp://aaaa.com/userCenter 那Url中的login和userCenter可以代表是哪个后端服务，那路由就需要设置正则匹配表达式去匹配http://aaaa.com/ 这部分，截取login、userCenter部分，说回我们的路由，App的路由需要通过设置Url的正则表达式来获取路径参数，同时我们必须知道这些参数的值和名称，那么我可以这样定义Url匹配的表达式scheme://host/path/:name([a-zA-Z_-]+)熟悉正则表达式的孩子都知道分组模式，path后name是key，([a-zA-Z_-]+)是规定name对应的value应该是什么格式的。那么routeParameters就是存放路径参数的 123456789101112//url@property (nonatomic, copy, readonly) NSURL *URL;//url上？以后的键值对参数@property (nonatomic, copy, readonly) NSDictionary *queryParameters;//url上匹配的路径参数@property (nonatomic, copy, readonly) NSDictionary *routeParameters;//原生参数，比方说要传给目标UIImage对象，NSArray对象等等@property (nonatomic, copy, readonly) NSDictionary *primitiveParams;//目标预留的callBack block,当完成处理以后,回到此Block，完成调用者的回调@property(nonatomic,copy)void(^targetCallBack)(NSError *error,id responseObject);//是否消费掉，一个request只能处理一次，该字段反应request是否被处理过@property(nonatomic)BOOL isConsumed; WLRRouteHandler设计handler对象要接收一个WLRRouteRequest对象来启动处理流程，前面经过我们的分析，这个handler应该担负起通过url和参数获取目标对象的职责，在一般的route处理中，目标往往是一个视图控制器，先实现这样一个通过url调用某一个视图控制器的并跳转处理的handler，那么应该是如下的： handler处理一个request请求是一个具有过程性的逻辑，WLRRouteHandler要作为一个基类，我们知道，这个handler在需要处理获取目标视图控制器-&gt;参数传递给目标视图控制器-&gt;视图控制器的转场-&gt;完成回调，那么我们需要设计这样的接口 123456789101112//即将开始处理request请求，返回值决定是否要继续相应request- (BOOL)shouldHandleWithRequest:(WLRRouteRequest *)request;//开始处理request请求-(BOOL)handleRequest:(WLRRouteRequest *)request error:(NSError *__autoreleasing *)error;// 根据request获取目标控制器-(UIViewController *)targetViewControllerWithRequest:(WLRRouteRequest *)request;//转场一定是从一个视图控制器跳转到另外一个视图控制器，该方法用以获取转场中的源视图控制器-(UIViewController *)sourceViewControllerForTransitionWithRequest:(WLRRouteRequest *)request;//改方法内根据request、获取的目标和源视图控制器，完成转场逻辑-(BOOL)transitionWithWithRequest:(WLRRouteRequest *)request sourceViewController:(UIViewController *)sourceViewController targetViewController:(UIViewController *)targetViewController isPreferModal:(BOOL)isPreferModal error:(NSError *__autoreleasing *)error;//根据request来返回是否是模态跳转- (BOOL)preferModalPresentationWithRequest:(WLRRouteRequest *)request; WLRRouteMatcher设计一个matcher应该具有根据url和参数判断是否匹配某个url表达式的逻辑 matcher对象必须拥有url的匹配表达式，类似于 scheme://host/path/:name([a-zA-Z_-]+) ，也有拥有该表达式真正的正则表达式，^scheme://host/path/([a-zA-Z_-]+)$ 1234567@interface WLRRouteMatcher : NSObject//url匹配表达式@property(nonatomic,copy)NSString * routeExpressionPattern;//url匹配的正则表达式@property(nonatomic,copy)NSString * originalRouteExpression;+(instancetype)matcherWithRouteExpression:(NSString *)expression;-(WLRRouteRequest *)createRequestWithURL:(NSURL *)URL primitiveParameters:(NSDictionary *)primitiveParameters targetCallBack:(void(^)(NSError *, id responseObject))targetCallBack; 设计-(WLRRouteRequest *)createRequestWithURL:(NSURL *)URL primitiveParameters:(NSDictionary *)primitiveParameters targetCallBack:(void(^)(NSError *, id responseObject))targetCallBack;这个方法，可以通过传入url和参数，检查是否返回request请求，来表示该WLRRouteMatcher对象所拥有的匹配表达式与url是否能够匹配，这句话有点绕，看不懂的多看几遍。 WLRRouterWLRRouter是路由实体对象，后端开发者对于路由挂载的概念非常了解，其实这样一个路由实体对象可以完成对URL的拦截和处理并返回结果，事实上，根据前面的梳理和总结，WLRRouter对象内部应该保存了需要匹配拦截的URL表达式，而前面我们知道Url的匹配表达式是存储在WLRRouteMatcher对象中的，并且一个Url传入检查是否匹配也是Matcher对象提供的功能，对于匹配上的Url需要有对应的Handler处理，所以Router对象的内部存在Machter对象和Handler对象一一对应的关系，并且拥有注册Url表达式对应到Handler的功能，也具有传入Url和参数就能匹配到Handler的功能，还要有一个检测Url是否能有对应Handler处理的功能，所以应该是： 这里有两种注册的方法，注册handler的就不需再多描述，另外一个是注册Block的回调形式，因为有时候可能会需要一些简单的Url拦截，去做一些事情，这里面的Block需要返回一个request对象，这是因为，如果Block没有对request的回调做处理，Router应该处理调用者的回调问题，否则就会出现调用者设置了回调的Block而没有人调用回来，这样就尴尬了。 1234567891011121314151617181920212223242526272829303132/** 注册一个route表达式并与一个block处理相关联 @param routeHandlerBlock block用以处理匹配route表达式的url的请求 @param route url的路由表达式，支持正则表达式的分组，例如app://login/:phone({0,9+})是一个表达式，:phone代表该路径值对应的key,可以在WLRRouteRequest对象中的routeParameters中获取 */-(void)registerBlock:(WLRRouteRequest *(^)(WLRRouteRequest * request))routeHandlerBlock forRoute:(NSString *)route;/** 注册一个route表达式并与一个block处理相关联 @param routeHandlerBlock handler对象用以处理匹配route表达式的url的请求 @param route url的路由表达式，支持正则表达式的分组，例如app://login/:phone({0,9+})是一个表达式，:phone代表该路径值对应的key,可以在WLRRouteRequest对象中的routeParameters中获取 */-(void)registerHandler:(WLRRouteHandler *)handler forRoute:(NSString *)route;/** 检测url是否能够被处理，不包含中间件的检查 @param url 请求的url @return 是否可以handle */-(BOOL)canHandleWithURL:(NSURL *)url;/** 处理url请求 @param URL 调用的url @param primitiveParameters 携带的原生对象 @param targetCallBack 传给目标对象的回调block @param completionBlock 完成路由中转的block @return 是否能够handle */-(BOOL)handleURL:(NSURL *)URL primitiveParameters:(NSDictionary *)primitiveParameters targetCallBack:(void(^)(NSError *, id responseObject))targetCallBack withCompletionBlock:(void(^)(BOOL handled, NSError *error))completionBlock; 梳理总结：从以上我们规划的几个类的接口，我们可以清楚的看到Router工作的流程。 首先实例化Router对象 实例化Handler或者是Block，通过Router的注册接口使得一个Url的匹配表达式对应一个Handler或者是一个block Router内部会将Url的表达式形成一个Matcher对象进行保存，对应的Handler或处理的Block会与Matcher一一对应，怎么对应呢？应该使用路由表达式进行关联 Router通过handle方法，接收一个Url的请求，内部遍历所有的Matcher对象，将Url和参数转换为Request对象，如果能转换为Request对象则说明能匹配，如果不能则说明该Url不能被路由实体处理 拿到Request对象以后，则根据Matcher对应的路由表达式找到对应的Handler或者是Block 根据Handler的几个关键方法，传入Request对象，按照顺序完成处理逻辑的触发，最后如果有request当中包含有目标的回调，则将处理结果通过回调的Block响应给调用方 Handler完成处理后，Router完成本次路由请求 Tips:很多开发者把敏捷开发当做来了需求不管三七二十一，一把梭子就是干，不断写不断改。🙊其实敏捷开发是一种模式，并不简单是快速迭代的意思。初入行的程序员其实都是coder(编码员)，基本上在靠模仿代码和代码套路去工作，真正的Programmer(程序设计师)是在设计代码，科班出身的程序员往往在进阶过程中突然发现大学里面的软件工程有多么重要，其实设计能力的培养需要有一个正规的流程，就像本教程的大纲一样，发现问题-&gt;需求分析-&gt;总体设计-&gt;具体实现-&gt;测试-&gt;发布维护，有了清晰的流程，把你的精力和时间按照不同的阶段进行分配和投入，你就会豁然开朗，比方说在总体设计过程中，就需要你以宏观的功能流程去考虑大体的模块有几个，模块的关系是怎样，每个模块的核心职责是什么，建议根据需求去画一个逻辑流程图，将每个逻辑分支都补全，再根据流程图规划总体框架，总体框架通过类图来表达，每个类的属性和行为都确定以后，再进入具体设计阶段就非常轻松和容易了，同时类图画完，技术方案的可行性和实现所需时间也就非常容易精确评估了 给架子填充骨血(具体实现)：有了上面大体上的架子我们就能相信只要按照这个架子，就能完成你在需求分析阶段规划的功能目标，现在我们要做的就是在我们设计的这个牛逼的框架里填充血肉，去实现它，这部分是一个非常有意思的过程，在上一步你已经获得了相当大的信心，在这一步你只需要按照规定尽力去实现，在有信心的情况下，你会思维活跃，因为你明确了要实现何种功能的目标，大脑会自动根据目标和现在差距不断想考出各种办法去弥补这样的差距，你所做的就是不断尝试你大脑迸发出的这些代码，选择最有效、可读性最好、性能最好和代码最健壮的代码。 WLRRouteRequest:了解了以上，我们从WLRRouteRequest入手。其实WLRRouteRequest跟NSURLRequest差不多，不过WLRRouteRequest继承NSObject，实现NSCopying协议，我们再来看一下头文件的声明： 123456789101112131415161718192021222324252627#import &lt;Foundation/Foundation.h&gt;@interface WLRRouteRequest : NSObject&lt;NSCopying&gt;//外部调用的URL@property (nonatomic, copy, readonly) NSURL *URL;//URL表达式，比方说调用登录界面的表达式可以为：AppScheme://user/login/138********，那URL的匹配表达式可以是：/login/:phone([0-9]+),路径必须以/login开头，后面接0-9的电话号码数字，当然你也可以直接把电话号码的正则匹配写全@property(nonatomic,copy)NSString * routeExpression;//如果URL是AppScheme://user/login/138********?callBack=&quot;&quot;,那么这个callBack就出现在这@property (nonatomic, copy, readonly) NSDictionary *queryParameters;//这里面会出现{@&quot;phone&quot;:@&quot;138********&quot;}@property (nonatomic, copy, readonly) NSDictionary *routeParameters;//这里面存放的是内部调用传递的原生参数@property (nonatomic, copy, readonly) NSDictionary *primitiveParams;//自动检测窃取回调的callBack 的Url@property (nonatomic, strong) NSURL *callbackURL;//目标的viewcontrolller或者是组件可以通过这个@property(nonatomic,copy)void(^targetCallBack)(NSError *error,id responseObject);//用以表明该request是否被消费@property(nonatomic)BOOL isConsumed;//简便方法，用以下标法取参数- (id)objectForKeyedSubscript:(NSString *)key;//初始化方法-(instancetype)initWithURL:(NSURL *)URL routeExpression:(NSString *)routeExpression routeParameters:(NSDictionary *)routeParameters primitiveParameters:(NSDictionary *)primitiveParameters targetCallBack:(void(^)(NSError * error,id responseObject))targetCallBack;-(instancetype)initWithURL:(NSURL *)URL;//默认完成目标的回调-(void)defaultFinishTargetCallBack;@end 初始化方法就是将三个存放入参的字典初始化，并直接将Url上的？以后的参数取出来： 1234567891011121314151617181920212223242526-(instancetype)initWithURL:(NSURL *)URL{ if (!URL) { return nil; } self = [super init]; if (self) { _URL = URL; _queryParameters = [[_URL query] WLRParametersFromQueryString]; } return self;}-(instancetype)initWithURL:(NSURL *)URL routeExpression:(NSString *)routeExpression routeParameters:(NSDictionary *)routeParameters primitiveParameters:(NSDictionary *)primitiveParameters targetCallBack:(void (^)(NSError *, id))targetCallBack{ if (!URL) { return nil; } self = [super init]; if (self) { _URL = URL; _queryParameters = [[_URL query] WLRParametersFromQueryString]; _routeExpression = routeExpression; _routeParameters = routeParameters; _primitiveParams = primitiveParameters; self.targetCallBack = targetCallBack; } return self;} 在调用方设置传入callBack的时候，因为request是消费型的，所以将TargetCallBack重新包装，在回调的时候需要将isConsumed属性设置为YES，表示该request已经被处理消耗，这里实现的比较简单，其实request是应该具有状态的，比方说未处理，处理中，已处理，实现一个优雅的状态机会更好的表达逻辑： 123456789101112-(void)setTargetCallBack:(void (^)(NSError *, id))targetCallBack{ __weak WLRRouteRequest * weakRequest = self; if (targetCallBack == nil) { return; } self.isConsumed = NO; _targetCallBack = ^(NSError *error, id responseObject){ weakRequest.isConsumed = YES; targetCallBack(error,responseObject); }; } 默认的回调方法是为了处理响应者没有触发回调，则需要有默认的回调给调用者： 12345-(void)defaultFinishTargetCallBack{ if (self.targetCallBack &amp;&amp; self.isConsumed == NO) { self.targetCallBack(nil,@&quot;正常执行回调&quot;); }} WLRRouteHandlerhttps://www.jianshu.com/p/3a902f274a3d WLRRouteMatcherhttps://www.jianshu.com/p/3a902f274a3d WLRRouterhttps://www.jianshu.com/p/3a902f274a3d 以上我们可以看到，Router将匹配的逻辑单独封装到WLRRouteMatcher对象中，将匹配后的结果生成WLRRouteRequest实例以携带足够完整的数据，同时将真正处理视图控制器的转场或者是组件的加载或者是未来可能拓展的handler业务封装到WLRRouteHandler实例中，匹配逻辑对应的处理逻辑干净分离，Matcher对象将匹配逻辑单独封装，如果有一天需要增加多种URL的匹配逻辑，则可以更换Matcher或者添加Matcher就可以，处理逻辑可以通过继承扩展或者冲洗WLRRouteHandler的生命周期函数来更好的处理回调业务。如果WLRRouteHandler不能提供足够多的扩展性，则可以使用block回调最大限度的进行扩展。 以上，就是路由部分的整体实现。 Tips:在具体的实现的过程中，不要被总体设计给套死，前面设计了几个类，设计了几个层次以后，就画地为牢，在实现具体细节的时候仍可以继续进行局部设计，分割逻辑，控制代码复杂度，一方面你要注重在局部进行进一步设计和分割的时候带来的究竟是什么，可读性？可维护性？还是帮助你能够进行思考？有时候，注释可能更加能给你的逻辑带来清晰的表述，毕竟几千行的代码很正常，如果没有经验，可能逻辑重点记得不太清楚，那就需要动动笔头好好记录一下，再进一步实现，将复杂逻辑进行适当分割，是一种常见的策略 最终的路由整体类图和如何设计代码的总结整体的设计就是如此，我们走了一趟从发现问题到最后通过封装一个路由组件解决问题的过程，在这里，对于新手程序员我想讨论一些有意义的东西，也就是手把手写出一个路由的过程中，思路从何而来？代码设计怎么从无到有？对于一个逻辑问题怎么转换为代码实践？授之以鱼不如授之以渔，在这个过程中我们总结一下哪些是比较重要的： 对于经常重复的工作和低效的工作流程有没有敏锐的察觉，是否有想解决的愿望，毕竟能解决了你能节省更多时间，何乐而不为？可有些人宁愿加班做一些简单的重复工作也不希望直面挑战创造一些解决问题的新方式 在思考问题的解决方案的时候，大脑有没有将问题抽象一下去寻找有没有相应解决问题的模式、方法论或者是方案？(比如说发现路由方案，发现中介者模式，发现分割思想和职责单一，发现软件工程的流程) 天赋并不是最重要的，甚至天赋是一种结论而不是条件，大脑不擅长同时处理很多事情，在你设计或者编码的过程中，通过标准化流程能最大程度的提高大脑思考的效率，比方说，总结问题就是总结问题而不要变总结变思考解决方案，否则会片面或者影响问题的客观描述，总结问题-&gt;需求分析-&gt;总体设计-&gt;具体实现，这就是一个简单的大脑思考流程，如果想不清楚，通过思维导图、流程图、UML建模等等，去把大脑需要同时照顾到的东西呈现在眼前，你只要去对比对照，一步步来就可以了，并不是有人特别有天赋，而是有人比你有方法 发散很重要，每当你从出现问题到解决问题以后，你可以安心的天马行空的头脑风暴，在这个过程中你可能对解决问题的方式进行重新审视和回顾，发现不足，甚至可以对其扩展使其能解决更多问题，就比如你会考虑路由的效率、安全，从而诞生中间件、异步事件的想法，这点很重要，但却很简单 自我引导能力，看了文章一步步做了出来，其实就跟做菜一样，你学会了一道菜该怎么做，但为什么别人能做，或者说别人从无到有的过程经历了什么，你为什么没想到，这种想法会激励你自我引导，从而提升自己，进一步改变自己，如果你从来没有对他人为什么能做到感到好奇，那就说明你的自我意识和自我引导力不够强，需要停下每天写代码的手，认真思考一下人生了 路由的安全有两个方面可以去做 WLRRouteHandler实例中， -(BOOL)shouldHandleWithRequest:(WLRRouteRequest *)request中可以检测request中的参数，比方说效验source或者是效验业务参数完整等 WLRRouter实例中handleURL方法，将按照中间件注册的顺序回调中间件，而我们可以在中间件中实现风控业务、认证机制、加密验签等等，中间件的实现大家可查看源码 路由的效率目前我们实现的路由是一个同步阻塞型的，在处理并发的时候可能会出现一些问题，或者是在注册比较多的route表达式以后，遍历和匹配的过程会损耗性能，比较好的实现方式是，将Route修改成异步非阻塞型的，但是API全部要换成异步API，起步我们先把同步型的搞定，随后我们将会参照promise范式来进行异步改造，提升路由效率。 路由的使用在大部分App实践MVVM架构或者更为复杂的VIPER架构的时候，除了迫切需要一个比较解耦的消息传递机制，如何更好的剥离目标实体的获取和配合UIKit这一层的转场逻辑是一项比较复杂的挑战，路由实际上是充当MVVM的ViewModel中比较解耦的目标获取逻辑和VIPER中Router层，P与V的调用全部靠Router转发。 在实施以组件化为目的的工程化改造中，如何抽离单独业务为组件，比较好的管理业务与业务之间的依赖，就必须使用一个入侵比较小的Route，WLRRoute入侵的地方在于WLRRouteHandler的transitionWithRequest逻辑中，通过一个UIViewController的扩展，给 targetViewController.wlr_request = request;设置了WLRRouteRequest对象给目标业务，但虽然如此，你依旧可以重写WLRRouteHandler的transitionWithRequest方法，来构建你自己参数传递方式，这一点完全取决于你如何更好的使得业务无感知而使用路由。","link":"/2019/05/08/iOS路由设计/"},{"title":"paper3:Skyway: Connecting Managed Heaps in Distributed Big Data Systems","text":"前言课程所需，才是驱动啊。 标题Skyway: Connecting Managed Heaps in Distributed Big Data SystemsSkyway：连接分布式大数据系统中的托管堆 Abstract诸如Java和Scala之类的托管语言普遍用于大规模分布式系统的开发。在托管运行时，在跨机器执行数据传输时，这是在大数据系统中经常执行的任务，系统需要在通过网络发送对象之前将对象序列化为字节序列。接收字节的远程节点然后将它们反序列化为对象。这个过程既有消耗性能又浪费资源：（1）对象序列化/反序列化大量使用反射，昂贵的运行时操作和/或（2）序列化/反序列化功能需要手写和容易出错。本文介绍了Skyway，这是一种基于JVM的技术，可以直接连接不同（本地或远程）JVM进程的托管堆。在Skyway下，源堆中的对象可以直接写入远程堆，而无需更改其格式。 Skyway完全消除了调用序列化/反序列化功能的需要（1），从而节省了CPU时间，并且（2）减少要求开发人员手写串行化功能，从而为任何基于JVM的系统提供性能优势。 CCS Concepts • Information systems → Data manage- ment systems; • Software and its engineering → Mem- ory management; Keywords Big data, distributed systems, data transfer, serialization and deserialization 1 Introduction现代大数据系统需要经常在群集中混洗数据 - 像Hadoop这样的map / reduce框架会在执行减少之前对每个map worker的结果进行洗牌;像Spark这样的数据流系统支持许多需要在节点之间存储数据的RDD转换。由于大多数这些系统都是用托管语言（如Java和Scala）编写的，因此数据表示为托管堆中的对象。跨节点传输对象很复杂，涉及图1所示的三个过程。（1）序列化过程将整个对象图从o转换为二进制序列。此过程重新格式化每个对象 - 除其他外，它提取对象数据，剥离对象标题，删除存储在对象中的所有引用，以及更改某些元数据的表示。 （2）该字节序列被传送到接收机。 （3）反序列化过程读出字节序列，相应地创建对象，最终在接收器机器的托管堆中重建对象图。 问题虽然已经开发了许多序列化/反序列化（S / D）库[3,22,32]，但在它们的实现中存在很大的不足。 我们自己的经验（§2）和之前工作的证据[27]表明，S / D占Spark中执行时间的30％。 为了解释为什么S / D如此昂贵，我们讨论了处理三个关键信息，这些信息必须从o可以获得的每个对象中提取，传输和重构：（1）对象数据（即原始类型） “elds”，（2）对象引用（即引用类型“elds”），以及（3）对象类型。 (1)对象数据访问：S / D库需要调用反射函数，例如Reflection.getField和Reflection.setField，在发送方以枚举和访问每个字段来抽取，在接收方然后回写，每个原始对象都是单独的。在大数据系统中，每个数据传输涉及数百万个对象，这些对象会调用这些函数数百万次或更多次。Reflection是一种非常昂贵的运行时操作。它允许程序动态检查或调用类，方法，字段或属性；在没有类型信息静态可用的在以耗时的字符串查找为代价，在性能关键任务中是不可取的。 （2）类型表示：每个类型由托管运行时中的特殊（元）对象表示，并由该类型的对象的标头引用。 但是，类型引用不能用于表示字节序列中的类型，因为表示相同类型的元对象可能在不同的运行时中具有不同的地址。 Java序列化程序通过包含类名及其所有父类的字符串表示每种类型。 该设计使元数据（即，类型字符串）消耗通过网络传输的大部分字节序列。 此外，必须使用反射来在接收器节点上的对象重新创建期间解析每个字符串的类型。（3）参考调整：需要调整传输对象的引用类型字段中包含的引用，因为这些对象将被放置在接收器节点上的不同地址中。Java序列化程序使用rection来获取引用对象的内容并将其内联到引用对象的二进制表示中。它使用反射构造从接收器机器上的o可到达的所有对象，然后通过反射设置参考字段与刚创建的引用对象的地址。 最近的进展许多第三方库已经开发出来。特别是，Kryo [22]是Spark推荐的库。 Kryo要求开发人员（1）为数据传输中涉及的类型手动定义S / D函数，这样可以加速对象数据访问，以及（2）在所有节点上以一致的顺序手动注册这些类型，这使得可以使用整数来表示类型。其他图书馆[3,11,32]遵循类似的原则。然而，基本的“数据传输的缺点仍然存在于Kryo中 - 需要为发送方和接收方的每个传输对象调用用户定义的函数。由于这些调用的数量非常大发送和接收，序列化和反序列化期间的S / D功能仍然占用数据处理任务运行时的很大一部分。此外，使用Kryo的开发人员承受着巨大的负担。开发人员很难理解涉及多少和哪些类型，更不用说一致地注册这些类型并为每种类型开发正确和有效的S / D功能。例如，考虑一个HashMap对象。它的序列化涉及其键值阵列，所有键/值对以及每个键/值对象。它的反序列化需要重新创建键和值对象，将它们配对，并另外重新设置键值/值对以正确地重新创建键值数组，因为键的哈希值可能已更改。 Our Solution – Skyway现有S / D库的关键问题在于，对于现有的JVM，除了第一次反汇编并将它们推送到（不同的）二进制格式之外，没有其他路由来传输对象，然后重新组装和拉动 他们备份到远程堆中。 在本文中，我们主张在托管堆之间建立一个“kyway”（如图1所示），这样数据对象就不再需要被推送到较低的级别进行传输。 Skyway增强了JVM，使对象图可以从堆移动到堆，并在移动后立即在远程节点上使用。 具体而言，给定应用程序指定的根对象（例如，Spark中的RDD对象），Skyway增强型JVM从o开始执行类似GC的堆遍历，将每个可到达对象复制到输出缓冲区，并执行轻量级调整存储在对象中的与机器相关的元数据，而无需更改对象格式。 然后可以将此输出缓冲区作为整体直接复制到远程堆中，并在传输后几乎立即使用。 这为现有和未来的大数据系统提供了以下好处：（1）Skyway完全消除了访问字段和类型的成本，节省了计算成本; （2）开发人员不需要手写任何S / D功能。 为了实现这些目标，Skyway比所有现有的S / D库更加有效地解决了上述三个问题，如下所述。 首先，Skyway通过更改JVM，将每个对象作为一个整体进行传输，这完全消除了访问单个数据字段的需要。 此外，由于对象的哈希码被高速缓存在对象的头部中，因此传输每个对象的整体保留了对象的原始哈希码，从而可以在接收器节点上使用基于哈希的数据结构而无需重新划分对象。 在传统的S / D中花费大量时间的过程。 其次，Skyway通过采用自动全局类型编号过程来表示类型 - 主节点维护所有类型及其ID的注册表，并且每个工作节点与主服务器通信以在类加载时获得其类的ID。 此过程使群集中的所有类都可以进行全局编号，而无需任何开发人员干预，因此每个ID可用于在不同节点上唯一标识相同的类。 第三，Skyway采用了一种有效的“复制”技术来调整参考。 当对象被复制到输出缓冲区中时，存储在它们中的指针在线性时间中被相对化 - 它们从绝对地址变为相对地址。 在接收到缓冲器时，接收器节点上的Skyway客户端执行输入缓冲器的另一线性扫描，以对该缓冲器中的相对信息进行绝对化。 Skyway可能会在网络上推送比S / D库更多的字节，因为它传输整个每个对象，但S / D库不传输对象头。 然而，大量证据[44]表明实际系统中的瓶颈正在从I / O转向计算，因此，我们认为这种设计能够实现正确的设计交易 - 计算成本的节省显着超过额外的网络 在现代网络上传输的额外字节所产生的I / O成本。 我们的实证结果表明，即使在1000Mb / s以太网上（例如，大多数数据中心使用带宽较高的网络），在Spark中为真实图形数据集传输50％以上的数据（总共约100GB）只会增加执行力 4％（在网络和读取I / O上），而通过消除S / D调用实现的节省超过20％。 它为什么有效？值得注意的是，Skyway不是通用的序列化器。 我们深入了解Skyway为大数据处理工作的原因是双重的。 首先，数据处理应用程序经常洗涤数百万个对象，并且在强烈划分的阶段这样做。 因此，批量发送对象而不改变其格式可提供显着的执行效率。 其次，使用现代网络技术可以快速传输额外的字节，而不会产生太多开销。 我们在OpenJDK 8中实现了Skyway。我们对Java序列化器基准测试集JSBS [34]，Spark [45]和Flink [2]的评估表明，（1）Skyway优于JSBS上所有90个现有的S / D库 ，它使用基于媒体内容的数据集 - 例如，它比Kryo快2.2倍，比Java序列化器快67.3倍; （2）与Kryo和Java序列化器相比，Skyway在四个真实数据集中对四个代表性分析任务的整体Spark性能提高了16％和36％; （3）对于Flink的另一个真实系统，与Flink高度优化的内置串行器相比，Skyway的整体性能提高了19％。 2 Background and Motivation2.1 BackgroundWhen Does S/D Happen? Spark在整个执行过程中进行S / D.有两类S / D任务：闭包序列化和数据序列化。关闭S / D发生在驾驶员和工人之间。由于驱动程序启动了Spark程序，因此驱动程序需要在远程工作程序上执行部分程序。图2显示了一个Spark程序，它从文本文件（第27行）读取一系列字符串，每个字符串代表一个日期。它接下来通过在RDD上调用map函数来解析这些字符串（第28行）。 map转换采用lambda表达式（即闭包）作为输入，它通过调用将字符串转换为Date对象的parse函数来解析每个字符串。最后，调用RDD操作collect以将所有Date对象引入驱动程序。当这个程序由驱动程序执行时，Spark会在工作节点上调度闭包的执行（即传递给map的lambda表达式）。因此需要Closure序列化来将闭包及其所需的一切从驱动程序传输到每个工作节点。在此示例中，闭包引用在其作用域之外创建的对象解析器。因此，解析器也需要在闭包序列化期间进行序列化。这解释了为什么DateParser类需要实现Java Serializable接口。 2.2 Motivation为了理解现实世界中的S / D成本，我们在Spark上进行了一系列实验。 我们在3个工作节点的小集群上执行Spark，每个节点都有2个Xeon（R）CPU E5-2640 v3处理器，32GB内存，1个SSD，运行CentOS 6.8。 这三个节点是通过In“niBand连接的大型集群的一部分。我们在LiveJournal图[4]上运行TriangleCounting算法，计算图形边缘引起的三角形数量。它被广泛用于社交网络分析，用于分析图形 连接属性[38]。我们使用Oracle JDK 8（build 25.71）并让每个slave运行一个执行器 - 每个slave上的单线程执行使我们可以很容易地测量性能的细分。 输入图大约为1.2GB，我们给每个JVM一个20GB的堆 - 一个足够大的堆来执行内存计算 - 就像Spark中的推荐做法一样。钨排序用于重排数据。 图4. Skyway的系统架构。 紫色和橙色矩形分别代表输入（堆内）缓冲区和输出（native）缓冲区; 对象传输沿着红色箭头。 #3 Design Overview本节概述了Skyway，解释了Skyway如何针对三个目标 - 正确性，高效率和易于集成。 图4显示了Skyway的系统架构，包括三个主要部分。 首先，为了实现正确的数据传输，Skyway修改JVM以在每个克隆对象中进行对象遍历，对象克隆和调整。 其次，为了实现高效的数据传输，Skyway谨慎地维护输入和输出缓冲区，并跨机器传输内容。 第三，为了使Skyway易于使用，Skyway库为应用程序开发人员提供了一组易于使用且向后兼容的API。 3.1 CorrectnessSkyway调整每个传输对象的机器特定部分，以保证执行的正确性。 首先，Skyway在发送期间使用自动维护的全局类型ID填充对象标题的类型字段，然后在接收节点上使用正确的类型表示替换它。其次，Skyway在发送期间用相对化参考替换存储在对象的所有非原始字段中的引用，并在接收期间将它们转回正确的绝对引用。 详情见§4.2。最后，当对象移动到另一台机器时，需要重置某些元数据，如GC位和锁定位。Skyway在发送时重置这些标志，并且在接收时不需要访问它们。 Skyway还为异构集群提供支持，其中不同机器上的JVM可支持不同的对象格式。 如果发送方和接收方节点具有不同的JVM规范，则Skyway会在将其复制到输出缓冲区时调整每个对象的格式（例如，标头大小，指针大小或标头格式）。 这仅在发送方节点上产生额外成本，而接收方节点不支付使用传输对象的额外成本。 对于同构群集，任何节点都不会产生这种平台调整成本。 Skyway使用的唯一假设是发送方和接收方使用每个与传输相关的类的相同版本 - 如果同一类的两个版本具有不同的字段，则对象读取将失败。 但是，这种假设并不是Skyway独有的; 它也需要适用于所有其他序列化器。 ##3.2 Efficiencykyway使用类似GC的遍历来发现可从一组根对象访问的对象图。 为了提高效率，Skyway使用缓冲-Skyway将遍历期间遇到的每个对象复制到发送节点上的缓冲区（即输出缓冲区），并将缓冲区内容流式传输到接收节点上的相应缓冲区（即输入缓冲区）。 输出和输入缓冲器都经过精心设计，以提高效率。 还支持多线程数据传输（参见§4）。Skyway输出缓冲区由接收器隔离 - 具有相同目标的对象被放入相同的输出缓冲区。 每个目标只存在一个这样的输出缓冲区。 发送对象后，可以安全地清除输出缓冲区。 Skyway输入缓冲区由发送方隔离，因此来自不同发送方的数据对象可以在不同步的情况下同时写入。 请注意，接收方节点的堆实际上可能包含每个发送方的多个输入缓冲区，每个输入缓冲区都保存在发送方的不同轮次中发送的对象。 除非开发人员使用API明确释放缓冲区，否则Skyway不会重用旧的输入缓冲区 - 例如Spark缓存内存中的所有RDD，因此Skyway会保留所有输入缓冲区。 输出缓冲区位于堆外本机内存中 - 它们不会干扰GC，如果这些缓冲区位于托管堆中，它可以在发送数据对象之前回收它们。 从托管堆分配输入缓冲区，以便来自远程节点的数据直接写入堆中，并可立即使用。 此外，尽管每个输入缓冲区在图4中显示为消耗连续的堆空间，但我们允许它跨越多个小内存块，原因有两个。 首先，由于流式传输，接收器可能不知道发送的字节数，因此，确定输入缓冲器大小是困难的。 其次，分配大的连续空间可能很快导致内存碎片，这可以通过使用较小的内存块来有效地减轻（§4.3）。 流式Skyway为这些缓冲器提供的一个重要特性：对于输出缓冲器，如果我们在所有对象都进入之前不发送数据，那么它既无时间且又耗费空间。 对于输入缓冲器，流式传输将允许计算与数据传输并行执行。 支持流式传输会带来许多挑战，例如，如何在没有多次扫描的情况下调整指针以及如何管理接收器节点上的内存（4.2）。 3.3 Ease of IntegrationSkyway旨在为应用程序开发人员提供简单的界面。 Skyway不仅应该支持全新系统的开发，还应该支持Spark等现有系统的简单S / D库集成。 为此，Skyway提供了一组与标准Java序列化程序直接兼容的高级Java API。 Skyway提供SkywayObjectOutputStream和SkywayObjectInputStream类，它们是标准ObjectOutputStream和Object- InputStream的子类。 这两个类为Skyway（本机）实现readObject和writeObject方法创建了一个接口。 SkywayObjectOutput- Stream / SkywayObjectInputStream对象与输出/输入缓冲区相关联。 我们还创建了SkywayFileOutputStream / SkywayFileInputStream和SkywaySocketOutputStream / SkywaySocketInputStream类，可以像使用Java序列化程序一样使用Skyway轻松编程。 将程序从使用其原始库切换到使用Skyway需要修改代码。 例如，我们根本不需要更改对象写入/读取调用，例如stream.writeObject（o）。 唯一的修改是（1）将流式例化为SkywayFileOutputStream对象而不是任何其他类型的ObjectOutputStream对象，以及（2）使用API函数shuffleStart识别混洗阶段。 由于我们所有的输出数据都需要在下一个调整阶段开始之前被清除（第4步），因此Skyway需要开发人员的标记才能知道何时清除这些数据。 识别分支阶段通常很简单 - 在许多系统中，一个分支阶段由一个shuffle函数实现，开发人员可以简单地在函数的开头调用shuffleStart。 另请注意，编写为在大数据系统上运行的用户程序（如图2中的程序）大多不直接使用S / D库，因此可以在没有更改的情况下从Skyway获益。 最后，Skyway提供了一个界面，允许开发人员在传输后轻松更新某些对象字段，例如出于语义原因重新初始化某些字段。 例如，下面的代码片段使用用户定义的函数updateTimeStamp在传输Record对象时返回的值更新类Record中的eld时间戳。 当然，我们希望很少使用此接口 - 在我们的实验中从未发生传输后需要更新对象数据内容。 4 Implementation我们在Oracle的生产JVM Open-JDK 1.8.0（版本25.71）中实现了Skyway。 除了实现我们的对象传输技术之外，我们还修改了类加载器子系统，对象/堆布局和Parallel Scavenge垃圾收集器，它是OpenJDK 8中的默认GC。我们还为开发人员提供了一个Skyway库。 图5.用于全局类编号的类型注册表。 4.1 Global Class NumberingSkyway开发了一种分布式类型注册系统，该系统可以自动允许不同JVM实例上同一类的不同表示共享相同的整数ID。 该系统完全消除了在数据传输期间使用字符串表示类型的需要（如在标准Java序列化程序中）或人类开发人员参与理解和注册类（如在Kryo中）。 Skyway类型注册在每个JVM内部运行，并维护一个类型注册表，它将每个类型字符串映射到其唯一的整数ID。 驱动程序JVM为所有类分配ID; 它维护一个完整的类型注册表，涵盖已经加载到集群中的所有类，并且自计算开始以来已为驱动程序所知。 每个工作者JVM都有一个注册表视图，它是驱动程序上类型注册表的一个子集; 它检查驱动程序以获取它加载的每个类的ID，并且在本地注册表视图中不存在。 图5显示了这些注册表的一个示例。 算法1描述了在驱动程序和工作程序JVM上运行的算法。 用户通过插入客户端代码中的API调用来选择驱动程序。 例如，对于Spark，可以自然地将运行Spark驱动程序的JVM指定为Skyway驱动程序，并且所有Spark工作程序节点都运行Skyway工作程序。 应用程序提供了容错功能 - 例如，在崩溃时，Spark会在配备Skyway的JVM上重新启动系统; Skyway的驱动程序JVM将在托管Spark驱动程序的节点上启动。 开始时，驱动程序通过在JVM完成其启动逻辑之后扫描其自己的已加载类来填充注册表（第4行 - 第8行）。 接下来，驱动程序通过运行监听端口的守护程序线程来切换到后台，以处理来自工作程序的查找请求（第10行 - 第19行）。 Skyway使用driver和worker之间的拉动式沟通。 在启动worker JVM时，它首先请求（第22行）并通过“EQU EQUET_VIEW”消息从driver驱动程序获得（第12行）当前完整类型注册表。 这为每个工作者JVM提供了在其启动时到目前为止在集群中加载的所有类的视图。 这种设计背后的基本原理是，这个worker JVM所需的大多数类可能已经由驱动程序或其他工作者注册。 因此，批量获取ID比制作单独的远程获取请求更有效。 我们修改每个worker JVM上的类加载器，以便在加载类时，加载器获取该类的ID。加载器首先在其自己的JVM中查询注册表视图。 如果它无法找到该类，则继续通过“LOOKUP”消息与类名字符串进行通信（第29行 - 第34行）。 如果字符串存在于其自己的注册表中，则驱动程序返回ID，或者创建新ID并将其与类名称一起注册（第18行）。 一旦工作人员收到此ID，它就会更新其注册表视图（第34行）。 最后，worker JVM将此ID写入类的元对象（第35行）。 在JVM术语中，元对象称为“klass”（如图5所示）。 我们在每个klass中添加一个额外的字段以容纳其ID。 ??? 在反序列化期间，如果我们在worker JVM上遇到一个unload的类，Skyway会指示类加载器加载缺少的类，因为类型注册表知道完整的类名。 虽然其他选项（例如，低冲突散列函数，例如MD和SHA系列）可以实现为每个类分配唯一ID的相同目标，但Skyway不能使用它们，因为它们不能用于恢复类名。 与通过网络在每个对象上发送类型字符串的标准Java序列化程序相比，Skyway在整个计算过程中为每台机器上的每个类最多发送一次类型字符串。 当然，在Skyway下传递的字符串数量要小几个数量级。 与Kryo相比，Skyway自动注册所有类，并且无需开发人员了解哪些类将涉及数据传输，从而显着减少了人力。 4.2 Sending Object GraphOverview当在SkywayObjectOutputStream对象上调用writeObject（root）时，Skyway开始遍历并发送可从root访问的对象图。 算法2描述了复制从用户指定的根可到达的对象图的单线程逻辑，我们将在本节后面讨论多线程扩展。 在高层次上，Skyway模仿基于BFS的GC遍历。 它维护一个灰色队列，保存已访问但尚未处理的每个对象的记录，以及将该对象放置在输出缓冲区ob中的位置addr。 主循环的每次迭代（第8行）以灰色处理顶部记录并执行三个任务。 首先，基于从灰色检索的对象 - 地址对（s，addr），将对象s克隆到从addr计算的位置处的缓冲区ob中（第10行）。 如果Skyway检测到接收方JVM与发送方JVM具有不同的规范，则在缓冲区中克隆也会调整克隆的格式，遵循用户提供的配置文件，该文件指定不同JVM中的对象格式。 其次，更新克隆的标题（第12-22行）。 第三，对于s的每个参考类型字段f，如果尚未访问o，则Skyway将引用的对象o推入工作队列灰色，然后用相对化的地址（输出缓冲区中的s位置）更新f，这将是 在接收机上进行快速参考调整（第15 - 27行）。 当对象被复制到本机存储器中的缓冲区时，可以刷新缓冲区（即，流式传输过程）。 第10行的分配触发了冲洗 - 分配首先检查该缓冲区是否仍有空间用于对象s; 如果没有，则刷新缓冲区ob和ob的值。 ob.flushedBytes增加了缓冲区的大小。 Reference Relativization想象一下，对象s的参考场f指向一个对象o。 Skyway需要在输出缓冲区中调整f，因为o可以放在接收器节点上的不同地址。 Skyway用ob中相对地址替换克隆字段f，其中o将被克隆到。这将允许接收器节点在确定输入缓冲区的起始地址后，轻松地为输入缓冲区中的每个引用计算正确的绝对值。 我们首先描述整体相对化算法，然后讨论Skyway如何解决由流和多阶段数据混洗引起的三个挑战。 Algorithm2 find如算法2的第15-27行所示，对于每个参考类型字段s.f，Skyway遵循引用以找到对象（o）。 Skyway确定当前数据重排阶段是否访问过o; 细节很快就会讨论。 如果不是（第20行），我们知道o将被克隆到ob.allocableAddr位置的输出缓冲区的末尾。 我们使用此位置来填充o的baddr字段（第22行），并将ob.allocableAddr提高大小为o，以跟踪ob中下一个克隆对象的起始地址。 如果访问过o（第26行），我们将从其对象标题中baddr字段的最低七个字节中检索其在输出文件中的位置，稍后我们将对此进行解释。 然后，我们使用此缓冲区位置newAddr更新f的克隆，在该位置将放置o的克隆（第27行）。 第一个挑战与streaming有关。 当Skyway尝试使用o的克隆的输出缓冲区位置更新f（f指向o）时，此克隆可能已经流出并且不再存在于物理输出缓冲区中。 因此，Skyway必须仔细存储这样的缓冲区位置信息，使其在整个数据处理阶段都可用。 Skyway使用额外的字段baddr将缓冲区位置保存在原始对象的标头中，而不是克隆。 修改后的对象布局如图6（a）所示。 当通过来自另一个对象o’的引用再次到达o时，o中的baddr将用于更新o’的克隆中的引用。 第二个挑战也与streaming有关。存储在对象s的baddr中以及其在灰色队列中的记录中的缓冲区位置都表示在s之前已经提交给输出缓冲区中的其他对象的累积字节。 但是，当Skyway克隆到缓冲区时，需要考虑物理缓冲区可能已多次刷新的流式传输效果。 因此，Skyway将先前刷新的字节数减去ob。 来自addr的ushedBytes计算应该复制s的地址中的实际地址（第10行）。 第三个挑战是由于多阶段数据混洗。 由于一个对象可能涉及多个混洗阶段，我们需要将其baddr字段的使用分开用于不同的混洗阶段。 Skyway使用sID来唯一地识别混洗阶段。每当Skyway更新baddr字段时，当前的sID将被写为baddr的最高字节的前缀。 因此，Skyway可以容易地检查baddr字段中的内容是在数据混洗（即，有效）的相同阶段期间还是在较早阶段（即，无效）期间计算的。 示例在算法2的第2 - 5行和第19 - 20行。在前一种情况下，如果root已经在同一个shuffing阶段被复制（由于另一个根对象启动的复制过程），Skyway只是创建一个向后 指向其在缓冲区中的位置的引用（第30行）。 Skyway提供了一个API函数shuffleStart，开发人员可以使用它来标记一个shuffing阶段。 调用shuffleStart时，sID会递增。 4.3 Receiving Object Graph通过精心设计发送，接收逻辑更加简单。 为了从发送方接收对象，接收方JVM“首先准备一个输入缓冲区，其大小是用户可调的，用于其托管堆中的发送方来存储传输的对象。这里的一个微妙问题是发送方节点可能使用多个流（在多个线程中）同时向同一接收节点发送数据。为了避免竞争条件，接收节点为每个发送方的每个流创建一个输入缓冲区，这样不同的流/线程就可以在不同步的情况下传输数据。我们创建 超大缓冲区，以适应大小超过常规缓冲区的对象。 填充输入缓冲区后，Skyway会对缓冲区执行线性扫描，以对类型和指针进行绝对化。 对于每个对象的klass字段，Skyway查询本地注册表视图以根据类型ID获取正确的klass指针，并将指针写入字段。 对于存储在参考字段中的相对地址，Skyway将其替换为+ s，其中s是此输入缓冲区的起始地址。 有一个与流媒体相关的挑战。 由于Skyway在分配缓冲区时可能不知道传入数据的总大小，因此固定长度的一个缓冲区可能不够大。 Skyway通过支持链接的块来解决这个问题 - 当旧的块耗尽空间时，可以创建新块并将其链接到旧块。 Skyway不允许对象跨越多个块以提高效率。 此外，当缓冲区包含多个块时，需要改变上面讨论的地址转换。 我们首先需要计算相对地址a将落入哪个块。然后，因为以前的块可能没有完全填充，我们需要计算第i个块中的a的ffset。 假设s i是块i的起始地址，因此，s i + o ffset是a的最终绝对地址。 该地址将用于替换每个指针字段中的a。 由于每个输入缓冲区对应于不同的发送方，我们可以安全地开始计算以处理每个流式传输的对象。 这不会产生安全问题，因为来自不同节点的对象不能相互引用。 但是，我们确实需要阻止对数据进行流式处理的计算，直到完成绝对化传递。 接收到对象后，接收器JVM上的Skyway客户端必须使这些对象在垃圾回收中可访问。 Skyway在托管堆的旧代（tenured）中分配所有输入缓冲区。 在Skyway中，我们使用Parallel Scavenge GC（即OpenJDK 8中的默认GC），它使用一个卡片表，将对象分组为固定大小的桶，并跟踪哪些桶包含带有年轻指针的对象。 因此，我们在Skyway中添加了对适当更新卡表的支持，以表示从每次数据传输生成的新指针。 5 Evaluation为了彻底评估Skyway，我们进行了三组实验，一组是广泛使用的基准测试套件，另外两组是广泛部署的系统Spark和Flink。 第一组实验侧重于将Skyway与所有现有的S / D库进行比较 - 由于大多数这些库无法直接插入实际系统，我们使用了Java序列化器基准集（JSBS）[34]，这是设计的 特别是评估Java / Scala序列化器，以了解Skyway在现有S / D库中的位置。 JSBS最初设计用于评估单机S / D. 我们修改了这个程序，使其在分布式环境中工作; 细节很快就会讨论。 在第二组和第三组实验中，我们修改了Spark和Flink代码，以替换使用Kryo和Java序列化程序（在Spark中）和内置序列化程序（在Flink中）与Skyway，以评估的好处 Skyway到现实世界的分布式系统。 我们所有的实验都在一个拥有11个节点的集群上运行，每个节点都有2个Xeon（R）CPU E5-2640 v3处理器，32GB内存，1个100GB SSD，运行CentOS 6.8并通过1000Mb / s以太网连接。 每个节点运行8个作业实例。 每个节点上的JVM配置为30GB堆。 JSBS包含多个工作负载，在这些工作负载下重复执行每个串行器和解串器。 每个工作负载包含多个媒体内容对象，这些对象由原始int和long字段以及引用类型字段组成。 驱动程序创建了数百万个这样的对象，每个对象的JSON格式大约为1KB。 这些对象被序列化为内存中的字节数组，然后将其反序列化回堆对象。 为了理解传输由不同序列化器生成的字节序列的成本，化器生成的字节序列的成本，我们修改了基准测试，将其转换为分布式程序 - 每个节点序列化这些对象，将生成的字节广播到所有其他节点，并反序列 为了执行这个程序，我们参与了ve节点并重复执行了1000次这个过程。 报告每个对象的平均S / D时间和网络成本。 以上及其几何平均值。 Spark上Skyway和Kryo的性能总结：在总运行时间，序列化时间，写入I / O时间和反序列化时间，读取I / O时间（包括网络成本）方面标准化为基线（Java序列化器）， 和生成的字节序列的大小。 值越低表示性能越好。 每个单元格显示百分比范围及其几何平均值。","link":"/2019/01/02/paper3-Skyway-Connecting-Managed-Heaps-in-Distributed-Big-Data-Systems/"},{"title":"pytorch学习","text":"Introduction to Neural Network 在人工神经网络里，没有产生新连接，网络固定不变 反向传播: 对比预测答案和真实答案的差别，再将差别去 反向传播 调整参数，提高正确率 详细的训练: 每个神经元都有一个自己的activate function, 刺激行为 神经网络: 梯度下降公式-》 优化问题optimization 求导求微分，Gradient Descent 沿着梯度去下降，得到最小的W值 -&gt; 优化问题 迁移学习: 拆掉输出层，保留分辨能力，添加其他层，进行另外的功能 PytorchPyTorch 是 Torch 在 Python 上的衍生. 因为 PyTorch 是一个使用 PyTorch 语言的神经网络库, Torch 很好用, 但是 Lua 又不是特别流行, 所有开发团队将 Lua 的 Torch 移植到了更流行的语言 Python 上. 是的 PyTorch 一出生就引来了剧烈的反响. 为什么呢? 而且如果你知道 Numpy, PyTorch 说他就是在神经网络领域可以用来替换 numpy 的模块. 神经网络在做什么神经网络在学习拟合线条(回归): PyTorch 和 Tensorflow据 PyTorch 自己介绍, 他们家的最大优点就是建立的神经网络是动态的, 对比静态的 Tensorflow, 他能更有效地处理一些问题, 比如说 RNN 变化时间长度的输出. 而我认为, 各家有各家的优势和劣势, 所以我们要以中立的态度. 两者都是大公司, Tensorflow 自己说自己在分布式训练上下了很大的功夫, 那我就默认 Tensorflow 在这一点上要超出 PyTorch, 但是 Tensorflow 的静态计算图使得他在 RNN 上有一点点被动 (虽然它用其他途径解决了), 不过用 PyTorch 的时候, 你会对这种动态的 RNN 有更好的理解. Torch 或 NumpyTorch 自称为神经网络界的 Numpy, 因为他能将 torch 产生的 tensor 放在 GPU 中加速运算 (前提是你有合适的 GPU), 就像 Numpy 会把 array 放在 CPU 中加速运算. 所以神经网络的话, 当然是用 Torch 的 tensor 形式数据最好咯. 就像 Tensorflow 当中的 tensor 一样. 1234567import torchimport numpy as npnp_data = np.arange(6).reshape((2, 3))torch_data = torch.from_numpy(np_data)tensor2array = torch_data.numpy()np.sin(data) == torch.sin(data) 除了简单的计算, 矩阵运算才是神经网络中最重要的部分. 所以我们展示下矩阵的乘法. 注意一下包含了一个 numpy 中可行, 但是 torch 中不可行的方式. variable就是存放神经网络参数的东西，并且神经网络优化一般都是优化类型为variable的节点 123456789# matrix multiplication 矩阵点乘data = [[1, 2], [3, 4]]tensor = torch.FloatTensor(data)# correct methodprint( \"\\nmatrix multiplication (matmul)\", \"\\nnumpy\", np.matmul(data, data), \"\\ntorch\", torch.mm(tensor, tensor)) 变量 (Variable)在 Torch 中的 Variable 就是一个存放会变化的值的地理位置. 里面的值会不停的变化. 就像一个裝鸡蛋的篮子, 鸡蛋数会不停变动. 那谁是里面的鸡蛋呢, 自然就是 Torch 的 Tensor 咯. 如果用一个 Variable 进行计算, 那返回的也是一个同类型的 Variable. 我们定义一个 Variable: 12345678910import torchfrom torch.autograd import Variable # torch 中 Variable模块# 先 生鸡蛋tensor = torch.FloatTensor([[1, 2], [3, 4]])# 把鸡蛋 放到篮子中， require_grad是参不参与误差反向传播,要不要计算梯度variable = Variable(tensor, requires_grad = True)print( tensor) Variable 计算, 梯度我们再对比下Tensor的计算和variable的计算 1234567import torchfrom torch.autograd import Variabletensor = torch.FloatTensor([[1, 2], [3, 4]])t_out = torch.mean(tensor*tensor)v_out = torch.mean(variable*variable)print(t_out) # 7.5print(v_out) # 7.5 到目前为止, 我们看不出什么不同, 但是时刻记住, Variable 计算时, 它在背景幕布后面一步步默默地搭建着一个庞大的系统, 叫做计算图, computational graph. 这个图是用来干嘛的? 原来是将所有的计算步骤 (节点) 都连接起来, 最后进行误差反向传递的时候, 一次性将所有 variable 里面的修改幅度 (梯度) 都计算出来, 而 tensor 就没有这个能力啦. v_out = torch.mean(variable*variable) 就是在计算图中添加的一个计算步骤, 计算误差反向传递的时候有他一份功劳, 我们就来举个例子: 12345678910111213141516171819import torch from torch.autograd import Variable # 导入 Variable 模块tensor = torch.FloatTensor([[1, 2], [3, 4]]) variable = Variable(tensor, requires_grad = True)t_out = torch.mean(tensor*tensor)v_out = torch.mean(variable*variable)print(t_out)print(v_out)v_out.backward() # 模拟v_out误差反向传播# Variable是计算图是的一部分# v_out 是 1/4 * sum(variable*variable) 这是计算图中的 v_out 计算步骤# 针对 v_out 的梯度是 d(v_out)/d(variable) = 1/2 * variable# 初始化的梯度print( \"variable.grad: \",variable.grad) 获取Variable 里面的数据直接print(variable)只会输出Variable 形式的数据，很多时候用不了(plt画图)，转化成Tensor 123print(variable) # Variable 形式print(Variable.data) # tensor 形式print(variable.data.numpy()) # numpy形式 激励函数 Activation Function为什么需要？解决 日常生活总不能用 线性方程解决的问题 Linear; NonLinear; y = Wx =&gt; y = AF(Wx) ; W 就是我们要求的参数, y 是预测值, x 是输入值. 激励函数 AF 就是指的激励函数(本身就是非线性的，必须可微分). 激励函数拿出自己最擅长的”掰弯利器”, 套在了原函数上 用力一扭, 原来的 Wx 结果就被扭弯了。 你甚至可以创造自己的激励函数来处理自己的问题, 不过要确保的是这些激励函数必须是可以微分的, 因为在 backpropagation 误差反向传递的时候, 只有这些可微分的激励函数才能把误差传递回去。当你使用特别多层的神经网络, 在掰弯的时候, 玩玩不得随意选择利器. 因为这会涉及到梯度爆炸, 梯度消失的问题。卷积神经网络 Convolutional neural networks 的卷积层中, 推荐的激励函数是 relu. 在循环神经网络中 recurrent neural networks, 推荐的是 tanh 或者是 relu pytorch activation function 神经网络中的每一层出来都是线性的函数关系，而在日常生活许多都是非线性关系。因此我们需要用激活函数将线性网络转成非线性结果。就是让神经网络可以描述非线性问题的步骤, 是神经网络变得更强大. Torch中的激励函数Torch中的激励函数: relu, sigmoid, tanh, softplus 123456import torchimport torch.nn.funcational as F # 激励函数在此from torch.autograd import Variable# 做数据x = torch.linespace(-5, 5, 200) # x data (Tensor), shape=(100, 1)x = Variable(x) 接着就是做生成不同的激励函数数据: 1234567x_np = x.data.numpy() # 换成 numpy, array,出图用# 常用的 激励函数y_relu = F.relu(x).data.numpy() # 0 -&gt; y_sigmoid = F.sigmoid(x).data.numpy() # 0 ~ 1y_tanh = F.tanh(x).data.numpy() # -1 ~ 1y_softplus = F.softplus(x).data.numpy() # 0 ~# y_softmax = F.softmax(x) softmax 比较特殊， 不能直接显示，他是关于概率的，用于分类 12345678910111213141516171819202122232425import matplotlib.pyplot as plt # python 的可视化模块plt.figure(1, figsize = (8, 6))plt.subplot(221)plt.plot(x_np, y_relu, c='red', label=\"relu\")plt.ylim((-1, 5))plt.legend(loc=\"best\")plt.subplot(223)plt.plot(x_np, y_sigmoid, c=\"blue\", label=\"sigmoid\")plt.ylim((-0.1, 1.2))plt.legend(loc=\"best\")plt.subplot(222)plt.plot(x_np, y_tanh, c=\"orange\", label=\"tanh\")plt.ylim((-1.1, 1.1))plt.legend(loc=\"best\")plt.subplot(224)plt.plot(x_np, y_softplus, c=\"yellow\", label=\"softplus\")plt.ylim((-1, 5))plt.legend(loc=\"best\")plt.show() 神经网络神经网络分为两类: 回归和分类； 回归，就是结果为一些系列连续的值，如f(房价) = W(大小，地点等)； 分类，就是判断结果的类别， 如图像中判断 是 猫还是狗 关系拟合(回归)来见证神经网络是如何通过简单的形式将一群数据用一条线条来表示. 或者说, 是如何在数据当中找到他们的关系, 然后用神经网络模型来建立一个可以代表他们关系的线条. 建立数据集我们创建一些假数据来模拟真实的情况. 比如一个一元二次函数: y = a * x^2 + b, 我们给 y 数据加上一点噪声来更加真实的展示它. 123456789import torchimport matplotlib.pyplot as pltx = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1) # x data (tensor),shape=(100, 1)y = x.pow(2) + 0.2*torch.rand(x.size()) # nosiy y data (tensor), shape=(100, 1)# 动画plt.scatter(x.data.numpy(), y.data.numpy())plt.show() 建立神经网络建立一个神经网络我们可以直接运用 torch 中的体系. 先定义所有的层属性(__init__()), 然后再一层层搭建(forward(x))层与层的关系链接. 建立关系的时候, 我们会用到激励函数, 12345678910import torchimport torch.nn.funcational as Fclass Net(torch.nn.Module): # 1. 继承 torch 的Module def __init__(self): # 2. 初始化模块， 去继承， 搭建完成每层的定义，信息 super(Net, self).__init__() # 2. 继承关系 __init__ 功能 pass def forward(self, x): # 3. 完成层与层之间的 forward的联系, 前向传播 pass 1234567891011121314151617181920212223242526import torchimport torch.nn.funcational as Fclass Net(torch.nn.Module): # 1. 继承 torch 的Module def __init__(self, n_features, n_hidden, n_output): # 2. 初始化模块， 去继承， 搭建完成每层的定义，信息; 神经元的个数 super(Net, self).__init__() # 2. 继承关系 __init__ 功能 # 4. 每一层的信息都是 模块的一个属性,属性的内容就是一层神经网络: 多少个输入，多少个输出，做什么操作 # 4. 这是只是搭建了各层神经元的内容 self.hidden = torch.nn.Linear(n_features, n_hidden) self.predict = torch.nn.Linear(n_hidden, n_output) pass def forward(self, x): # 3. 完成层与层之间的 forward的联系, 前向传播流程 # 5. 层与层之间的关系， 整个搭建过程，用激活函数激活 # 正向传播输入值, 神经网络分析出输出值 x = F.relu(self.hidden(x)) x = self.predict(x) # 输出层不需要用激活函数去截断 return xnet = Net(n_features=1, n_hidden=10, n_output=1)print(net)# net 的结构\"\"\"Net ( (hidden): Linear (1 -&gt; 10) (predict): Linear (10 -&gt; 1))\"\"\" 训练网络训练步骤很简单: 1234567891011# 设置 优化函数；设置 误差函数； 迭代训练: 喂数据给模型，计算得到 loss，更新梯度，参数优化# optimizer 是训练工具 优化参数optimizer = torch.optim.SGD(net.parameters(), lr=0.2) # 传入 net所有参数,学习率loss_func = torch.nn.MSELoss() # 预测值和真实值的误差计算公式(均方差)回归 mean squre error； 分类: cross entropyfor t in range(100): prediction = net(x) # 喂给 net 训练数据 x； 输出 预测值 loss = loss_func(prediction, y) # 计算 误差, 拿去反向传播 optimizer.zero_grad() # 清空上一步的残余更新参数值 loss.backward() # 误差反向传播，计算参数更新值 optimizer.step() # 用optimizer去优化 将参数更新到施加到 net 的parameters 上 可视化训练过程理解如何训练 1234567891011121314151617181920import matplotlib.pyplot as pltplt.ion() # 画图plt.show()optimizer = torch.optim.SGD(net.parameters(), lr=0.2)loss_func = torch.nn.MSELoss()for t in range(200): prediction = net(x) loss = loss_func(prediction, y) optimizer.zero_grad() loss.backward() optimizer.step() if t%5 == 0: # plt and show learning process plt.cla() plt.scatter(x.data.numpy(), y.data.numpy()) plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5) plt.text(0.5, 0. \"Loss=%.4f\" % loss.data.numpy(), fontdict={'size': 20, 'color': 'red'}) plt.pause(0.1) 区分类型(分类)Classification 分类; x 是32bit FloatTensor, y 64bit FloatTensor 建立数据集创建一些假数据来模拟真实的情况. 比如两个二次分布的数据, 不过他们的均值都不一样. 1234567891011121314import torchimport matplotlib.pyplot as plt# 模拟分类数据n_data = torch.ones((100, 2)) # 数据基本形态， shape=(100, 2)x0 = torch.normal(2*n_data, 1) # 类型 0 x (tensor), shape=(100,2)y0 = torch.zeros((100,)) # y0 = torch.zeros((100, )) 类型 0 y data (tensor), shape=(100,)x1 = torch.normal(-2*n_data, 1) # 类型 1 x data (tensor),shape=(100, 2)y1 = torch.ones((100,)) # 类型1 y data (tensor),shape=(100,)# pytorch的数据形式 x = torch.cat((x0, x1), 0).type(torch.FloatTensor) # FloatTensor = 32-bit floatingy = torch.cat((y0, y1),).type(torch.LongTensor) # LongTensor = 64-bit integerplt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=y.data.numpy(), s=100, lw=0, cmap='RdYlGn')plt.show() 建立神经网络建立一个神经网络我们可以直接运用 torch 中的体系: 先定义所有的层属性(__init__()) , 定义神经元个数 再一层层搭建(forward(x))层于层的关系链接.建立关系的时候, 我们会用到激励函数 12345678910111213141516import torchimport torch.nn.functional as F # activationclass Net(torch.nn.Module): # 神经元的节点数 def __init__(self, n_feature, n_hidden, n_output): super(Net, self).__init__() # 继承 __init__ 功能 self.hidden = torch.nn.Linear(n_feature, n_hidden) self.predict = torch.nn.Linear(n_hidden, n_output) pass def forward(self, x)： x = F.relu(self.hidden(x)) x = self.predict(x) return x# 初始化net = Net(n_feature=2, n_hidden=10, n_output=2) # 几个类别就几个 outputprint(net) 训练网络用优化器 训练 网络参数；loss 反向传播 1234567891011optimizer = torch.optim.SGD(net.parameters(), lr=0.02) # set up optimizer, 传入参数，学习率# 算误差时，注意真实值！ 不是one-hot形式的， 而是1D Tensor，(batch,)# 但是预测值是2D tensor (batch, n_classes)loss_func = torch.nn.CrossEntropyLoss()for t in range(100): out = net(x) # 喂给 net 训练数据 x, 输出分析值 loss = loss_func(out, y) # 计算两者的误差 optimizer.zero_grad() # 清空上一步的残余更新参数值 loss.backward() # 误差反向传播, 计算参数更新值 optimizer.step() # 将参数更新值施加到 net 的 parameters 上 可视化训练过程1234567891011121314151617181920212223import matplotlib.pyplot as pltplt.ion()plt.show()optimizer = torch.optim.SGD(net.parameters(), lr=0.02)loss_func = torch.nn.CrossEntropyLoss()for t in range(100): out = net(x) loss = loss_func(out, y) optimizer.zero_grad() loss.backward() optimizer.step() if t%2 == 0: plt.cla() # 过了一道 softmax 的激励函数后的最大概率才是预测值 prediction = torch.max(F.softmax(out), 1)[1] pred_y = prediction.data.numpy().squeeze() target_y = y.data.numpy() plt.scatter(x.data.numpy()[:,0], x.data.numpy()[:,1], c=pred_y, s=100, lw=0, cmap='RdYlGn') accuracy = sum(pred_y == target_y) /200 plt.text(1.5, -4, 'Accuracy=%.2f'%accuracy, fontdict={'size':20, 'color':'red'}) plt.pause(0.1)plt.ioff() #停止画图plt.show() 快速搭建之前的搭法 12345678910111213import torchclass Net(torch.nn.Module): def __init__(self, n_feature, n_hidden, n_output): super(Net, self).__init__() self.hidden = torch.nn.Linear(n_feature, n_hidden) self.output = torch.nn.Linear(n_hidden, n_output) pass def forward(self, x): x = F.relu(self.hidden(x)) x = self.output(x) return xnet1 = Net(1, 10, 1)print(net) # 这是我们用这种方式搭建的 net1 我们用 class 继承了一个 torch 中的神经网络结构, 然后对其进行了修改, 不过还有更快的一招, 用一句话就概括了上面所有的内容! 1234567891011121314151617181920net2 = torch.nn.Sequential( # Sequential 一层一层的积累神经层,激活函数也是一层神经 torch.nn.Linear(1, 10), torch.nn.ReLU(), # 调用了类的构造方法 torch.mm.Linear(10, 1))print(net1)\"\"\"Net ( (hidden): Linear (1 -&gt; 10) (predict): Linear (10 -&gt; 1))\"\"\"print(net2)\"\"\"Sequential ( (0): Linear (1 -&gt; 10) (1): ReLU () (2): Linear (10 -&gt; 1))\"\"\" 我们会发现 net2 多显示了一些内容, 这是为什么呢? 原来他把激励函数也一同纳入进去了, 但是 net1 中, 激励函数实际上是在 forward() 功能中才被调用的. 这也就说明了, 相比 net2, net1 的好处就是, 你可以根据你的个人需要更加个性化你自己的前向传播过程, 比如(RNN). 不过如果你不需要七七八八的过程, 相信 net2 这种形式更适合你. 保存提取训练好了一个模型, 我们当然想要保存它, 留到下次要用的时候直接提取直接用 保存我们快速建造数据，搭建神经网络 1234567891011121314151617181920212223242526272829303132import torchtorch.manual_seed(1)# fake datax = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1) # x data (tensor),shape=(100, 1)y = x.pow(2)+0.2*torch.rand(x.size()) # noisy y data (tensor), shape=(100, 1)def save(): # set up net net1 = torch.nn.Sequential( torch.nn.Linear(1, 10), torch.nn.ReLu(), torch.nn.Linear(10, 1) ) optimizer = torch.optim.SGD(net1.parameters(), lr=0.02) loss_func = torch.nn.MSELoss() # train for t in range(2000): prediction = net1(x) loss = loss_func(prediction, y) optimizer.zero_grad() loss.backkward() optimizer.step() #接下来我们有两种途径来保存 torch.save(net1, 'net.pkl') # save entire net torch.save(net1.state_dict(), 'net_params.pkl') # 只保存网络中的参数(速度快，占内存少) # plot result prediction = net1(x) plt.figure(1, figsize=(10, 3)) plt.subplot(131) plt.title('Net1') plt.scatter(x.data.numpy(), y.data.numpy()) plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5) 提取网络提取整个神经网络, 网络大的时候可能会比较慢. 12345678def restore_net(): # restore entire net1 to net2 net2 = torch.load('net.pkl') prediction = net2(x) plt.subplot(132) plt.title('Net2') plt.scatter(x.data.numpy(), y.data.numpy()) plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5) 只提取网络参数提取所有的参数, 然后再放到你的新建网络中. 1234567891011121314def restore_params(): # set up net3 net3 = torch.nn.Sequential( torch.nn.Linear(1, 10) torch.nn.ReLU() torch.nn.Linear(10, 1) ) # load parameters to net3 net3.load_state_dict(torch.load('net_params.pkl')) prediction = net3(x) plt.subplot(133) plt.title('Net3') plt.scatter(x.data.numpy(), y.data.numpy()) plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5) 显示结果1234567# 保存 net1 (1. 整个网络, 2. 只有参数)save()# 提取整个网络restore_net()# 提取网络参数, 复制到新网络restore_params()plt.show() 批训练Torch中有个帮助整理数据结构，DataLoader，用来包装自己的数据，进行批训练，批训练途径: DataLoaderDataLoader 是 torch 给你用来包装你的数据的工具. 所以你要讲自己的 (numpy array 或其他) 数据形式装换成 Tensor, 然后再放进这个包装器中. 使用 DataLoader 有什么好处呢? 就是他们帮你有效地迭代数据。 1234567891011121314151617181920212223, ximport torchimport torch.utils.data as Datatorch.manual_seed(1)BATCH_SIZE = 5 # 批训练的数据个数# produce dataloader: tensor -&gt; dataset -&gt; torch dataset -&gt; dataloaderx = torch.linspace(1, 10, 10) # x data (torch tensor)y = torch.linspace(10, 1, 10) # y data (torch tensor)# 先转换成 torch 能识别的Datasettorch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)# 把dataset 放入 DataLoderloader = Data.DataLoader( dataset=torch_dataset, # torch TensorDataset format batch_size = BATCH_SIZE, # mini batch size/ shuffle=True, # 打乱 num_workers=2, # 多线程)for epoch in range(3): # 训练所有 !整套! 数据 3 次 for step, (batch_x, batch_y) in enumerate(loader): # 每一步 loader释放一小批数据来学习 # 训练的地方 # 打印数据 print('Epoch: ', epoch, '| Step: ', step, '| batch x: ', batch_x.numpy(), '| batch y: ', batch_y.numpy()) 可以看出, 每步都导出了5个数据进行学习. 然后每个 epoch 的导出数据都是先打乱了以后再导出. 真正方便的还不是这点. 如果我们改变一下 BATCH_SIZE = 8, 这样我们就知道, step=0 会导出8个数据, 但是, step=1 时数据库中的数据不够 8个, 这时怎么办呢: 这时, 在 step=1 就只给你返回这个 epoch 中剩下的数据就好了. 加速神经网络训练 (Speed Up Training)包括以下几种模式: Stochastic Gradient Descent (SGD) Momentum AdaGrad RMSProp Adam Stochastic Gradient Descent (SGD) 最基础的方法就是 SGD 啦, 想像红色方块是我们要训练的 data, 如果用普通的训练方法, 就需要重复不断的把整套数据放入神经网络 NN训练, 这样消耗的计算资源会很大. 我们换一种思路, 如果把这些数据拆分成小批小批的, 然后再分批不断放入 NN 中计算, 这就是我们常说的 SGD 的正确打开方式了. 每次使用批数据, 虽然不能反映整体数据的情况, 不过却很大程度上加速了 NN 的训练过程, 而且也不会丢失太多准确率.如果运用上了 SGD, 你还是嫌训练速度慢, 那怎么办? 事实证明, SGD 并不是最快速的训练方法, 红色的线是 SGD, 但它到达学习目标的时间是在这些方法中最长的一种. 我们还有很多其他的途径来加速训练. Momentum 更新方法 大多数其他途径是在更新神经网络参数那一步上动动手脚. 传统的参数 W 的更新是把原始的 W 累加上一个负的学习率(learning rate) 乘以校正值 (dx).这种方法可能会让学习过程曲折无比, 看起来像 喝醉的人回家时, 摇摇晃晃走了很多弯路. 所以我们把这个人从平地上放到了一个斜坡上, 只要他往下坡的方向走一点点, 由于向下的惯性, 他不自觉地就一直往下走, 走的弯路也变少了. 这就是 Momentum 参数更新. 另外一种加速方法叫AdaGrad. AdaGrad 更新方法 这种方法是在学习率上面动手脚, 使得每一个参数更新都会有自己与众不同的学习率, 他的作用和 momentum 类似, 不过不是给喝醉酒的人安排另一个下坡, 而是给他一双不好走路的鞋子, 使得他一摇晃着走路就脚疼, 鞋子成为了走弯路的阻力, 逼着他往前直着走. 他的数学形式是这样的. 接下来又有什么方法呢? 如果把下坡和不好走路的鞋子合并起来, 是不是更好呢? 没错, 这样我们就有了 RMSProp 更新方法. RMSProp 更新方法 有了 momentum 的惯性原则 , 加上 adagrad 的对错误方向的阻力, 我们就能合并成这样. 让 RMSProp同时具备他们两种方法的优势. 不过细心的同学们肯定看出来了, 似乎在 RMSProp 中少了些什么. 原来是我们还没把 Momentum合并完全, RMSProp 还缺少了 momentum 中的 这一部分. 所以, 我们在 Adam 方法中补上了这种想法. Adam 更新方法 计算m 时有 momentum 下坡的属性, 计算 v 时有 adagrad 阻力的属性, 然后再更新参数时 把 m 和 V 都考虑进去. 实验证明, 大多数时候, 使用 adam 都能又快又好的达到目标, 迅速收敛. 所以说, 在加速神经网络训练的时候, 一个下坡, 一双破鞋子, 功不可没. Optimizer 优化器伪数据为了对比各种优化器的效果, 我们需要有一些数据, 今天我们还是自己编一些伪数据, 这批数据是这样的: 1234567891011121314151617import torchimport torch.utils.data as Dataimport torch.nn.functional as Fimport matplotlib.pyplot as plttorch.manual_seed(1) # 使每次随机产生的数一样LR = 0.01BATCH_SIZE = 32EPOCH = 12# fake datax = torch.unsqueeze(torch.linpsace(-1, 1, 1000), dim=1)y = x.pow(2) + 0.1*torch.normal(torch.zeros(*x.size()))# plot datasetplt.scatter(x.numpy(), y.numpy())plt.show()# 使用上节提到 data loader: x,y -&gt; torch dataset -&gt; data loadertorch_dataset = Data.TensorDataset(x, y)loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2) 每个优化器优化一个神经网络为了对比每一种优化器, 我们给他们各自创建一个神经网络, 但这个神经网络都来自同一个 Net 形式. 123456789101112131415161718# 默认的 network 形式class Net(torch.nn.Module): def __init__(self): super(Net, self).__init__() self.hidden = torch.nn.Linear(1, 20) # hidden layer self.predict = torch.nn.Linear(20, 1) # output layer def forward(self, x): x = F.relu(self.hidden(x)) # activation function for hidden layer x = self.predict(x) # linear output return x# 为每个优化器创建一个 netnet_SGD = Net()net_Momentum = Net()net_RMSprop = Net()net_Adam = Net()nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam] 优化器 Optimizer接下来在创建不同的优化器, 用来训练不同的网络. 并创建一个 loss_func 用来计算误差. 我们用几种常见的优化器, SGD, Momentum, RMSprop, Adam. 12345678# different optimizersopt_SGD = torch.optim.SGD(net_SGD.parameters(), lr=LR)opt_Momentum = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8)opt_RMSprop = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)opt_Adam = torch.optim.Adam(net.net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]loss_func = torch.nn.MSELoss()loss_his = [[], [], [], []] # 记录 training 时不同神经网络的 loss 训练/出图1234567891011for epoch in range(EPOCH): print(epoch) for step, (b_x, b_y) in enumerate(loader): # 对每个优化器, 优化属于他的神经网络 for net, opt, l_his in zip(nets, optimizers, loss_his): output = net(b_x) # get output for every net loss = loss_func(output, b_y) # compute loss for every net opt.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients opt.step() # apply gradients l_his.append(loss.data.numpy()) # loss recoder","link":"/2019/08/15/pytorch学习/"},{"title":"restartAndConsist","text":"原因中断了一段时间的写作，由于学习，自身的懒散，时间的原因。自我要反思，明白好的习惯至少需要21天才能养成，恶习的形成只用一天，便足以破坏好的习惯。写博客，曾经的坚持被放弃了，现在要重拾博客，不再放弃。 为什么重写博客？一为督促自身每天的反思，我需要一段清醒时间，我忽然发现人其实都是一个很容易受诱惑，容易进入沉浸式体验而不自知的人。他们也许是没认识到，也许是不想认识到，每日看似忙碌的工作，对他们的自身的成长，技能，知识的扩展并毫无用处。简而言之，他们只是以为自己在过生活，其实只是被生活驱赶而已。这些感悟有些扯远了。说回来，我觉得一定要有一段清醒的时间，曾子曰：吾日三省吾身。这一段时间应该没有干扰，不去工作，没有其他想法，只是静静的思考自己的这一天，这一段时间，自身的所作所为，回顾审视自己。时间不用太长，10分钟，半个小时，兴之所至，兴尽则归。然后便是去改变，去突破，去成长，这才是有价值的人生。 二为高效学习学习一定要总结，学过的知识，只有反复咀嚼，温故而知新，孔子学习的名言。刻意练习，勤练不坠。学习，总结，再学习，再总结，这才是真正高效的学习方法，而这总结离不开博客。另外：也督促自己做事干净利索，不拖拉，高效做事。 三为提升逻辑写作是一门长久的能力，作文能力，逻辑表达能力，总-分-总等结构，妙语生花。写作，规划自己的语言逻辑，强迫自己表达清晰，简明了当，也要生动形象，考验自己。 四为读书做记录自己的读书记录，论文阅读记录。社会，科技，历史，人文，金融，政治等学科的学习与总结。 五为规划未来博客的建设未来主要将记录: 代码学习的总结， 论文阅读记录， 自己的总结, 自己的读书。 坚持目标无规矩，无计划，如何成功？因此，希望自己: 每天学习知识学习总结-java，golang等知识; 每周日上传一篇论文阅读； 两周上传一篇读书记录。","link":"/2018/09/27/restartAndConsist/"},{"title":"spring-bootSummary","text":"Spring-boot 快速搭建SSM工程 Spring boot Starter原理解密 Starter的作用 自动包依赖 自动配置Bean 以前的配置方法 xml文件配置 注解注入 基于java代码配置 通过 autoconfigure jar包 疑问1: 这些bean的依赖关系是如何自动处理的? 疑问2: 这些bean配置需要的参数是如何规定并获取的? 分享-互联网Java高级开发工程师必备技能 中级开发工程师: 工程会使用 如何快速成为Java高级工程师？ 找到自己的差距 快速学习补短板，针对目标快速补短板 如何快速学习提升： 自学 要找学习资料 碰到问题学很久 多线程 Q1: 用多线程的目的是什么? 充分利用cpu资源，并发做多件事 Q2: 单核cpu机器上不适不适合用多线程? 适合，如果单线程，线程中需要等待IO，CPU就空闲了 Q3：线程什么是让出CPU 阻塞时， wait, sleep yeild over Q4: 线程是什么？ 一条代码执行流，完成一组代码的执行；这一组代码，称为一个任务 Q5: cpu是做什么？ 执行代码 Q6: 线程是不是越多越好？ for example? Q7： 该如何正确使用多线程？ 线程池原理解密 任务用什么表示？ Runnable, Callable 仓库用什么？ Q8: 如何确定数量的线程？ 如何正确使用Java的线程池APIJava并发包中提供了丰富的线程池实现 只用满足所有核心线程都在工作，且队列已满，才会创建非核心线程。 java + docker 使用","link":"/2019/06/19/spring-bootSummary/"},{"title":"二分旋转数组","text":"题目https://leetcode-cn.com/problems/xuan-zhuan-shu-zu-de-zui-xiao-shu-zi-lcof/ 解法 暴力解法， 0(n)不符合要求 有序，要到0(logn)，考虑二分查找，排序数组的查找问题首先考虑使用 二分法 解决，其可将遍历法的 线性级别 时间复杂度降低至 对数级别 。 定下大概思路， 接着弄清楚算法流程: 入口，设置 i, j 指针分别指向 numbers 数组左右两端，m = (i + j) // 2为每次二分的中点 循环二分： 比较 number[m] &gt; number[j] number[m] &lt; number[j] 特殊 number[m] == number[j] 是否可以用 numbers[m] 和 numbers[i] 比较做代替？解析： 不可以。因为做比较的目的是判断 mm 在哪个排序数组中。但在 numbers[m] &gt; numbers[i]情况下，无法判断 m 在哪个排序数组中。 123456789101112131415161718192021222324252627282930313233class Solution { public int minArray(int[] numbers) { if (numbers == null || numbers.length ==0) { return Integer.MIN_VALUE; } int len = numbers.length; int i=0, j = len-1; while(i&lt;j) { int m = i + (j-i)/2; // 错误示范！！！！， 以后写条件判断，if() else if(); 变量改变了条件。 if (numbers[m] &gt; numbers[j]) { i = m+1; } if (numbers[m] &lt; numbers[j]) { j = m; } if (numbers[m] == numbers[j]) { j--; } // ------ if (numbers[m] &gt; numbers[j]) { i = m+1; } else if(numbers[m] &lt; numbers[j]) { j = m; } else { j--; } } return numbers[i]; }}","link":"/2020/06/09/二分旋转数组/"},{"title":"分布式机器学习论文","text":"虽说具体要做的东西目前还在思考比较多，从之前的 【整理一下看过的论文】 里面把相关的论文理出来了。 大致分成三个方面： Distributed Machine Learning System Distributed Deep Learning System Large Scale Neural Network Training 虽说重点主要集中在后面两块上，不过其他方面的机器学习毕竟发展的时间比深度学习更早，分布式系统方面还是有参考价值的。 把第二三两部分分开整理主要考虑一个是偏框架和算法设计，一个是偏向针对某个具体的应用问题做的大规模实现。 Distributed Machine Learning System2013 NIPS - More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server 话说我第一眼看这个系列的文章还以为讲的就是深度学习了，后来才发现这些全是主要针对机器学习的。 分布式方法还是主要基于传统的 Parameter Server 对 Worker 的形式，但是提出了一种 SSP(Stale Synchronous Parallel) 模型来解决普通的同步或者异步模式下训练会有的问题。SSP 模型大致是说会在本地维护一个参数的 cache，每个工作的 node 直接从本地的 cache 中拿数据，跟 PS 之间的同步问题应该是另外处理，这样就把每个工作 node 等待网络的时间给降下来了。 Introduction 中首先分析了机器学习训练中可能有的问题：massive data volume 和 massive model size，数据量太大和模型太大这两个问题即使可以通过尽可能地缩减、压缩，也终有个尽头，未来总有解决不了的时候，因此不得不需要用到分布式的训练环境。 分布式机器学习系统需要解决的最终目标是： 最大化利用计算资源（把更多的时间花在运算上） 训练完之后要能支持 inference 保证正确性（保证分布式之后网络仍然是能够收敛的） 文中对 PS 的定义是：一个共享的键值对存储模型，同时要具备读取和更新参数的同步机制。共享的键值对存储方式能简化编程复杂度，数据同步是为了保证整个训练过程的正确性。 SSP 这种模型的重点在于像前面说的本地保留尽可能新的旧参数，这里给了个微软一篇技术报告的引用： Replicated Data Consistency Explained Through Baseball 如果说同步可以达到更好的 quality，异步可以达到更好的 quantity 的话，SSP 是取了这两种方案的折中，最终比这两种都要有效。 规定每个 worker 运行时有个 clock 计数器（时间戳），记的是当前到了第几轮迭代。给定一个阈值 s，SSP 模型遵循以下几个规则： 最慢的和最快的 worker 之间跑的参数之间的 clock 差不能超过 s，否则最快的 worker 就要强制等待后面较慢的 worker 算完了赶上来 当一个当前 clock 值是 c 的 worker 提交一个参数更新时，那个更新的时间戳为 c 当一个当前 clock 值是 c 的 worker 读取参数时，根据上述规则，它读取到的更新至少会包含 c-s-1 时间戳之前的所有更新内容 一个 worker 读取到的更新总会比它自己产生的所有结果更新 事实上作者用 c 和 s 这两个定义就把 BSP 和异步给总结到一起了。如果 s 为 0，相当于所有的 worker 都是完全同步的，当 s 是无穷大的时候，就是完全异步的。 这个 s 就是 SSP 模型中需要 overlap 的部分，s 太小则可能经常有些 worker 需要等待，并行性提不上去，s 太大影响问题整体的收敛率。 具体实现上，作者用了一个类似 cache 页表的 SSP table，思想大致也是类似的。在多个 worker 跑在单个节点中的情况下，节点内还可以再多做一层线程间的数据 cache。 2014 ATC - Exploiting bounded staleness to speedup Big Data analytics这篇 paper 的内容是上面那篇的后续研究（话说作者都差不多），大体上是对上篇内容作了更多的补充和提升。作者在实验中发现应用了 bounded staleness 思想的 BSP 模型到最后效果跟 SSP 是差不多的，因此对这其中的详细情况和原因也作了深入分析。 引入 bounded staleness 思想的 BSP 模型这里成为 A-BSP，BSP 是每次迭代之后一个大同步，A-BSP 就是把这个限制放开点，每 n 次迭代之后一个大同步。用上面一篇提过的归纳方式，BSP、A-BSP、SSP 都可以归到一起。 SSP 说起来确实有点像是在 A-BSP 的基础上做个流水线的感觉。当整个分布式系统中，各个线程做的任务不太平衡，有明显较慢的 worker 存在时，SSP 能更好地提高并行度（负载均衡？）。但 SSP 相对 A-BSP 的通信次数是更多的，当各个 worker 的运行速度差距很小时，可能用 A-BSP 会有更好的效果。 这里用来维护数据 cache 的数据结构叫 LazyTable，其实就是前面 SSP Table 的升级版，实现大致都是类似的。 另外这里还做了预取和容错。 2014 SoCC - Exploiting Iterative-ness for Parallel ML ComputationsTo be read. 2014 OSDI - Scaling distributed machine learning with the parameter serverTo be read. 2015 - Distributed Machine Learning via Sufficient Factor BroadcastingSFB。 To be read. 2015 - Efficient Machine Learning for Big Data: A Review机器学习与大数据结合方面的一篇综述。 To be read. 2015 - Petuum: A new Platform for Distributed Machine Learning on Big Data一种分布式机器学习的平台设计。 To be read. 2015 EuroSys - Malt: distributed data-parallelism for existing ml applications一个叫 Malt 的库，用于把常用的机器学习应用从单机改造成分布式的。 To be read. 2015 SoCC - Managed Communication and Consistency for Fast Data-Parallel Iterative Analytics一套叫 Bosen 的分布式机器学习系统。 To be read. 2016 EuroSys - STRADS: A Distributed Framework for Scheduled Model Parallel Machine Learning.To be read. 2016 UAI - Lighter-Communication Distributed Machine Learning via Sufficient Factor BroadcastingSFB。 To be read. 2017 - Machine learning on big data Opportunities and challengesTo be read. Distributed Deep Learning System其他资料 【知乎专栏 - ML@Scale】 2012 NIPS - Large Scale Distributed Deep NetworksGoogle 的第一代深度学习系统 Distbelief，由 Jeffrey Dean 大佬带头，其实是 TensorFlow 的前身。 现在说的深度神经网络最初的来源就是传统机器学习里面的受限玻尔兹曼机（Restricted Boltzmann Machine，RBM）以及多层感知器（Multilayer Perceptron，MLP）等等想法，这俩演变出来了一个叫深度信念网络（Deep Belief Network，DBN）的东西，基本跟今天的 DNN 已经很像了。Distbelief 里面的 Belief 指的应该就是 DBN 吧。 出发点基本跟后来的文章大同小异，文章的重点在于： Downpour SGD：前面这个单词是描述倾盆大雨的，不知道整个词组应该怎么翻译比较好； 基于 Sandblaster 框架实现了一个 L-BFGS，用于处理整个训练过程的数据并行和模型并行。 相比普通的 SGD + 普通的 L-BFGS 实现要快上不少。 异步 SGD 之前在机器学习领域很少被用在非凸的优化问题上，但是经过试验验证之后发现异步 SGD 用在神经网络上效果很好，尤其是配合 Adagrad 这种学习率修正算法的时候。然后在资源足够的情况下，L-BFGS 的性能不会弱于 SGD。 论文中做到的最大规模是把一个模型拆到 32 个节点上进行模型并行。 2013 ICML - Deep Learning with COTS HPC首次把分布式机器学习里面的数据并行和模型并行引入深度学习。 主要实现在 InfiniBand 网络上，然后偏重在模型并行上。 To be read. 2014 ICASSP - On parallelizability of stochastic gradient descent for speech DNNS这篇文章是从理论上对比了模型并行和数据并行中分布式 SGD 训练的效率，指出增大 minibatch 的大小可以提高数据并行训练的效率。 To be read. 2014 OSDI - Project Adam: Building an Efficient and Scalable Deep Learning Training System微软在分布式 DL 训练方面做的工作。 To be read. 2014 Proceedings of the VLDB Endowment - Mariana： Tencent Deep Learning Platform and its Applications腾讯做的一个叫 Mariana 的深度学习平台。 To be read. 2015 ACM MM - SINGA: Putting Deep Learning in the Hands of Multimedia Users一套叫 SINGA 的分布式深度学习框架。 To be read. 2016 - Asynchrony begets momentum, with an application to deep learning分析了异步与动量法调整的学习率之间的影响关系。 动量法是一种梯度下降里面对学习率自动调节的方法。 To be read. 2016 - How to scale distributed deep learning用 ImageNet 对比了同步和异步 SGD 的实测结果，指出可能在更大规模下其实同步 SGD 效果更好。 To be read. 2016 SoCC - Ako: Decentralised deep learning with partial gradient exchange去中心化（不再采用 PS-Worker 方式）的分布式深度学习思路。 To be read. 2016 Eurosys - GeePS: Scalable Deep Learning on Distributed GPUs with a GPU-Specialized Parameter Server这篇文章很厉害了，主要内容是说做了一套叫 GeePS 的 Parameter Server 机制，主要针对 GPU 做了特别的优化和改进，克服了数据并行和模型并行，BSP 和异步这些老方法中存在的问题，最终结果性能爆炸。GeePS 还支持在卡上跑超过显存容量的网络，直接解决了对模型并行的需求。 背景介绍部分分析了一下 Parameter Server 模式存在的问题，这里也有提到前面 13 年那篇 SSP 模型方面的工作（…其实这篇论文还是同一拨人做的…）。要应用 PS 模式到 GPU 上，采用多个 worker 配合多个 ps，每个物理节点内都有 ps 和 worker 这种形式会比较合适。 但是直接简单地把 ps 搬到 GPU 上效果非常不好，后文接下来就是讲他们如何解决这个问题，即他们提出来的 GeePS 是怎么做的。 第一个优化策略是在 GPU 上做一个参数 cache，嗯这个思想大概跟往异步 PS 里面引入本地 cache（应用 SSP 模型）是一个道理。因为前面的性能瓶颈主要在每一次推送参数更新上了，引入了 CPU 跟 GPU 之间的数据传输之后总会把整个计算过程卡下来。 这种方法提升性能的关键在于能够成功地把计算和 CPU/GPU 的数据拷贝给 overlap 开，这样就能最大化 GPU 的使用率啦，GPU 只要拿到了新的 input 数据就能一直跑。 第二个策略是数据的输入和参数的更新都是以 batch 为单位的，利用了 GPU 的 SIMD 特性，增加了数据的吞吐量，这一块详细的要见他们前面的一篇文章（Exploiting Iterative-ness for Parallel ML Computations）。 第三个就厉害了，目测应该工作量挺大的。为了解决模型太大，GPU 上放不下的问题，他们手动维护了一个 GPU 和 CPU 之间的内存池，每次把不用的数据换到主存上，把下一波计算需要用的数据换到 GPU 上。 用手动维护的内存池完全接管整个 GPU 的内存分配、释放操作，CPU 跟 GPU 之间的数据传输用另外一个线程在后台完成，把计算时间和数据拷贝延迟完全 overlap 开。由于神经网络层与层之间的顺序性是显示存在的，因此数据在 GPU 显存上的换入换出就是完全可以做到的了。 第四点是对 PS 模式下异步方式的思考，虽说把 BSP 改成异步的可以增加计算资源的利用率，但是收敛速度会放慢是肯定的，之前的不少研究也是在这两个方面作了取舍，才能让最终训练到相同效果的总体时间更短。这篇文章在同步延迟能够保证的情况下，测试结果偏向于用 BSP 收敛效果会更好。 中间的详细实现先放着，留待之后回来看。 2017 ATC - Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on GPU Clustersemm…这篇文章跟上一篇还是同一拨人做的。Motivation、目标什么的基本上差不多，工作方向上从不同的角度出发来做。 文章首先指出了限制分布式深度学习可扩展性的两个瓶颈： 每次更新的梯度都可能是大矩阵，很容易就把网络带宽给占满； 由于神经网络运算的迭代特性，在完成一轮迭代之后才更新参数。因此其通信表现是短时间内有一个通信量的暴增，而其他时间没有通信。 本文解决问题的思路也从这两点开始出发： 把要更新的梯度矩阵做一定的划分，通过重新调度，在时间上把整个通信平摊掉； 想办法减少每次更新的梯度矩阵的大小，从整体通信量上做文章。 最后要达到的效果呢，也是分两方面：首先整个系统的吞吐量增加了，同时迭代的收敛速度并不受影响，不需要增加迭代次数就能达到一样的效果。 减少梯度矩阵大小用的是一个叫 SFB（Sufficient Factors Broadcasting）的技术，具体的文章在这里。 作者认为核心的瓶颈还是主要在通信上。 这里给出了一个对 Alexnet 的粗略分析（不是特别准确，但是基本上是同一个量级），假定计算跟通信能够完全 overlap 开，用 Taitan X 节点来分布式训练大约也要至少 26 Gbps 的网络带宽，这个压力对一般的以太网来说基本上是很难应对的。当然 overlap 计算和通信这事本身就很难做到了。 作者首先分析了整个训练过程的思路： 定义训练第 l 层网络的前后向为 𝑓𝑙𝑡ftl 和 𝑏𝑙𝑡btl，那么整个训练的计算部分是这样的： 𝐶𝑡=[𝑓1𝑡,𝑓2𝑡,…,𝑓𝐿𝑡,𝑏𝐿𝑡,𝑏𝐿−1𝑡,…,𝑏1𝑡]Ct=[ft1,ft2,…,ftL,btL,btL−1,…,bt1] 加上通信同步部分，𝑂𝑙𝑡Otl 和 𝐼𝑙𝑡Itl 分别表示第 l 层网络参数的输出和输入（更新）： 𝑆𝑡=[𝑂𝑡,𝐼𝑡]=[𝑂𝐿𝑡,𝑂𝐿−1𝑡,…,𝑂1𝑡,𝐼𝐿𝑡,𝐼𝐿−1𝑡,…,𝐼1𝑡]St=[Ot,It]=[OtL,OtL−1,…,Ot1,ItL,ItL−1,…,It1] 这样组成的一次 𝐶𝑡,𝑆𝑡Ct,St 就是一次迭代的完整过程了。 那么就提出了这里的第一个思路： 分层把通信和计算给 overlap 开，这里称为无等待的反向传播算法（WFBP） 通信的数据依赖关系只在同一层内，即： 𝑏𝑙𝑡btl 结束之后就可以马上做 𝑂𝑙𝑡Otl 和 𝐼𝑙𝑡Itl 了，即第 l 层的参数同步可以跟第 l-1 以及以后层的后向计算同时进行。示意图如下： 这种思路尤其适用于参数集中在后几层（例如后几层是全连接），然后计算集中在前几层（例如前几层是卷积）这样的网络（例如 VGG 和 AdamNet），这样就能把顶层的通信时间掩藏在底层的计算时间中。 大概是这种样子： [𝐶𝑡,𝑆𝑡]=[𝑓1𝑡,𝑓2𝑡,…,𝑓𝐿𝑡,𝑏𝐿𝑡,𝑏𝐿−1𝑡,𝑏𝐿−2𝑡…,𝑏1𝑡] [𝑂𝐿𝑡,𝑂𝐿−1𝑡,…,𝑂1𝑡] 𝐼𝐿𝑡,𝐼𝐿−1𝑡,…,𝐼1𝑡(1)[Ct,St]=[ft1,ft2,…,ftL,btL,btL−1,btL−2…,bt1] [OtL,OtL−1,…,Ot1] [ItL,ItL−1,…,It1] 但是这样对于带宽受限的网络来说仍然不够，所以有了接下来的第二个思路： 采用 PS 和 SFB 混合的通信机制 正如上面所分析的，事实上不只是神经网络不同层的计算和通信可以无关，不同层之间的通信也是完全可以独立的，因此考虑对不同层数据的特点采用不同的方式进行组织通信。PS 模式或者 SFB 模式： 并且神经网络的结构是只要训练开始了，自始至终都不会再改变，因此可以事先算出参数的总量来估计整个网络会产生的通信开销，在训练开始前就能够选择合适的通信组织方式。 这里又举了个例子： VGG19，假定 batch size K = 32，8 个 PS 和 8 个 Worker，参数均分在 8 台 PS 上，全连接层 M 和 N 都是 4096。2 个全连接层每一步迭代产生 2 4096 4096 = 34M 个参数。 PS 模式下：每个 Worker 每次要发送 34M 个参数，每个 PS 管 34M/8 个参数，但是要从 8 个 Worker 那里接收，所以总的通信数据量还是 34M。如果实际物理机就 8 台，每台上面跑一个 PS 和一个 Worker，那么每台机上本地更新 34M/8 的数据，每次要送出去 7/8 34M 的数据，再收回来 7 个 34M/8 的数据，一共 2 7/8 * 34M = 58.7M 的数据量。 SFB 模式下：（这种通信模式我还没细看，所以不知道为什么这么算）没有 PS，直接是 8 个 Worker 进行数据处理，每个节点需要负责的通信量只有 2 K (M + N)(P1 -1) = 2 32 8192 * 7 = 3.7M 的数据量。虽然收到数据之后要恢复成梯度矩阵需要额外的运算，但是这跟节省下来的通信时间相比是可以忽略不计的。 卷积层的更新梯度由于是不可分解的，并且是稀疏的，所以还是采用 PS 模式更好。 所以根据不同层的参数特性采用不同的通信机制的方法是有很大潜力的。 整个 Poseidon 系统的设计分成三个部分： Coordinator：用于维护网络模型和集群设备之间的配置关系 集群中 worker 和 ps 的数量，对应的 ip 地址，网络模型的结构等等。负责建立各节点之间的通信端口，分析网络模型，决定哪些参数用 PS 哪些参数用 SFB。 当然这些数据是在初始化的时候就完成了。 KV store：一个共享内存的键值对存储部件，其实就是 Parameter Server 为什么这里的 PS 要突出”键值对“这个概念呢？ Poseidon 在这里做了一个操作，参数不是按照层来划分的（！！！），而是把所有的参数按照 2MB 划分之后再均分到各个 PS 上去，这样就能保证每个 PS 上面存储的数据量尽可能地一致，让各个节点需要的网络带宽尽可能地平均。 另外还有 checkpoint 的设计，保存多个阶段的参数用于容错恢复等等。 Client Library：用于适配到不同的深度学习框架中 对每个层都单独创建一个 syncer 负责处理其参数一致性。 在 CPU 上维护一个线程池（用于后台处理网络通信），在 GPU 上维护一个 stream 池（用于后台处理 CPU 和 GPU 之间的数据拷贝）。 之前作者的文章里面也提到了 BSP 的收敛性相对异步来说始终还是更稳定的，有了上面那个分层同步、通信几乎完全掩藏在计算背后的设计之后，BSP 跟异步之间的延迟差距可能已经能够减少到足以忽略的程度了。 因此在参数一致性方面的维护上，Poseidon 直接采用了 BSP 同步。 worker 这边，每个 client library 维护一个长为 syncer 数的 01 向量，每次迭代开始前初始化为 0，当一层的数据同步完了之后设为 1，当整个向量全 1 时就可以进入下一个迭代了。 PS 这边，KV stroe 在每次迭代开始前维护一个值为 0 的计数器，每当有一个 KV 更新计数加一，当计数器达到 worker 的数量时，发起一次参数广播。 整个训练过程的伪代码： 12 Copy 后面是 evaluation，测试结果就是很强…各方面都很强。 2015 - Poseidon: A system architecture for efficient GPU-based deep learning on multiple machines这篇 paper 是在上面那篇发出来之前放在 arxiv.org 上的，应该算是正篇前的一些基础工作，看完 2017 年的正篇再看下这个。 2017 - Can Decentralized Algorithms Outperform Centralized Algorithms A Case Study for Decentralized Parallel Stochastic Gradient Descent又一篇去中心化思路的文章。 To be read. 2017 ICML - Device Placement Optimization with Reinforcement LearningGoogle 做的关于模型并行中 Op 和 Device 对应关系的研究。 在多块卡上做模型并行时需要考虑把整个网络的哪些部分分在哪种设备上，这篇文章的思路是先生成一套 Op 和 Device 的对应关系，然后扔进 TensorFlow 里面跑，跑完结果再反馈到一个强化学习的网络里面去，让网络去自动重新调整 Op 和 Device 的对应关系，直到得到一套最高效的分配方案。 666666！ 2017 - ChainerMN: Scalable Distributed DeepLearning Framework下面列的那个 15 分钟跑完 ImageNet 的日本机构用的是他们自己写的框架，名字就叫 ChainerMN。 To be read. 2018 NIPS - Pipe-SGD: A Decentralized Pipelined SGD Framework for Distributed Deep Net Training刚刚看到的一篇 NIPS 的工作，嗯，从题目就基本上可以知道写的是啥了。 Introduction 中首先讨论了一下网络层、数据量的增长带来的计算量的需求，尽管各种神经网络加速器（GPU、ASIC 等等）和并行计算的发展缓解了很大的压力，从另一方面却更加剧了多机通信的 Overhead 造成的影响，毕竟前后向的计算时间是减少了，但是通信时间不变啊，在整体一次训练迭代中占的比重就增大了。 这篇文章首先分析了神经网络训练过程中的通信时间和计算时间的关系，然后提出了一套模型来评估各方面因素对它们的影响（例如延迟、集群规模、网络带宽、神经网络模型等等），在这套评估模型的基础上接下来推出了 Pipe-SGD 框架来实现他们想要的训练机制，最终在评估测试中得到了很好的结果。 Background 部分是对神经网络以及分布式训练的介绍，值得一提的是最后提到好多人为了减少通信时间会采用压缩的方案，例如低比特量化、传输时直接扔掉某些层的梯度等等。 既然 Background 里面提到了压缩，说明后面他们的框架中也有压缩方面的工作。 下图分别是中心化的 PS-Worker 异步模式、去中心化的同步模式以及本文的流水线模式的示意图。 马后炮一下，看到 c 图的时候我惊了一下……大概去年年底的时候也想到了类似这种思路，无奈限于一直做的框架都是 TensorFlow，没想到应该怎么在 TF 里面实现，于是一直没有把想法变成实现。 另外一点是，由于 TensorFlow 本身的数据流设计，一轮迭代中的计算和通信本就能够很大程度上自动 overlap 起来，因此即使在 TF 中实现了也可能意义不大。 这里把每一次的训练迭代分成三个部分：模型更新、梯度计算、梯度传输。 在一个常规的同步训练过程中，每一轮迭代中的三个部分是相互依赖的，如果用 T 表示总的训练迭代次数，则整个训练时间可以表示成这样： 𝑙𝑡𝑜𝑡𝑎𝑙_𝑠𝑦𝑛𝑐=𝑇∗(𝑙𝑢𝑝+𝑙𝑐𝑜𝑚𝑝+𝑙𝑐𝑜𝑚𝑚)ltotal_sync=T∗(lup+lcomp+lcomm) Pipe-SGD 的思想呢则是把这种通信和计算的依赖关系解开，如果一个 Worker 上是多条流水线交替进行，则通信和计算的时间就有机会完全 overlap 开了，本轮迭代的数据只与前 K 轮的迭代数据相关，而相邻的两次迭代之间则完全没有依赖了。在这种方式下，总的时间可以表示为： 𝑙𝑡𝑜𝑡𝑎𝑙_𝑝𝑖𝑝𝑒=𝑇/𝐾∗(𝑙𝑢𝑝+𝑙𝑐𝑜𝑚𝑝+𝑙𝑐𝑜𝑚𝑚)ltotal_pipe=T/K∗(lup+lcomp+lcomm) 更进一步的分析是，由于网络中的计算和通信时间往往很难完全对等起来，事实上整个计算过程是资源受限（Resource Bound）的，则最后实际受限的结果是要看计算和通信哪部分更花时间： 𝑙𝑡𝑜𝑡𝑎𝑙_𝑝𝑖𝑝𝑒=𝑇∗𝑚𝑎𝑥(𝑙𝑢𝑝+𝑙𝑐𝑜𝑚𝑝,𝑙𝑐𝑜𝑚𝑚)ltotal_pipe=T∗max(lup+lcomp,lcomm) 下面更进一步地给出了一个更详细的时间模型，把网络延迟、模型的参数量、传输带宽、worker 数量等等都包含在内了，最终分析完了给出了这样一个结论： Pipe-SGD 选择 K=2 为最佳，整个计算过程是计算受限的，并且应该采用顺序的梯度传输原则。 这里的顺序梯度更新原则指的是把梯度计算和梯度传输这两个过程完全分开，即计算完整个网络的所有梯度之后再一次性传输完，在 TensorFlow 中默认并不存在这种分离关系，可以认为是自动的分层梯度传输，一层梯度算完之后马上就可以开始传输过程，并且这个传输并不影响后续其他层的梯度计算。 …… 话说我觉得这里的结论直接想也很容易得出来啊，可能严谨的数学模型推论就是人家为什么能中 NIPS 的原因吧。 另外还有为什么这里要特意提到压缩呢？ 下图的 a 是无压缩情况下的通信和 Reduce 计算关系，可以看到即使采用了流水线的方式，Reduce 的实现序列上还是会有大量的空闲。 而如 b 图所示，梯度压缩操作减少了传输时间，但却会略微增加一些计算量，而这种增加的计算量到了流水线中反而可以很好地被隐藏掉。 。。。 最后的实验验证部分说实话我是比较失望的，4 台单 GPU 节点组成的集群实在不是个太大的规模啊……而且这里也没有给出与其他框架的对比性能来。 2018 - Beyond data and model parallelism for deep neural networks看的时候这篇文章还在 SysML19 审稿中。 主要工作是建立了一套名为 SOAP（Sample，Operation，Attribute，Parameter）的并行方案搜索空间，设计了一个专门的模拟器来评估要训练的神经网络的性能情况，然后在这个搜索空间里面找到最佳的并行方案。听上去有点玄，不过摘要中说可以达到比 State-of-the-Art 还要大 3.8 倍的吞吐量，还是很厉害的。 Introduction 部分提了下 DNN 并行的大背景，目前市面上常见的框架中都只是提供了一些简单的并行方案，要达到理想的性能通常需要研究者自己去根据网络的特性手工调整。举例来说 Google 14 年有个把前半部分的卷积用数据并行，最后的全连接改用模型并行的方案，以及 Google 对自己机器翻译模型做的一些并行方案设计。另外还有依靠强化学习等方法去找到最佳的并行方案（例如 Google 的模型并行设备分配工作，以及这里引用了另外一个他们自己的工作）。 Google 对模型并行做的强化学习每次是试跑一遍得到实际执行时间来作为一种方案的结果（常规操作没毛病），因此当搜索空间很大的时候就不可避免地需要花费很长时间。那这篇文章的关键就在于他们模拟器的设计，用模拟器直接评估网络的执行性能自然是是要比实跑快很多的（这里说快了好几个数量级），问题是这样评估出来的结果是否可靠？ 文章主要的依据有两个： 大多数神经网络的模型都是由常见的结构组成，只会有很少量的特殊网络层； 神经网络每一层的运行时间基本上只与硬件本身相关，输入规模固定之后基本不会受其他因素影响。 因此文章中的模拟器的做法是预先得到不同输入规模下各个网络层的执行时间，之后再用这些数据来评估网络模型的执行时间。相比 Google 的做法，这样可以更快，而且速度快了以后整体的方案搜索过程对硬件资源的需求也就少的多了（Google 用了 160 个 4 GPU 节点，本文只需要单个节点）。后面对优化方案的搜索采用的是马尔科夫链蒙特卡洛的方法，然后各种测试的效果都很不错。 相关工作的对比里面还有提到用网络流来优化任务调度的（666666）。 Large Scale Neural Network Training这个分类下面主要是实际应用层面的工作。 2013 ICASSP - Building High-level Features Using Large Scale Unsupervised Learning谷歌做的无监督学习图像分类的工作。 在一个超过 1000 个节点的集群上用了模型并行和异步 SGD。 文中的网络叫稀疏深度自编码器（Sparse Deep Autoencoder），基本上跟现在的 DNN 差不多，应该是当时深度学习这个概念还没有完全定型。 实现方面没有具体写的很清楚，主要参看前面12年NIPS的那篇工作吧。 这里的几篇各家拼 ImageNet 训练速度的笔记整理到新帖里了： 【Faster and Faster – ImageNet】","link":"/2019/08/29/分布式机器学习论文/"},{"title":"你好，Hexo","text":"走上博客之路，望坚持！经过漫长的纠结，终于暂定下来了，要严格要求自己坚持下去。雄关漫道真如铁，而今迈步从头越","link":"/2018/04/16/你好，Hexo/"},{"title":"yaml入门","text":"YAML 语言教程配置文件的学习，yaml专门用来写配置文件，本文介绍 YAML 的语法，以 JS-YAML 的实现为例。你可以去在线 Demo 验证下面的例子 一、简介YAML 语言（发音 /ˈjæməl/ ）的设计目标，就是方便人类读写。它实质上是一种通用的数据串行化格式。 它的基本语法规则如下： 大小写敏感 使用缩进表示层级关系 缩进时不允许使用Tab键，只允许使用空格。 缩进的空格数目不重要，只要相同层级的元素左侧对齐即可 # 表示注释，从这个字符一直到行尾，都会被解析器忽略。 YAML 支持的数据结构有三种。 对象：键值对的集合，又称为映射（mapping）/ 哈希（hashes） / 字典（dictionary） 数组：一组按次序排列的值，又称为序列（sequence） / 列表（list） 纯量（scalars）：单个的、不可再分的值 二、对象对象的一组key-value键值对，使用冒号结构表示。 animal:pets 转为 JavaScript 如下。 1{ animal: 'pets' } Yaml 也允许另一种写法，将所有键值对写成一个行内对象。 1hash: { name: Steve, foo: bar } 转为 JavaScript 如下。 1{ hash: { name: 'Steve', foo: 'bar' } } 三、数组一组连词线开头的行，构成一个数组。 123- Cat- Dog- Goldfish 转为 JavaScript 如下。 1[ 'Cat', 'Dog', 'Goldfish' ] 数据结构的子成员是一个数组，则可以在该项下面缩进一个空格。 1234- - Cat - Dog - Goldfish 转为 JavaScript 如下。 1[ [ 'Cat', 'Dog', 'Goldfish' ] ] 数组也可以采用行内表示法。 1animal: [Cat, Dog] 转为 JavaScript 如下。 1{ animal: [ 'Cat', 'Dog' ] } 四、复合结构对象和数组可以结合使用，形成复合结构。 123456789languages: - Ruby - Perl - Python websites: YAML: yaml.org Ruby: ruby-lang.org Python: python.org Perl: use.perl.org 转为 JavaScript 如下 123456{ languages: [ 'Ruby', 'Perl', 'Python' ], websites: { YAML: 'yaml.org', Ruby: 'ruby-lang.org', Python: 'python.org', Perl: 'use.perl.org' } } 五、纯量纯量是最基本的、不可再分的值。以下数据类型都属于 JavaScript 的纯量。 字符串 布尔值 整数 浮点数 Null 时间 日期 数值直接以字面量的形式表示。number: 12.30 转为 JavaScript 如下{ number: 12.30 } 布尔值用true和false表示。isSet:True 转为 JavaScript 如下。{isSet: true} null用~表示。parent: ~ 转为 JavaScript 如下。{parent: null} 时间采用 ISO8601 格式。 1iso8601: 2001-12-14t21:59:43.10-05:00 转为 JavaScript 如下。 1{ iso8601: new Date('2001-12-14t21:59:43.10-05:00') } 日期采用复合 iso8601 格式的年、月、日表示。 1date: 1976-07-31 转为 JavaScript 如下 1{ date: new Date('1976-07-31') } YAML 允许使用两个感叹号，强制转换数据类型。 12e: !!str 123f: !!str true 转为 JavaScript 如下。 1{ e: '123', f: 'true' } 六、字符串字符串是最常见，也是最复杂的一种数据类型。字符串默认不使用引号表示。 1str: 这是一行字符串 转为 JavaScript 如下。 1{ str: '这是一行字符串' } 如果字符串之中包含空格或特殊字符，需要放在引号之中。 1str: '内容： 字符串' 转为 JavaScript 如下。 1{ str: '内容: 字符串' } 单引号和双引号都可以使用，双引号不会对特殊字符转义。 12s1: '内容\\n字符串's2: \"内容\\n字符串\" 转为 JavaScript 如下。 1{ s1: '内容\\\\n字符串', s2: '内容\\n字符串' } 单引号之中如果还有单引号，必须连续使用两个单引号转义。 1str: 'labor''s day' 转为 JavaScript 如下。 1{ str: 'labor\\'s day' } 字符串可以写成多行，从第二行开始，必须有一个单空格缩进。换行符会被转为空格。 123str: 这是一段 多行 字符串 转为 JavaScript 如下。 1{ str: '这是一段 多行 字符串' } 多行字符串可以使用|保留换行符，也可以使用&gt;折叠换行。 123456this: | Foo Barthat: &gt; Foo Bar 转为 JavaScript 代码如下。 1{ this: 'Foo\\nBar\\n', that: 'Foo Bar\\n' } +表示保留文字块末尾的换行，-表示删除字符串末尾的换行。 12345678910s1: | Foos2: |+ Foos3: |- Foo 转为 JavaScript 代码如下。 1{ s1: 'Foo\\n', s2: 'Foo\\n\\n\\n', s3: 'Foo' } 字符串之中可以插入 HTML 标记。 12345message: | &lt;p style=\"color: red\"&gt; 段落 &lt;/p&gt; 转为 JavaScript 如下。 1{ message: '\\n&lt;p style=\"color: red\"&gt;\\n 段落\\n&lt;/p&gt;\\n' } 七、引用锚点&amp;和别名*，可以用来引用。 1234567891011defaults: &amp;defaults adapter: postgres host: localhost development: database: myapp_development &lt;&lt;: *defaultstest: database: myapp_test &lt;&lt;: *defaults 等同于下面的代码。 12345678910111213defaults: adapter: postgres host: localhostdevelopment: database: myapp_development adapter: postgres host: localhosttest: database: myapp_test adapter: postgres host: localhost &amp;用来建立锚点（defaults），&lt;&lt;表示合并到当前数据，*用来引用锚点。 下面是另一个例子。 12345- &amp;showell Steve - Clark - Brian - Oren - *showell 转为 JavaScript 代码如下。 1[ 'Steve', 'Clark', 'Brian', 'Oren', 'Steve' ] 八、函数和正则表达式的转换这是 JS-YAML 库特有的功能，可以把函数和正则表达式转为字符串。 123# example.ymlfn: function () { return 1 }reg: /test/ 解析上面的 yml 文件的代码如下。 1234567891011var yaml = require('js-yaml');var fs = require('fs');try { var doc = yaml.load( fs.readFileSync('./example.yml', 'utf8') ); console.log(doc);} catch (e) { console.log(e);} 从 JavaScript 对象还原到 yaml 文件的代码如下。 1234567891011121314151617var yaml = require('js-yaml');var fs = require('fs');var obj = { fn: function () { return 1 }, reg: /test/};try { fs.writeFileSync( './example.yml', yaml.dump(obj), 'utf8' );} catch (e) { console.log(e);} python yaml用法详解一、PyYaml load() :返回一个对象 先创建一个yml文件，config.yml: 12345678910name: Tom Smithage: 37spouse: name: Jane Smith age: 25children: - name: Jimmy Smith age: 15 - name1: Jenny Smith age1: 12 读取yml文件： 1234import yamlf = open(r'E:\\AutomaticTest\\Test_Framework\\config\\config.yml')y = yaml.load(f)print (y) 结果： {'name': 'Tom Smith', 'age': 37, 'spouse': {'name': 'Jane Smith', 'age': 25}, 'children': [{'name': 'Jimmy Smith', 'age': 15}, {'name1': 'Jenny Smith', 'age1': 12}]} load_all()生成一个迭代器 如果string或文件包含几块yaml文档，你可以使用yaml.load_all来解析全部的文档。 123456789101112import yamlf = '''---name: Jamesage: 20---name: Lilyage: 19'''y = yaml.load_all(f)for data in y: print(data) 执行结果： 12{&apos;name&apos;: &apos;James&apos;, &apos;age&apos;: 20}{&apos;name&apos;: &apos;Lily&apos;, &apos;age&apos;: 19} yaml.dump 将一个python对象生成为yaml文档 1234567import yamlaproject = {'name': 'Silenthand Olleander', 'race': 'Human', 'traits': ['ONE_HAND', 'ONE_EYE'] }print(yaml.dump(aproject，)) ssh zhejianglab727-4@10.0.104.95","link":"/2019/08/23/yaml入门/"},{"title":"tutorialGO","text":"基础包，变量和函数包每个 Go 程序都是由包构成的。程序从main包开始运行。 本程序通过导入路径&quot;fmt&quot;和&quot;math/rand&quot;来使用这两个包。 按照约定，包名与导入路径的最后一个元素一致。例如&quot;math/rand&quot;包中的源码均以package rand语句开始。 注意： 此程序的运行环境是固定的，因此 rand.Intn总是会返回相同的数字。 （要得到不同的数字，需为生成器提供不同的种子数，参见 rand.Seed。） 导入此代码用圆括号组合了导入，这是“分组”形式的导入语句。[推荐] 1234import ( &quot;fmt&quot; &quot;math&quot;) 当然你也可以编写多个导入语句，例如： 12import &quot;fmt&quot;import &quot;math&quot; 导出名在Go中，如果一个名字以大写字母开头，那么它就是已导出的。例如，Pizza就是个已导出名，Pi也同样，它导出自math包。 pizza和pi并未以大写字母开头，所以它们是未导出的。 在导入一个包时，你只能引用其中已导出的名字。任何“未导出”的名字在该包外均无法访问。 执行代码，观察错误输出。 123456789package mainimport ( \"fmt\" \"math\")func main() { // fmt.Println(math.pi); // math.pi undefined fmt.Printlin(math.Pi)} 函数函数可以没有参数或接受多个参数。add接受两个int类型的参数。 注意类型在变量名之后。 123func add(x int, y int) int { return x + y} 当连续两个或多个函数的已命名形参类型相同时，除最后一个类型以外，其它都可以省略。x int, y int可以写作 123func add(x, y int) int{ return x+y} 与C语言对比参考 C系列语言定义int x， 是定义一个表达式， 包含变量， 声明表达式的类型，从右向左读取。 C系列之外的语言则是: 123x: intp: pointer to inta: array[3] of int 从左向右读取，写出一个变量， 声明的类型(Note：指针类型除外) GO中的指针 在GO语法中，将括号放到类型的左边，但表达式则是将括号放在右边 12345var a []intx = a[1]var p *intvalue := *ppointer := &amp;value 多值返回函数可以返回任意数量的返回值。 123456789package mainimport \"fmt\"func swap(a, b string) (string, string){ return b, a}func main(){ a, b := swap(\"hello\", \"world\") // '' 与 \"\"不同 fmt.Println(\"a, b:\", a, b)} 命名返回值 Go 的返回值可被命名，它们会被视作定义在函数顶部的变量。 返回值的名称应当具有一定的意义，它可以作为文档使用。 没有参数的 return 语句返回已命名的返回值。也就是 直接 返回。 直接返回语句应当仅用在下面这样的短函数中。在长的函数中它们会影响代码的可读性。 12345678910package mainimport \"fmt\"func split(sum int) (x, y int) { x = sum * 4 /9 y = sum - x return}func main() { fmt.Println(split(17))} 变量var语句用于声明一个变量列表，跟函数的参数列表一样，类型在最后。 就像在这个例子中看到的一样，var 语句可以出现在包或函数级别。 12345678910package mainimport ( \"fmt\")var c, python, java bool;var i int;// var x float32, y int16; syntax errorfunc main() { fmt.Println(i, c, python, java);} 变量的初始化变量声明可以包含初始值，每个变量对应一个。 如果初始化值已存在，则可以省略类型；变量会从初始值中获得类型。 1234567package mainimport \"fmt\"var i, j int = 1, 2func main() { var c, python, java = true, false, \"no!\" fmt.Println(i, j, c, python, java)} 短变量声明在函数中，简洁赋值语句:=可在类型明确的地方代替var声明。 函数外的每个语句都必须以关键字开始（var, func 等等），因此 :=结构不能在函数外使用。 123456789package mainimport \"fmt\"func main() { var i, j int = 1, 2 k := 3 c, python, java := true, false, \"no!\" fmt.Println(i, j, k, c, python, java)} 总结： 声明变量必须只用两种 使用var申明列表, 如var x, y = 1, &quot;dd&quot;;; 而:=可以代替var 做简洁申明并且必须进行初始化， python, java := &quot;ss&quot;, 2; 基本类型GO的基本类型 12345678boolstringint int8 int16 int32 int64uint uint8 uint16 uint32 uint64 uintptrbyte // uint8 的别名rune // int32 的别名 表示一个 Unicode 码点float32 float64complex64 complex128 本例展示了几种类型的变量。 同导入语句一样，变量声明也可以“分组”成一个语法块。 int, uint 和 uintptr 在 32 位系统上通常为 32 位宽，在 64 位系统上则为 64 位宽。 123456789101112131415161718192021222324package mainimport ( &quot;fmt&quot; &quot;math/cmplx&quot;)var ( ToBe bool = false MaxInt uint64 = 1&lt;&lt;64-1 z complex128 = cmplx.Sqrt(-5+12i))var i, j float32 = 1, 2var a, b, x, y = 1, 2, true, false // var 申明一个变量列表(类型可省略)func main() { k, w := 1, 2; fmt.Println(k, w); // fmt.Println 打印字符串并换行,fmt.Printf获取变量输出(推荐使用) // fmt.Println(&quot;Type: %T Value: %v\\n&quot;, ToBe, ToBe); // fmt.Println(&quot;Type: %T value: %v\\n&quot;, MaxInt, MaxInt); fmt.Printf(&quot;Type %T value: %v\\n&quot;, ToBe, ToBe); fmt.Printf(&quot;Type: %T value: %v\\n&quot;, MaxInt, MaxInt); fmt.Printf(&quot;Type: %T value: %v\\n&quot;, z, z);} 零值没有明确初始值的变量声明会被赋予它们的零值零值：数值类型为 0；布尔类型为false；字符串为” “空字符串 123456789package mainimport \"fmt\"func main() { var i int var f float64 var b bool var s string fmt.Printf(\"%v %v %q %q\\n\", i, f, b, s)// %v 打印值；%T 打印类型，%q 打印全部信息} 类型转换表达式T(v)将值v转换为类型T一些关于数值的转换： 1234567var i int = 42var f float64 = float64(i)var u int = uint(f)// 或者，更加简单的形式：i := 42f := float64(i)u := uint(f) 与 C 不同的是，Go 在不同类型的项之间赋值时需要显式转换。试着移除例子中 float64 或 uint 的转换看看会发生什么。 12345678910111213141516package mainimport ( \"fmt\" \"math\")func main() { var x, y int = 3, 4 // var f float64 = math.Sqrt(float64(x*x + y*y)) // var u uint = uint(f) // var f = math.Sqrt(float64(x*x + y*y)) // var u = uint(f) f := math.Sqrt(float64(x*x + y*y)) u := uint(f) // fmt.Printf(x, f, u) fmt.Printf针对string fmt.Println(x, f, u)} 类型推导在声明一个变量而不指定其类型时(即不使用带:=或者var=表达式)，变量的类型由右值推导得出。当右值声明了类型时，新变量的类型与其相同： 12var i intj := i // j 也是一个 int 不过当右边包含未指明类型的数值常量时，新变量的类型就可能是int, float64, complex128, 取决于常量的精度类型: 123456789i := 2 // intf := 3.142 // float64g := 0.867 + 0.5i //complex128package mainimport \"fmt\"func main() { v := 42 // 修改这里！ fmt.Printf(\"v is of type %T\\n\", v)} 常量常量的声明与变量类似，只不过是使用const关键字。常量可以是字符、字符串、布尔值或数值。常量不能用:=语法声明。 1234567891011package mainimport \"fmt\"const Pi = 3.14func main() { const World = \"世界\" fmt.Println(\"Hello\", World) fmt.Println(\"Happy\", Pi, \"Day\") const Truth = true fmt.Println(\"Go rules?\", Truth)} 数值常量数值常量是高精度的 值。一个未指定类型的常量由上下文来决定其类型。int类型最大可以存储一个 64 位的整数，有时会更小。 123456789101112131415161718192021package mainimport ( \"fmt\")const ( // create a huge number by shifting 1 bit left 100 spaces // In other words, the binary number that is 1 followed by 100 zeros Big = 1 &lt;&lt; 100 // shift it right again in 99 places, so we end up with 1&lt;&lt;1 or 2. the binary number Small = Big &gt;&gt; 99)func needInt(x int) int { return 10*x + 1}func needFloat(x float64) float64 { return x * 0.1}func main() { // fmt.Println(Big) fmt.Println(Small) fmt.Println(needInt(Small)) fmt.Println(needFloat(Small)) fmt.Println(needFloat(Big))} 流程控制语句forGo 只有一种循环结构：for循环。基本的 for 循环由三部分组成，它们用分号隔开： 初始化语句：在第一次迭代前执行 条件表达式：在每次迭代前求值 后置语句：在每次迭代的结尾执行 [注意] 初始化语句通常为一句短变量声明，该变量声明仅在 for 语句的作用域中可见。一旦条件表达式的布尔值为 false，循环迭代就会终止。和C、Java、JavaScript之类的语言不同，Go的for语句后面没有小括号，大括号 { } 则是必须的。初始化语句和后置语句是可选的。 12345678910111213141516171819202122232425262728293031323334package mainimport ( \"fmt\")func main() { // var sum int = 1 // var sum = 1 // var ( // sum int = 1 // ) sum := 0; for i:=1; i &lt;= 10; i++ { sum += i fmt.Println(sum) } fmt.Println(sum)}// exchangepackage mainimport ( \"fmt\")func main() { // var sum = 0 // var sum int = 0 // var ( // sum int = 1 // ) sum := 1 for ;sum&lt; 10; { sum += sum fmt.Println(sum) }} for 是 Go 中的 “while”此时你可以去掉分号，因为 C 的while在 Go 中叫做for。 1234567891011package mainimport ( \"fmt\")func main() { sum := 1 for sum &lt; 1000 { sum += sum } fmt.Println(sum)} 无限循环如果省略循环条件，该循环就不会结束，因此无限循环可以写得很紧凑。 12345678package mainimport ( \"fmt\")func main() { for { }} ifGo 的 if 语句与 for 循环类似，表达式外无需小括号 ( )，而大括号 { } 则是必须的。 12345678910111213141516package main import ( \"fmt\" \"math\")func sqrt(x float64) string { if x &lt; 0 { // return string(math.Sqrt(-x)) + \"i\" error // return math.Sqrt(x) cannot convert float64 into string return sqrt(-x)+\"i\" } return fmt.Sprint(math.Sqrt(x)) // use Sprint to convert float64 into string}func main() { fmt.Println(sqrt(2), sqrt(-4))} if 的简短语句同 for 一样， if 语句可以在条件表达式前执行一个简单的语句。该语句声明的变量作用域仅在 if 之内。 123456789101112131415161718package mainimport ( \"fmt\" \"math\")func pow(x, n, lim float64) float64 { if v := math.Pow(x, n); v &lt; lim { return v } return lim}func main() { fmt.Println( pow(3, 2, 10), pow(3, 3, 20), )} if 和 else在 if 的简短语句中声明的变量同样可以在任何对应的 else 块中使用。 （在 main 的 fmt.Println 调用开始前，两次对 pow 的调用均已执行并返回。） 12345678910111213141516171819package mainimport ( \"math\" \"fmt\")func pow(x, n, lim float64) float64 { if v := math.Pow(x, n); v &lt; lim { return v } else { fmt.Printf(\"%g &gt;= %g\\n\", v, lim) } return lim}func main() { fmt.Println( pow(3, 2, 10), pow(3, 3, 20), )} 练习：循环与函数我们来实现一个平方根函数：用牛顿法实现平方根函数。计算机通常使用循环来计算 x 的平方根。从某个猜测的值 z 开始，我们可以根据 z² 与 x 的近似度来调整 z，产生一个更好的猜测：z -= (z*z - x) / (2*z)重复调整的过程，猜测的结果会越来越精确，得到的答案也会尽可能接近实际的平方根。 123456789101112131415161718package mainimport ( \"fmt\")func Sqrt(x float64) float64 { z := float64(1) for i := 0; i &lt; 10; i++ { z -= (z*z - x)/(2*z) fmt.Println(z) } return z}func main() { fmt.Println(Sqrt(2))} switchswitch 是编写一连串 if - else 语句的简便方法。它运行第一个值等于条件表达式的 case 语句。GO的switch语句类似于 C、C++、Java、JavaScript 和 PHP 中的，不过 Go 只运行选定的 case，而非之后所有的 case。 实际上，Go 自动提供了在这些语言中每个 case 后面所需的 break 语句。 除非以 fallthrough 语句结束，否则分支会自动终止。Go 的另一点重要的不同在于 switch 的 case 无需为常量，且取值不必为整数。 123456789101112131415161718package mainimport ( \"fmt\" \"runtime\")func main() { fmt.Print(\"Go runs on: \") switch os := runtime.GOOS; os { case \"darwin\": fmt.Println(\"OS X.\") case \"runtime\": fmt.Println(\"Linux.\") default: // freebsd, openbsd, // plan9, windows... fmt.Printf(\"%s.\", os) }} switch 的求值顺序switch 的 case 语句从上到下顺次执行，直到匹配成功时停止。 1234switch i {case 0:case f():} 在 i==0 时 f 不会被调用。）注意： Go 练习场中的时间总是从 2009-11-10 23:00:00 UTC 开始，该值的意义留给读者去发现。 12345678910111213141516171819202122package main import ( \"fmt\" \"time\")func main() { fmt .Println(\"when is Saturday?\"); today := time.Now().Weekday(); // fmt.Println(today); // fmt.Printf(\"%v %T\\n\", today, today); // fmt.Printf(\"%v %T\\n\", time.Now(), time.Now()); switch time.Saturday { case today + 0 : fmt.Println(\"Today\"); case today + 1 : fmt.Println(\"Tomorrow\"); case today + 2 : fmt.Println(\"in two days\"); default: fmt.Println(\"too far away\"); }} 没有条件的 switch没有条件的 switch 同 switch true 一样。这种形式能将一长串 if-then-else 写得更加清晰。 123456789101112131415161718package mainimport ( \"time\" \"fmt\")func main() { t := time.Now() switch { case t.Hour() &lt; 12: fmt.Println(\"Good morning\") case t.Hour() &lt; 18: fmt.Println(\"Good afternoon\") case t.Hour() &lt; 24: fmt.Println(\"Good night\") default: fmt.Println(\"Good day\") }} deferdefer 语句会将函数推迟到外层函数返回之后执行。推迟调用的函数其参数会立即求值，但直到外层函数返回前该函数都不会被调用。 12345678package mainimport ( \"fmt\")func main() { defer fmt.Println(\"hello defer\")//相对main函数 fmt.Println(\"world\")} defer 栈推迟的函数调用会被压入一个栈中。当外层函数返回时，被推迟的函数会按照后进先出的顺序调用。 1234567891011package mainimport ( \"fmt\")func main() { fmt.Println(\"counting\") for i := 0; i &lt; 10; i++ { defer fmt.Println(i) } fmt.Println(\"done\")} 更多类型 struct, slice and 映射指针Go 拥有指针。指针保存了值的内存地址(指针就是一个指向地址的数据)。在go中获得该变量的内存地址 用&amp;a, 别名而已，并没有占用内存空间。实际上他们是同一个东西，在内存中占用同样的一个存储单元。go中所有的都是按值传递，对于复杂类型，传的是指针的拷贝 类型 *T 是指向 T 类型值的指针。其零值为 nil。var p *int &amp; 操作符会生成一个指向其操作数的指针(&amp;取地址值)。 12i :=42p = &amp;i * 操作符表示指针指向的底层值(*取内容值)。 123456789101112131415161718fmt.Println(*p)// 通过指针 p 读取 i*p = 21 // 通过指针 p 设置 i// examplepackage main import ( \"fmt\")func main() { i := 43; // p := &amp;i; var p *int = &amp;i; j := *p; j = 1; *p = 3; fmt.Printf(\"value: %v type: %T\\n\", i, i); fmt.Printf(\"value: %v type: %T\\n\", p, p); fmt.Printf(\"value: %v type: %T\\n\", j, j); } 这也就是通常所说的“间接引用”或“重定向”。与 C 不同，Go 没有指针运算。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package mainimport ( \"fmt\")func main() { // i, j := 43, 8782; // // var p *int = &amp;i; // p := &amp;i; // pointer p to i // fmt.Printf(\"address: %v value: %v\\n\", p, *p); // fmt.Printf(\"%p\\n\", &amp;j) // Learnin pointer var m map[int]string = map[int]string { 0: \"00\", 1: \"11\", }; mm := m; //deep copy m value to mm var values = 4; // 如果是简单的类型，深拷贝 values2 := values; fmt.Printf(\"values value: %v address: %p\\n\", values, &amp;values); fmt.Printf(\"values2 value: %v address: %p\\n\", values2, &amp;values2); var mapLiu map[string]string = map[string]string { \"0\": \"www\", \"1\": \"xxxx\", }; mapLiu2 := mapLiu; // 如果是复杂类型， 浅拷贝 fmt.Printf(\"mapLiu value: %v address: %p\\n\", mapLiu, mapLiu); fmt.Printf(\"mapLiu2 value: %v address: %p\\n\", mapLiu2, mapLiu2); fmt.Printf(\"m value: %v address: %p\\n\",m, m); fmt.Printf(\"mm value: %v address: %p\\n\", mm, mm); // fmt.Printf(\"m value: %v address: %v\\n\",m, &amp;m); // fmt.Printf(\"mm value: %v address: %v\\n\", mm, &amp;mm); changeMap(m); //(1) go中所有的都是按值传递，对于复杂类型，传的是指针的拷贝`` // changeMap(&amp;m); // (2) 直接传指针 也是传指针的拷贝 fmt.Printf(\"m value: %v address: %p\\n\",m, m); fmt.Printf(\"mm value: %v address: %p\\n\", mm, mm); // （3） // 形参 和 实参 // param := 3; // fmt.Printf(\"param value: %v address: %p\\n\",param, &amp;param); // changeParam(param); // fmt.Printf(\"param value: %v address: %p\\n\",param, &amp;param);}// (1)func changeMap(mmm map[int]string) { mmm[1] = \"eeee\"; fmt.Printf(\"changeMap func value: %v address: %p\\n\", mmm, &amp;mmm);}// (2)// func changeMap(mmmm *map[int]string) {// // temp := *mmmm;// // temp[0] = \"啛啛喳喳\";// // fmt.Printf(\"func changeMap value: %v address: %p\\n\", temp, &amp;temp);// mmmm = nil;// // *mmmm = nil;// fmt.Printf(\"func changeMap value: %v address: %p\\n\", mmmm, mmmm);// }// (3)// func changeParam(x int) {// x = 2222;// fmt.Printf(\"func changeParam value: %v address: %p\\n\", x, &amp;x);// } 结构体一个结构体（struct）就是一个 字段的集合。 1234567891011package mainimport ( \"fmt\")type Vertex struct { X int Y int}func main() { fmt.Println(Vertex{1, 2})} 结构体字段结构体字段使用点号来访问。 12345678910111213package mainimport ( \"fmt\")type Vertex struct { X int Y int}func main() { v := Vertex{1, 2} v.X = 3 fmt.Println(v)} 结构体指针 结构体字段可以通过结构体指针来访问。 如果我们有一个指向结构体的指针 p，那么可以通过 (*p).X 来访问其字段 X。不过这么写太啰嗦了，所以语言也允许我们使用隐式间接引用，直接写 p.X 就可以。 12345678910111213141516package mainimport ( \"fmt\")type Vertex struct { X int Y int}func main() { v := Vertex{1, 2} p := &amp;v // (*p).X = 4 // fmt.Println(*p) p.X = 3e3 fmt.Println(*p)} 结构体文法 结构体文法通过直接列出字段的值来新分配一个结构体。 使用 Name: 语法可以仅列出部分字段。（字段名的顺序无关。） 特殊的前缀 &amp; 返回一个指向结构体的指针。 123456789101112131415161718192021package main import ( \"fmt\")type Vertex struct { X, Y int}var ( v1 = Vertex{1, 2}; // has type Vertex v2 = Vertex{X: 1}; // Y: 0 is implicit v3 = Vertex{}; // X:0 Y:0 // p = &amp;Vertex{1, 2} // has type *Vertex p = &amp;v1;)func main() { var pointer *int; fmt.Println(pointer); fmt.Println(v1, v2, v3, p); fmt.Printf(\"v1 address: %p value: %v\\n\", &amp;v1, v1); fmt.Printf(\"p address: %p value: %v\\n\", p, *p); // 指针为同一个地址} 数组类型 [n]T 表示拥有 n 个 T 类型的值 的数组。表达式 会将变量 a 声明为拥有有 10 个整数的数组。将数组看作一个特殊的struct，结构的字段名对应数组的索引，同时成员的数目固定 1var a [10]int 数组的长度是其类型的一部分，因此数组不能改变大小。这看起来是个限制，不过没关系，Go 提供了更加便利的方式来使用数组。 12345678910111213141516171819// arraypackage main import ( \"fmt\")func main() { var a [2]string; a[0] = \"hellp\"; a[1] = \"ekedd\"; fmt.Println(a); fmt.Println(a[0], a[1]); primes := [6]int{2, 3, 5, 7, 9, 11}; fmt.Println(primes); var m map[int]string = map[int]string { 1: \"www0\", 2: \"sss\", }; fmt.Println(m);} 切片(slices)每个数组的大小都是固定的。而切片则为数组元素提供动态大小的、灵活的视角。在实践中，切片比数组更常用。类型[]T表示一个元素类型为T的切片切片通过两个下标来界定，即一个上界和一个下界，二者以冒号分隔:a[low : high] 即 var s []int = primes[1 : 4]他会选择一个半开区间，左闭右开(美国人的习惯)以下表达式创建了一个切片，包含了a中下标从1到3的元素:a[1:3] 123456789101112//slicespackage mainimport ( \"fmt\")func main() { // var primes [6]int = [6]int{2, 3, 5, 7, 11, 13} // fmt.Println(primes) primes := [6]int{2, 3, 5, 7, 11, 13} var s []int = primes[1 : 4] fmt.Println(s)} 切片就像数组的引用(slices are like references to arrays)切片并不存储任何数据，它只是描述了底层数组中的一段(引用)。更改切片的元素会修改其底层数组中对应的元素。与它共享底层数组的切片都会观测到这些修改。 123456789101112131415161718192021// slices are like references to arrayspackage mainimport ( \"fmt\")func main() { // var names [4]string = [4]string{} names := [4]string{ \"John\", \"Paul\", \"George\", \"Ringo\", } fmt.Println(names) a := names[0 : 2] b := names[1 : 3] fmt.Println(a, b) b[0] = \"XXXX\" fmt.Println(a, b) fmt.Println(names)} 切片文法(slices literals)切片文法类似于没有长度的数组文法。这是一个数组文法：[3]bool{true, true, false}下面这样则会创建一个和上面相同的数组，然后构建一个引用了它的切片：[]bool{true, true, false} 12345678910111213141516171819202122232425262728293031323334353637383940// slices literalspackage mainimport ( \"fmt\")func main() { // create array var array [4]int = [4]int{1, 2, 3, 4} fmt.Printf(\"value: %v, Type: %T\\n\", array, array) array2 := [4]bool{false, true, false, true} fmt.Printf(\"value: %v Type: %T\\n\", array2, array2) // create slices var q []int = []int{3, 4, 6, 7} fmt.Printf(\"value: %v Type:%T\\n\", q, q) q2 := []bool{true, false, true, true, false} fmt.Printf(\"value: %v Type: %T\\n\", q2, q2) // create slices with struct var s1 []struct{ i int b bool } = []struct { i int b bool }{ {2, false}, {3, true}, {1, true}, {6, false}, } fmt.Printf(\"value:%v Type: %T\\n\", s1, s1) s := []struct{ i int b bool }{ {1, true}, {3, false}, {5, false}, } fmt.Printf(\"value:%v Type: %T\\n\", s, s)} 切片的默认行为在进行切片时，你可以利用它的默认行为来忽略上下界。切片下界的默认值为 0，上界则是该切片的长度。对于数组var a [10]int来说，以下切片是等价的： 1234a[0:10]a[:10]a[0:]a[:] 12345678910111213141516// slices defaultspackage mainimport ( \"fmt\")func main() { s := []int{2, 3, 4, 6, 8} s1 := s[1:4] fmt.Println(s, s1) s2 := s[:2] fmt.Println(s, s2) s2[0] = 100 // slices are like references to array fmt.Println(s, s1) fmt.Println(s, s2)} 切片的长度与容量切片拥有 长度 和 容量。切片的长度就是它所包含的元素个数。切片的容量是从切片后的第一个元素开始数，到其底层数组元素末尾的个数。切片 s 的长度和容量可通过表达式 len(s) 和 cap(s) 来获取。 len()获取的切片的得到的长度，cap是指员原来数组的长度。你可以通过重新切片来扩展一个切片，给它提供足够的容量。试着修改示例程序中的切片操作，向外扩展它的容量，看看会发生什么。 12345678910111213141516171819202122232425// slices gopackage main import ( \"fmt\")func printSlices(s []int) { fmt.Printf(\"len=%d cap=%d %v\\n\", len(s), cap(s), s);}func main() { s := []int{3, 4, 5,7, 8, 10}; printSlices(s); // Slices the slice to give it zero length s = s[:0]; printSlices(s); // extend th length s = s[:4]; printSlices(s); // drop its first two values s = s[:2]; printSlices(s); //len=2 cap=6 [3 4] s = s[:6]; printSlices(s); //len=6 cap=6 [3 4 5 7 8 10] s = s[3:4]; printSlices(s); //len=1 cap=3 [7]} nil切片切片的的零值是nilnil切片的长度和容量为0切没有底层数组。 1234567891011121314// nil slicespackage mainimport ( \"fmt\")func main() { // var s = []int //error []int is not expression // var s []int = []int{1, 2, 3} &lt;==&gt; s := []int{1, 2, 3} var s []int fmt.Println(s, len(s), cap(s)) if s == nil { fmt.Println(\"nil\") }} 用 make 创建切片切片可以用内建函数 make 来创建，这也是你创建动态数组的方式。make 函数会分配一个元素为零值的数组并返回一个引用了它的切片：a := make([]int, 5) // len(a)=5要指定它的容量，需向 make 传入第三个参数： 123b := make([]int, 0, 5) // len(b)=0, cap(b)=5b = b[:cap(b)] // len(b)=5, cap(b)=5b = b[1:] // len(b)=4, cap(b)=4 1234567891011121314151617181920212223// use make to create slicespackage mainimport ( \"fmt\")func printSlice(s string, x []int) { fmt.Printf(\"%s len=%d cap=%d value=%v type=%T\\n\", s, len(x), cap(x), x, x)}func main() { // x := [3]string{\"Лайка\", \"Белка\", \"Стрелка\"} // s := x[:] // a slice referencing the storage of x // 创建 一个元素值为0的数组并返回一个引用了它的切片 a := make([]int, 5) // len(a) = 5 cap(a) = 5 printSlice(\"a\", a) // 创建 一个指定容量， 需要向make传入第三个参数 // b := make([]int, 3, 5) // len(b)=3, cap(b)=5 b := make([]int, 0, 5); // len(b)=0, cap(b)=5 printSlice(\"b\", b); // b len=0 cap=5 value=[] type=[]int c := b[:3]; printSlice(\"c\", c); // c len=3 cap=5 value=[0 0 0] type=[]int d := c[2:5]; printSlice(\"d\", d); // d len=3 cap=3 value=[0 0 0] type=[]int} [Note]一个切片是一个数组片段的描述。它包含了指向数组的指针，片段的长度， 和容量（片段的最大长度）。前面使用 make([]byte, 5) 创建的切片变量 s ,其中[]int指向数组指针；长度是切片引用的元素数目。容量是底层数组的元素数目（从切片指针开始）。 reference 切片的切片切片可包含任何类型，甚至包括其它的切片。 1234567891011121314151617181920212223// slices of slicespackage main import ( \"fmt\" \"strings\")func main() { // create a tic-tac-toe board board := [][]string { []string{\"_\", \"_\", \"_\"}, []string{\"_\", \"_\", \"_\"}, []string{\"_\", \"_\", \"_\"}, }; // board[0][0] = \"X\"; board[2][2] = \"O\"; board[1][2] = \"X\"; board[1][0] = \"O\"; board[0][2] = \"X\"; for i:=0; i &lt; len(board); i++ { fmt.Printf(\"%s\\n\", strings.Join(board[i], \" \")); }} 向切片追加元素为切片追加新的元素是种常用的操作，为此 Go 提供了内建的 append 函数。内建函数的文档对此函数有详细的介绍。func append(s []T, vs ...T) []Tappend 的第一个参数 s 是一个元素类型为 T 的切片，其余类型为 T 的值将会追加到该切片的末尾。append 的结果是一个包含原切片所有元素加上新添加元素的切片。当 s 的底层数组太小，不足以容纳所有给定的值时，它就会分配一个更大的数组。返回的切片会指向这个新分配的数组。 123456789101112131415161718192021222324// appendpackage main import ( \"fmt\")func printSlices(s []int) { fmt.Printf(\"len=%d cap=%d value=%v type=%T\\n\", len(s), cap(s), s, s);}func main() { var s []int; printSlices(s); // append works on nil slices s = append(s, 0); printSlices(s); // the slices grows as needed s = append(s, 1); printSlices(s); // we can add more than one element at time s = append(s, 2, 4, 5, 6); printSlices(s); // s1 := []int{7, 4, 2}; // 不是同一种类型 // s = append(s, s1); // printSlices(s);} Rangefor 循环的 range 形式可遍历切片或映射。当使用 for 循环遍历切片时，每次迭代都会返回两个值。第一个值为当前元素的下标，第二个值为该下标所对应元素的一份副本。 1234567891011// Rangepackage mainimport ( \"fmt\")var pow = []int{1, 2, 4, 8, 16, 32, 64, 128}func main() { for i, v := range pow { fmt.Printf(\"2**%d = %d\\n\", i, v) }} 可以将下标或值赋予 _ 来忽略它。若你只需要索引，去掉 , value 的部分即可。 12345678910111213package mainimport( \"fmt\")func main() { pow := make([]int, 10) for i := range pow { pow[i] = 1 &lt;&lt; uint(i) // ==2**i } for _, value := range pow { fmt.Printf(\"%d\\n\", value) }} 练习：切片实现 Pic。它应当返回一个长度为 dy 的切片，其中每个元素是一个长度为 dx，元素类型为 uint8 的切片。当你运行此程序时，它会将每个整数解释为灰度值（好吧，其实是蓝度值）并显示它所对应的图像。 图像的选择由你来定。几个有趣的函数包括 (x+y)/2, x*y, x^y, x*log(y) 和 x%(y+1)。 （提示：需要使用循环来分配 [][]uint8 中的每个 []uint8；请使用 uint8(intValue) 在类型之间转换；你可能会用到 math 包中的函数。） 12345678910111213141516171819package mainimport \"golang.org/x/tour/pic\"import \"fmt\"func Pic(dx, dy int) [][]uint8 { var rgb [][]uint8 fmt.Println(dx, dy) for i := 0; i&lt;dy; i++ { for j:=0; j &lt;dx; j++ { rgb[i][j] = uint8((i+j)/2) } } return rgb}func main() { pic.Show(Pic)} 映射(map)映射将key映射到value。映射的零值为 nil 。nil 映射既没有键，也不能添加键。make函数会返回给定类型的映射(map)，并将其初始化备用。map type is type: map[string]main.Vertex 12345678910111213141516171819// mappackage mainimport ( \"fmt\")type Vertex struct { Lat, Long float64}// initialize mapvar m map[string]Vertexfunc main() { m = make(map[string]Vertex) m[\"Bell Labs\"] = Vertex{ // 40.22234, -74.33113 // error 40.22234, -74.33113, } fmt.Printf(\"value: %v type: %T\\n\",m, m) fmt.Println(m[\"Bell Labs\"])} map的文法(写法)映射的文法与结构体相似，不过必须有键名。 12345678910111213141516171819202122// map literalspackage mainimport ( \"fmt\")type Vertex struct { Lat, Long float64}// var s []intvar s = []int{1, 2, 3}// map 类型 map[string]Vertexvar m = map[string]Vertex{ \"Bell Labs\": Vertex{ 40.441245, -68.421167889, }, \"Google\": Vertex{ 37.3425, -124.341, },}func main() { fmt.Println(m)} 若顶级类型只是一个类型名，你可以在文法的元素中省略它。 1234567891011121314151617181920package mainimport ( \"fmt\")type Vertex struct { Lat, Long float64}var m = map[string]Vertex{ // \"Bell Labs\": Vertex{ // 30.4422, -44.44212, // }, // \"Google\": Vertex { // 39.442, -55.224, // }, \"Bell\": {-22.3444, 33.312}, \"Google\":{-33.442, 33.51},}func main() { fmt.Println(m)} 修改映射(map)在map m中插入或者修改元素： m[key] = elem获取元素: elem = m[key]删除元素: delete(m, key)通过双赋值检测某个键是否存在: elem, ok = m[key]若key在m中，ok为true；否则, ok为false.若key不在map中，那么elem是该map的零值。即当从映射中读取某个不存在的键时，结果是映射的元素类型的零值。注: 若elem或者ok还未声明，你可以使用短变量声明: elem, ok := m[key] 123456789101112131415package main import ( \"fmt\")func main() { m := make(map[string]int); m[\"Answer\"] = 42; fmt.Println(\"The value:\", m[\"Answer\"]); m[\"Answer\"] = 48; fmt.Println(\"Thw value:\", m[\"Answer\"]); delete(m, \"Answer\"); fmt.Println(\"The value:\", m[\"Answer\"]); v, ok := m[\"Answer\"]; fmt.Println(\"The value:\", v, \"Present?\", ok);} 函数值函数也是值。它们可以像其它值一样传递。函数值可以用作函数的参数或返回值。 1234567891011121314151617package main import ( \"math\" \"fmt\")func compute(fn func(float64, float64) float64) float64 { fmt.Println(fn); return fn(3, 4);}func main() { hypot := func(x, y float64) float64 { return math.Sqrt(x*x + y*y); } fmt.Println(hypot(5, 12)); fmt.Println(compute(hypot)); fmt.Println(compute(math.Pow));} 函数的闭包Go 函数可以是一个闭包。闭包是一个函数值，它引用了其函数体之外的变量。该函数可以访问并赋予其引用的变量的值，换句话说，该函数被“绑定”在了这些变量上。例如，函数adder返回一个闭包。每个闭包都被绑定在其各自的 sum 变量上。 1234567891011121314151617181920package mainimport ( \"fmt\")func adder() func(int) int { sum := 0; return func(x int) int { sum += x; return sum; }}func main() { pos, neg := adder(), adder(); for i:=0; i&lt;10; i++ { fmt.Println( pos(i), neg(-i), ); }} 练习：斐波纳契闭包实现一个 fibonacci 函数，它返回一个函数（闭包），该闭包返回一个斐波纳契数列 (0, 1, 1, 2, 3, 5, ...)。 12345678910111213141516171819package mainimport \"fmt\"// fibonacci is a function that returns// a function that returns an int.func fibonacci() func() int { last = 0; return func(x int) int { };}func main() { f := fibonacci() for i := 0; i &lt; 10; i++ { fmt.Println(f()) }} 方法与接口方法Go 没有类。不过你可以为结构体类型定义方法。方法就是一类带特殊的 接收者 参数的函数。方法接收者在它自己的参数列表内，位于 func 关键字和方法名之间。在此例中，Abs 方法拥有一个名为 v，类型为 Vertex 的接收者。 123456789101112131415package mainimport ( \"fmt\" \"math\")type Vertex struct { X, Y float64}func (v Vertex) Abs() float64 { return math.Sqrt(v.X*v.X + v.Y*v.Y);}func main() { v := Vertex{3, 4}; fmt.Println(v.Abs());} 方法即是函数(Methods(class) are functions)记住：方法只是个带接收者参数的函数。现在这个 Abs 的写法就是个正常的函数，功能并没有什么变化。 123456789101112131415161718192021package mainimport ( \"fmt\" \"math\")type Vertex struct { X, Y float64}// define a method// func (v Vertex) Abs() float64 {// return math.Sqrt(v.X*v.X + v.Y*v.Y);// }// functionfunc Abs(v Vertex) float64 { return math.Sqrt(v.X*v.X + v.Y*v.Y);}func main() { v := Vertex {3, 4}; // fmt.Println(v.Abs()); fmt.Println(Abs(v));} 你也可以为非结构体类型声明方法。如例子中的一个带Abs方法的数值类型MyFloat。你只能为在同一个包内定义的类型的接受者声明方法，而不能为其他包内定义的类型(包括int之类的内建类型)的接受者声明方法。(注：就是接收者的类型定义和方法声明必须在同一个包内；不能为内建类型声明方法。) 1234567891011121314151617181920212223242526272829303132package mainimport ( \"math\" \"fmt\")type MyFloat float64func (f MyFloat) Abs() float64 { if f &lt; 0 { return float64(-f); } return float64(f);}func main() { f := MyFloat(-math.Sqrt2); fmt.Println(f.Abs());}// example 2package mainimport ( \"fmt\")type MyInt intfunc (Myint MyInt) Abs() int { if Myint &lt; 0 { return int(-Myint); } return int(Myint);}func main() { myint := MyInt(-2); fmt.Println(myint.Abs());} 通用：声明的方法为在类中添加方法，必须是初始化类，生成对象调用方法。 指针接收者你可以为指针接收者声明方法。这意味着对于某类型 T，接收者的类型可以用 T 的文法。（此外，T 不能是像 int 这样的指针。）例如，这里为 Vertex 定义了 Scale 方法。指针接收者的方法可以修改接收者指向的值（就像 Scale 在这做的）。由于方法经常需要修改它的接收者，指针接收者比值接收者更常用。试着移除第 16 行 Scale 函数声明中的 ，观察此程序的行为如何变化。若使用值接收者，那么 Scale 方法会对原始 Vertex 值的副本进行操作。（对于函数的其它参数也是如此。）Scale 方法必须用指针接受者来更改 main 函数中声明的 Vertex 的值。 12345678910111213141516171819202122232425262728293031package main import ( \"math\" \"fmt\")type Vertex struct { X, Y float64}func (v Vertex) Abs() float64 { return math.Sqrt(v.X*v.X + v.Y*v.Y);}func (v *Vertex) Scale(f float64) { // (*v).X = (*v).X * f; // (*v).Y = (*v).Y * f; v.X *= f; v.Y *= f; // fmt.Printf(\"2 value: %v type: %T address: %p\\n\", v, v, v);}// func (v Vertex) Scale(f float64) {// v.X = v.X * f;// v.Y = v.Y * f;// fmt.Printf(\"2 value: %v type: %T address: %p\\n\", v, v, &amp;v);// }func main() { v := Vertex{3, 4}; fmt.Printf(\"1 value: %v type: %T address: %p\\n\", v, v, &amp;v); v.Scale(10); fmt.Printf(\"3 value: %v type: %T address %p\\n\", v, v, &amp;v); fmt.Println(v.Abs());} 指针与函数把 Abs 和 Scale 方法重写为函数。接着移除Scale中的*。 1234567891011121314151617181920package mainimport ( \"math\" \"fmt\")type Vertex struct { X, Y float64}func Abs(v Vertex) float64 { return math.Sqrt(v.X*v.X + v.Y*v.Y);}func Scale(v *Vertex, f float64) { v.X = v.X * f; v.Y = v.Y * f;} func main() { v := Vertex{3, 4}; Scale(&amp;v, 10); fmt.Println(Abs(v));} 方法与指针重定向比较前两个程序，你大概会注意到带指针参数的函数必须接受一个指针： 123var v VertexScaleFunc(v, 5) // 编译错误！ScaleFunc(&amp;v, 5) // OK 而以指针为接受者的方法被调用时，接受者既能为值又能为指针： 1234var v Vertexv.Scale(5) // OKp := &amp;vp.Scale(10) // OK 对于语句 v.Scale(5)，即便 v 是个值而非指针，带指针接收者的方法也能被直接调用。 也就是说，由于 Scale 方法有一个指针接收者，为方便起见，Go 会将语句 v.Scale(5) 解释为 (&amp;v).Scale(5)。 123456789101112131415161718192021222324252627282930313233343536373839package main import ( \"fmt\")type Vertex struct { X, Y float64}// define methodfunc (v *Vertex) Scale(f float64) { // (*v).X *= f; // (*v).Y *= f; v.X *= f; v.Y *= f;}// define functionfunc ScaleFunction(v *Vertex, f float64) { v.X *= f; v.Y *= f;}func main() { // v := Vertex{3, 4} // v.Scale(2) // ScaleFunction(&amp;v, 10) // p := &amp;Vertex{4, 3} // p.Scale(3) // ScaleFunction(p, 8) // fmt.Println(v, p) v := Vertex{3, 4}; v.Scale(10); fmt.Println(\"1:\",v); ScaleFunction(&amp;v, 2); fmt.Println(\"2:\", v); // p := &amp;v; p := &amp;Vertex{4, 3}; p.Scale(2); fmt.Println(\"3:\", p); ScaleFunction(p, 10); fmt.Println(\"4:\", p);} 方法与指针重定向（续）同样的事情也发生在相反的方向。接受一个值作为参数的函数必须接受一个指定类型的值： 123var v Vertexfmt.Println(AbsFunc(v)) // OKfmt.Println(AbsFunc(&amp;v)) // 编译错误！ 而以值为接收者的方法被调用时，接收者既能为值又能为指针： 1234var v Vertexfmt.Println(v.Abs()) // OKp := &amp;vfmt.Println(p.Abs()) // OK 这种情况下，方法调用 p.Abs() 会被解释为 (*p).Abs()。 12345678910111213141516171819202122package mainimport ( \"fmt\" \"math\")type Vertex struct { X, Y float64}func (v Vertex) Abs() float64 { return math.Sqrt(v.X*v.X + v.Y*v.Y);}func AbsFunc(v Vertex) float64 { return math.Sqrt(v.X*v.X + v.Y*v.Y);}func main() { v := Vertex{3, 4}; fmt.Println(v.Abs()); fmt.Println(AbsFunc(v)); p := &amp;Vertex{4, 3}; fmt.Println(p.Abs()); fmt.Println(AbsFunc(*p));} 选择值或指针作为接收者使用指针接收者的原因有二：首先，方法能够修改其接收者指向的值。其次，这样可以避免在每次调用方法时复制该值。若值的类型为大型结构体时，这样做会更加高效。在本例中，Scale 和 Abs 接收者的类型为 *Vertex，即便 Abs 并不需要修改其接收者。通常来说，所有给定类型的方法都应该有值或指针接收者，但并不应该二者混用。（我们会在接下来几页中明白为什么。） 12345678910111213141516171819202122232425262728293031323334353637383940package mainimport ( \"fmt\" \"math\")type Vertex struct { X, Y float64}// define methodfunc (v *Vertex)Scale(f float64) { v.X *= f; v.Y *= f;}func (v *Vertex)Abs() float64 { return math.Sqrt(v.X*v.X + v.Y*v.Y);}// define functionfunc ScaleFunc(v *Vertex, f float64){ v.X *= f; v.Y *= f;}func AbsFunc(v *Vertex) float64 { return math.Sqrt(v.X*v.X + v.Y*v.Y);}func main() { v :=Vertex{3, 4}; v.Scale(2); // (&amp;v).Scale(2); equal fmt.Println(v); // ScaleFunc(v, 20); cannot use v(type Vertex) as type *Vertex ScaleFunc(&amp;v, 20); fmt.Println(v); v2 := &amp;Vertex{4, 5}; // fmt.Printf(\"Before scaling: %v, Abs: %v\\n\", v2, v2.Abs()); // fmt.Printf(\"Before scaling: %p, Abs: %v\\n\", v2, v2.Abs()); fmt.Printf(\"Before scaling: %+v, Abs: %v\\n\", v2, v2.Abs()); v2.Scale(5); fmt.Printf(\"After Scaling: %+v, Abs: %v\\n\", v2, v2.Abs());} 接口接口类型 是由一组方法签名定义的集合接口类型的值可以保存任何实现了这些方法的值。 123456789101112131415161718192021222324252627282930313233package mainimport ( \"fmt\" \"math\")type MyFloat float64func (f MyFloat) Abs() float64 { if f &lt; 0 { return float64(-f); } return float64(f);}type Vertex struct { X, Y float64}func (v *Vertex) Abs() float64 { return math.Sqrt(v.X*v.X + v.Y*v.Y);} // define interfacetype Abser interface { Abs() float64}func main() { // initlize var a Abser; f := MyFloat(-math.Sqrt2); v := Vertex{3, 4}; a = f; // a MyFloat implements Abser a = &amp;v; // a *Vertex implements Abser // 下面一行，v是一个Vertex(而不是 *Vertex)，所以没有实现Abser a = v; fmt.Println(a.Abs());} 接口与隐式实现类型通过实现一个接口的所有方法 来实现该接口。既然无需专门显示声明，也就没有“implement”关键字。隐式接口从接口的实现中解耦了定义，这样接口的实现可以出现在任何包中，无需提前准备。因此，也就无需再每一个实现上增加新的接口名称，这样同时也鼓励了明确的接口定义。 12345678910111213141516171819package mainimport ( \"fmt\")type I interface { M();}type T struct { S string;}// This method means type T implements the inteface I,// but we don't need to explicitly declare that it does so.func (t T) M() { fmt.Println(t.S);}func main() { var i I = T{\"hellosdsfs\"}; i.M();} 接口值在内部，接口值可以看做包含值和具体类型的元组： (value, type)接口值保存了一个具体底层类型的具体值。接口值调用方法时会执行其底层类型的同名方法。 12345678910111213141516171819202122232425262728293031323334package mainimport ( \"fmt\" \"math\")// 1.声明接口type I interface { M();}// 2.声明一个结构体Ttype T struct { S string}// 3.结构体T声明结构体的方法，实现接口的方法，隐式实现func (t *T) M() { fmt.Println(t.S);}type F float64func (f F) M() { fmt.Println(f);}func main() { var i I; i = &amp;T{\"Hello\"}; describe(i); i.M(); // 接口 有具体类型的类的去实现，方法也相应的类的方法去实现。 i = F(math.Pi); describe(i); i.M();}func describe(i I) { fmt.Printf(\"(%v, %T)\\n\", i, i);} 底层值为 nil 的接口值即便接口内的具体值为 nil，方法仍然会被 nil 接收者调用。在一些语言中，这会触发一个空指针异常，但在 Go 中通常会写一些方法来优雅地处理它（如本例中的 M 方法）。注意： 保存了 nil 具体值的接口其自身并不为 nil。 123456789101112131415161718192021222324252627282930313233package mainimport ( \"fmt\")// 1.声明一个接口Itype I interface { M();}// 2.声明具体类型T 结构体type T struct { S string}// 3.具体类型结构体T(reciever) 实现接口的方法(隐式实现)func (t *T) M() { if t == nil { fmt.Println(\"&lt;nil&gt;\"); return; } fmt.Println(t.S);}func main() { var i I; var t *T; i = t; describe(i); i.M(); i = &amp;T{\"Hellowqww\"}; describe(i); i.M();}func describe(i I) { fmt.Printf(\"(%v %T)\\n\", i, i);} nil 接口值nil 接口值既不保存值也不保存具体类型。为 nil 接口调用方法会产生运行时错误，因为接口的元组内并未包含能够指明该调用哪个 具体 方法的类型。 123456789101112131415package mainimport ( \"fmt\")type I interface { M();}func main() { var i I; describe(i); i.M();}func describe(i I) { fmt.Printf(\"(%v %T)\\n\",i, i);} 空接口指定了零个方法的接口值被称为 空接口： interface{}空接口可保存任何类型的值。（因为每个类型都至少实现了零个方法。）空接口被用来处理未知类型的值。例如，fmt.Print 可接受类型为 interface{} 的任意数量的参数。 123456789101112131415package mainimport ( \"fmt\")func main() { var i interface{}; describe(i); i = 42; describe(i); i = \"hello\"; describe(i);}func describe(i interface{}) { fmt.Printf(\"(%v %T)\\n\", i, i);} 类型断言类型断言 提供了访问接口值底层具体值的方式。t := i.(T)该语句断言接口值 i 保存了具体类型 T，并将其底层类型为 T 的值赋予变量 t。若 i 并未保存 T 类型的值，该语句就会触发一个panic。为了 判断 一个接口值是否保存了一个特定的类型，类型断言可返回两个值：其底层值以及一个报告断言是否成功的布尔值。t, ok := i.(T)若 i 保存了一个 T，那么 t 将会是其底层值，而 ok 为 true。否则，ok 将为 false 而 t 将为 T 类型的零值，程序并不会产生恐慌。请注意这种语法和读取一个映射时的相同之处。 1234567891011121314151617package mainimport ( \"fmt\")func main() { // 声明一个接口， var i interface{} = \"hello\"; fmt.Printf(\"(%v %T)\\n\",i, i); s := i.(string); fmt.Println(s); s, ok := i.(string); fmt.Println(s, ok); f, ok := i.(float64); fmt.Println(f, ok); f = i.(float64); fmt.Println(f);} 类型选择类型选择 是一种按顺序从几个类型断言中选择分支的结构。类型选择与一般的 switch 语句相似，不过类型选择中的 case 为类型（而非值）， 它们针对给定接口值所存储的值的类型进行比较。 12345678switch v := i.(type) {case T: // v 的类型为 Tcase S: // v 的类型为 Sdefault: // 没有匹配，v 与 i 的类型相同} 类型选择中的声明与类型断言 i.(T) 的语法相同，只是具体类型 T 被替换成了关键字 type。此选择语句判断接口值 i 保存的值类型是 T 还是 S。在 T 或 S 的情况下，变量 v 会分别按 T 或 S 类型保存 i 拥有的值。在默认（即没有匹配）的情况下，变量 v 与 i 的接口类型和值相同。 12345678910111213141516171819package mainimport ( \"fmt\")func do (i interface{}) { switch v := i.(type) { case int: fmt.Printf(\"Twice %v is %v\\n\", v, v*2); case string: fmt.Printf(\"%q is %v bytes long\\n\", v, len(v)); default: fmt.Printf(\"I don't know about type %T!\\n\", v); }}func main() { do(11); do(\"hell9\"); do(true);} Stringerfmt 包中定义的 Stringer 是最普遍的接口之一。 123type Stringer interface { String() string} Stringer 是一个可以用字符串描述自己的类型。fmt 包（还有很多包）都通过此接口来打印值。 12345678910111213141516171819package mainimport ( \"fmt\")// type Stringer interface {// String() string;// }type Person struct { Name string Age int}func (p Person) String() string { return fmt.Sprintf(\"%v (%v years)\\n\", p.Name, p.Age);}func main() { a := Person{\"Arthur Dent\", 43}; z := Person{\"Zaphod Beelebrox\", 9001}; fmt.Println(a, z);} 练习：Stringer通过让 IPAddr 类型实现 fmt.Stringer 来打印点号分隔的地址。例如，IPAddr{1, 2, 3, 4} 应当打印为 &quot;1.2.3.4&quot;。 123456789101112131415161718192021package mainimport \"fmt\"type IPAddr [4]byte// TODO: Add a \"String() string\" method to IPAddr.func (ipAddr IPAddr) String() string { return fmt.Sprintf(\"%v.%v.%v.%v\\n\", ipAddr[0],ipAddr[1],ipAddr[2],ipAddr[3]);}func main() { hosts := map[string]IPAddr{ \"loopback\": {127, 0, 0, 1}, \"googleDNS\": {8, 8, 8, 8}, } for name, ip := range hosts { fmt.Printf(\"%v: %v\\n\", name, ip) }} 错误Go 程序使用 error 值来表示错误状态。与 fmt.Stringer 类似，error 类型是一个内建接口： 123type error interface { Error() string} （与 fmt.Stringer 类似，fmt 包在打印值时也会满足 error。）通常函数会返回一个 error 值，调用的它的代码应当判断这个错误是否等于 nil 来进行错误处理。 123456i, err := strconv.Atoi(\"42\")if err != nil { fmt.Printf(\"couldn't convert number: %v\\n\", err) return}fmt.Println(\"Converted integer:\", i) error 为 nil 时表示成功；非 nil 的 error 表示失败。 1234567891011121314151617181920212223package mainimport ( \"time\" \"fmt\")type MyError struct { When time.Time What string}func (e *MyError) Error() string { return fmt.Sprintf(\"at %v, %s\", e.When, e.What);}func run() error { return &amp;MyError{ time.Now(), \"it didn't work\", };}func main() { if err := run(); err != nil { fmt.Println(err); }} 练习：错误从之前的练习中复制 Sqrt 函数，修改它使其返回 error 值。Sqrt 接受到一个负数时，应当返回一个非 nil 的错误值。复数同样也不被支持。创建一个新的类型type ErrNegativeSqrt float64并为其实现func (e ErrNegativeSqrt) Error() string方法使其拥有 error 值，通过 ErrNegativeSqrt(-2).Error() 调用该方法应返回 &quot;cannot Sqrt negative number: -2&quot;。注意： 在 Error 方法内调用 fmt.Sprint(e) 会让程序陷入死循环。可以通过先转换 e 来避免这个问题：fmt.Sprint(float64(e))。这是为什么呢？修改 Sqrt 函数，使其接受一个负数时，返回 ErrNegativeSqrt 值。 1234567891011121314package mainimport ( \"fmt\")func Sqrt(x float64) (float64, error) { return 0, nil}func main() { fmt.Println(Sqrt(2)) fmt.Println(Sqrt(-2))} Readerio 包指定了 io.Reader 接口，它表示从数据流的末尾进行读取。Go 标准库包含了该接口的许多实现，包括文件、网络连接、压缩和加密等等。io.Reader 接口有一个 Read 方法：func (T) Read(b []byte) (n int, err error)Read 用数据填充给定的字节切片并返回填充的字节数和错误值。在遇到数据流的结尾时，它会返回一个 io.EOF 错误。示例代码创建了一个 strings.Reader 并以每次 8 字节的速度读取它的输出。 123456789101112131415161718192021package mainimport ( \"fmt\" \"io\" \"strings\")func main() { r := strings.NewReader(\"Hello, Reader!\") b := make([]byte, 8) for { n, err := r.Read(b) fmt.Printf(\"n = %v err = %v b = %v\\n\", n, err, b) fmt.Printf(\"b[:n] = %q\\n\", b[:n]) if err == io.EOF { break } }} 练习：Reader实现一个 Reader 类型，它产生一个 ASCII字符 'A' 的无限流。 1234567891011package mainimport \"golang.org/x/tour/reader\"type MyReader struct{}// TODO: Add a Read([]byte) (int, error) method to MyReader.func main() { reader.Validate(MyReader{})} 练习：rot13Reader有种常见的模式是一个 io.Reader 包装另一个 io.Reader，然后通过某种方式修改其数据流。 例如，gzip.NewReader 函数接受一个 io.Reader（已压缩的数据流）并返回一个同样实现了 io.Reader 的 *gzip.Reader（解压后的数据流）。 编写一个实现了 io.Reader 并从另一个 io.Reader 中读取数据的 rot13Reader，通过应用 rot13 代换密码对数据流进行修改。 rot13Reader 类型已经提供。实现 Read方法以满足 io.Reader。 1234567891011121314151617package mainimport ( \"io\" \"os\" \"strings\")type rot13Reader struct { r io.Reader}func main() { s := strings.NewReader(\"Lbh penpxrq gur pbqr!\") r := rot13Reader{s} io.Copy(os.Stdout, &amp;r)} 图像image 包定义了 Image 接口： 123456package imagetype Image interface { ColorModel() color.Model Bounds() Rectangle At(x, y int) color.Color} 注意： Bounds 方法的返回值 Rectangle 实际上是一个 image.Rectangle，它在 image 包中声明。color.Color 和 color.Model 类型也是接口，但是通常因为直接使用预定义的实现 image.RGBA 和 image.RGBAModel 而被忽视了。这些接口和类型由 image/color 包定义。 123456789101112package mainimport ( \"fmt\" \"image\")func main() { m := image.NewRGBA(image.Rect(0, 0, 100, 100)) fmt.Println(m.Bounds()) fmt.Println(m.At(0, 0).RGBA())} 练习：图像还记得之前编写的图片生成器吗？我们再来编写另外一个，不过这次它将会返回一个 image.Image 的实现而非一个数据切片。 定义你自己的 Image 类型，实现必要的方法并调用 pic.ShowImage。 Bounds 应当返回一个 image.Rectangle ，例如 image.Rect(0, 0, w, h)。 ColorModel 应当返回 color.RGBAModel。 At 应当返回一个颜色。上一个图片生成器的值 v 对应于此次的 color.RGBA{v, v, 255, 255}。 12345678910package mainimport \"golang.org/x/tour/pic\"type Image struct{}func main() { m := Image{} pic.ShowImage(m)} 并发goroutineGo 程（goroutine）是由 Go 运行时管理的轻量级线程。go f(x, y, z)会启动一个新的 goroutine并执行f(x, y, z)。f, x, y 和 z 的求值发生在当前的 Go 程中，而f的执行发生在新的 Go 程中。Go 程在相同的地址空间中运行，因此在访问共享的内存时必须进行同步。sync 包提供了这种能力，不过在 Go 中并不经常用到，因为还有其它的办法（见下一页）。 12345678910111213141516package main import ( \"fmt\" \"time\")func say(s string) { for i:= 1; i &lt; 5; i++ { time.Sleep(100*time.Millisecond); fmt.Println(s); }}func main() { go say(\"hellod\"); // fmt.Printf(\"address: %p\", say); say(\"easonchen\");} 信道(Channels)信道是带有类型的管道，你可以通过它用信道操作符 &lt;- 来发送或者接收值。 12ch &lt;- v // 将 v 发送至信道 ch。v := &lt;-ch // 从 ch 接收值并赋予 v。 （“箭头”就是数据流的方向。）和映射与切片一样，信道在使用前必须创建：ch := make(chan int)默认情况下，发送和接收操作在另一端准备好之前都会阻塞。这使得 Go 程可以在没有显式的锁或竞态变量的情况下进行同步。以下示例对切片中的数进行求和，将任务分配给两个 Go 程。一旦两个 Go 程完成了它们的计算，它就能算出最终的结果。 12345678910111213141516171819package mainimport ( \"fmt\")func sum(s []int, c chan int) { sum := 0; for _, v := range s { sum += v; } c &lt;- sum;// 将和送入 c}func main() { s := []int{1, 2, 3, 5, 7, 9}; c := make(chan int); go sum(s[:len(s)/2], c); go sum(s[len(s)/2:], c); x, y := &lt;-c, &lt;-c; // 接受者 fmt.Println(x, y, x+y);} 带缓冲的信道信道可以是 带缓冲的。将缓冲长度作为第二个参数提供给 make 来初始化一个带缓冲的信道：ch := make(chan int, 100)仅当信道的缓冲区填满后，向其发送数据时才会阻塞。当缓冲区为空时，接受方会阻塞。修改示例填满缓冲区，然后看看会发生什么。 1234567891011121314package mainimport ( \"fmt\")func main() { ch := make(chan int, 4); ch &lt;- 1; ch &lt;- 2; ch &lt;- 3; ch &lt;- 4; fmt.Println(&lt;-ch); fmt.Println(&lt;-ch);} range 和 close发送者可通过 close 关闭一个信道来表示没有需要发送的值了。接收者可以通过为接收表达式分配第二个参数来测试信道是否被关闭：若没有值可以接收且信道已被关闭，那么在执行完v, ok := &lt;-ch之后 ok 会被设置为 false。循环 for i := range c 会不断从信道接收值，直到它被关闭。Note： 只有发送者才能关闭信道，而接收者不能。向一个已经关闭的信道发送数据会引发程序恐慌（panic）。Another Note： 信道与文件不同，通常情况下无需关闭它们。只有在必须告诉接收者不再有值需要发送的时候才有必要关闭，例如终止一个 range 循环。 12345678910111213141516171819package mainimport ( \"fmt\")func fibonacci(n int, c chan int) { x, y := 0, 1; for i:=0; i&lt;n; i++ { c &lt;- x; x, y = y, x+y; } close(c);}func main() { c := make(chan int, 10); go fibonacci(cap(c), c); for i := range c { fmt.Println(i); }} select 语句select 语句使一个 Go 程可以等待多个通信操作。select 会阻塞到某个分支可以继续执行为止，这时就会执行该分支。当多个分支都准备好时会随机选择一个执行。 123456789101112131415161718192021222324252627package mainimport ( \"fmt\")func fibonacci (c, quit chan int) { x, y := 0, 1; for { select { case c &lt;- x: x, y = y, x+y; case &lt;- quit: fmt.Println(\"quit\"); return; } }}func main() { c := make(chan int); quit := make(chan int); go func() { for i := 0; i &lt; 10; i++ { fmt.Println(&lt;-c); } quit &lt;- 0; }(); fibonacci(c, quit);} 默认选择当 select 中的其它分支都没有准备好时，default 分支就会执行。为了在尝试发送或者接收时不发生阻塞，可使用 default 分支： 123456select {case i := &lt;-c: // 使用 idefault: // 从 c 中接收会阻塞时执行} 123456789101112131415161718192021package mainimport ( \"fmt\" \"time\")func main() { tick := time.Tick(100*time.Millisecond); boom := time.After(500*time.Millisecond); for { select { case &lt;- tick: fmt.Println(\"tick\"); case &lt;- boom: fmt.Println(\"boom\"); return; default: fmt.Println(\" .\") time.Sleep(50*time.Millisecond); } }} 练习：等价二叉查找树 实现 Walk 函数。 测试 Walk 函数。 函数 tree.New(k) 用于构造一个随机结构的已排序二叉查找树，它保存了值 k, 2k, 3k, …, 10k。 创建一个新的信道 ch 并且对其进行步进： go Walk(tree.New(1), ch)然后从信道中读取并打印 10 个值。应当是数字 1, 2, 3, …, 10。 用 Walk 实现 Same 函数来检测 t1 和 t2 是否存储了相同的值。 测试 Same 函数。 Same(tree.New(1), tree.New(1)) 应当返回 true，而 Same(tree.New(1), tree.New(2)) 应当返回 false。 12345678package mainimport \"golang.org/x/tour/tree\"// Walk 步进 tree t 将所有的值从 tree 发送到 channel ch。func Walk(t *tree.Tree, ch chan int)// Same 检测树 t1 和 t2 是否含有相同的值。func Same(t1, t2 *tree.Tree) boolfunc main() {} sync.Mutex我们已经看到信道非常适合在各个 Go 程间进行通信。 但是如果我们并不需要通信呢？比如说，若我们只是想保证每次只有一个 Go 程能够访问一个共享的变量，从而避免冲突？ 这里涉及的概念叫做_互斥（mutual_exclusion）_ ，我们通常使用 _互斥锁（Mutex）_ 这一数据结构来提供这种机制。 Go 标准库中提供了 sync.Mutex 互斥锁类型及其两个方法： LockUnlock我们可以通过在代码前调用 Lock 方法，在代码后调用 Unlock 方法来保证一段代码的互斥执行。参见 Inc 方法。 我们也可以用 defer 语句来保证互斥锁一定会被解锁。参见 Value 方法。 123456789101112131415161718192021222324252627282930313233package mainimport ( \"fmt\" \"sync\" \"time\")// SafeCounter 的并发使用是安全的type SafeCounter struct { v map[string]int mux sync.Mutex}// Inc增加给定key的计数器的值func (c *SafeCounter) Inc(key string) { c.mux.Lock(); // Lock之后同一时刻只有一个goroutine能访问c.v c.v[key]++; c.mux.Unlock();}// Value返回给定key的计数器的当前值func (c *SafeCounter) Value(key string) int { c.mux.Lock() // Lock之后同一时刻只有一个goroutine能访问c.v defer c.mux.Unlock(); return c.v[key];}func main() { c := SafeCounter{v: make(map[string]int)}; for i :=0; i&lt;1000; i++ { go c.Inc(\"Somekey\"); } time.Sleep(time.Second); fmt.Println(c.Value(\"Somekey\"));} ###练习：Web 爬虫在这个练习中，我们将会使用 Go 的并发特性来并行化一个 Web 爬虫。修改 Crawl 函数来并行地抓取 URL，并且保证不重复。提示：你可以用一个 map 来缓存已经获取的 URL，但是要注意 map 本身并不是并发安全的！ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package mainimport ( \"fmt\")type Fetcher interface { // Fetch 返回 URL 的 body 内容，并且将在这个页面上找到的 URL 放到一个 slice 中。 Fetch(url string) (body string, urls []string, err error)}// Crawl 使用 fetcher 从某个 URL 开始递归的爬取页面，直到达到最大深度。func Crawl(url string, depth int, fetcher Fetcher) { // TODO: 并行的抓取 URL。 // TODO: 不重复抓取页面。 // 下面并没有实现上面两种情况： if depth &lt;= 0 { return } body, urls, err := fetcher.Fetch(url) if err != nil { fmt.Println(err) return } fmt.Printf(\"found: %s %q\\n\", url, body) for _, u := range urls { Crawl(u, depth-1, fetcher) } return}func main() { Crawl(\"https://golang.org/\", 4, fetcher)}// fakeFetcher 是返回若干结果的 Fetcher。type fakeFetcher map[string]*fakeResulttype fakeResult struct { body string urls []string}func (f fakeFetcher) Fetch(url string) (string, []string, error) { if res, ok := f[url]; ok { return res.body, res.urls, nil } return \"\", nil, fmt.Errorf(\"not found: %s\", url)}// fetcher 是填充后的 fakeFetcher。var fetcher = fakeFetcher{ \"https://golang.org/\": &amp;fakeResult{ \"The Go Programming Language\", []string{ \"https://golang.org/pkg/\", \"https://golang.org/cmd/\", }, }, \"https://golang.org/pkg/\": &amp;fakeResult{ \"Packages\", []string{ \"https://golang.org/\", \"https://golang.org/cmd/\", \"https://golang.org/pkg/fmt/\", \"https://golang.org/pkg/os/\", }, }, \"https://golang.org/pkg/fmt/\": &amp;fakeResult{ \"Package fmt\", []string{ \"https://golang.org/\", \"https://golang.org/pkg/\", }, }, \"https://golang.org/pkg/os/\": &amp;fakeResult{ \"Package os\", []string{ \"https://golang.org/\", \"https://golang.org/pkg/\", }, },}","link":"/2018/09/28/tutorialGO/"},{"title":"区块链技术学习指引","text":"1. 引言给迷失在如何学习区块链技术的同学一个指引，区块链技术是随比特币诞生，因此要搞明白区块链技术，应该先了解下比特币。但区块链技术不单应用于比特币，还有非常多的现实应用场景，想做区块链应用开发=&gt;可进一步阅读以太坊系列。教程地址：https://learnblockchain.cn/2018/01/11/guide/#more 比特币如果你是还不知比特币是什么，那就看看比特币总结： 一种基于分布式网络的数字货币；需要网络节点共识 2. 基础入门接下来可以通过下面这几篇文章了解比特币大概的运行原理： 区块链运行原理 通过这篇可以了解到区块链是一个怎样的结构 比特币所有权和隐私问题 通过这篇可以了解到地址私钥 非对称加密应用 等概念 比特币如何挖矿 通过这篇了解工作量证明 比特币如何达成共识 - 最长链的选择 通过这篇可以了解共识机制。 补充阅读什么是拜占庭将军问题 3. 进阶在基础入门之后，可以进一步阅读以下几篇，理解分布式网络，交易验证。 分析比特币网络:一种去中心化、点对点的网络架构 比特币区块结构Merkle树及简单支付验证分析 比特币脚本及交易分析-智能合约的雏形看完上面这些，区块链应该理解差不多了，就可以尝试实现一个简单的区块链了。参考这篇用Python从零开始创建区块链。 4. 以太坊一个技术要落地还得靠应用， 以太坊就这样一个建立在区块链技术之上，去中心化的应用平台。可以阅读几下几篇，这部分以开发为主，需要多花时间实践。 以太坊开发入门 智能合约开发环境搭建及Hello World合约 搭建智能合约开发环境Remix IDE及使用 以太坊客户端Geth命令用法-参数详解 Geth控制台使用实战及Web3.js使用 如何搭建以太坊私有链 智能合约及应用开发 程序员如何切入区块链去中心化应用开发 一步步教你开发、部署第一个Dapp应用 一步步教你创建自己的数字货币（代币）进行ICO 实现一个可管理、增发、兑换、冻结等高级功能的代币 如何通过以太坊智能合约来进行众筹（ICO） 剖析非同质化代币ERC721–全面解析ERC721标准 Web3与智能合约交互实战 Web3监听合约事件 如何编写一个可升级的智能合约 美链BEC合约漏洞技术分析","link":"/2018/11/15/区块链技术学习指引/"},{"title":"动态规划-爬楼梯","text":"题目https://leetcode-cn.com/problems/climbing-stairs 动态规划 解题动态规划不难发现，这个问题可以被分解为一些包含最优子结构的子问题，即它的最优解可以从其子问题的最优解来有效地构建，我们可以使用动态规划来解决这一问题。 第 i 阶可以由以下两种方法得到： 在第 (i-1)阶后向上爬一阶。 在第 (i-2) 阶后向上爬 2 阶。 所以到达第 i阶的方法总数就是到第 (i-1)阶和第 (i-2)阶的方法数之和。 dp[i] = dp[i-1]+dp[i-2]; 1234567891011121314class Solution { public int climbStairs(int n) { // int[] dp = new int[n]; dp[0] = 1; dp[1] = 2; for(int i=2; i&lt;n; i++) { dp[i] = dp[i-1] + dp[i-2]; } return dp[n-1]; }} 记忆化递归12345678910111213141516171819public class Solution { public int climbStairs(int n) { int memo[] = new int[n + 1]; return climb_Stairs(0, n, memo); } public int climb_Stairs(int i, int n, int memo[]) { if (i &gt; n) { return 0; } if (i == n) { return 1; } if (memo[i] &gt; 0) { return memo[i]; } memo[i] = climb_Stairs(i + 1, n, memo) + climb_Stairs(i + 2, n, memo); return memo[i]; }} https://leetcode-cn.com/problems/qing-wa-tiao-tai-jie-wen-ti-lcof/ 12345678910111213141516171819class Solution { public int numWays(int n) { if (n == 0) { return 1; } if (n &lt;3) { return n; } int[] dp = new int[n]; dp[0] = 1; dp[1] = 2; int mod = (int)1e9+7; for(int i=2; i&lt;n; i++) { dp[i] = (dp[i-1] + dp[i-2]) % mod; } return dp[n-1]; }} 本质上 寻找 前后项的关系， 从上到下 递归分解 ！！ 从下到上 状态方程","link":"/2020/06/08/动态规划-爬楼梯/"},{"title":"大数据学习之Spark","text":"简介基于内存计算的大数据并行计算框架;一个可用于大规模数据快速处理的快速、通用引擎。为Apache分布式计算三大框架[hadoop, Spark, Storm]之一。Spark目的：使数据分析更快，不仅运行速度快，也要能快速、容易地编写程序。为了程序更快，Spark提供了内存运算，减少了迭代计算时的IO开销。由于Hadoop中MapReduce存在诸多缺陷，Spark可以解决。 特点 运算速度快：使用先进的DAG（Directed Acyclic Graph，有向无环图）执行引擎，以支持循环数据流与内存计算，基于内存的执行速度可比Hadoop MapReduce快上百倍，基于磁盘的执行速度也能快十倍； 容易使用：Spark支持使用Scala、Java、Python和R语言进行编程，简洁的API设计有助于用户轻松构建并行程序，并且可以通过Spark Shell进行交互式编程； 通用性：Spark提供了完整而强大的技术栈，包括SQL查询、流式计算、机器学习和图算法组件，这些组件可以无缝整合在同一个应用中，足以应对复杂的计算； 运行模式多样：Spark可运行于独立的集群模式中，或者运行于Hadoop中，也可运行于Amazon EC2等云环境中，并且可以访问HDFS、Cassandra、HBase、Hive等多种数据源。优势HADOOP problem: hadoop 本身诸多缺陷，最主要：MapReduce计算模型延迟过高，无法胜任实时、快速计算的需求，只适用离线批处理； 表达能力有限：所有计算必须转成Map和Reduce； 磁盘IO开销大：每次执行都需从磁盘读取数据，计算完成后需要将中间结果写入磁盘 延迟高：一次计算可能需要分解成一系列按顺序执行的MapReduce，任务之间的衔接由于涉及到IO开销，延迟高；必须等到上一个任务完成才能执行下一个任务Spark 优势: Spark计算也是MapReduce，但不局限与Map和Reduce，还提供了多种数据集操作类型，编程模型比MapReduce更灵活 提供了内存计算，中间结果直接存内存 基于DAG的任务调度执行机制，要优于MapReduce的迭代执行机制最大的特点：将计算数据，中间结果都存储在内存中，减少IO开销。MapReduce要写不少底层代码；Spark封装了API。Spark主要用了替代Hadoop中MapReduce计算模型。 生态系统实际应用中，大数据处理主要包括： 复杂的批量数据处理：时间跨度长在数十分钟到数小时之间； 基于历史数据的交互式查询：时间跨度在数十秒到数分钟之间； 基于实时数据流的数据处理：时间跨度通常在数百毫秒到数秒之间。可以利用Hadoop MapReduce来进行批量数据处理，可以用Impala来进行交互式查询（Impala与Hive相似，但底层引擎不同，提供了实时交互式SQL查询），对于流式数据处理可以采用开源流计算框架Storm。多个软件难以统一；Spark的设计遵循：一个软件栈满足不同应用场景，既能提供内存计算框架，也可支持SQL及时查询，实时流式计算，机器学习和图计算。Spark可以部署在资源管理器YARN之上，提供一站式的大数据解决方案。因此，Spark所提供的生态系统足以应对上述三种场景，即同时支持批处理、交互式查询和流数据处理。 Spark的生态系统: Spark Core、Spark SQL、Spark Streaming、MLLib和GraphX。 Spark Core：Spark Core包含Spark的基本功能，如内存计算、任务调度、部署模式、故障恢复、存储管理等。Spark建立在统一的抽象RDD之上，使其可以以基本一致的方式应对不同的大数据处理场景；通常所说的Apache Spark，就是指Spark Core； Spark SQL：Spark SQL允许开发人员直接处理RDD，同时也可处理HIVE，HBase等外部数据。Spark SQL特点：能够统一处理关系表和RDD。可使用SQL命令进行查询。 Spark Streaming：Spark Streaming支出高吞吐量、可容错的实时流数据处理：核心是将流式计算分解成一系列短小的批处理作业。 MLlib（机器学习）:MLlib提供； 常用的机器学习算法实现，聚类，分类。回归、协同过滤。 Graphx(图计算): Graphx是Spark用于图计算的API。运行架构基本概念 RDD:是弹性分布式数据集(Resilient Distributed Dataset)，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型 DAG: Distributed Acyclic Graph（有项无环图）,反映了RDD之间的依赖关系 Executor: 运行在工作节点(Worker Node)上的一个进程，负责运行任务，并为应用程序存储数据。 应用：用户编写的Spark应用程序 任务: 运行在Executor上的工作单元 作业：一个作业包含多个RDD及作用于相应RDD上的各种操作； 阶段：是作业的基本调度单位，一个作业分为多组任务，每组任务被称为“阶段”,也是”任务集”[应用[作业[阶段[任务…]…]…]] 架构设计如图9-5所示，Spark运行架构包括集群资源管理器（Cluster Manager）、运行作业任务的工作节点（Worker Node）、每个应用的任务控制节点（Driver）和每个工作节点上负责具体任务的执行进程（Executor）。其中，集群资源管理器可以是Spark自带的资源管理器，也可以是YARN或Mesos等资源管理框架。与Hadoop MapReduce计算框架相比，Spark所采用的Executor有两个优点：一是利用多线程来执行具体的任务（Hadoop MapReduce采用的是进程模型），减少任务的启动开销；二是Executor中有一个BlockManager存储模块，会将内存和磁盘共同作为存储设备，当需要多轮迭代计算时，可以将中间结果存储到这个存储模块里，下次需要时，就可以直接读该存储模块里的数据，而不需要读写到HDFS等文件系统里，因而有效减少了IO开销；或者在交互式查询场景下，预先将表缓存到该存储系统上，从而可以提高读写IO性能。如下图: 总体而言，如下图所示，在Spark中，一个应用（Application）由一个任务控制节点（Driver）和若干个作业（Job）构成，一个作业由多个阶段（Stage）构成，一个阶段由多个任务（Task）组成。当执行一个应用时，任务控制节点会向集群管理器（Cluster Manager）申请资源，启动Executor，并向Executor发送应用程序代码和文件，然后在Executor上执行任务，运行结束后，执行结果会返回给任务控制节点，或者写到HDFS或者其他数据库中。 Spark运行基本流程如下图所示，Spark的基本流程：（1）当一个Spark应用被提交时，首先需要为这个应用构建起基本的运行环境，即由任务控制节点（Driver）创建一个SparkContext，由SparkContext负责和资源管理器（Cluster Manager）的通信以及进行资源的申请、任务的分配和监控等。SparkContext会向资源管理器注册并申请运行Executor的资源；（2）资源管理器为Executor分配资源，并启动Executor进程，Executor运行情况将随着“心跳”发送到资源管理器上；（3）SparkContext根据RDD的依赖关系构建DAG图，DAG图提交给DAG调度器（DAGScheduler）进行解析，将DAG图分解成多个“阶段”（每个阶段都是一个任务集），并且计算出各个阶段之间的依赖关系，然后把一个个“任务集”提交给底层的任务调度器（TaskScheduler）进行处理；Executor向SparkContext申请任务，任务调度器将任务分发给Executor运行，同时，SparkContext将应用程序代码发放给Executor；（4）任务在Executor上运行，把执行结果反馈给任务调度器，然后反馈给DAG调度器，运行完毕后写入数据并释放所有资源。 总体而言，Spark运行架构具有以下特点：（1）每个应用都有自己专属的Executor进程，并且该进程在应用运行期间一直驻留。Executor进程以多线程的方式运行任务，减少了多进程任务频繁的启动开销，使得任务执行变得非常高效和可靠；（2）Spark运行过程与资源管理器无关，只要能够获取Executor进程并保持通信即可；（3）Executor上有一个BlockManager存储模块，类似于键值存储系统（把内存和磁盘共同作为存储设备），在处理迭代计算任务时，不需要把中间结果写入到HDFS等文件系统，而是直接放在这个存储系统上，后续有需要时就可以直接读取；在交互式查询场景下，也可以把表提前缓存到这个存储系统上，提高读写IO性能；（4）任务采用了数据本地性和推测执行等优化机制。数据本地性是尽量将计算移到数据所在的节点上进行，即“计算向数据靠拢”，因为移动计算比移动数据所占的网络资源要少得多。而且，Spark采用了延时调度机制，可以在更大的程度上实现执行过程优化。比如，拥有数据的节点当前正被其他的任务占用，那么，在这种情况下是否需要将数据移动到其他的空闲节点呢？答案是不一定。因为，如果经过预测发现当前节点结束当前任务的时间要比移动数据的时间还要少，那么，调度就会等待，直到当前节点可用。","link":"/2018/11/14/大数据学习之Spark/"},{"title":"pytorch学习2","text":"卷积神经网络 CNN (Convolutional Neural Network) 卷积 和 神经网络卷积神经网络是如何运作的吧, 举一个识别图片的例子, 我们知道神经网络是由一连串的神经层组成,每一层神经层里面有存在有很多的神经元. 这些神经元就是神经网络识别事物的关键. 每一种神经网络都会有输入输出值, 当输入值是图片的时候, 实际上输入神经网络的并不是那些色彩缤纷的图案,而是一堆堆的数字. 就比如说这个. 当神经网络需要处理这么多输入信息的时候, 也就是卷积神经网络就可以发挥它的优势的时候了. 那什么是卷积神经网络呢? 先把卷积神经网络这个词拆开来看. “卷积” 和 “神经网络”. 卷积也就是说神经网络不再是对每个像素的输入信息做处理了,而是图片上每一小块像素区域进行处理, 这种做法加强了图片信息的连续性. 使得神经网络能看到图形, 而非一个点. 这种做法同时也加深了神经网络对图片的理解. 具体来说, 卷积神经网络有一个批量过滤器, 持续不断的在图片上滚动收集图片里的信息,每一次收集的时候都只是收集一小块像素区域, 然后把收集来的信息进行整理, 这时候整理出来的信息有了一些实际上的呈现, 比如这时的神经网络能看到一些边缘的图片信息, 然后在以同样的步骤, 用类似的批量过滤器扫过产生的这些边缘信息, 神经网络从这些边缘信息里面总结出更高层的信息结构,比如说总结的边缘能够画出眼睛,鼻子等等. 再经过一次过滤, 脸部的信息也从这些眼睛鼻子的信息中被总结出来. 最后我们再把这些信息套入几层普通的全连接神经层进行分类, 这样就能得到输入的图片能被分为哪一类的结果了. 我们截取一段 google 介绍卷积神经网络的视频, 具体说说图片是如何被卷积的. 下面是一张猫的图片, 图片有长, 宽, 高 三个参数. 对! 图片是有高度的! 这里的高指的是计算机用于产生颜色使用的信息. 如果是黑白照片的话, 高的单位就只有1, 如果是彩色照片, 就可能有红绿蓝三种颜色的信息, 这时的高度为3. 我们以彩色照片为例子. 过滤器就是影像中不断移动的东西, 他不断在图片收集小批小批的像素块, 收集完所有信息后, 输出的值, 我们可以理解成是一个高度更高,长和宽更小的”图片”. 这个图片里就能包含一些边缘信息. 然后以同样的步骤再进行多次卷积, 将图片的长宽再压缩, 高度再增加, 就有了对输入图片更深的理解. 将压缩,增高的信息嵌套在普通的分类神经层上,我们就能对这种图片进行分类了. 池化(pooling) 研究发现, 在每一次卷积的时候, 神经层可能会无意地丢失一些信息. 这时, 池化 (pooling) 就可以很好地解决这一问题. 而且池化是一个筛选过滤的过程, 能将 layer 中有用的信息筛选出来, 给下一个层分析. 同时也减轻了神经网络的计算负担 (具体细节参考). 也就是说在卷集的时候, 我们不压缩长宽, 尽量地保留更多信息, 压缩的工作就交给池化了,这样的一项附加工作能够很有效的提高准确性. 有了这些技术,我们就可以搭建一个属于我们自己的卷积神经网络啦. 流行的 CNN 结构 比较流行的一种搭建结构是这样, 从下到上的顺序, 首先是输入的图片(image), 经过一层卷积层 (convolution), 然后在用池化(pooling)方式处理卷积的信息, 这里使用的是 max pooling 的方式. 然后在经过一次同样的处理, 把得到的第二次处理的信息传入两层全连接的神经层 (fully connected),这也是一般的两层神经网络层,最后在接上一个分类器(classifier)进行分类预测. CNN 卷积神经网络1. MNIST手写数据123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import torchimport torch.nn as nnimport torch.utils.data as Dataimport torchvision # 数据库 模块 下载 处理数据import matplotlib.pyplot as plttorch.manual_seed(1)# Hyper parametersEPOCH = 1 # 训练整批数据的多少次，BATCH_ZISE = 50LR = 0.001DOWNLOAD_MNIST = True # # Mnist 手写数字# 1. use torchvision download mnist datasettrain_data = torchvision.datasets.MNIST( root='./mnist/' # 保存位置 train=True, # this is training data transform=torchvision.transforms.ToTensor(), # 转换PIL.Image or numpy.ndarray 或 torch.FloaterTensor (C x H x W), 训练的时候 normalize成[0.0, 1.0]区间 download=DOWNLOAD_MNIST,)# 2. package data with dataloader # 批训练 50 example, 1 channel, 28*28 (50, 1, 28, 28)train_loader = Data.Dataloader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)#plot one example print(train_data.train_data.size()) # torch.Size([60000, 28, 28])print(train_data.data.size()) # torch.Size([60000, 28, 28])print(train_loader.dataset.data.size()) # (6000, 28, 28)print(train_data.train_labels.size()) # (60000,)print(train_data.targets.size())print(train_loader.dataset.targets.size())plt.imshow(train_data.train_data[0].numpy(), cmap='gray')plt.title(\"%i\" % train_data.train_data.labels[0])plt.show()plt.imshow(train_data.data[0].numpy(), cmap='gray')plt.title(\"%i\" % train_data.targets[0])plt.show()# 同样, 我们除了训练数据, 还给一些测试数据, 测试看看它有没有训练好.test_data = torchvision.datasets.MNIST(root='./mnist', train=False)# 为了节约时间, 我们测试时只测试前2000个# shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)test_x = torch.unsqueeze(test_data.test_data, dim=1).type(torch.FloatTensor)[:2000]/255.test_y = test_data.test_labels[:2000]# tensor.nump(); torch.from_numpy()# minst data(numpy or PIL image) -&gt;train_data(tensor) -&gt;train_loader(tensor with batchsize) 2. CNN模型和以前一样, 我们用一个 class 来建立 CNN 模型.这个 CNN 整体流程是 卷积(Conv2d) -&gt; 激励函数(ReLU) -&gt; 池化, 向下采样 (MaxPooling) -&gt; 再来一遍 -&gt; 展平多维的卷积成的特征图 -&gt; 接入全连接层 (Linear) -&gt; 输出 12345678910111213141516171819202122232425262728293031323334353637383940414243class CNN(nn.Module)： def __init__(self): super(CNN, self).__init__() self.conv1 = nn.Sequential( # input shape (1, 28, 28) nn.Conv2d( in_channels=1, # input height out_channels=16, # n_filters kernel_size = 5, # filter size stride=1, # filter movement/step padding=2, # 如想conv2d出来的图片长宽没有变化，padding=(kernel_size-1)/2 当 stride=1 )， # out shape (16, 28, 28) nn.ReLU(), # activation nn.MaxPool2d(2), # output (16, 14, 14) ) self.conv2 = nn.Sequential( # input shape (16, 14, 14) nn.Conv2d(16, 32, 5, 1, 2), nn.ReLU(), # activation nn.MaxPool2d(2) # output shape (32, 7, 7) ) self.out = nn.Linear(32*7*7, 10) # fully connected layer, output 10 classes def forward(self, x): x = self.conv1(x) x = self.conv2(x) x = x.view(x.size(0), -1) # 展平多维平面图 成 (batch_size, 32*7*7) output = self.out(x) return output, xcnn = CNN()print(cnn) # net architecture\"\"\"CNN ( (conv1): Sequential ( (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (1): ReLU () (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1)) ) (conv2): Sequential ( (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (1): ReLU () (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1)) ) (out): Linear (1568 -&gt; 10))\"\"\" 3. 训练下面我们开始训练, 将 x y 都用 Variable 包起来, 然后放入 cnn 中计算 output, 最后再计算误差. 下面代码省略了计算精确度 accuracy 的部分, 如果想细看 accuracy 代码的同学, 请去往我的 github 看全部代码. 1234567891011121314151617181920212223242526272829303132optimizer = torch.optim.Adam(cnn.parameters(), lr=LR) # optimize all cnn parametersloss_func = nn.CrossEntropyLoss() # the target label is not one-hotted# training and testingfor epoch in range(EPOCH): for step, (b_x, b_y) in enumerate(train_loader): # 分配batch data, normalize x when iterate train_loader output = cnn(b_x)[0] loss = loss_func(output, b_y) optimizer.zero_grad() loss.backward() optimizer.step() if step % 100 == 0: test_output, last_layer = cnn(test_x) pred_y = torch.max(test_output, 1)[1].data.squeeze() accuracy = (pred_y == test_y).sum().item() / float(test_y.size(0)) print('Epoch: ', epoch, '| train loss: %.4f' % loss.data, '| test accuracy: %.2f' % accuracy)\"\"\"...Epoch: 0 | train loss: 0.0306 | test accuracy: 0.97Epoch: 0 | train loss: 0.0147 | test accuracy: 0.98Epoch: 0 | train loss: 0.0427 | test accuracy: 0.98Epoch: 0 | train loss: 0.0078 | test accuracy: 0.98\"\"\"# print 10 predictions from test datatest_output, _ = cnn(test_x[:10])pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()print(pred_y, 'prediction number')print(test_y[:10].numpy(), 'real number')'''[5 0 4 1 9 2 1 3 1 4] prediction number[5 0 4 1 9 2 1 3 1 4] real number''' 什么是循环神经网络 RNN (Recurrent Neural Network)在语言分析, 序列化数据中穿梭自如的循环神经网络 RNN RNN 的用途只想着斯蒂芬乔布斯这个名字 , 请你再把他逆序念出来. 斯布乔(*#&amp;, 有点难吧. 这就说明, 对于预测, 顺序排列是多么重要. 我们可以预测下一个按照一定顺序排列的字, 但是打乱顺序, 我们就没办法分析自己到底在说什么了. 序列数据 我们想象现在有一组序列数据 data 0,1,2,3. 在当预测 result0 的时候,我们基于的是 data0, 同样在预测其他数据的时候, 我们也都只单单基于单个的数据. 每次使用的神经网络都是同一个 NN. 不过这些数据是有关联 顺序的 , 就像在厨房做菜, 酱料 A要比酱料 B 早放, 不然就串味了. 所以普通的神经网络结构并不能让 NN 了解这些数据之间的关联. 处理序列数据的神经网络 那我们如何让数据间的关联也被 NN 加以分析呢? 想想我们人类是怎么分析各种事物的关联吧, 最基本的方式,就是记住之前发生的事情. 那我们让神经网络也具备这种记住之前发生的事的能力. 再分析 Data0 的时候, 我们把分析结果存入记忆. 然后当分析 data1的时候, NN会产生新的记忆, 但是新记忆和老记忆是没有联系的. 我们就简单的把老记忆调用过来, 一起分析. 如果继续分析更多的有序数据 , RNN就会把之前的记忆都累积起来, 一起分析. 我们再重复一遍刚才的流程, 不过这次是以加入一些数学方面的东西. 每次 RNN 运算完之后都会产生一个对于当前状态的描述 , state. 我们用简写 S(t) 代替, 然后这个 RNN开始分析 x(t+1) , 他会根据 x(t+1)产生s(t+1), 不过此时 y(t+1) 是由 s(t) 和 s(t+1) 共同创造的. 所以我们通常看到的 RNN 也可以表达成这种样子. RNN 的应用RNN 的形式不单单这有这样一种, 他的结构形式很自由. 如果用于分类问题, 比如说一个人说了一句话, 这句话带的感情色彩是积极的还是消极的. 那我们就可以用只有最后一个时间点输出判断结果的RNN. 又或者这是图片描述 RNN, 我们只需要一个 X 来代替输入的图片, 然后生成对图片描述的一段话. 或者是语言翻译的 RNN, 给出一段英文, 然后再翻译成中文. 什么是 LSTM 循环神经网络RNN 的弊端 之前我们说过, RNN 是在有顺序的数据上进行学习的. 为了记住这些数据, RNN 会像人一样产生对先前发生事件的记忆. 不过一般形式的 RNN 就像一个老爷爷, 有时候比较健忘. 为什么会这样呢? 想像现在有这样一个 RNN, 他的输入值是一句话: ‘我今天要做红烧排骨, 首先要准备排骨, 然后…., 最后美味的一道菜就出锅了’. 现在请 RNN 来分析, 我今天做的到底是什么菜呢. RNN可能会给出“辣子鸡”这个答案. 由于判断失误, RNN就要开始学习 这个长序列 X 和 ‘红烧排骨’ 的关系 , 而RNN需要的关键信息 ”红烧排骨”却出现在句子开头, 再来看看 RNN是怎样学习的吧. 红烧排骨这个信息原的记忆要进过长途跋涉才能抵达最后一个时间点. 然后我们得到误差, 而且在 反向传递 得到的误差的时候, 他在每一步都会 乘以一个自己的参数 W. 如果这个 W 是一个小于1 的数, 比如0.9. 这个0.9 不断乘以误差, 误差传到初始时间点也会是一个接近于零的数, 所以对于初始时刻, 误差相当于就消失了. 我们把这个问题叫做梯度消失或者梯度弥散 Gradient vanishing. 反之如果 W 是一个大于1 的数, 比如1.1 不断累乘, 则到最后变成了无穷大的数, RNN被这无穷大的数撑死了, 这种情况我们叫做梯度爆炸, Gradient exploding. 这就是普通 RNN 没有办法回忆起久远记忆的原因. LSTM LSTM 就是为了解决这个问题而诞生的. LSTM 和普通 RNN 相比, 多出了三个控制器. (输入控制, 输出控制, 忘记控制). 现在, LSTM RNN 内部的情况是这样. 他多了一个 控制全局的记忆, 我们用粗线代替. 为了方便理解, 我们把粗线想象成电影或游戏当中的 主线剧情. 而原本的 RNN 体系就是 分线剧情. 三个控制器都是在原始的 RNN 体系上, 我们先看 输入方面 , 如果此时的分线剧情对于剧终结果十分重要, 输入控制就会将这个分线剧情按重要程度 写入主线剧情 进行分析. 再看 忘记方面, 如果此时的分线剧情更改了我们对之前剧情的想法, 那么忘记控制就会将之前的某些主线剧情忘记, 按比例替换成现在的新剧情. 所以 主线剧情的更新就取决于输入 和忘记 控制. 最后的输出方面, 输出控制会基于目前的主线剧情和分线剧情判断要输出的到底是什么.基于这些控制机制, LSTM 就像延缓记忆衰退的良药, 可以带来更好的结果. 什么是自编码 (Autoencoder)自编码 autoencoder 是一种什么码呢. 他是不是 条形码? 二维码? 打码? 其中的一种呢? NONONONO. 和他们统统没有关系. 自编码是一种神经网络的形式.如果你一定要把他们扯上关系, 我想也只能这样解释啦. 压缩与解压 有一个神经网络, 它在做的事情是 接收一张图片, 然后 给它打码, 最后 再从打码后的图片中还原. 太抽象啦? 行, 我们再具体点. 假设刚刚那个神经网络是这样, 对应上刚刚的图片,看出图片其实是经过了压缩,再解压的这一道工序. 当压缩的时候, 原有的图片质量被缩减, 解压时用信息量小却包含了所有关键信息的文件恢复出原本的图片. 为什么要这样做呢? 原来有时神经网络要接受大量的输入信息, 比如输入信息是高清图片时, 输入信息量可能达到上千万, 让神经网络直接从上千万个信息源中学习是一件很吃力的工作. 所以, 何不压缩一下, 提取出原图片中的最具代表性的信息, 缩减输入信息量, 再把缩减过后的信息放进神经网络学习. 这样学习起来就简单轻松了. 所以, 自编码就能在这时发挥作用. 通过将原数据白色的X 压缩, 解压 成黑色的X, 然后通过对比黑白 X ,求出预测误差, 进行反向传递, 逐步提升自编码的准确性. 训练好的自编码中间这一部分就是能总结原数据的精髓. 可以看出, 从头到尾, 我们只用到了输入数据 X, 并没有用到 X 对应的数据标签, 所以也可以说自编码是一种非监督学习. 到了真正使用自编码的时候. 通常只会用到自编码前半部分. 编码器 Encoder 这 部分也叫作 encoder 编码器. 编码器能得到原数据的精髓, 然后我们只需要再创建一个小的神经网络学习这个精髓的数据,不仅减少了神经网络的负担, 而且同样能达到很好的效果. 这是一个通过自编码整理出来的数据, 他能从原数据中总结出每种类型数据的特征, 如果把这些特征类型都放在一张二维的图片上, 每种类型都已经被很好的用原数据的精髓区分开来. 如果你了解 PCA 主成分分析, 再提取主要特征时, 自编码和它一样,甚至超越了 PCA. 换句话说, 自编码 可以像 PCA 一样 给特征属性降维. 解码器 Decoder至于解码器 Decoder, 我们也能那它来做点事情. 我们知道, 解码器在训练的时候是要将精髓信息解压成原始信息, 那么这就提供了一个解压器的作用, 甚至我们可以认为是一个生成器 (类似于GAN). 那做这件事的一种特殊自编码叫做 variational autoencoders, 你能在这里找到他的具体说明. 有一个例子就是让它能模仿并生成手写数字. AutoEncoder (自编码/非监督学习)神经网络也能进行非监督学习, 只需要训练数据, 不需要标签数据. 自编码就是这样一种形式. 自编码能自动分类数据, 而且也能嵌套在半监督学习的上面, 用少量的有标签样本和大量的无标签样本学习. 如果对自编码还没有太多概念, 强烈推荐我的这个动画短片, 让你秒懂自编码. 这次我们还用 MNIST 手写数字数据来压缩再解压图片. 然后用压缩的特征进行非监督分类. 1. 训练数据自编码只用训练集就好了, 而且只需要训练 training data 的 image, 不用训练 labels. 1234567891011121314151617181920212223import torch import torch.nn as nnimport torch.utils.data as Dataimport torchvisionfrom matplotlib import cmimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dimport numpy as np# hyper parameterEPOCH = 10BATCH_SIZE = 64LR = 0.005DOWNLOAD_MNIST = TRUE N_NEST_IMG = 5 # 显示5张图片# 下载数据train_data = torchvision.dataset.MNIST( root=\"./mnist\", train=True, # training data transform=torchvision.transforms.ToTensor(), # convert a PIL.Image or numpy.ndarray to torch.FloatTensor() of shape(C, H, W) and normalize in the range [0.0, 1.0] download=DOWNLOAD_MNIST,)# 转换train_loader = Data.Dataloader(dataset=train_data, batchsize=BATCH_SIZE,shuffle=True) 这就是一张我们要训练的手写数字 4. 2. AutoEncoderAutoEncoder 形式很简单, 分别是 encoder 和 decoder, 压缩和解压, 压缩后得到压缩的特征值, 再从压缩的特征值解压成原图片. 1234567891011121314151617181920212223242526272829class AutoEncoder(nn.Module): def __init__(self): super(AutoEncoder, self).__init__() # 压缩 self.encoder = nn.Sequential( nn.Linear(28*28, 128) nn.Tanh(), nn.Linear(128, 64), nn.Tanh(), nn.Linear(64, 12), nn.Tanh(), nn.Linear(12, 3), # 压缩成3个特征，进行 3D图像可视化 ) # 解压 self.decoder = nn.Sequential( nn.Linear(3, 12), nn.Tanh(), nn.Linear(12, 64), nn.Tanh(), nn.Linear(64, 128), nn.Tanh(), nn.Linear(128, 28*28), nn.sigmoid(), # 激励函数让输出在(0,1) ) def forward(self, x): encoded = self.encoder(x) decoded = self.decoder(encoded) return encoded, decodedautoEncoder = AutoEncoder() 3. 训练训练, 并可视化训练的过程. 我们可以有效的利用 encoder 和 decoder 来做很多事, 比如这里我们用 decoder 的信息输出看和原图片的对比, 还能用 encoder 来看经过压缩后, 神经网络对原图片的理解. encoder 能将不同图片数据大概的分离开来. 这样就是一个无监督学习的过程. 123456789101112optimizer = torch.optim.Adam(autoEncoder.parameters(), lr=LR)loss_func = nn.MSELoss()for epoch in range(EPOCH): for step, (x, b_label) in enumerate(train_loader): b_x = x.view(-1, 28*28) # batch x, shape (batch, 28*28) b_y = x.view(-1, 28*28) # batch x, shape (batch, 28*28) encoded, decoded = autoAncoder(b_x) loss = loss_func(b_y, decoded) # mean square error optimizer.zero_grad() # clear gradients for this training step loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients 画3D图如上；3D 的可视化图挺有趣的, 还能挪动观看, 更加直观, 好理解. 123456789101112131415161718# 要观看的数据view_data = train_data.data[:200].view(-1, 28*28).type(torch.FloatTensor)/255.encoded_data, _ = autoencoder(view_data) # 提取压缩的特征值fig = plt.figure(2)ax = Axes3D(fig) # 3D 图# x, y, z 的数据值X = encoded_data.data[:, 0].numpy()Y = encoded_data.data[:, 1].numpy()Z = encoded_data.data[:, 2].numpy()values = train_data.train_labels[:200].numpy() # 标签值for x, y, z, s in zip(X, Y, Z, values): c = cm.rainbow(int(255*s/9)) # 上色 ax.text(x, y, z, s, backgroundcolor=c) # 标位子 ax.set_xlim(X.min(), X.max())ax.set_ylim(Y.min(), Y.max())ax.set_zlim(Z.min(), Z.max())plt.show() 什么是 强化学习 (Reinforcement Learning)强化学习是机器学习大家族中的一大类, 使用强化学习能够让机器学着如何在环境中拿到高分, 表现出优秀的成绩. 而这些成绩背后却是他所付出的辛苦劳动, 不断的试错, 不断地尝试, 累积经验, 学习经验. 从无到有强化学习是一类算法, 是让计算机实现从一开始什么都不懂, 脑袋里没有一点想法, 通过不断地尝试, 从错误中学习, 最后找到规律, 学会了达到目的的方法. 这就是一个完整的强化学习过程. 实际中的强化学习例子有很多. 比如近期最有名的 Alpha go, 机器头一次在围棋场上战胜人类高手, 让计算机自己学着玩经典游戏 Atari, 这些都是让计算机在不断的尝试中更新自己的行为准则, 从而一步步学会如何下好围棋, 如何操控游戏得到高分. 既然要让计算机自己学, 那计算机通过什么来学习呢? 虚拟老师原来计算机也需要一位虚拟的老师, 这个老师比较吝啬, 他不会告诉你如何移动, 如何做决定, 他为你做的事只有给你的行为打分, 那我们应该以什么形式学习这些现有的资源, 或者说怎么样只从分数中学习到我应该怎样做决定呢? 很简单, 我只需要记住那些高分, 低分对应的行为, 下次用同样的行为拿高分, 并避免低分的行为. 比如老师会根据我的开心程度来打分, 我开心时, 可以得到高分, 我不开心时得到低分. 有了这些被打分的经验, 我就能判断为了拿到高分, 我应该选择一张开心的脸, 避免选到伤心的脸. 这也是强化学习的核心思想. 可以看出在强化学习中, 一种行为的分数是十分重要的. 所以强化学习具有分数导向性. 我们换一个角度来思考.这种分数导向性好比我们在监督学习中的正确标签. 对比监督学习 我们知道监督学习, 是已经有了数据和数据对应的正确标签, 比如这样. 监督学习就能学习出那些脸对应哪种标签. 不过强化学习还要更进一步, 一开始它并没有数据和标签. RL 算法们 强化学习是一个大家族, 他包含了很多种算法, 我们也会一一提到之中一些比较有名的算法, 比如有通过行为的价值来选取特定行为的方法, 包括使用表格学习的 q learning, sarsa, 使用神经网络学习的 deep q network, 还有直接输出行为的 policy gradients, 又或者了解所处的环境, 想象出一个虚拟的环境并从虚拟的环境中学习 等等. 什么是 DQN今天我们会来说说强化学习中的一种强大武器, Deep Q Network 简称为 DQN. Google Deep mind 团队就是靠着这 DQN 使计算机玩电动玩得比我们还厉害. 之前我们所谈论到的强化学习方法都是比较传统的方式, 而如今, 随着机器学习在日常生活中的各种应用, 各种机器学习方法也在融汇, 合并, 升级. 而我们今天所要探讨的强化学习则是这么一种融合了神经网络和 Q learning 的方法, 名字叫做 Deep Q Network. 这种新型结构是为什么被提出来呢? 原来, 传统的表格形式的强化学习有这样一个瓶颈. 我们使用表格来存储每一个状态 state, 和在这个 state 每个行为 action 所拥有的 Q 值. 而当今问题是在太复杂, 状态可以多到比天上的星星还多(比如下围棋). 如果全用表格来存储它们, 恐怕我们的计算机有再大的内存都不够, 而且每次在这么大的表格中搜索对应的状态也是一件很耗时的事. 不过, 在机器学习中, 有一种方法对这种事情很在行, 那就是神经网络. 我们可以将状态和动作当成神经网络的输入, 然后经过神经网络分析后得到动作的 Q 值, 这样我们就没必要在表格中记录 Q 值, 而是直接使用神经网络生成 Q 值. 还有一种形式的是这样, 我们也能只输入状态值, 输出所有的动作值, 然后按照 Q learning 的原则, 直接选择拥有最大值的动作当做下一步要做的动作. 我们可以想象, 神经网络接受外部的信息, 相当于眼睛鼻子耳朵收集信息, 然后通过大脑加工输出每种动作的值, 最后通过强化学习的方式选择动作. 更新神经网络 接下来我们基于第二种神经网络来分析, 我们知道, 神经网络是要被训练才能预测出准确的值. 那在强化学习中, 神经网络是如何被训练的呢? 首先, 我们需要 a1, a2 正确的Q值, 这个 Q 值我们就用之前在 Q learning 中的 Q 现实来代替. 同样我们还需要一个 Q 估计 来实现神经网络的更新. 所以神经网络的的参数就是老的 NN 参数 加学习率 alpha 乘以 Q 现实 和 Q 估计 的差距. 我们整理一下. 我们通过 NN 预测出Q(s2, a1) 和 Q(s2,a2) 的值, 这就是 Q 估计. 然后我们选取 Q 估计中最大值的动作来换取环境中的奖励 reward. 而 Q 现实中也包含从神经网络分析出来的两个 Q 估计值, 不过这个 Q 估计是针对于下一步在 s’ 的估计. 最后再通过刚刚所说的算法更新神经网络中的参数. 但是这并不是 DQN 会玩电动的根本原因. 还有两大因素支撑着 DQN 使得它变得无比强大. 这两大因素就是 Experience replay 和 Fixed Q-targets. DQN 两大利器 简单来说, DQN 有一个记忆库用于学习之前的经历. 在之前的简介影片中提到过, Q learning 是一种 off-policy 离线学习法, 它能学习当前经历着的, 也能学习过去经历过的, 甚至是学习别人的经历. 所以每次 DQN 更新的时候, 我们都可以随机抽取一些之前的经历进行学习. 随机抽取这种做法打乱了经历之间的相关性, 也使得神经网络更新更有效率. Fixed Q-targets 也是一种打乱相关性的机理, 如果使用 fixed Q-targets, 我们就会在 DQN 中使用到两个结构相同但参数不同的神经网络, 预测 Q 估计 的神经网络具备最新的参数, 而预测 Q 现实 的神经网络使用的参数则是很久以前的. 有了这两种提升手段, DQN 才 能在一些游戏中超越人类. DQN模型: 接收外界的变化值；自身的记忆库；学习过程 DQN 强化学习Torch 是神经网络库, 那么也可以拿来做强化学习, 之前我用另一个强大神经网络库 Tensorflow 来制作了这一个 从浅入深强化学习教程, 你同样也可以用 PyTorch 来实现, 这次我们就举 DQN 的例子, 我对比了我的 Tensorflow DQN 的代码, 发现 PyTorch 写的要简单很多.强化学习教程 模块导入和参数设置这次除了 Torch 自家模块, 我们还要导入 Gym 环境库模块 什么是批标准化 (Batch Normalization)普通数据标准化 Batch Normalization, 批标准化, 和普通的数据标准化类似, 是将分散的数据统一的一种做法, 也是优化神经网络的一种方法. 在之前 Normalization 的简介视频中我们一提到, 具有统一规格的数据, 能让机器学习更容易学习到数据之中的规律. 每层都做标准化 在神经网络中, 数据分布对训练会产生影响. 比如某个神经元 x 的值为1, 某个 Weights 的初始值为 0.1, 这样后一层神经元计算结果就是 Wx = 0.1; 又或者 x = 20, 这样 Wx 的结果就为 2. 现在还不能看出什么问题, 但是, 当我们加上一层激励函数, 激活这个 Wx 值的时候, 问题就来了. 如果使用 像 tanh 的激励函数, Wx 的激活值就变成了 ~0.1 和 ~1, 接近于 1 的部已经处在了 激励函数的饱和阶段, 也就是如果 x 无论再怎么扩大, tanh 激励函数输出值也还是 接近1. 换句话说, 神经网络在初始阶段已经不对那些比较大的 x 特征范围 敏感了. 这样很糟糕, 想象我轻轻拍自己的感觉和重重打自己的感觉居然没什么差别, 这就证明我的感官系统失效了. 当然我们是可以用之前提到的对数据做 normalization 预处理, 使得输入的 x 变化范围不会太大, 让输入值经过激励函数的敏感部分. 但刚刚这个不敏感问题不仅仅发生在神经网络的输入层, 而且在隐藏层中也经常会发生. 只是时候 x 换到了隐藏层当中, 我们能不能对隐藏层的输入结果进行像之前那样的normalization 处理呢? 答案是可以的, 因为大牛们发明了一种技术, 叫做 batch normalization, 正是处理这种情况. BN 添加位置 Batch normalization 的 batch 是批数据, 把数据分成小批小批进行 stochastic gradient descent. 而且在每批数据进行前向传递 forward propagation 的时候, 对每一层都进行 normalization 的处理, BN 效果Batch normalization 也可以被看做一个层面. 在一层层的添加神经网络的时候, 我们先有数据 X, 再添加全连接层, 全连接层的计算结果会经过 激励函数 成为下一层的输入, 接着重复之前的操作. Batch Normalization (BN) 就被添加在每一个全连接和激励函数之间. 之前说过, 计算结果在进入激励函数前的值很重要, 如果我们不单单看一个值, 我们可以说, 计算结果值的分布对于激励函数很重要. 对于数据值大多分布在这个区间的数据, 才能进行更有效的传递. 对比这两个在激活之前的值的分布. 上者没有进行 normalization, 下者进行了 normalization, 这样当然是下者能够更有效地利用 tanh 进行非线性化的过程. 没有 normalize 的数据 使用 tanh 激活以后, 激活值大部分都分布到了饱和阶段, 也就是大部分的激活值不是-1, 就是1, 而 normalize 以后, 大部分的激活值在每个分布区间都还有存在. 再将这个激活后的分布传递到下一层神经网络进行后续计算, 每个区间都有分布的这一种对于神经网络就会更加有价值. Batch normalization 不仅仅 normalize 了一下数据, 他还进行了反 normalize 的手续. 为什么要这样呢? BN 算法 我们引入一些 batch normalization 的公式. 这三步就是我们在刚刚一直说的 normalization 工序, 但是公式的后面还有一个反向操作, 将 normalize 后的数据再扩展和平移. 原来这是为了让神经网络自己去学着使用和修改这个扩展参数 gamma, 和 平移参数 β, 这样神经网络就能自己慢慢琢磨出前面的 normalization 操作到底有没有起到优化的作用, 如果没有起到作用, 我就使用 gamma 和 belt 来抵消一些 normalization 的操作. 最后我们来看看一张神经网络训练到最后, 代表了每层输出值的结果的分布图. 这样我们就能一眼看出 Batch normalization 的功效啦. 让每一层的值在有效的范围内传递下去. BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布， 就是每次处理之后，用BN拉回N（0,1），这样就避免了梯度爆炸和梯度消失 Batch Normalization 批标准化批标准化通俗来说就是对每一层神经网络进行标准化 (normalize) 处理, 我们知道对输入数据进行标准化能让机器学习有效率地学习。 如果把每一层后看成这种接受输入数据的模式, 那我们何不 “批标准化” 所有的层呢 那我们就看看下面的两个动图, 这就是在每层神经网络有无 batch normalization 的区别啦.","link":"/2019/08/25/pytorch学习2/"},{"title":"目标检测总结","text":"目标检测的知识整理 知识体系分类：one-stage和two-stage两大类，如one-stage和two-stage两大类，R-CNN和YOLO系列等 正文导言：目标检测的任务是找出图像中所有感兴趣的目标（物体），确定它们的位置和大小，是机器视觉领域的核心问题之一。由于各类物体有不同的外观，形状，姿态，加上成像时光照，遮挡等因素的干扰，目标检测一直是机器视觉领域最具有挑战性的问题。本文将针对目标检测（Object Detection）这个机器视觉中的经典任务进行解析，抛砖引玉。 什么是目标检测？ 目标检测的任务是找出图像中所有感兴趣的目标（物体），确定它们的位置和大小，是机器视觉领域的核心问题之一。由于各类物体有不同的外观，形状，姿态，加上成像时光照，遮挡等因素的干扰，目标检测一直是机器视觉领域最具有挑战性的问题。 计算机视觉中关于图像识别有四大类任务： 分类-Classification：解决“是什么？”的问题，即给定一张图片或一段视频判断里面包含什么类别的目标。 定位-Location：解决“在哪里？”的问题，即定位出这个目标的的位置。 检测-Detection：解决“是什么？在哪里？”的问题，即定位出这个目标的的位置并且知道目标物是什么。 分割-Segmentation：分为实例的分割（Instance-level）和场景分割（Scene-level），解决“每一个像素属于哪个目标物或场景”的问题。 目标检测要解决的核心问题除了图像分类之外，目标检测要解决的核心问题是： 1.目标可能出现在图像的任何位置。 2.目标有各种不同的大小。 3.目标可能有各种不同的形状。 如果用矩形框来定义目标，则矩形有不同的宽高比。由于目标的宽高比不同，因此采用经典的滑动窗口+图像缩放的方案解决通用目标检测问题的成本太高。 目标检测的应用目标检测在很多领域都有应用需求。其中被广为研究的是人脸检测，行人检测，车辆检测等重要目标的检测。人脸检测在SIGAI上一篇文章“人脸识别算法演化史”中已经简单介绍，后面我们会针对这个问题撰写综述文章。 行人检测行人检测在视频监控，人流量统计，自动驾驶中都有重要的地位，后续也会有相关综述文章。 车辆检测车辆检测在智能交通，视频监控，自动驾驶中有重要的地位。车流量统计，车辆违章的自动分析等都离不开它，在自动驾驶中，首先要解决的问题就是确定道路在哪里，周围有哪些车、人或障碍物。 其他应用交通标志如交通灯、行驶规则标志的识别对于自动驾驶也非常重要，我们需要根据红绿灯状态，是否允许左右转、掉头等标志确定车辆的行为。 除了这些常见目标的检测之外，很多领域里也需要检测自己感兴趣的目标。比如工业中材质表面的缺陷检测，硬刷电路板表面的缺陷检测等。 农业中农作物表面的病虫害识别也需要用到目标检测技术： 人工智能在医学中的应用目前是一个热门的话题，医学影像图像如MRI的肿瘤等病变部位检测和识别对于诊断的自动化，提供优质的治疗具有重要的意义。 目标检测相关算法DPM算法与人脸、行人等特定类型的目标检测不同，通用目标检测要同时检测出图像中的多类目标，难度更大。处理这一问题的经典方法是DPM（Deformable Part Model），正如其名，这是可变形的组件模型，是一种基于组件的检测算法。该模型由Felzenszwalb在2008年提出，并发表了一系列的CVPR，NIPS文章，蝉联三届PASCAL VOC目标检测冠军，拿下了2010年PASCAL VOC的“终身成就奖”。 在深度卷积神经网络（DCNN）出现之前，DPM算法一直是目标检测领域最优秀的算法，它的基本思想是先提取DPM人工特征（如下图所示），再用latentSVM分类。这种特征提取方式存在明显的局限性，首先，DPM特征计算复杂，计算速度慢；其次，人工特征对于旋转、拉伸、视角变化的物体检测效果差。这些弊端很大程度上限制了算法的应用场景。 Alexnet现代深度神经网络的想法早在2006年就被 Geoffrey Hinton 提出，直到2012年，Alex Krizhevsky凭借著名的Alexnet卷积神经网络模型以领先第二名10%的成绩夺得ILSVRC2012图像分类比赛冠军，深度学习技术才真正走进主流学术界和工业界的视野。深度神经网络的出现颠覆了传统的特征提取方式，凭借其强大的表达能力，通过丰富的训练数据和充分的训练能够自主学习有用的特征。这相比传统的人工发现特征并根据特征设计算法的方式是质的飞跃。 通过卷积神经网络可以学到物体在各个层次的抽象表达（关于卷积神经网络的原理以及为什么会有效，SIGAI会在接下来的文章中介绍）：深度学习得到的层次特征表达 OverFeat2013年纽约大学Yann LeCun团队中Zhang xiang等提出的OverFeat在ILSVRC2013比赛中获得了多项第一，他们改进了Alexnet，提出了使用同一个卷积网络完成了多个任务的方法。该方法充分利用了卷积神经网络的特征提取功能，它把分类过程中提取到的特征同时又用于定位检测等各种任务，只需要改变网络的最后几层，就可以实现不同的任务，而不需要从头开始训练整个网络的参数。这充分体现和发掘了CNN特征共享的优点。 该文主要的亮点是： 1.共享卷基层用于多任务学习。 2.全卷积网络思想。 3.在特征层进行滑窗操作（Sliding Window）避免大量重复运算，这也是后来的系列算法不断沿用和改进的经典做法。 OverFeat几个明显的缺陷： 1.采用了多尺度贪婪的划窗策略，导致计算量还是很大 。 2.由于当时并没有太优秀的backbone网络，共享特征层的表征能力不是太强，没有考虑多尺度特征融合，对小目标效果差，整体的检测效果不尽如人意。ILSVRC 2013数据集上的mAP（可以简单的理解为检测准确率）为24.3%。 经典的卷积神经网络有一个问题是它只能接受固定大小的输入图像，这是因为第一个全连接层和它之前的卷积层之间的权重矩阵大小是固定的，而卷积层、全连接层本身对输入图像的大小并没有限制。而在做目标检测时，卷积网络面临的输入候选区域图像大小尺寸是不固定的。 下面用一个例子说明怎么让一个已经设计好的DCNN模型，可以支持任意大小图片输入，其中一种方案是全卷积网络（FCN），即去掉所有全连接层，全部由卷积层来替代： FCN并不是把5×5的图片展平成一维向量再进行计算，而是直接采用5×5的卷积核对一整张图片进行卷积运算。比如16×16大小的特征图片，那么会是什么样的结果？请看下面的示意图： 这个时候就会发现，网络最后的输出是一张2×2大小的特征图片。可以发现采用FCN网络，可以输入任意大小的图片。需要注意的是网络最后输出的特征图片大小不再总是1×1而是一个与输入图片大小相关。 OverFeat有很多创新，但是不能算是目标检测典型的Pipeline，所以我们单独提了出来。下面将从R-CNN开始介绍目前基于DCNN物体检测发展脉络。 卷积神经网络用于目标检测之后，进展神速，在短期内大幅度的提高了算法的精度，推动这一技术走向实用。 基于DCNN的目标检测算法发展路线图 R-CNNRegion CNN(简称R-CNN)由Ross Girshick （江湖人称RBG大神，Felzenszwalb的学生）提出，是利用深度学习进行目标检测的里程碑之作，奠定了这个子领域的基础。这篇文章思路清奇，在DPM方法经历多年瓶颈期后，显著提升了检测率（ILSVRC 2013数据集上的mAP为31.4%）。RBG是这个领域神一样的存在，后续的一些改进方法如Fast R-CNN、Faster R-CNN、YOLO等相关工作都和他有关。 R-CNN检测时的主要步骤为： 1.使用Selective Search算法从待检测图像中提取2000个左右的区域候选框，这些候选框可能包含要检测的目标。 2.把所有侯选框缩放成固定大小（原文采用227×227）。 3.用DCNN提取每个候选框的特征，得到固定长度的特征向量。 4.把特征向量送入SVM进行分类得到类别信息，送入全连接网络进行回归得到对应位置坐标信息。 R-CNN不采用滑动窗口方案的原因一是计算成本高，会产生大量的待分类窗口；另外不同类型目标的矩形框有不同的宽高比，无法使用统一尺寸的窗口对图像进行扫描。用于提取特征的卷积网络有5个卷积层和2个全连接层，其输入是固定大小的RGB图像，输出为4096维特征向量。对候选区域的分类采用线性支持向量机，对每一张待检测图像计算所有候选区域的特征向量，送入支持向量机中进行分类；同时送入全连接网络进行坐标位置回归。 R-CNN虽然设计巧妙，但仍存在很多缺点： 1.重复计算。R-CNN虽然不再是穷举，但通过Proposal（Selective Search）的方案依然有两千个左右的候选框，这些候选框都需要单独经过backbone网络提取特征，计算量依然很大，候选框之间会有重叠，因此有不少其实是重复计算。 2.训练测试不简洁。候选区域提取、特征提取、分类、回归都是分开操作，中间数据还需要单独保存。 3.速度慢。前面的缺点最终导致R-CNN出奇的慢，GPU上处理一张图片需要十几秒，CPU上则需要更长时间。 4.输入的图片Patch必须强制缩放成固定大小（原文采用227×227），会造成物体形变，导致检测性能下降。 SPPNet此后MSRA的Kaiming He等人在R-CNN的基础上提出了SPPNet，该方法虽然还依赖候选框的生成，但将提取候选框特征向量的操作转移到卷积后的特征图上进行，将R-CNN中的多次卷积变为一次卷积，大大降低了计算量（这一点参考了OverFeat）。 R-CNN的卷积网络只能接受固定大小的输入图像。为了适应这个图像尺寸，要么截取这个尺寸的图像区域，这将导致图像未覆盖整个目标；要么对图像进行缩放，这会产生扭曲。在卷积神经网络中，卷积层并不要求输入图像的尺寸固定，只有第一个全连接层需要固定尺寸的输入，因为它和前一层之间的权重矩阵是固定大小的，其他的全连接层也不要求图像的尺寸固定。如果在最后一个卷积层和第一个全连接层之间做一些处理，将不同大小的图像变为固定大小的全连接层输入就可以解决问题。 SPPNet引入了Spatial Pyramid pooling层，对卷积特征图像进行空间金字塔采样获得固定长度的输出，可对特征层任意长宽比和尺度区域进行特征提取。具体做法是对特征图像区域进行固定数量的网格划分，对不同宽高的图像，每个网格的高度和宽度是不规定的，对划分的每个网格进行池化，这样就可以得到固定长度的输出。下图是SPP操作示意图： 相比R-CNN，SPPNet的检测速度提升了30倍以上。下图是R-CNN和SPPNet 检测流程的比较： 下图是SPPNet的原理： SPPNet 检测框架图 SPPNet和R-CNN一样，它的训练要经过多个阶段，中间特征也要进行存储；backbone网络参数沿用了分类网络的初始参数，没有针对检测问题进行优化。 Fast RCNNRoss Girshick 针对SPPNet做了进一步改进提出的FRCNN ，其主要创新是RoI Pooling 层，它将不同大小候选框的卷积特征图统一采样成固定大小的特征。ROI池化层的做法和SPP层类似，但只使用一个尺度进行网格划分和池化。该层可以直接求导，训练时直接将梯度传导到backbone网络进行优化。FRCNN针对R-CNN和SPPNet在训练时是多阶段的和训练的过程中很耗费时间空间的问题进行改进。将深度网络和后面的SVM分类两个阶段整合到一起，使用一个新的网络直接做分类和回归。使得网络在Pascal VOC上的训练时间从R-CNN的84小时缩短到9.5小时，检测时间更是从45秒缩短到0.32秒。 重要的是Fast RCNN的backbone网络也可以参与训练了！！！ Faster RCNNSPPNet和Faster RCNN都需要独立的候选区域生成模块，这个模块计算量很大，而且不易用GPU加速。针对这个问题，Shaoqin Ren 等人在Faster RCNN基础上提出Faster R-CNN ，在主干网络中增加了RPN （Region Proposal Network）网络，通过一定规则设置不同尺度的锚点（Anchor）在RPN的卷积特征层提取候选框来代替Selective Search等传统的候选框生成方法，实现了网络的端到端训练。候选区域生成、候选区域特征提取、框回归和分类全过程一气呵成，在训练过程中模型各部分不仅学习如何完成自己的任务，还自主学习如何相互配合。这也是第一个真正意义上的深度学习目标检测算法。 注：Shaoqin Ren实现的matlab版本中RPN阶段和FRCNN阶段是分开训练的，但是在实际的实践中（RBG实现的Python版本）发现二者可以一起优化训练，而且精度没有损失，可以说Faster RCNN真正实现了端到端的训练。 Fast RCNN（左） 和 Faster RCNN（右）框架结构对比 R-FCN由于现在的主流网络层数越来越多，基于Faster RCNN检测框架的方法的计算量受到了3个因素的影响： 1.基础网络的复杂度 2.候选框数量的多少 3.分类和位置回归子网络的复杂度（每个候选框的box都会独立进行前向计算）。 一般来说直接优化前两点性价比不太高。如果直接优化RoI-wise subnetwork是否可行呢,将子网络的深度尽可能减少？分类是要增加物体的平移不变性（不同的位置都是同一个物体）；目标检测时减少物体的平移变化（目标检测需要得到物体所在的位置）。通常我们所用的网络都是ImageNet的分类任务训练得到的，在目标检测的时候进行Finetune。由于得到的初始模型基于分类任务，那么会偏向于平移不变性，这和目标检测就出现了矛盾。 MSRA的Jifeng Dai等人提出了R-FCN，通过position-positive score maps（位置敏感得分图）来解决这个矛盾。位置敏感得分图通过预测RoI中不同部位的类别投票表决产生该RoI的类别预测。引用原文中的例子，“如果我们的算法要识别婴儿，那么把一个目标区域分成九宫格，其中算法认为其中五个格子中的区域分别像婴儿的头、四肢和躯干，那么根据投票机制，就认为这个目标区域里的是一个婴儿。这很符合我们人类的判断逻辑。” R-FCN沿用了 Faster RCNN 的框架结构，不同的是在Faster R-CNN的基础上通过引入位置敏感得分图，将RoI-wise subnetwork消灭了，直接在位置敏感得分图上利用ROI Pooling进行信息采样融合分类和位置信息。 R-FCN 网络框架结构 ResNet101为例，不同检测框架复用卷积网络层数 Mask R-CNN2017年Kaiming He等提出了Mask R-CNN ，并获得ICCV2017 Best Paper Award。作者指出，Faster R-CNN在做下采样和RoI Pooling时都对特征图大小做了取整操作，这种做法对于分类任务基本没有影响，但对检测任务会有一定影响，对语义分割这种像素级任务的精度影响则更为严重。为此，作者对网络中涉及特征图尺寸变化的环节都不使用取整操作，而是通过双线性差值填补非整数位置的像素。这使得下游特征图向上游映射时没有位置误差，不仅提升了目标检测效果，还使得算法能满足语义分割任务的精度要求。 以上介绍的检测方法都属于two-stage的方案，即分为候选区域生成和区域分类两步，接下来我们将介绍几种single-stage的经典方法。 YOLO系列2015年，随着YOLO算法的出现，深度学习目标检测算法开始有了两步（two-stage）和单步（single-stage）之分。区别于R-CNN系列为代表的两步检测算法，YOLO舍去了候选框提取分支（Proposal阶段），直接将特征提取、候选框回归和分类在同一个无分支的卷积网络中完成，使得网络结构变得简单，检测速度较Faster R-CNN也有近10倍的提升。这使得深度学习目标检测算法在当时的计算能力下开始能够满足实时检测任务的需求。 算法将待检测图像缩放到统一尺寸，为了检测不同位置的目标，将图像等分成的网格，如果某个目标的中心落在一个网格单元中，此网格单元就负责预测该目标。 YOLOv1只针对最后7x7的特征图进行分析，使得它对小目标的检测效果不佳，当多个目标出现在一个Grid Cell时不容易区分。 YOLOv1原理图 YOLOv1在7X7特征图上对每个Grid cell进行操作 YOLOv2改进了YOLOv1的网络结构，除加入时下热门的批量归一化层以及残差网络结构以外，还针对性的训练了一个高分辨率的分类网络（448x448）然后利用该网络训练检测网络，单单通过提升输入的分辨率，mAP获得了4%的提升。YOLOv1利用单个grid cell拼接成全连接层完成边框的预测，导致丢失较多的空间信息，定位不准,作者在这一版本中进行了优化改进： 1.借鉴了Faster R-CNN中的Anchor思想，但是作者在实践中发现用基于规则选择的Anchor效果并没有得到提升，实验中作者对Pascal VOC和COCO的数据集进行了统计分析（聚类分析）选择针对行的Anchor的尺寸性能的到了明显提升。 2.作者在使用anchor boxes时发现模型收敛不稳定，尤其是在早期迭代的时候。大部分的不稳定现象出现在预测box的 (x,y) 坐标的优化上。因此作者就没有采用直接预测offset的方法，而使用了预测相对于grid cell的坐标位置的办法，利用logistic函数把ground truth归一化到0到1之间，坐标值被归一化后，模型优化会更稳定。 YOLOv3在YOLOv2的基础上使用了全新设计的Darknet53残差网络并结合FPN网络结构，在网络后两个特征图上采样后于网络前期相应尺寸的特征图聚合再经过卷积网络后得到预测结果。这些改进使得YOLOv3用三分之一的时间达到与SSD相当的精确度。在 COCO test-dev 上 mAP@0.5 达到 57.9%，与RetinaNet（FocalLoss论文所提出的单阶段网络）的结果相近，但速度快4倍。 YOLOv3的模型比之前的版本复杂了不少，可以通过改变模型结构的大小来权衡速度与精度。 YOLOv3的改进点： 多尺度预测（FPN） 更好的Backbone网络（Darknet53残差网络） 分类损失采用binary cross-entropy损失函数替换Softmax损失函数（Softmax会选择分数最高的类别判定为当前框所属的类别，而现实中一个目标可能属于多个类别标签） SSDSSD对YOLO进行了改进，达到了和两阶段方法相当的精度，同时又保持了较快的运行速度。SSD也采用了网格划分的思想，和Faster RCNN不同的是它将所有的操作整合在一个卷积网络中完成。为了检测不同尺度的目标，SSD对不同卷积层的特征图像进行滑窗扫描；在前面的卷积层输出的特征图像中检测小的目标，在后面的卷积层输出的特征图像中检测大的目标。它的主要特点是： 1.基于多尺度特征图像的检测：在多个尺度的卷积特征图上进行预测，以检测不同大小的目标，一定程度上提升了小目标物体的检测精度。 2.借鉴了Faster R-CNN中的Anchor boxes思想，在不同尺度的特征图上采样候选区域，一定程度上提升了检测的召回率以及小目标的检测效果。下图是SSD的原理： FPNFPN（Feature Pyramid Network）方法同时利用低层特征高分辨率和高层特征的高语义信息，通过融合这些不同层的特征达到提升预测的效果的作用。FPN中预测是在每个融合后的特征层上单独进行的，这和常规的特征融合方式有所不同。 FPN 网络结构如下图d（其中YOLO使用b结构，SSD使用c结构）所示，它的结构具有相当的灵活性，可以和各种特征提取网络结合作为检测算法的基础网络。在后文中会看到，目前大多数state-of-art的模型都采用了这种结构。其中RetinaNet在FPN的基础上使用了ResNet网络提取特征，并用Focal Loss损失改善单步目标检测算法中普遍存在的前景类和背景类损失不均衡的问题。这些基于FPN结构的检测算法能够在增加网络深度、获取更丰富语义信息的同时从浅层特征图中获取更丰富且高分辨率的图像特征，这使得这种网络结构在实际应用中表现出优异的性能。 目前主流检测框架有4种使用特征的形式： 1.图像金字塔。即将图像缩放到不同的大小，然后不同大小的图像生成对应的特征。这种方法的缺点是增加了时间成本。有些算法会在检测时采用这种图像金字塔的方案。 2.单一尺度特征层。SPPNet，Fast RCNN，Faster RCNN采用这种方式，即仅采用网络最后一层卷积层的特征。 3.SSD采用这种多尺度特征融合的方式，但是没有上采样过程，即从网络不同层抽取不同尺度的特征做预测，这种方式不会增加额外的计算量。SSD算法中没有用到足够低层的特征（在SSD中，最低层的特征是VGG网络的conv4_3），而足够低层的特征对于检测小物体是很有帮助的。 4.FPN采用bottom-up与top-down的结构，实现了低层特征和高层语义特征的融合，提高了特征映射的信息密度和分辨率，提高了小目标物体的检测效果；区别于SSD，FPN每层都是独立预测的。 COCO2017排行榜最后我们来看通用目标检测算法的最新进展。下图是MSCOCO 2017年目标检测竞赛的领先算法： 其中排名第一的模型为旷视科技（face++）提交的MegDet。他们的方案没有在检测算法方面做过多优化（采用的是ResNet50+FPN），而是在并行训练规模上做了优化。训练硬件环境是由128个GPU组成的集群，通过改进跨GPU批量归一化算法和学习率变化策略，将batch size增大到256张，这使得批量归一化层中使用的批均值和方差更能够反应总体特征，有效提升了归一化效果，从而大幅提升训练速度并且得到了非常好的结果。 排名第二的方案PAN改进了FPN算法，如下图所示。它在FPN的基础上不仅增加了一个降采样网络（b），还聚合使用了多个不同尺度特征图上的预测候选框（c）。该模型不仅在这一届的COCO目标检测竞赛中名列第二，而且取得了语义分割任务的冠军。 第三名的模型出自MSRA之手，他们同样没有对检测算法本身做过多改进，在FPN基础上使用了Xception网络结构和SoftNMS，但与以往不同的是使用了可变卷积层DCN（deformable convnet）替代了传统卷积，使得卷积层能够根据图片的语义信息调整卷积核感受点的位置，达到提升网络特征提取能力的目的。下图是可变卷积层的原理： 排名第四的结果是用以FPN+ResNeXt网络为作为基础网络的Mask R-CNN算法得到的。后面大多数成绩优异的模型都是R-FCN、FPN、Faster-RCNN等经典模型结合DCN、Attention机制、先进分类网络结构和模型融合等技术而形成的算法。","link":"/2019/11/28/目标检测总结/"},{"title":"剪枝网络","text":"The Lottery Ticket Hypothesis: Finding sparse, trainable neural networks 抽奖彩票假说: 寻找稀疏，可训练的神经网络 📝心得：实验结果扎实可信(改进明显)，主题简单明确，未来科研究方向多 摘要神经网络剪枝技术可以将训练好的神经网络参数减少90%以上，在不影响准确率的情况下，降低存储要求并提高计算性能。然而当前的经验的是: 通过剪枝产生的稀疏参数矩阵从头开始训练很困难，同样也很难提高训练性能。 我们发现标准的剪枝技术揭示了一些子网络，这些子网络的初始化可以进行高效的训练。基于以上这些结果，我们阐明了彩票假说：当进行隔离训练时，密集、随机初始化、前向传播的网络包含子网络(获胜的彩票)在同样的迭代次数和在测试集上将会达到与原始网络相同的精度。我们发现的子网络(the winning tickets) 赢过了初始的抽彩给奖法(the initialization lottery)：他们之间的联系具有初始的权值使训练更加高效。 !作者提出了一个算法来识别wining tickets，以及一系列来支持彩票假说和随机初始化的重要性的实验。我们持续发现子网络(the winning tickets)比全接连和卷积前向网络的10-20%还少，在MNIST和CIFAR10数据集上。超过这个大小，我们发现子网络(the winning tickets)比原始网络更快达到更高的准确率。 Introduction从神经网络中消除不必要的权重(剪枝)(LeCun等，1990; Hassibi＆Stork，1993; Han等，2015; Li等，2016)可以减少90%以上的参数数量并不减少准确率。这样做降低了训练的神经网络的规模和能源消耗，使推理更加高效。然而，如果一个神经网络可以被减小规模，为什么我们不训练一个更小的参数结构，反而注意在让训练更高效？当代的经验是说通过剪枝的结构从一开始难以训练，其准确率会低于原始准确率。 思考一个例子。在图1，我们随机采样和训练全连接和卷积网络的子网络在mnist和cifar10的数据集上。随机抽样(Random sampling)模拟了LeCun等人使用的非结构化剪枝的效果。（1990）和Han等人。 （2015年）。在各种稀疏度水平上，虚线追踪在迭代中的最小验证集的损失和测试精度。网络越稀疏，学习越慢，最终的测试精度越低。 [1] 从头开始训练修剪过的模型比重新训练之前之前修剪过模型更糟糕，这可能表明训练网络容量小的困难。”（Li et al。，2016）“在再训练期间，最好保留权重修剪过程中存在的连接的初始训练阶段比重新初始化修剪过的图层…梯度下降能够在网络初始训练时找到一个好的解决方案，但不能在重新初始化某些层并重新训练它们之后找到一个好的解决方案“。 [2]作为网络学习速度的代理，我们使用迭代，在该迭代中，早期停止的标准是将结束训练。我们在本文中使用的迭代是早期停止标准是训练期间最小验证损失的那个迭代。有关更多详细信息，请参阅附录C. 注: 在多次迭代中，我们选取那个迭代== 一个早期停止的标准用来结束训练。早期停止标准是训练期间最小验证损失的那个迭代 \u0001\u0001 图1：MNIST的Lenet架构和CIFAR10的Conv-2，Conv-4和Conv-6架构的早期停止（左）和迭代（右）的迭代次数（见图2）从不同尺寸开始训练时。虚线是随机抽样的稀疏网络（十次试验的平均值）。实线是赢彩票the winning tickets（五次试验的平均值）密集、随机初始化、前向传播。 左1：X轴剪枝后保留的权重的比例，Y轴早期停止的迭代 即 在多轮迭代中选取的最小验证集的损失的那一次迭代。 wining tickets 在相同的比例的参数下，收敛速度更快 左2：X轴剪枝后保留的权重的比例，Y轴早期停止的迭代 即 在多轮迭代中选取的最小验证集的损失的那一次迭代。不同网络保留的网络参数越多，需要达到最小验证集损失的训练迭代次数也越大。相同网络参数越少，达到训练标准的次数越大。 右1：X轴剪枝后保留的权重的比例，Y轴：达到最小验证集损失时在测试集上的准确率。保留的参数比例由大到小时，准确率先升后降，wining tickets效果好于随机采样。 右2：X轴剪枝后保留的权重的比例，Y轴：达到最小验证集损失时在测试集上的准确率。保留的参数比例由大到小时，准确率先升后降，复杂网络准确率越高。 在本文中，我们表明，一直存在较小的子网络，从一开始就训练，并且至少与其较大的对应网络一样快速地学习，同时达到相似的测试精度。图1中的实线显示了我们发现的网络。基于这些结果，我们陈述了彩票假设。 The Lottery Ticket Hypothesis.彩票假设。随机初始化的、密集的、神经网络包含一个子网络，经过初始化，当经过隔离训练时，它可以达到训练后最多相同迭代次数的原始网络的测试精度。 更正式的，考虑一个密集的、前向传播的网络$f(x ; \\theta)$,初始化参数为$\\theta=\\theta_{0} \\sim \\mathcal{D}{\\theta}$。当在训练集上用随机梯度下降SGD优化时，$f$经过$j$轮迭代达到了最小的验证损失$l$，测试正确率为$a$。 另外，考虑用掩码$m \\in{0,1}^{|\\theta|}$训练$f(x ; m \\odot \\theta)$，参数初始化为$m \\odot \\theta{0}$。当在同样的训练集(m固定)上用SGD来优化时，$f$达到了最小的验证损失$l^{\\prime}$ 经过了$j^{\\prime}$轮迭代，达到了测试正确率$a^{\\prime}$. 彩票假说预测 $\\exists m$使 $j^{\\prime} \\leq j$ (相应的训练时间)，$a^{\\prime} \\geq a$(相应的正确率)和$|m|_{0} \\ll|\\theta|$ (更少的参数)。 我们发现标准的剪枝技术会自动从全连接和卷积的前馈网络中发现这种可训练的子网络。我们指定这些可训练的子网络$f(x ; m \\odot \\theta)$ 是中奖票(winning tickets)，因为我们发现的那些已经通过能够学习的权重和连接的组合超过了(the initialization lottery)初始化抽奖。当他们的参数随机重新初始化($f\\left(x ; m \\odot \\theta_{0}^{\\prime}\\right) \\text { where } \\theta_{0}^{\\prime} \\sim \\mathcal{D}_{\\theta}$)，我们的中奖票(winning tickets)不再匹配原始网络的性能，提供证据表明这些较小的网络不能有效训练，除非他们适当初始化。 Identifying winning tickets. 我们通过训练网络并剪枝其最小等级的权重来找到获胜彩票。剩余的未剪枝通过连接构成了中奖票的参数结构。对于我们的工作独一无二而言，每个未经传输的连接的权值在被训练之前将被重置为从原始网络初始化。这形成了我们的中心实验： 随机初始化神经网络$f\\left(x ; \\theta_{0}\\right)\\left(\\text { where } \\theta_{0} \\sim \\mathcal{D}_{\\theta}\\right)$ 训练一个网络 用 $j$轮迭代，达到参数 $\\theta_{j}$ [ $f$经过$j$轮迭代达到了最小的验证损失$l$，测试正确率为$a$。] 剪枝$p\\%$的$\\theta_{j}$参数，通过创建一个mask $m$ [剪枝 最小等级的权值] 把$\\theta_{0}$中的未剪枝的参数值重新设置到保留的参数中，得到the winning tickets $f\\left(x ; m \\odot \\theta_{0}\\right)$ 如上所述，这种修剪方法是一次性的：一次网络训练，p％的权重被修剪，幸存的权重被重置。然而，在本文中，我们专注于迭代修剪，反复训练，修剪和重置网络n轮 rounds; 每一rounds剪枝掉上一轮保留参数的$p^{\\frac{1}{n}} \\%$ 。我们结果证明 迭代剪枝发现中彩票 比得上原始网络的正确率，参数规模小于一次性剪枝的规模。 Results.我们辨识找到winning tickets 在全连接结构对mnist和卷积结构对cifar10 通过几个优化策略(SGD, 动量, adam) 用不同技术如删除，权重衰退，batchnorm和残差连接。我们用无结构化的剪枝技术，所以中票的彩票是稀疏的。在更深的网络中，我们这个基于剪枝的策略 来找到 winning tickets 对学习率是很敏感的：它需要热身warmup才能在以更高的学习率获得中奖彩票。我们发现的中奖彩票(winning tickets)是原始网络规模的10-20％（或更小）（较小的尺寸）。在这个尺寸下，它们在最多相同的迭代次数（相应的训练时间）内达到或超过原始网络的测试精度（相称的准确度）。当随机重新初始化时，中奖彩票(the winning tickets)表现得更糟，这意味着单独的结构无法解释获胜彩票成功。 The Lottery Ticket Conjecture.彩票票猜想。回到我们的动机问题，我们将我们的假设扩展为一个未经测试的猜想，即SGD寻找并训练一组良好初始化的权重。密集，随机初始化的网络比剪枝产生的稀疏网络更容易训练，因为有更多可能的子网络，训练可以从中恢复中奖票参数。 Contributions. 我们证明剪枝可以找到可训练的子网络，这些子网络达到了测试精度，与原始网络相比，它们可以在相同数量的迭代中得到。 我们表明剪枝可以获得比原始网络学得更快的中奖彩票参数，同时达到更高的测试精度和更好的推理。 我们提出将彩票假设作为解释这些发现的神经网络组成的新视角。 图2：本文测试的体系结构。卷积是3x3。 Lenet来自LeCun等人。 （1998）。 Conv-2/4/6是VGG的变种（Simonyan＆Zisserman，2014）。 Resnet-18来自He等人。 （2016）。 CIFAR10的VGG-19改编自Liu等人。 （2019）。初始化是Gaussian Glorot（Glorot＆Bengio，2010）。括号表示层周围的残余连接。 Implications.在本文中，我们实证研究了彩票假设。既然我们已经证明了中奖票的存在，我们希望利用这些知识： 提高训练性能。由于中奖彩票(winning tickets)可以从一开始就被隔离训练，我们希望我们可以设计出尽可能早地搜索中奖彩票和剪枝的训练方案。 设计更好的网络。获奖票(Winning tickets)证揭示了稀疏的架构和初始化的组合，这些组合特别适应学习。我们可以从中奖彩票(winning tickets)中获取灵感，设计具有有助于学习的相同属性的新架构和初始化方案。我们甚至可以将为一项任务发现的中奖票转移给许多其他人。 提高我们对神经网络的理论认识。我们可以研究为什么随机初始化的前馈网络似乎包含中奖票和对优化理论研究的潜在影响（Du等，2019）和泛化（Zhou等，2018; Arora等，2018）。 2 Winning tickets在全连接网络中在本节中，我们评估了应用于在MNIST上训练的全连接网络的彩票假设。我们使用Lenet-300-100架构（LeCun等，1998），如图2所示。我们遵循第1节中的大纲：在随机初始化和训练网络之后，我们修剪网络并将剩余连接重置为其原始初始化的参数值。我们使用简单的逐层修剪启发：删除每层中具有最低幅度的权重的百分比（如Han等人（2015））。与输出的连接的部分以网络其余部分比率的一半进行修剪。我们将在附录G中探索其他超参数，包括学习速率，优化策略（SGD，动量），初始化方案和网络规模。 图3：随着训练的进行，Lenet（迭代修剪）的测试准确性。每条曲线是五次试验的平均值。标签是$P_{m}$ - 修剪后网络中剩余的权重比重。误差线是任何试验的最小值和最大值。 Notation符号. $P_{m}=\\frac{|m|{0}}{|\\theta|}$ 是mask m的稀疏程度，比如： $P{m}=25 \\%$是表示75%的参数被剪掉 G.5 I NITIALIZATION D ISTRIBUTION To this point, we have considered only a Gaussian Glorot (Glorot &amp; Bengio, 2010) initialization scheme for the network. Figure 30 performs the lottery ticket experiment while initializing the Lenet architecture from Gaussian distributions with a variety of standard deviations. The networks were optimized with Adam at the learning rate chosen earlier. The lottery ticket pattern continues to appear across all standard deviations. When initialized from a Gaussian distribution with standard deviation 0.1, the Lenet architecture maintained high validation accuracy and low early-stopping times for the longest, approximately matching the performance of the Glorot-initialized network. 到目前为止，我们只考虑了网络的Gaussian Glorot（Glorot＆Bengio，2010）初始化方案。图30执行彩票实验，同时从具有各种标准偏差的高斯分布初始化Lenet架构。亚当以之前选择的学习率对网络进行了优化。彩票模式继续出现在所有标准偏差中。当从具有标准偏差0.1的高斯分布初始化时，Lenet架构保持高验证准确度和较低的早停时间，最长，与Glorot初始化网络的性能大致匹配。 Iterative pruning.迭代剪枝。 我们获得的中奖彩票（The winning tickets）比原来的网络学得更快。图3绘制了在不同程度上迭代修剪的中奖彩票（The winning tickets）时的平均测试精度。误差线是五次运行的最小值和最大值。对于第一次修剪，网络学习得越快，测试精度越高，修剪得越多（图3中的左图）。中奖彩票，占原网络重量的51.3％(.i.e, $P_{m}=51.3 \\%$)比原始网络更快地达到更高的测试准确度，但比P m = 21.1％时更慢。当P m &lt;21.1％时，学习速度减慢（中图）。当P m = 3.6％时，中奖票据会回归到原始网络的性能。本文中重复了类似的模式。 在每次迭代中的迭代修剪20％时（蓝色）, 图4 a 总结了所有修剪级别方法的此行为。左边是每个网络达到最小验证损失的迭代（即，当早期停止标准将停止训练时）与修剪后剩余的权重百分比相关的迭代;中间是该迭代的测试精度。我们使用迭代来满足早期停止标准，作为网络学习速度的代理。 当P m从100％降低到21％时，The winning tickets学得更快，此时提前停止比原始网络早38％ 原始-5K，prune- 3.1k。进一步修剪导致学习速度变慢，当P m = 3.6％时，返回到原始网络的早期停止性能。随着修剪，测试集精度提高，当P m = 13.5％时，测试精度提高0.3个百分点以上;在此之后，当P m = 3.6％时，精度降低，返回到原始网络的水平。 在早期停止时，训练准确性（图4a，右）随着修剪以类似的模式增加以测试准确性，似乎暗示winning tickets更有效地优化但不能更好地推广。然而，在迭代50,000（图4b），尽管几乎所有网络的训练精度达到100％，但迭代修剪的winning tickets仍然可以看到测试精度提高高达0.35个百分点（附录D，图12）。这意味着，winning tickets的训练准确度和测试准确度之间的差距较小，这表明泛化的改进。 Random reinitialization.随机重新初始化。为了测量a winning ticket的初始化的重要性，我们保留a winning ticket的结构（即，掩码m），但是随机地采样新的初始化参数$\\theta_{0}^{\\prime} \\sim \\mathcal{D}_{\\theta}$。我们随机重新初始化(each winning ticket)每张中奖彩票三次，在图4中每点累计15次。我们发现初始化对获胜彩票的效果至关重要。图3中的右图显示了迭代修剪的这个实验。除了原始网络和P m = 51％和21％的获奖门票是随机重新初始化实验。the winning tickets获奖彩票在被修剪时学得更快的地方，但随机重新初始化时，它们学得越来越慢。 该实验的更广泛的结果是图4a中的橙色线。与获奖彩票不同，重新初始化的网络学习速度比原始网络慢，并且在修剪后会失去测试准确性。平均重新初始化的迭代中奖票的测试精度从原始准确度下降，相比于当the winning ticket的P m = 21.1％时和2.9％。当P m = 21％时，中彩票的最小验证损失比重新初始化时快2.51倍，并且准确度提高了半个百分点。所有网络达到100％的训练精度，Pm≥5％; 因此, 图4b表明中彩票比随机重新初始化时更好地泛化。该实验支持彩票假设’强调初始化：原始的初始化承受和修剪的好处，而随机重新初始化的性能立即受到影响并且稳定地减少。 图4：在一次性和迭代修剪下Lenet的早期停止迭代和准确性。平均五次试验;最小值和最大值的误差线。在迭代50,000，对于迭代中奖票，Pm≥2％的训练精度≈100％（见附录D，图12）。 One-shot pruning.一次性剪枝。虽然迭代修剪iterative pruning提取较小的中奖票winning tickets，但重复地训练意味着它们被找到的代价大。一次性修剪可以在没有重复训练的情况下识别获奖彩票winning tickets。图4c显示了一次性修剪（绿色）和随机重新初始化（红色）的结果; 一次性修剪确实能获得中奖彩票。当67.5％≥Pm≥17.6％时，平均中奖彩票比原始网络更早达到最低验证准确度。当95.0％≥Pm≥5.17％时，测试精度高于原始网络。但是，迭代修剪的获奖彩票可以更快地学习，并在较小的网络规模下达到更高的测试精度。图4c中的绿线和红线在图4a的对数轴上再现，使得该性能差距清晰。由于我们的目标是确定最小的获奖门票，因此我们将重点放在本文其余部分的迭代修剪上。 3 Winning tickets在卷积网络中在这里，我们将彩票假设应用于CIFAR10上的卷积网络，增加了学习问题的复杂性和网络的规模。我们考虑图2中的Conv-2，Conv-4和Conv-6架构，它们是VGG（Simonyan＆Zisserman，2014）系列的缩小版本。网络有两个，四个或六个卷积层，后面是两个完全连接的层;每两个卷积层之后发生最大池化。这些网络覆盖了从近乎完全连接到传统卷积网络的范围，其中参数少于Conv-2中卷积层中的参数不到的1％和Conv-6中的接近三分之二。 Finding winning tickets.图5（上）中的实线显示了Conv-2（蓝色），Conv-4（橙色）和Conv-6（绿色）的迭代彩票实验来自图2中的每层修剪率。来自Lenet的第2节重复：随着网络被修剪，与原始网络相比，它学得更快，测试准确度也提高了。在这种情况下，结果更加明显。 图5：在迭代修剪和随机重新初始化时，Conv-2/4/6架构的早期停止迭代和测试及训练准确性。每条实线是五次试验的平均值; 每条虚线是15次重新初始化的平均值（每次试验三次）。右下图描绘了在与原始网络的最后一次训练迭代相对应的迭代中获胜门票的测试准确度（Conv-2为20,000，Conv-4为25,000，Conv-6为30,000）;在此迭代中，对于中奖彩票，Pm≥2％的训练精度≈100％（见附录D）。 达到50K，traning accuracy = 100% 中奖彩票达到最低验证损失比为Conv-2的快3.5倍（P m = 8.8％），比Conv-4快3.5倍（Pm = 9.2％），比Conv-6快2.5倍（P m = 15.1％） 。测试精度最高提高3.4个百分点对Conv-2的（P m = 4.6％），Conv-4的3.5个百分点（P m = 11.1％）和Conv-6的3.3个百分点（P m = 26.4％）。当P m&gt; 2％时，所有三个网络都保持在其原始平均测试精度之上。 ??? 与第2节一样，早期停止迭代的训练精度随着测试精度而提高。然而，在Conv-2的迭代20,000，Conv-4的25,000和Conv-6的30,000（对应于原始网络的最终训练迭代的迭代）中，当Pm≥2%时，所有网络的训练精度达到100％（附录D，图13）和中彩票仍然保持较高的测试精度（图5右下）。这意味着中彩票的测试和训练准确度之间的差距较小，表明它们泛化更好。 Random reinitialization.随机重新初始化，我们重复随机重新初始化实验按照第2节，即在图5中展示的虚线。这些网络再次花费更长的时间来学习在继续修剪时。正如MNIST上的Lenet（第2节）一样，随机重新初始化实验的测试精度下降得更快。然而，与Lenet不同，早期停止时的测试准确性最初保持稳定，甚至在Conv-2和Conv-4上也得到改善，表明 - 在适度的修剪 - 中奖彩票的结构可能会带来更好的准确性。 DropoutDropout（Srivastava et al。，2014; Hinton et al。，2012）通过在每次训练迭代中随机排除一小部分单元（即，随机抽样子网）来提高准确性。Baldi＆Sadowski（2013）将Dropout描述为同时训练所有子网的整体。由于彩票假设表明这些子网中的一个包含中奖彩票，因此很自然地会询问Dropout和我们发现中奖票的策略是否相互影响。 图6显示了Conv-2，Conv-4和Conv-6的训练结果，其dropout率为0.5。虚线是没有dropout的网络性能（图5中的实线）。 我们在dropout训练时继续获得中奖彩票。dropout提高了初始测试的准确性（分别为Conv-2，Conv-4和Conv-6的平均值分别为2.1,3.0和2.4个百分点），迭代修剪进一步提高（最多增加2.3,4.6和4.7百分点，平均分别）。迭代修剪学习变得比以前更快，但在Conv-2的情况下则不那么显着。 图6：Conv-2/4/6早期停止时的早期停止迭代和测试精度，当经过反复修剪和dropout训练时。虚线是在没有dropout的情况下训练的相同网络（图5中的实线）。 Conv-2的学习率为0.0003，Conv-4和Conv-6的学习率为0.0002。 图7：迭代修剪时VGG-19的测试精度（在30K，60K和112K迭代）。 这些改进表明我们的迭代修剪策略与dropout相互促进在互补的作用下。 Srivastava观察到dropout导致最终网络中的稀疏激活; dropout引起的稀疏性可能会使网络被修剪。如果是这样，针对权重的dropout技术（Wan et al。，2013）或学习每个权重的dropout概率（Molchanov等，2017; Louizos等）可以使中奖彩票更容易找到。 4 VGG 和Resnet对CIFAR10的数据在这里，我们研究了神经网络上的彩票假设，这些假设唤起了实践中使用的架构和技术。具体而言，我们考虑VGG型深度卷积网络（CIFAR10-Simonyan和Zisserman（2014）上的VGG-19）和残余网络（CIFAR10-He等人（2016）上的Resnet-18）。这些网络通过batchnorm，权重衰减，降低学习率计划和增强的训练数据进行训练。我们继续为所有这些架构找到中奖彩票; 然而，我们发现它们的方法，迭代修剪，对所使用的特定学习率很敏感。在这些实验中，我们不是测量早期停止时间（对于这些较大的网络，与学习速率计划纠缠在一起），而是在训练期间的几个时刻绘制准确度，以说明准确度提高的相对速率。 Global pruning.全局剪枝。在Lenet和Conv-2/4/6上，我们以相同的比例分别修剪每一层。对于Resnet-18和VGG-19，我们稍微修改了这个策略：我们在全局范围内修剪这些更深层的网络，在所有卷积层中共同去除最低幅度的权重。在附录I.1中，我们发现全局修剪确定了Resnet-18和VGG-19的较小中奖票。我们对此行为的推测解释如下：对于这些更深层次的网络，某些层具有比其他层更多的参数。例如，VGG-19的前两个卷积层有1728和36864个参数，而最后一个有235万个。当所有图层以相同的速率修剪时，这些较小的图层成为瓶颈，使我们无法识别最小的中奖票。全局修剪可以避免这种陷阱。 VGG-19.我们研究了Liu等人的适用于CIFAR10的变体VGG-19。 （2019）;我们使用相同的训练方案和超参数：160个epochs（112,480次迭代），其中SGD具有动量（0.9）并且在80和120个时期将学习率降低10倍。该网络有2000万个参数。图7显示了在两个初始学习率下对VGG-19进行迭代修剪和随机重新初始化的结果：0.1（在Liu等人（2019）中使用）和0.01。在较高的学习速率下，迭代修剪不能找到中奖票，并且性能并不比修剪后的网络随机重新初始化时更好。但是，在较低的学习速率下，通常的模式重新出现，子网络保持在原始精度的1个百分点内，而Pm≥3.5％。 （他们不是中彩票，因为它们与原始准确度不匹配。）当随机重新初始化时，子网络会失去准确性，因为它们与本文中的其他实验一样被修剪。尽管这些子网在训练早期学习的速度比未传播的网络快（图7左），但由于较低的初始学习速率，这种准确性优势在训练后期会逐渐消失。但是，这些子网仍然比重新初始化时学得更快。 图8：迭代修剪时Resnet-18的测试精度（在10K，20K和30K迭代）。 为了弥合较低学习率的彩票行为与较高学习率的准确性优势之间的差距，我们探讨了从0到初始学习率的线性学习率预热(warmup)对k次迭代的影响。在学习率0.1下以预热(warmup)（k = 10000，绿线）训练VGG-19可以将未剪枝网络的测试精度提高大约一个百分点。热身(warmup)可以获得中奖彩票，当Pm≥1.5％时超过此初始准确度。 Resnet-18.Resnet-18（He et al。，2016）是一个20层卷积网络，具有为CIFAR10设计的残差连接。它有271,000个参数。我们用SGD以动量（0.9）训练网络进行30,000次迭代，在20,000和25,000次迭代时将学习率降低10倍(decreasing the learning rate by a factor of 10)。图8显示了学习率0.1（在He等人（2016）中使用）和0.01的迭代修剪和随机重新初始化的结果。这些结果很大程度上反映了VGG的结果：迭代修剪以较低的学习率获得中奖彩票，但不是较高的学习率。较低学习率的最佳中奖门票的准确率（当41.7％≥Pm≥21.9％时，89.5％）在较高学习率（90.5％）下达不到原始网络的准确读。在较低的学习率下，中奖票最初学习得更快（图8的左图），但是在训练后的较高学习率（右图）下落后于未剪枝的网络。通过预热训练的中奖彩票在较高学习率下与未剪枝网络的准确性差距接近，在P m = 27.1％时，学习率0.03（预热，k = 20000）达到90.5％的测试准确度。对于这些超参数，当Pm≥11.8％时，我们仍然会获得中奖彩票。然而，即使有热身warmup，我们也无法找到超参数在我们可以用原始学习率0.1来识别中奖彩票。 5 讨论！关于神经网络修剪的现有工作（例如，Han等人（2015））证明由神经网络学习的函数通常可以用较少的参数来表示。修剪通常通过训练原始网络，移除连接和进一步调整来进行。实际上，刚开始的训练初始化修剪网络的权重，以便它可以在微调期间孤立地学习。我们试图确定类似的稀疏网络是否可以从一开始就学习。我们发现本文研究的架构可靠地包含这种可训练的子网，并且彩票假设提出该属性一般适用。我们对获奖门票的存在和性质的实证研究引发了许多后续问题。 The importance of winning ticket initialization.winning tickets 初始化的重要性。当随机重新初始化时，winning tickets学得更慢并且测试精度更低，这表明初始化对其成功很重要。这种行为的一种可能的解释是这些初始权重在训练之后接近它们的最终值 - 在最极端的情况下，它们已经被训练。然而，附录F中的实验表明相反 - 中奖票(winning tickets)权重比其他权重更进一步。这表明初始化的优势与优化算法，数据集和模型相关联。例如，the winning ticket的初始化可能落在损失情况的区域中，该区域特别适合于通过所选择的优化算法进行优化。 刘等人发现当被随机重新初始化时，被修剪的网络确实可训练，似乎与传统经验和我们的随机重新初始化实验相矛盾。举个例子，在VGG-19（我们共享相同的设置）上，他们发现网络被修剪高达80％并且随机重新初始化匹配原始网络的准确性。我们在图7中的实验证实了这种稀疏程度的这些发现（Liu等人没有提供数据）。然而，在进一步修剪之后，初始化很重要：当VGG-19被修剪高达98.5％时我们获得了winning tickets; 重新初始化时，这些winning tickets的准确度要低得多。我们假设 - 达到一定程度的稀疏性 - 高度参数化的网络可以被成功修剪，重新初始化和重新训练; 然而，超越这一点，极度修剪，不太严重的过参化的网络只能通过偶然的初始化来保持准确性。 The importance of winning ticket structure.winning tickets的参数结构。产生中奖彩票的初始化安排在特定的稀疏架构中。由于我们通过大量使用训练数据来发现中奖彩票，我们假设我们的中奖彩票的结构编码为手边的学习任务定制的归纳偏差。Cohen＆Shashua（2016）表明，嵌入在深层网络结构中的归纳偏差决定了它可以比浅层网络更有效地分离更多参数的数据类型; 虽然Cohen＆Shashua（2016）专注于卷积网络的汇集几何，但类似的效果可能与获奖门票的结构有关，即使在大量修剪时也可以学习。 ？The improved generalization of winning tickets.winning tickets的改进泛化。我们可靠地找到能够更好地泛化的中奖彩票，超过原始网络的测试精度，同时匹配其训练准确性。 测试精度随着我们的修剪而增加然后减小，形成一个Occam’s Hill（Rasmussen＆Ghahramani，2001），其中原始的，过度参数化的模型具有太多的复杂性（可能过度配置）并且极度修剪的模型太少。压缩和泛化之间关系的传统观点是紧凑的假设可以更好地泛化（Rissanen，1986）。最近的理论工作显示了神经网络的类似链接，证明了可以进一步压缩的网络的更紧密的泛化界限（Zhou等人（2018）用于修剪/量化，而Arora等人（2018）用于噪声鲁棒性）。彩票假设提供了对这种关系的补充观点 - 较大的网络可能明确地包含更简单的表示。 Implications for neural network optimization.神经网络优化的意义。Winning tickets的准确度可以达到原始未剪枝网络的准确度，但参数显着减少。这一观察结果与最近关于过度参数化在神经网络训练中的作用有关。例如，Du等人。 （2019）证明用SGD训练的足够过度参数化的双层relu网络（具有固定大小的第二层）会聚到全局最优。因此，一个关键问题是，winning ticket的存在是必须的？或者满足SGD优化，以便将神经网络优化到特定的测试精度。我们推测（但不是凭经验表明）SGD寻找并训练一个初始化良好的子网。通过这种逻辑，过度参数化的网络更容易训练，因为它们具有更多潜在中奖彩票的子网络组合。 6 缺点与未来工作我们只在较小的数据集（MNIST，CIFAR10）上考虑以视觉为中心的分类任务。我们不研究更大的数据集（即Imagenet（Russakovsky等，2015））：迭代修剪是计算密集型的，需要训练网络连续15次或更多次才能进行多次试验。在未来的工作中，我们打算探索更有效的方法来找到winning tickets，这样就可以在更加资源密集的环境中研究彩票假设。 稀疏修剪是我们找到中奖彩票的唯一方法。虽然我们减少了参数计数， 但最终的体系结构并未针对现代库或硬件进行优化。在未来的工作中，我们打算从广泛的当代文献中研究其他修剪方法，例如结构修剪（这将产生针对当代硬件优化的网络）和非大小修剪方法（可以产生较小的中奖票或更早地找到它们）。 [非结构化稀疏指的是被稀疏掉（置为0）的权重是随机分布在卷积参数矩阵中的，我们没法控制这些被稀疏掉的权重的位置。GPU的CUDA/CUDNN没法加速，具体数据可以看这里介绍的论文：https://xmfbit.github.io/2018/02/24/paper-ssl-dnn/ 但是如果是自己设计的专用硬件，是可以利用这种稀疏加速的，只是需要专门的硬件支持。] 我们获得的winning tickets具有初始化，这些初始化允许它们匹配未剪枝网络的性能，其大小对于随机初始化的网络太小以至于不能达到同样的效果。在未来的工作中，我们打算研究这些初始化的属性，这些属性与修剪后的网络架构的归纳偏差一致，使这些网络特别擅长学习。 在更深的网络（Resnet-18和VGG-19）上，迭代修剪无法找到winning tickets，除非我们用学习率预热(with learning rate warmup.)训练网络。在未来的工作中，我们计划探索为什么需要预热以及我们的识别winning tickets方案的其他改进是否可以消除对这些超参数修改的需求。 7 releated work在实践中，神经网络倾向于显着过度参数化。蒸馏（Ba＆Caruana，2014; Hinton等，2015）和修剪（LeCun等，1990; Han等，2015）依赖于可以在保持准确性的同时减少参数的事实。即使有足够的能力来记忆训练数据，网络自然也会学习更简单的功能（Zhang et al。，2016; Neyshabur et al。，2014; Arpit et al。，2017）。当代经验（Bengio et al。，2006; Hinton et al。，2015; Zhang et al。，2016）和图1表明，过度参数化的网络更容易训练。我们表明，密集网络包含稀疏子网，能够从原始初始化开始自行学习。其他几个研究方向旨在训练小型或稀疏网络。 附录A ACKNOWLEDGMENTS我们非常感谢IBM，它通过MIT-IBM Watson AI Lab为本文中的实验提供了必要的计算资源。 附录B ITERATIVE PRUNING STRATEGIES在本附录中，我们研究了构建迭代修剪策略的两种不同方法，我们在本文的主体中使用该方法来获得中奖票。 Strategy 1：Iterative pruning with resetting. Strategy 2: Iterative pruning with continued training. 这两种策略之间的区别在于，在每轮修剪之后，策略2使用已经训练的权重进行重新训练，而策略1在重新训练之前将网络权重重置回其初始值。在这两种情况下，在网络被充分修剪后，其权重将重置为原始初始化。在小尺寸，1比2更优化 图9和图10比较了我们在附录G和H中选择的超参数上Lenet和Conv-2/4/6架构的两种策略。在所有情况下，策略1保持更高的验证准确性和更快的早期停止时间网络规模。 附录C early stopping criterion在整篇文章中，我们感兴趣测量网络学习的速度。作为整个数量的代理，我们测量迭代 是 早期停止的标准将结束训练。我们使用的这个特别的标准是达到最小验证集损失的迭代次数。接下来，我们将解释这个标准。 验证和测试损失遵循一种模式，即在训练过程的早期阶段减少，达到最小值，然后随着模型过拟合对于训练数据而开始增加。图11显示了随着训练的进行，验证损失的一个例子;这些图使用Lenet，迭代修剪和Adam，学习率为0.0012。该图显示了与图3中的测试精度相对应的验证损失。 图9：使用重置和持续训练策略迭代修剪时，Lenet架构上迭代彩票实验的早期停止迭代和早期停止的准确性。 图10：使用重置和持续训练策略进行迭代修剪时，Conv-2，Conv-4和Conv-6架构上迭代彩票实验的早期停止迭代和早期停止的准确性。 图11：对应于图3的验证损失数据，即，在迭代修剪实验中针对若干不同修剪水平的训练进展的验证损失。每条线是在相同的迭代修剪水平下进行的五次训练的平均值;标签是修剪后保留的原始网络权重的百分比。每个网络都使用Adam训练，学习率为0.0012。左图显示的wining tickets比原始网络学得越快，损失也越来越低。中间的图表显示wining tickets，在达到最快的早停时间后学得越来越慢。右边的图表将wining tickets的loss与随机重新初始化网络的loss进行了对比。 Figure 11: The validation loss data corresponding to Figure 3, i.e., the validation loss as training progresses for several different levels of pruning in the iterative pruning experiment. Each line is the average of ﬁve training runs at the same level of iterative pruning; the labels are the percentage of weights from the original network that remain after pruning. Each network was trained with Adam at a learning rate of 0.0012. The left graph shows winning tickets that learn increasingly faster than the original network and reach lower loss. The middle graph shows winning tickets that learn increasingly slower after the fastest early-stopping time has been reached. The right graph contrasts the loss of winning tickets to the loss of randomly reinitialized networks. 图12：图4增加了50,000次迭代结束时训练精度的图表。 在所有情况下，验证损失最初都会下降，然后形成一个明确的底部，然后再次开始增加。我们早期停止的标准确定了这个底线。我们认为，更快到达这一时刻的网络可以“更快地”学习。为了支持这一概念，每个实验符合图3中早期停止标准的顺序与每个实验达到特定测试精度阈值的顺序相同在图3中。 在本文中，为了使这种学习速度具有背景，我们还在最小验证损失的迭代中呈现网络的测试精度。在论文的主体中，我们发现获奖门票在此时提前到达并提高了测试精度。 附录D TRAINING ACCURACY FOR LOTTERY TICKET EXPERIMENTS本附录附图4（第2节中关于MNIST的Lenet的准确性和早期停止迭代）和图5（第3节中Conv-2，Conv-4和Conv-6的准确性和早期停止迭代）在论文的主体中。这些图表显示了早期停止的迭代，早期停止的测试准确性，早期停止时的训练准确性以及训练过程结束时的测试准确性。但是，我们没有足够的空间在培训过程结束时包含培训准确性的图表，我们在本文的主体中断言，对于除了最严重修剪的网络之外的所有网络都是100％。在本附录中，我们在图12（对应于图4）和图13（对应于图5）中包括那些附加的图。正如我们在论文的主体中描述的那样，除了最经过重点修剪的网络之外，所有情况下的训练准确率都达到了100％。但是，获奖门票的培训准确率仍然比随机重新初始化的网络长100％。 附录E COMPARING RANDOM REINITIALIZATION AND RANDOM SPARSITY比较随机重新定位和随机稀疏性 在本附录中，我们的目标是了解随机重新初始化的中奖票和随机稀疏网络的相对性能。 通过使用原始初始化的迭代修剪找到网络（图14中的蓝色）。 通过迭代修剪找到的网络随机重新初始化（图14中的橙色）。 随机稀疏子网，其参数数量与通过迭代修剪找到的参数数量相同（图14中的绿色）。 图13：图5增加了训练过程结束时训练准确性的图表。 图14显示了本文中所有主要实验的比较。对于MNIST的完全连接的Lenet架构，我们发现随机重新初始化的网络优于随机稀疏性。然而，对于本文研究的所有其他卷积网络，两者之间的性能没有显着差异。我们假设MNIST的完全连接网络看到了这些好处，因为只有MNIST图像的某些部分包含有用的分类信息，这意味着网络某些部分的连接将比其他部分更有价值。对于卷积不太正确，卷积不限于输入图像的任何一部分。 附录F EXAMINING WINNING TICKETS测试wining tickets 在本附录中，我们将检查获奖门票的结构，以深入了解获奖门票为何能够有效学习，即使经过如此严格的修剪。在本附录中，我们将研究在MNIST培训的Lenet架构的获奖门票。除非另有说明，否则我们使用与第2节中相同的超参数：glorot初始化和adam优化。 F.1 WINNING TICKET INITIALIZATION (ADAM )图15显示了四个不同级别P m的中奖票证初始化的分布。为了澄清，这些是在修剪过程中幸存下来的连接的初始权重的分布。蓝色，橙色和绿色线分别显示第一个隐藏层，第二个隐藏层和输出层的权重分布。权重是从彩票实验的五个不同试验中收集的，但每个试验的分布与所有试验中的分布密切相关。直方图已经标准化，因此每条曲线下的面积为1。 附录G HYPERPARAMETER EXPLORATION FOR FULLY-CONNECTED NETWORKS本附录附有主要文件的第2部分。它探讨了第2节中评估的Lenet架构的超参数空间，并考虑了两个目的： 解释在论文正文中选择的超参数。 评估彩票实验模式扩展到其他超参数选择的程度。 G.1 实验方法本节考虑了完全连接的Lenet架构（LeCun等，1998），它在MNIST数据集上包含两个完全连接的隐藏层和一个十单元输出层。除非另有说明，否则隐藏层各有300和100个单位。 MNIST数据集由60,000个训练样例和10,000个测试示例组成。我们从训练集中随机抽取了5,000个示例验证集，并将剩余的55,000个训练样例用作本文其余部分的训练集（包括第2节）。本附录中的超参数选择实验使用验证集进行评估，以确定早期停止的迭代和早期停止的准确性;本文正文中的网络（利用这些超参数）在测试集上评估其准确性。训练集以60个例子的小批量呈现给网络;在每个时代，整个训练集都是混乱。 除非另有说明，否则每个图中的每一条线包括来自三个独立实验的数据。线本身跟踪实验的平均性能，误差条表示任何一个实验的最小和最大性能。 在本附录中，我们迭代地执行彩票实验，每次迭代的修剪率为20％（输出层为10％）;我们在本附录后面选择这种修剪率是合理的。网络的每一层都是独立修剪的。在彩票实验的每次迭代中，无论何时发生早期停止，网络都经过50,000次训练迭代的训练;换句话说，在训练过程中不考虑验证或测试数据，并且通过检查验证性能来追溯确定早期停止时间。我们每100次迭代评估验证和测试性能。 对于论文的主体，我们选择使用Adam优化器（Kingma＆Ba，2014）和Gaussian Glorot初始化（Glorot＆Bengio，2010）。虽然我们可以在其他超参数的彩票实验中获得更令人印象深刻的结果，但我们希望这些选择尽可能通用，以尽量减少我们的主要结果取决于手选超参数的程度。在本附录中，我们选择了在本文正文中使用的Adam的学习率。 此外，我们还考虑了其他各种超参数，包括其他优化算法（有和没有动量的SGD），初始化策略（具有各种标准偏差的高斯分布），网络大小（更大和更小的隐藏层）和修剪策略（更快）和较慢的修剪率）。在每个实验中，我们改变所选择的超参数，同时保持所有其他超参数的默认值（具有所选学习速率的Adam，Gaussian Glorot初始化，具有300和100单位的隐藏层）。本附录中提供的数据是通过Lenet架构的培训变体收集的3,000多次。 G.2 学习率在本小节中，我们对Lenet架构进行了彩票实验，并对Adam，SGD和SGD进行了优化，并具有各种学习率的动力。 在这里，我们选择在本文主体中用于Adam的学习率。我们选择学习率的标准如下： 在未剪枝的网络上，它应该最小化实现早期停止所需的训练迭代，并最大化该迭代的验证准确性。也就是说，即使我们没有运行彩票实验，它也应该是用于优化未剪枝网络的合理超参数。 在运行迭代彩票实验时，应该可以使用尽可能少的参数来匹配原始网络的早期停止迭代和准确性。 在满足（1）和（2）的那些选项中，它应该在保守（慢）方面，以便更有可能在各种条件下通过各种超参数有效地优化经过大量修剪的网络。 图26显示了在执行迭代彩票实验的迭代中的早期停止迭代和验证准确性，其中Lenet架构以各种学习速率与Adam优化。根据图26右侧的图表，在0.0002和0.002之间的几个学习速率在原始网络上实现了类似的验证准确度水平，并且在网络被修剪时将该性能保持在相似的水平。在这些学习率中，0.0012和0.002产生最快的早停时间并将其保持在最小的网络规模。我们选择0.0012是因为它在未净化的网络上具有更高的验证精度并且考虑到上述标准（3）。 我们注意到，在所有这些学习率中，彩票模式（其中学习变得更快并且验证准确性随着迭代修剪而增加）仍然存在。即使对于那些在50,000次迭代（2.5e-05和0.0064）内不满足早期停止标准的学习率，仍然显示出修剪精度的提高。 G.3 其他优化算法G.4 ITERATIVE PRUNING RATE 剪枝比例当在Lenet上运行迭代彩票实验时，我们以特定的速率分别修剪网络的每一层。也就是说，在训练网络之后，我们在将权重重置为其原始初始化并再次训练之前，修剪每层中k％的权重（输出层中权重的k/2％）。在本文的主体中，我们发现迭代修剪比单次修剪获得更少的中奖票，表明修剪过多的网络会立刻降低性能。在这里，我们探索k的不同值。 图29显示了每次修剪迭代修剪的网络数量对早期停止时间和验证准确性的影响。在最低修剪率（0.1和0.2）和更高的修剪率（0.4及以上）之间的早期停止学习速度和验证准确性方面存在切实差异。最低的修剪率可以达到更高的验证准确度，并将验证精度保持在较小的网络规模;它们还可以将较早的停止时间保持在较小的网络规模。对于本文的主体和本附录中的实验，我们使用0.2的修剪率，其保持0.1的准确性和学习速度的大部分，同时减少了获得较小网络规模所需的训练迭代次数。 在所有Lenet实验中，我们以网络其余部分的一半速率修剪输出层。由于输出层非常小（整个Lenet架构中的266,000个中的1,000个权重），我们发现修剪它比其他层更早到达收益递减点。 G.5 初始化分布到目前为止，我们只考虑了网络的Gaussian Glorot（Glorot＆Bengio，2010）初始化方案。图30执行彩票实验，同时从具有各种标准偏差的高斯分布初始化Lenet架构。亚当以之前选择的学习率对网络进行了优化。彩票模式继续出现在所有标准偏差中。当从具有标准偏差0.1的高斯分布初始化时，Lenet架构保持高验证准确度和较低的早停时间，最长，与Glorot初始化网络的性能大致匹配。 G.6 网络尺寸在本节中，我们考虑了Lenet架构，在第一个隐藏层中有300个单元，在第二个隐藏层中有100个单元。图31显示了Lenet架构迭代的早期停止迭代和验证准确性以及其他几个层大小。我们测试的所有网络都保持第一个隐藏层中单元与第二个隐藏层中单元之间的比例为3：1。 彩票假设自然会邀请一系列与网络规模相关的问题。一般来说，这些问题往往采取以下形式：根据彩票假设，做更大的网络，包含更多的子网，找到“更好”的中奖票？根据这个问题的一般性，有几个不同的答案。 如果我们按照其实现的准确度来评估中奖彩票，那么较大的网络会获得更好的中奖彩票。图31中的右图显示，对于任何特定数量的权重（即，x轴上的任何特定点），从最初较大的网络导出的获胜票证达到更高的准确度。换句话说，就准确性而言，线路按照网络尺寸的递增顺序从下到上大致排列。有可能的是，由于较大的网络具有更多的子网，因此梯度下降找到了更好的中奖票。或者，最初较大的网络即使在修剪到与较小网络相同数量的权重时也具有更多单元，这意味着它们能够包含最初较小网络无法表达的稀疏子网配置。 如果我们在获得早期停止所需的时间内评估中奖票，那么较大的网络就没那么大了。图31中的左图表明，通常，早期停止迭代在已经被修剪到相同数量的权重的不同初始大小的网络之间不会有很大变化。在非常接近的检查中，从最初较大的网络获得的获胜门票往往比从最初较小的网络获得的门票略微学习，但这些差异很小 如果我们根据其获得与原始网络相同精度的大小来评估中奖票，则大型网络没有优势。无论初始网络规模如何，图31中的右图显示，当获奖票据被修剪到大约9,000到15,000个权重之间时，它们会恢复到原始网络的准确性。 H 用于卷积网络的超参数探索Conv-2，Conv-4和Conv-6架构是针对CIFAR10（Krizhevsky＆Hinton，2009）数据集缩小的VGG（Simonyan＆Zisserman，2014）网络架构的变体。与VGG一样，网络由一系列模块组成。每个模块都有两层3x3卷积滤波器，后面跟一个带有步幅2的maxpool层。在所有模块之后是两个大小为256的全连接层，接着是一个大小为10的输出层;在VGG中，全连接层的大小为4096，输出层的大小为1000.与VGG一样，第一个模块每层有64个卷积，第二个模块有128个，第三个模块有256个，等等.Conv-2 ，Conv-4和Conv-6架构分别具有1,2和3个模块。 CIFAR10数据集包含50,000个32x32颜色（三通道）训练示例和10,000个测试示例。我们从训练集中随机抽取了5,000个示例验证集，并使用剩余的45,000个训练样例作为本文其余部分的训练集。在整个本附录中的超参数选择实验在验证集上进行评估，并且在测试集上评估本文主体中的实例（使用这些超参数）。训练集以60个例子的小批量呈现给网络;在每个时代，整个训练集都是混乱。 Conv-2，Conv-4和Conv-6网络使用Gaussian Glorot初始化进行初始化（Glorot＆Bengio，2010），并针对图2中指定的迭代次数进行训练。选择的训练迭代次数如此之多-pruned网络仍然可以在提供的时间内进行训练。在辍学实验中，训练迭代次数增加三倍，为退出正则化网络提供足够的时间进行训练。我们使用Adam优化这些网络，并在本附录中选择每个网络的学习速率。 H.4 迭代剪枝比例对于卷积网络架构，我们为卷积和完全连接的层选择不同的修剪速率。在Conv-2和Conv-4架构中，卷积参数占模型中参数总数的相对较小的一部分。通过更慢地修剪卷积，我们可能能够在保持性能的同时进一步修剪模型。换句话说，我们假设，如果所有层都被均匀地修剪，卷积层将成为一个瓶颈，使得更难以找到仍然能够学习的较低参数计数模型。对于Conv-6，反之亦然：由于其近三分之二的参数位于卷积层中，修剪完全连接的层可能成为瓶颈。 我们在本节中选择超参数的标准是找到修剪率的组合，允许网络达到尽可能低的参数计数，同时保持验证准确度等于或高于原始准确度和早期停止时间等于或低于原始网络。 图35显示了对Conv-2（顶部），Conv-4（中部）和Conv-6（底部）执行迭代彩票实验的结果，其具有不同的修剪率组合。 根据我们的标准，我们选择Conv-2的迭代卷积修剪率为10％，Conv-4为10％，Conv-6为15％。对于每个网络，10％到20％之间的任何比率似乎都是合理的。在所有卷积修剪率中，彩票模式继续出现。 H.5 学习率(dropout)为了训练Conv-2，Conv-4和Conv-6体系结构，我们重复了H.2节中的练习，以选择合适的学习速率。图32显示了对Conv-2（上图），Conv-4（中图）和Conv-6（下图）执行迭代彩票实验的结果，其中有丢失和Adam在各种学习速率下。受到辍学训练的网络需要更长的时间来学习，因此我们训练每个架构的迭代次数是实验的三倍而没有丢失：Conv-2的60,000次迭代，Conv-4的75,000次迭代以及Conv-6的90,000次迭代。我们以第H.4节确定的速率迭代地修剪这些网络。 在我们测试的所有学习率下，彩票模式通常都是准确的，随着网络的修剪而改进。然而，并非所有学习率都表明早停时间的减少。相反，Conv-2的学习率没有显示出其他彩票实验中所见的早停时间的明显改善。同样，Conv-4和Conv-6的学习速度更快，保持原始的早期停止时间，直到修剪到大约40％，此时早停时间稳步增加 附录I HYPERPARAMETER EXPLORATION FOR VGG-19 AND RESNET-18 ON CIFAR10CIFAR10上的VGG-19和RESNET-18的超参数探索 本附录附有第4节中的VGG-19和Resnet-18实验。它详细介绍了我们用于这些网络的修剪方案，培训制度和超参数 I.1 全局剪枝在我们使用Lenet和Conv-2/4/6架构的实验中，我们分别修剪每层中的一小部分参数（逐层修剪）。在我们使用VGG-19和Resnet-18的实验中，我们改为全球修剪;也就是说，我们共同修剪卷积层中的所有权重，而不考虑任何权重来源的特定层 图38（VGG-19）和39（Resnet-18）比较了第4节中超参数的全局修剪（实线）和分层修剪（虚线）所获得的中奖彩票。培训VGG-19时学习率0.1和预热迭代10,000，当Pm≥6.9％进行分层修剪而Pm≥1.5％进行全局修剪时，我们会获得中奖票。对于其他超参数，对于逐层修剪而言，与全局修剪相比，精度同样会下降。全球修剪也比Resnet-18的分层修剪获得更少的中奖票，但差异不如VGG-19差。 在第4节中，我们讨论了在更深层网络上进行全局修剪的有效性的基本原理。总之，这些深层网络中的层具有非常不同的参数数量（对于VGG-19尤为严重）;如果我们逐层修剪，我们推测参数较少的层会成为我们查找较小中奖票的能力的瓶颈。 改进先剪枝，再扩展","link":"/2019/05/20/剪枝网络/"},{"title":"如何使用Go编程","text":"如何使用Go编程引言一个简单Go包的开发，并介绍了用go工具来获取、 构建并安装Go包及命令的标准方式。 go 工具需要你按照指定的方式来组织代码。请仔细阅读本文档， 它说明了如何以最简单的方式来准备并运行你的Go安装。类似的视频讲解可在此处观看。 代码的组织工作空间go 工具为公共代码仓库中维护的开源代码而设计。 无论你会不会公布代码，该模型设置工作环境的方法都是相同的。 Go代码必须放在工作空间内。它其实就是一个目录，其中包含三个子目录： src 目录包含Go的源文件，它们被组织成包（每个目录[项目]都对应一个包）， pkg 目录包含包对象， bin 目录包含可执行命令。 go 工具用于构建源码包，并将其生成的二进制文件安装到 pkg 和 bin 目录中。 src 子目录通常包会含多种版本控制的代码仓库（例如Git或Mercurial）， 以此来跟踪一个或多个源码包的开发。 以下例子展现了实践中工作空间的概念： 12345678910111213141516171819202122232425bin/ streak # 可执行命令 todo # 可执行命令pkg/ linux_amd64/ code.google.com/p/goauth2/ oauth.a # 包对象 github.com/nf/todo/ task.a # 包对象src/ code.google.com/p/goauth2/ .hg/ # mercurial 代码库元数据 oauth/ oauth.go # 包源码 oauth_test.go # 测试源码 github.com/nf/ streak/ .git/ # git 代码库元数据 oauth.go # 命令源码 streak.go # 命令源码 todo/ .git/ # git 代码库元数据 task/ task.go # 包源码 todo.go # 命令源码 此工作空间包含三个代码库（goauth2、streak 和 todo），两个命令（streak 和 todo） 以及两个库（oauth 和 task）。命令和库从不同的源码包编译而来。稍后我们会对讨论它的特性。 GOPATH 环境变量GOPATH 环境变量指定了你的工作空间位置。它或许是你在开发Go代码时， 唯一需要设置的环境变量。 首先创建一个工作空间目录，并设置相应的 GOPATH。你的工作空间可以放在任何地方， 在此文档中我们使用 $HOME/work。注意，它绝对不能和你的Go安装目录相同。 （另一种常见的设置是 GOPATH=$HOME。） 12$ mkdir $HOME/work$ export GOPATH=$HOME/work 作为约定，请将此工作空间的 bin 子目录添加到你的 PATH 中： 1$ export PATH=$PATH:$GOPATH/bin 包路径标准库中的包有给定的短路径，比如 &quot;fmt&quot; 和 &quot;net/http&quot;。 对于你自己的包，你必须选择一个基本路径，来保证它不会与将来添加到标准库， 或其它扩展库中的包相冲突。 如果你将你的代码放到了某处的源码库，那就应当使用该源码库的根目录作为你的基本路径。 例如，若你在 GitHub 上有账户 github.com/user 那么它就应该是你的基本路径。 注意，在你能构建这些代码之前，无需将其公布到远程代码库上。只是若你某天会发布它， 这会是个好习惯。在实践中，你可以选择任何路径名，只要它对于标准库和更大的Go生态系统来说， 是唯一的就行。 我们将使用 github.com/user 作为基本路径。在你的工作空间里创建一个目录， 我们将源码存放到其中： 1$ mkdir -p $GOPATH/src/github.com/user 你的第一个程序要编译并运行简单的程序，首先要选择包路径（我们在这里使用 github.com/user/hello），并在你的工作空间内创建相应的包目录： 123$ mkdir $GOPATH/src/github.com/user/hellomkdir -p /Users/liulifeng/Workspaces/goWorkspaces/src/github.com/fallenk/hello 接着，在该目录中创建名为 hello.go 的文件，其内容为以下Go代码： 1234567package mainimport \"fmt\"func main() { fmt.Printf(\"Hello, world.\\n\")} 现在你可以用 go 工具构建并安装此程序了： 1$ go install github.com/user/hello !!注意，你可以在系统的任何地方运行此命令。go 工具会根据 GOPATH 指定的工作空间，在 github.com/user/hello 包内查找源码。 若在从包目录中运行 go install，也可以省略包路径： 12$ cd $GOPATH/src/github.com/user/hello$ go install 此命令会构建 hello 命令，产生一个可执行的二进制文件。 接着它会将该二进制文件作为 hello（在 Windows 下则为 hello.exe）安装到工作空间的 bin 目录中。 在我们的例子中为 $GOPATH/bin/hello，具体一点就是 $HOME/go/bin/hello。 go 工具只有在发生错误时才会打印输出，因此若这些命令没有产生输出， 就表明执行成功了。 现在，你可以在命令行下输入它的完整路径来运行它了： 12$ $GOPATH/bin/helloHello, world. 若你已经将 $GOPATH/bin 添加到 PATH 中了，只需输入该二进制文件名即可： 12$ helloHello, world. 若你使用源码控制系统，那现在就该初始化仓库，添加文件并提交你的第一次更改了。 再次强调，这一步是可选的：你无需使用源码控制来编写Go代码。 12345678$ cd $GOPATH/src/github.com/user/hello$ git initInitialized empty Git repository in /home/user/work/src/github.com/user/hello/.git/$ git add hello.go$ git commit -m &quot;initial commit&quot;[master (root-commit) 0b4507d] initial commit 1 file changed, 1 insertion(+) create mode 100644 hello.go 你的第一个库让我们编写一个库，并让 hello 程序来使用它。 同样，第一步还是选择包路径（我们将使用 github.com/user/stringutil） 并创建包目录： 123$ mkdir $GOPATH/src/github.com/user/stringutilmkdir -p /Users/liulifeng/Workspaces/goWorkspaces/src/github.com/fallenk/stringutil 接着，在该目录中创建名为 reverse.go 的文件，内容如下： 1234567891011// stringutil 包含有用于处理字符串的工具函数。package stringutil// Reverse 将其实参字符串以符文为单位左右反转。func Reverse(s string) string { r := []rune(s) for i, j := 0, len(r)-1; i &lt; len(r)/2; i, j = i+1, j-1 { r[i], r[j] = r[j], r[i] } return string(r)} 现在用 go build 命令来测试该包的编译： 1$ go build github.com/user/stringutil 当然，若你在该包的源码目录中，只需执行： 1$ go build 即可。这不会产生输出文件。想要输出的话，必须使用 go install 命令，它会将包的对象放到工作空间的 pkg 目录中。 确认 stringutil 包构建完毕后，修改原来的 hello.go 文件（它位于 $GOPATH/src/github.com/user/hello）去使用它： 1234567891011package main //向外展示的包名import ( \"fmt\" \"github.com/user/stringutil\")func main() { fmt.Printf(stringutil.Reverse(\"!oG ,olleH\"))} 来安装 hello 程序时，stringutil 包也会被自动安装。 运行此程序的新版本，你应该能看到一条新的，反向的信息： 12$ helloHello, Go! 做完上面这些步骤后，你的工作空间应该是这样的： 123456789101112bin/ hello # 可执行命令pkg/ linux_amd64/ # 这里会反映出你的操作系统和架构 github.com/user/ stringutil.a # 包对象src/ github.com/fallenk/ hello/ hello.go # 命令源码 stringutil/ reverse.go # 包源码 注意 go install 会将 stringutil.a 对象放到 pkg/linux_amd64 目录中，它会反映出其源码目录。 这就是在此之后调用 go 工具，能找到包对象并避免不必要的重新编译的原因。 linux_amd64 这部分能帮助跨平台编译，并反映出你的操作系统和架构。 Go的可执行命令是静态链接的；在运行Go程序时，包对象无需存在。 包名Go源文件中的第一个语句必须是 1package 名称 这里的 **名称** 即为导入该包时使用的默认名称。 （一个包中的所有文件都必须使用相同的 **名称**。） Go的约定是包名为导入路径的最后一个元素：作为 “crypto/rot13” 导入的包应命名为 rot13。 可执行命令必须使用 package main。 链接成单个二进制文件的所有包，其包名无需是唯一的，只有导入路径（它们的完整文件名） 才是唯一的。 共多关于Go的命名约定见 实效Go编程。 测试Go拥有一个轻量级的测试框架，它由 go test 命令和 testing 包构成。 你可以通过创建一个！名字以 _test.go 结尾的，包含名为 TestXXX 且签名为 func (t *testing.T) 函数的文件来编写测试。 测试框架会运行每一个这样的函数；若该函数调用了像 t.Error 或 t.Fail 这样表示失败的函数，此测试即表示失败。 我们可通过创建文件 $GOPATH/src/github.com/fallenk/stringutil/reverse_test.go 来为 stringutil 添加测试，其内容如下： 12345678910111213141516171819package stringutilimport \"testing\"func TestReverse(t *testing.T) { cases := []struct { in, want string }{ {\"Hello, world\", \"dlrow ,olleH\"}, {\"Hello, 世界\", \"界世 ,olleH\"}, {\"\", \"\"}, } for _, c := range cases { got := Reverse(c.in) if got != c.want { t.Errorf(\"Reverse(%q) == %q, want %q\", c.in, got, c.want) } }} 接着使用 go test 运行该测试： 12$ go test github.com/user/stringutilok github.com/user/stringutil 0.165s 同样，若你在包目录下运行 go 工具，也可以忽略包路径 12$ go testok github.com/user/stringutil 0.165s 更多详情可运行 go help test 或从 testing 包文档 中查看。 远程包像Git或Mercurial这样的版本控制系统，可根据导入路径的描述来获取包源代码。go 工具可通过此特性来从远程代码库自动获取包。例如，本文档中描述的例子也可存放到Google Code上的Mercurial仓库 code.google.com/p/go.example 中，若你在包的导入路径中包含了代码仓库的URL，go get 就会自动地获取、 构建并安装它： 123$ go get github.com/golang/example/hello$ $GOPATH/bin/helloHello, Go examples! 若指定的包不在工作空间中，go get 就会将会将它放到 GOPATH 指定的第一个工作空间内。（若该包已存在，go get 就会跳过远程获取， 其行为与 go install 相同） 在执行完上面的go get 命令后，工作空间的目录树看起来应该是这样的： 123456789101112131415161718192021bin/ hello # 可执行命令pkg/ linux_amd64/ code.google.com/p/go.example/ stringutil.a # 包对象 github.com/user/ stringutil.a # 包对象src/ code.google.com/p/go.example/ hello/ hello.go # 命令源码 stringutil/ reverse.go # 包源码 reverse_test.go # 测试源码 github.com/user/ hello/ hello.go # 命令源码 stringutil/ reverse.go # 包源码 reverse_test.go # 测试源码 hello 命令及其依赖的 stringutil 包都托管在Google Code上的同一代码库中。hello.go 文件使用了同样的导入路径约定， 因此 go get 命令也能够定位并安装其依赖包。 1import &quot;github.com/golang/example/stringutil&quot;","link":"/2019/05/15/如何使用Go编程/"},{"title":"工具命令","text":"小工具Mac下使用ssh操作远程服务器 背景Mac远程操控linux服务器，上传代码，安装环境；window版可以使用xshell 过程 安装 item2 更方便 打开终端：ssh root@xx.xxx.xxx.xxx，即 ssh 空格 用户名@目标机器IP地址，出现linux 服务器远程终端 问题:权限不足 1234567891011121314permission denied 或者authentication denied， 意思是权限不足 这里需要修改一个配置文件的值就可以通过了 vi /etc/ssh/ssh_config 点击i进入编辑状态，把PasswordAuthentication设成yes， 然后把PasswordAuthentication前面的#号去掉使之生效 然后使用命令关闭ssh服务 sudo launchctl unload -w /System/Library/LaunchDaemons/ssh.plist 然后重启，去掉un sudo launchctl load -w /System/Library/LaunchDaemons/ssh.plist 输入一个命令查看重启状态 sudo launchctl list | grep ssh 如果-0什么的一行，就是成功的 然后重新连接应该就能连上了 设置ssh生成SSHKEY;查看链接http://blog.csdn.net/xiaofei125145/article/details/30243535 上传文件–scp同理，可以使用上述远程连接的方式！基本命令： 12345678910111213141516171819202122root# scp --help usage: scp [-1246BCpqrv] [-c cipher] [-F ssh_config] [-i identity_file] [-l limit] [-o ssh_option] [-P port] [-S program] [[user@]host1:]file1 [...] [[user@]host2:]file2 -1 强制scp命令使用协议ssh1 -2 强制scp命令使用协议ssh2 -4 强制scp命令只使用IPv4寻址 -6 强制scp命令只使用IPv6寻址 -B 使用批处理模式（传输过程中不询问传输口令或短语） -C 允许压缩。（将-C标志传递给ssh，从而打开压缩功能） -p 保留原文件的修改时间，访问时间和访问权限。 -q 不显示传输进度条。 -r 递归复制整个目录。 -v 详细方式显示输出。scp和ssh(1)会显示出整个过程的调试信息。这些信息用于调试连接，验证和配置问题。 -c cipher 以cipher将数据传输进行加密，这个选项将直接传递给ssh。 -F ssh_config 指定一个替代的ssh配置文件，此参数直接传递给ssh。 -i identity_file 从指定文件中读取传输时使用的密钥文件，此参数直接传递给ssh。 -l limit 限定用户所能使用的带宽，以Kbit/s为单位。 -o ssh_option 如果习惯于使用ssh_config(5)中的参数传递方式， -P port 注意是大写的P, port是指定数据传输用到的端口号 -S program 指定加密传输时所使用的程序。此程序必须能够理解ssh(1)的选项。 使用实例：1，下载目录$scp -r root@172.30.4.42:/tmp/test2 ./将172.30.4.42linux系统中/tmp/test2目录copy到当前目录下面，在这172.30.4.42前面加了root@,提示输入密码，如果不加呢，会提示你输入用户名和密码 2，下载文件$ scp 172.30.4.42:/tmp/test2/aaa.php ./将172.30.4.42linux系统中/tmp/test2/aaa.php文件copy到当前目录下面 3，上传目录$ scp -r ./mytest 172.30.4.42:/tmp/test2将当前目录中的mytest目录上传到172.30.4.42服务器/tmp/test2目录下面。 4，上传文件$ scp ./mytest/password.php 172.30.4.42:/tmp/test2将当前目录中的mytest目录下的password.php上传到172.30.4.42服务器/tmp/test2目录下面。 使用Filezilla或者Transmit来上传文件，大文件Linux命令行记录Linux命令主要有以下9类： 系统维护及管理命令 date——显示和设置系统日期和时间setenv——查询或设置环境变量(set environment variable) 文件操作及管理命令ls——显示文件及目录find——查找文件 系统维护及管理命令kill——发送一个 signal 给某一个 processat——在指定的时间执行指令 磁盘及设备管理命令 df——检查文件系统的磁盘空间占用情况(disk free)du——显示磁盘空间的使用情况(disk usage)mount——挂载设备 用户管理命令 adduser——新增用户帐户userdel——删除用户帐号 文档操作命令 csplit——分割文件(Split a file into context-determined pieces)sort——对文件中的各行进行排序8.网络通信命令 netstat——显示网络连接、路由表和网络接口信息ifconfig——显示或设置网络设备 程序开发命令 cc——c编译link——链接 X Window管理命令 startx——启动X WindowXF86setup——图形界面下运行的配置程序 Shell程序组成：(1) 命令或Shell程序；(2) 位置参数；(3) 变量及特殊字符；(4) 表达式比较；(5) 控制流程语句，例如while，case等；(6) 函数。例：备份当前目录下的所有文件。 12345678mkdir backupfor file in ‘ls’do cp $ file backup/$ file if [$? –ne 0] then echo “copying $ file error” fidone shell预定义变量是由$符和另一个符号组成的，常用的shell预定义变量有： 1234567 $#：位置参数的数量 $*：所有位置参数的内容 $?：命令执行后返回的状态 $$：当前进程的进程号 $!：后台运行的最后一个进程号 $0：当前执行的进程名 其中，“$?”用于检查上一个命令执行是否正确(在Linux中，命令退出状态为0表示该命令正确执行，任何非0值表示命令出错)。 Windows的命令控制界面Windows命令主要有以下4类：(1) 系统信息命令time——Displays or sets the system time. 显示或设置系统时间date——Displays or sets the date. 显示或设置日期mem——Displays the amount of used and free memory in your system.driverquery——Enables an administrator to enumerate and display the list of installed device drivers as well as their properties.systeminfo——This command line tool enables an administrator to query for basic system configuration information. (2) 系统操作命令shutdown——关机runas——允许用户用其他权限运行指定的工具和程序，而不是用户当前登录提供的权限。taskkill——This command line tool can be used to end one or more processes. Processes can be killed by the process id or image name.(3) 文件系统命令copy——Copies one or more files to another location.del——删除文件mkdir——建立目录(4) 网络通信命令ping——检查网络是否能够连通netstat——显示当前正在活动的网络连接的详细信息route——主要用来管理本机路由表，可以查看，添加、修改或删除路由表条目。 123456 &amp;——同时执行多条命令，而不管命令是否执行成功。Usage：第一条命令 &amp; 第二条命令 [&amp; 第三条命令...] &amp;&amp;——同时执行多条命令，当碰到执行出错的命令后将不执行后面的命令，如果一直没有出错则一直执行完所有命令。Usage：第一条命令 &amp;&amp; 第二条命令 [&amp;&amp; 第三条命令...] ||——同时执行多条命令，当碰到执行正确的命令后将不执行后面的命令，如果没有出现正确的命令则一直执行完所有命令。Usage：第一条命令 || 第二条命令 [|| 第三条命令...] 直接在命令行输出命令Systeminfo &amp; mem 批处理 123456 @echo off mkdir test echo hello pause``` 例： @echo offmem&gt;%1\\meminfo.txtecho generate memoryinfo ok! @echo offtype %1*.txtecho type ok! @echo offmkdir testcall exam2.bat testcall exam3.bat testecho call ok!pause`","link":"/2018/05/11/工具命令/"},{"title":"第二篇博客","text":"经过一段时间的洗礼，重回github，将努力建设blog，简书，csdn，掘金。千里之行，始于足下。 博客的建立目前暂时运用github+hexo来建设。流程：github博客，本地网站hexo建立，链接 Github建立 注册github 本地设置RSA 配置fallenk.github.io hexo使用hexo地址 安装更新 npm install hexo-cli -g 快速开始 1.1 本地建立blog 12$ hexo init blog$ cd blog 以下命令都在blog目录下1.2 创建md文件$ hexo new &quot;Hello Hexo&quot;1.3 生成静态网页$ hexo generate 或者 $ hexo g1.4 开启服务$ hexo server 或者 $ hexo s可进行本地查看1.5 发布文件$ hexo deploy或者$ hexo d或者$ hexo g -d发布至网上1.6 清除缓存$ hexo clean cafe使用 安装 $ git clone https://github.com/giscafer/hexo-theme-cafe.git themes/cafe 使用主题 修改博客配置文件 _config.yml 主题属性 theme 为 cafe. 主题配置主题 themes/cafe/_config.yml文件内容参考说明配置 配置教程具体见_config.yml文件注释说明 图片托管使用七牛托管图片，等待审核再来修改 上传图片2.引用照片 ![image](你的外链地址)","link":"/2018/04/17/第二篇博客/"},{"title":"彩票假设paper阅读笔记","text":"The Lottery Ticket Hypothesis: Finding sparse, trainable neural networks 抽奖彩票假说: 寻找稀疏，可训练的神经网络 论文: THE LOTTERY TICKET HYPOTHESIS: FINDING SPARSE, TRAINABLE NEURAL NETWORKS 论文来源：ICLR 2019 论文链接：THE LOTTERY TICKET HYPOTHESIS 论文原作者：MIT CSAIL 的Jonathan Frankle 和 Michael Carbin #摘要 神经网络剪枝技术可以将训练好的神经网络参数减少90%以上，在不影响准确率的情况下，降低存储要求并提高计算性能。然而当前的经验的是: 通过剪枝产生的稀疏参数矩阵从头开始训练很困难，同样也很难提高训练性能。 作者发现标准的剪枝技术自然的揭示了子网络(wining ticket), 他们的初始化能够进行有效的训练。基于以上结果，作者提出了彩票假设: 密集、随机初始化的、前向传播的的网络包含子网络(wining tickets), 这个子网络—当进行隔离的训练时—在近乎相同的迭代次数中达到与原始网络相同的测试精度，我们发现The wining tickets超过了初始的网络：他们的连接有初始化的权值可以使训练更加高效。 ？？ !作者提出了一个算法来识别wining tickets，以及一系列来支持彩票假说和随机初始化的重要性的实验。作者持续发现the wining tickets 小于全连接和卷积网络结构的10-20%，在MNIST和CIFAR10数据集上。Above this size,？？作者发现the wining tickets学习的比原始网络更快，测试精度更高。 Introduction剪枝:消除不需要的网络的权值的技术 可以减少参数数量90%以上并不减少准确率。做这个可以减小尺寸或者训练好网络的能量消耗，使推理更加高效。然而，一个网络如果可以减小尺寸，我们为什么不训练这个更小的网络而是着重于使训练更加高效？当前的经验表明通过剪枝的网络更难训练从头开始，测试精度更低比原始网络。[训练一个剪枝后的网络从头开始 表现的效果比重新训练一个剪枝后的网络更差， 这个可能表明小容量的网络结构训练的困难性。 在重新训练时，最好是 保持初始训练阶段的联系的，经过剪枝后保留下的权值，而不是重新初始化被剪枝的层，梯度下降可以找一个好的方法，当网络初始化训练时，但不是重新初始化一些层，来重新训练他们] 考虑一个例子，在图1，我们随机抽样和训练来自基于MNIST全连接和基于CIFAR10的卷积网络的子网络。随机抽样模拟非结构化剪枝的效果。根据几个稀疏，虚线追踪在迭代中的最小验证集的损失[一个代理，网络学习的速度，我们使用迭代: 一个早期停止的标准结束训练。 我们使用的这个特殊的早期停止的标准 是 最小验证集损失在训练中]和测试精度。网络越稀疏，学习越慢，最终的准确率越低。 ppt: 背景资料: 压缩，治疗；-&gt; 剪枝； 论文讲解内容 论文 - Learning both Weights and Connections for Efficient Neural Networks作者的方法分为三个步骤： Train Connectivity: 按照正常方法训练初始模型。作者认为该模型中权重的大小表征了其重要程度 Prune Connection: 将初始模型中那些低于某个阈值的的权重参数置成00（即所谓剪枝） Re-Train: 重新训练，以期其他未被剪枝的权重能够补偿pruning带来的精度下降 为了达到一个满意的压缩比例和精度要求，2和3要重复多次","link":"/2019/05/29/彩票假设paper阅读笔记/"},{"title":"第一章Java程序设计概述","text":"Java的特性： 简单性 面向对象！ 分布式! 健壮性 安全性 体系结构中立 可移植性! 解释型! 高性能 多线程！ 动态性 第二章 Java程序设计环境JDK: 编写Java程序的程序员使用的软件，开发环境工具包 JRE：运行Java程序的用户使用的软件，运行环境 Standard Edition： SE，用于桌面或简单服务器应用的Java平台 EE：用于复杂服务器应用的Java平台 安装：库源文档和文档库源文件在JDK中以一个压缩文件 src.zip 的形式发布 ，必须将其解压缩后才能够访问源代码 。建议按照下面所的步骤进行操作。 确保JDK已经安转，并且在jdk/bin目录中执行路径汇中 在主目录中建立一个目录 javasrc 如果愿意 可以在一个终端窗口完成这个步骤 mkdir javasrc; 在jdk目录中找到src.zip 将src.zip文件解压到javasrc中 12345678910public class Welcome { public static void main(String[] args) { String greeting = \"Welcome to Core Java!\"; System.out.println(greeting); for(int i=0; i&lt;greeting.length(); i++) { System.out.println(\"=\"); } System.out.println(); }} 编写Welcome.java文件；类型名注意大小写 12javac Welcome.javajava Welcome 从命令行编译和运行一个 Java 程序 javac是一个java的编译器；将Welcome.java编译成Welcome.class; java程序启动Java虚拟机；虚拟机执行编译器放在class文件的字节码 第三章 Java的基本程序设计结构主要介绍 程序设计的基本概念(如数据类型，分支以及循环) 一个java demo12345public class FirstSample { public static void main(String[] args) { System.out.println(\"We will not use 'Hello World'\"); }} java区分大小分 main 不是Main！！ 关键字 public 访问修饰符(access modifier)；用于控制程序的其他部分对这段代码的访问级别 关键字class 表明Java程序中基本处理的单位是类，所有内容都在类中； class紧跟类名；名字必须字母开头，后跟字母和数字的组合。类名以大写字母开头，驼峰法；源代码命名必须与公共类同名！！ 正确命名后，javac FirstSample.java编译这段代码得到一个包含这个类字节码的文件。Java编译器将字节码文件自动命名为FirstSample.class，同一目录下。 运行 java已编译程序java FirstSample; Java虚拟机将从指定类中的main方法开始执行(方法即java中的函数)； Java 中的所有函數都属于某个类的方法 （ 标准术语将其称为方法而不是成员函数 ）。因此 Java 中的 main 方法必须有一个外壳类。Java 中的 main 方法必须是静态的 最后关键字 void 表示这个方法没有返回值。如果 main 方法正常退出 ， 那么 Java 应用程序的退出代码为 0，表示成功地运行了程序。如果希望在终止程序时返回其他的代码，那就需要调用System.exit方法 使用括号{} 来划分程序的各个部分；一对大括号表示方法体的开始与结束。 在上面这个 main 方法体中只包含了一条语句 其功能是 将一个文本行输出到控制台上；使用了 System . out 对象并调用了它的 println 方法。点号(.)用于调用方法。等价于函数调用。object.methond(parameters) 数据类型java是强类型语言；意味着必须为每个变量申明一种类型。共有8中基本类型； 4种整型(int 4字节 刚过20亿；short 2字节； long 8字节； byte 1字节-128-127)； 2种浮点类型； float 4字节； double 8字节 ！！！； 1种用于Unicode编码的字符单元的字符类型char； 单个字符，单引号括起来； 转义序列 \\u； 从 \\u0000 到\\uffff; 转义序列 \\ u 还可以出现在加引号的字符常量或字符串之外public static void main(String\\u005B\\u005D args ) 注释中 \\u 替换成特殊字符； 回车，换行 最好将字符串作为抽象数据类型处理 1种用于表示真值的boolean类型； false 或 true； 不能和整型转换 还有一个算术包， 大数值，是一个Java对象，不是类型 变量java中， 每个变量都有一个类型(type)；int i; double salary;变量的声明尽可能地靠近变量第一次使用的地方 $ 是一个合法的Java字符，但不要用，只用在Java编译器中。 变量初始化： 声明变量， 初始化； int vacationDays = 12; 常量: 使用 final 表示常量。 final double PI = 3.14; 该变量只赋值一次，不能再次更改了。全部大写。const是保留字； 使用final 定义常量；希望某个常量在一个类中多个方法中使用，称为类常量。使用 static final设置一个类常量。 static final CM_PI = 3.14; 运算符-; +; /; %; 整数被0产生异常；浮点数被除0得到无穷大或者NaN; 数学函数与常量在Math类中；Math.sqrt(); print方法处理System.out对象；Math类中sqrt方法处理不是对象是静态方法。 double y = Math.pow(x, a); floorMod 方法的目的是解决一个长期存在的有关整数余数的问题； 考虑表达式 n % 2 ， 所有人都知道 ， 如果 n 是偶数; 这个表达式为 0 ; 如果 n 是奇数 表达式则为 1 当然 除 非 n 是负数 如果 n 为负，表达式为-1. 数值类型之间的转换经常需要将一种数值类型转换为另一种数值类型。图 3 - 1 给出了数值类型之间的合法转换 有 6 个实心箭头 ， 表示无信息丢失的转换 ； 有 3 个虚箭头 ， 表示可能有精度 损失的转换 强制类型转换int 类型的值将会自动地转换为 double 类型。 double x = 9.997; int nx = (int) x; x += 4; (一般地 ， 要把运算符放在 = 号左边 如 *= 或 ％= ) 123456int m = 7 ; int n = 7 ;int a = 2 * ++m ; // now a is 16 , m is 8int b = 2 * n++ ; // now b is 14 , n is 8expression1&amp;&amp;expression2； 短路效果condition？expression1：expression2 在表达式中，前缀形式会先完成加 1 ; 而后缀形式会使用变量原来的值; 单独使用没有区别。 位运算符处理整型时，可以直接对组成整型数值的各个位完成操作 123456789101112131415&amp;(\"and\") 按位与运算符，无短路效果 |(\"or\") 参加运算的两个对象，按二进制位进行“或”运算。^(\"xor\") 异或运算符；运算规则：0^0=0；0^1=1；1^0=1；1^1=0；应位为“异”（值不同），则该位结果为1“异或运算”的特殊作用：（1）使特定位翻转找一个数，对应X要翻转的各位，该数的对应位为1，其余位为零，此数与X对应位异或即可。例：X=10101110，使X低4位翻转，用X ^ 0000 1111 = 1010 0001即可得到。（2）与0相异或，保留原值 ，X ^ 0000 0000 = 1010 1110。~(\"not\") 取反运算符；按二进制位进行“取反”运算。使一个数的最低位为零，可以表示为：a&amp;~1。&lt;&lt; 左移运算符；将一个运算对象的各二进制位全部左移若干位（左边的二进制位丢弃，右边补0）。左移1位后a = a * 2; &gt;&gt; 操作数每右移一位，相当于该数除以2。int fourthBit操作数每右移一位，相当于该数除以2。FromRight = ( n &amp; ( 1 « 3 ) ) » 3 ;&gt;&gt;&gt; 无符号右移运算符； 运算符0填充高位; &gt;&gt; 用符号位填充高位 不存在 &lt;&lt;&lt; 枚举类型可以自定义枚举类型 。 1234enum Size { SMALL , MEDIUM , LARGE , EXTRA LARCE } ;//现在 ，可以声明这种类型的变量Size s = Size.MEDIUM;//Size 类型的变量只能存储这个类型声明中给定的某个枚举值 或者 null 值 字符串注意:String 类是不可改变的，所以你一旦创建了 String 对象，那它的值就无法改变了。 从概念上讲，Java字符串是Unicode字符串序列。例如，”Java\\u2122”由5个字符Unicode字符J、a、v、a和”TM”特殊字符。无内置字符串类型；在类库中提供预定义类，叫String。 每个用双引号括起来的字符串都是Strin类的一个实例。 子串，String类中的substring方法可以提取子串。[ );substring 的工作方式有一个优点 容易计算子串的长度 字符串 s . substring ( a , b ) 的长度为 b - a。子串 “ Hel ” 的长度为 3 - 0 = 3 拼接允许使用+号连接 （ 拼接 ） 两个字符串。当将一个字符串与一个非字符串的值进行拼接时,后者被转换成字符串。 不可变字符串String 类没有提供用于修改字符串的方法。如果希望将 greeting 的内容修改为 “ Help ! ”，不能直接地将 greeting 的最后两个位置的字符修改为’p’和!’. greeting = greeting.substring(0, 3)+”p!”; 由于不能修改 Java 字符串中的字符 ，所以在 Java 文档中将 String类对象称为不可变字符串. 如同数字 3 永远是数字 3一样， 字符串”hello”包含字符h,e,l,l,o代码单元序列，不能修改其中的任何一个字符。 可以修改字符串变量greeting，让他引用另外一个字符串。 看起来好像修改一个代码单元要比创建一个新字符串更 加简洁，对，也不对。通过拼接 “ Hel ” 和 p ! ” 来创建一个新字符串的效率确实不高。但是 ，不可变字符串却有一个优点 ：编译器可以让字符串共享 。 为了弄清具体的工作方式，可以想象将各种字符串存放在公共的存储池中。字符串变量指向存储池中相应的位置，如果复制一个字符串变量，原始字符串与复制的字符串共享相同的字符。 Java 字符串大致类似于 char * 指针 ，不是认为是字符型数组；greeting = “ess”; 指向新串时，Java自动回收垃圾 检测字符串相等使用equals方法； s.quals(t); 或 “Hello”.equals(s); 而不区分大小写 ，可以使用 equalsIgnoreCase方法 空串与NULL串空串 “” 是长度为 0 的字符串；可以调用以下代码检查一个字符串是否为空 ： if(str.length() == 0)或者if(str.equals(“”)); 空串是一个Java对象，有自己的串长度(0)和内容(空)。 String变量还可以存一个特殊的值，名为null，表示目前没有任何对象与该变量关联。if(str==null) 有时要检查一个字符串既不是null也不为空串，需要使用以下： if(str != null &amp;&amp; str.length() != 0) ; 在null值上调用方法，会出现错误。 码点与代码单元码点 （ code point ) 是指与一个编码表中的某个字符对应的代码值。 在Unicode标准中，码点采用16进制书写，加上前缀U+,例如U+0041为拉丁字母A的码点。Unicode的码点可以分成17个代码级别。第一个代码级别，称为多语言级别，从U+0000到U+FFFF;其余从U+10000到U+10FFFF,包括辅助字符串;两个代码单元。 UTF 16 编码采用不同长度的编码表示所有 Unicode 码点。在基本的多语言级别中，每个字符用 16 位表示，称为代码单元，而辅助单元采用一对连续的代码单元进行编码。如八元数集。 char 类型描述了 UTF-16 编码中的一个代码单元。 Java 字符串由 char 值序列组成。从 3.3 . 3 节 “ char 类型 ” 已经看到，char数据类型是一个采用UTF - 16 编码表示 码点的代码单元。大多数的常用Unicode使用一个代码单元就可以表示，而辅助字符需要一对代码单元表示。length 方法将返回采用 UTF 16 编码表示的给定字符串所需要的代码单元数量 。 12345678910111213141516171819public class StringTest { public static void main(String[] args) { String greeting = &quot;Hello&quot;; int n = greeting.length(); System.out.println(n); // 字符串所需要的代码单元数量 // 想要得到实际的长度，即码点数量，可以调用: int cpCount = greeting.codePointCount(0, greeting.length()); System.out.println(cpCount); // 调用 s.charAt(n) 将返回位置 n 的代码单元，n 介于0~s.length()-1之间 char first = greeting.charAt(0); System.out.println(first); char last = greeting.charAt(4); System.out.println(last); // 想得到第i个码点 int index = greeting.offsetByCodePoints(0, 4); int cp = greeting.codePointAt(index); System.out.println(cp); }} String APIString类中包含了50多个方法。都有用。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122 java.lang.String char charAt (int index) 返回index所指定的代码单元(字符1：1； 除了辅助字符2个) String concat(String str) 将两字符串连接 boolean endsWith(String str) 测试字符串是否以str结尾 boolean equals(Object obj) 比较两对象 char[] getBytes 将字符串转换成字符数组返回 char[] getBytes(String str) 将指定的字符串转成制服数组返回 boolean startsWith(String str) 测试字符串是否以str开始 int length() 返回字符串的长度 String replace(char old ,char new) 将old用new替代 char[] toCharArray 将字符串转换成字符数组 String toLowerCase() 将字符串内的字符改写成小写 String toUpperCase() 将字符串内的字符改写成大写 String valueOf(Boolean b) 将布尔方法b的内容用字符串表示 String valueOf(char ch) 将字符ch的内容用字符串表示 String valueOf(int index) 将数字index的内容用字符串表示 String valueOf(long l) 将长整数字l的内容用字符串表示 String substring(int1,int2) 取出字符串内第int1位置到int2的字符串 1.构造方法//直接初始化String str = \"abc\";//使用带参构造方法初始化char[] char = {'a','b','c'};String str1 = new String(\"abc\");String str2 = new String(str);String str3 = new String(char);2.求字符串长度和某一位置字符String str = new String(\"abcdef\");int strlength = str.length();//strlength = 7char ch = str.charAt(4);//ch = efor (int i = 0; i &lt; greeting.length(); i++) { System.out.println(greeting.charAt(i)); } char[] cArray=greeting.toCharArray(); for(char cc:cArray){ }3.提取子串用String类的substring方法可以提取字符串中的子串，该方法有两种常用参数:1)public String substring(int beginIndex)//该方法从beginIndex位置起，从当前字符串中取出剩余的字符作为一个新的字符串返回。2)public String substring(int beginIndex, int endIndex)//该方法从beginIndex位置起，从当前字符串中取出到endIndex-1位置的字符作为一个新的字符串返回。作者：HCherisherString str1 = new String(\"abcdef\");String str2 = str1.substring(2);//str2 = \"cdef\"String str3 = str1.substring(2,5);//str3 = \"cde\"4.字符串比较1)public int compareTo(String anotherString)//该方法是对字符串内容按字典顺序进行大小比较，通过返回的整数值指明当前字符串与参数字符串的大小关系。若当前对象比参数大则返回正整数，反之返回负整数，相等返回0。2)public int compareToIgnoreCase(String anotherString)//与compareTo方法相似，但忽略大小写。3)public boolean equals(Object anotherObject)//比较当前字符串和参数字符串，在两个字符串相等的时候返回true，否则返回false。4)public boolean equalsIgnoreCase(String anotherString)//与equals方法相似，但忽略大小写。作者：HCherisherString str1 = new String(\"abc\");String str2 = new String(\"ABC\");int a = str1.compareTo(str2);//a&gt;0int b = str1.compareToIgnoreCase(str2);//b=0boolean c = str1.equals(str2);//c=falseboolean d = str1.equalsIgnoreCase(str2);//d=true5.字符串链接public String concat(String str)//将参数中的字符串str连接到当前字符串的后面，效果等价于\"+\"String str = \"aa\".concat(\"bb\").concat(\"cc\");//相当于String str = \"aa\"+\"bb\"+\"cc\";6.字符串中单个字符查找1)public int indexOf(int ch/String str)//用于查找当前字符串中字符或子串，返回字符或子串在当前字符串中从左边起首次出现的位置，若没有出现则返回-1。2)public int indexOf(int ch/String str, int fromIndex)//改方法与第一种类似，区别在于该方法从fromIndex位置向后查找。3)public int lastIndexOf(int ch/String str)//该方法与第一种类似，区别在于该方法从字符串的末尾位置向前查找。4)public int lastIndexOf(int ch/String str, int fromIndex)//该方法与第二种方法类似，区别于该方法从fromIndex位置向前查找作者：HCherisherString str = \"I really miss you !\";int a = str.indexOf('a');//a = 4int b = str.indexOf(\"really\");//b = 2int c = str.indexOf(\"gg\",2);//c = -1int d = str.lastIndexOf('s');//d = 6int e = str.lastIndexOf('s',7);//e = 77.大小写转换1)public String toLowerCase()//返回将当前字符串中所有字符转换成小写后的新串2)public String toUpperCase()//返回将当前字符串中所有字符转换成大写后的新串8.字符串中字符的替换1)public String replace(char oldChar, char newChar)//用字符newChar替换当前字符串中所有的oldChar字符，并返回一个新的字符串。2)public String replaceFirst(String regex, String replacement)//该方法用字符replacement的内容替换当前字符串中遇到的第一个和字符串regex相匹配的子串，应将新的字符串返回。3)public String replaceAll(String regex, String replacement)//该方法用字符replacement的内容替换当前字符串中遇到的所有和字符串regex相匹配的子串，应将新的字符串返回。String str = \"asdzxcasd\";String str1 = str.replace('a','g');//str1 = \"gsdzxcgsd\"String str2 = str.replace(\"asd\",\"fgh\");//str2 = \"fghzxcfgh\"String str3 = str.replaceFirst(\"asd\",\"fgh\");//str3 = \"fghzxcasd\"String str4 = str.replaceAll(\"asd\",\"fgh\");//str4 = \"fghzxcfgh\"9.其他方法1)String trim()//截去字符串两端的空格，但对于中间的空格不处理。String str = \" a bc \";String str1 = str.trim();int a = str.length();//a = 6int b = str1.length();//b = 42)boolean statWith(String prefix)或boolean endWith(String suffix)//用来比较当前字符串的起始字符或子字符串prefix和终止字符或子字符串suffix是否和当前字符串相同，重载方法中同时还可以指定比较的开始位置offset。作者：HCherisherString str = \"abcdef\";boolean a = str.statWith(\"ab\");//a = trueboolean b = str.endWith(\"ef\");//b = true3)contains(String str)//判断参数s是否被包含在字符串中，并返回一个布尔类型的值。String str = \"abcdef\";str.contains(\"ab\");//truestr.contains(\"gh\");//false4)String[] split(String str)//将str作为分隔符进行字符串分解，分解后的字字符串在字符串数组中返回。String str = \"abc def ghi\";String[] str1 = str.split(\" \");//str1[0] = \"abc\";str1[1] = \"def\";str1[2] = \"ghi\";10.类型转换字符串转基本类型java.lang包中有Byte、Short、Integer、Float、Double类的调用方法：public static byte parseByte(String s)public static short parseShort(String s)public static short parseInt(String s)public static long parseLong(String s)public static float parseFloat(String s)public static double parseDouble(String s)int n = Integer.parseInt(\"12\");float f = Float.parseFloat(\"12.34\");double d = Double.parseDouble(\"1.124\");//将char '8' 转换为int 8String str = String.valueOf('8');String str = String.valueOf(8.1);int num = Integer.parseInt(str);进制转换使用Long类中的方法得到整数之间的各种进制转换的方法：Long.toBinaryString(long l)//二进制Long.toOctalString(long l)//十进制Long.toHexString(long l)//十六进制Long.toString(long l, int p)//p作为任意进制 1234567String a = \"Hello World!\"; String b = \"Hello World!\"; String c = new String(\"Hello World!\"); String d = \"Hello\"+\" \"+\"World!\";System.out.println(a == b);//trueSystem.out.println(a == c);//falseSystem.out.println(a == d);//true 首先String不属于8种基本数据类型，String是一个对象。因为对象的默认值是null，所以String的默认值也是null；但它又是一种特殊的对象，有其它对象没有的一些特性。 在这里，我们先不谈堆，也不谈栈，只先简单引入常量池这个简单的概念。常量池(constant pool)指的是在编译期被确定，并被保存在已编译的.class文件中的一些数据。它包括了关于类、方法、接口等中的常量，也包括字符串常量。 Java会确保一个字符串常量只有一个拷贝。因为例子中的a和b都是字符串常量，它们在编译期就被确定了，所以a==b为true；而”Hello”和” “以及”World!”也都是字符串常量，当一个字符串由多个字符串常量连接而成时，它自己肯定也是字符串常量，所以d也同样在编译期就被解析为一个字符串常量，所以d也是常量池中”Hello World!”的一个引用。所以我们得出a==b==d;用new String() 创建的字符串不是常量,不能在编译期就确定，所以new String()创建的字符串不放入常量池中，它们有自己的地址空间。 存在于.class文件中的常量池，在运行期被JVM装载，并且可以扩充。String的intern()方法就是扩充常量池的一个方法；当一个String实例str调用intern()方法时，Java查找常量池中是否有相同Unicode的字符串常量，如果有，则返回其的引用，如果没有，则在常量池中增加一个Unicode等于str的字符串并返回它的引用，看例3就清楚了。 12345678910String a = \"Hello\"; String b = new String(\"Hello\"); String c = new String(\"Hello\"); System.out.println( a == b );//falseSystem.out.println( “**********” ); b.intern(); c = c.intern(); //把常量池中\"Hello\"的引用赋给c System.out.println( a == b);//false虽然执行了b.intern()但没有赋值给bSystem.out.println( a == b.intern() );//true System.out.println( a == c ); //true String是不可变的 :这一说又要说很多，大家只要知道String的实例一旦生成就不会再改变了，比如说：String str=”aa”+”bb”+” “+”cc”;就是有4个字符串常量，首先”aa”和”bb”生成了”aabb”存在内存中，后”aabb”又和” “ 生成 ”aabb “存在内存中，最后又和生成了”aabb cc”，并把这个字符串的地址赋给了str,就是因为String的“不可变”产生了很多临时变量，这也就是为什么建议用StringBuffer的原因了 三、StringBuffer和StringBuiler我们对String、StringBuffer、StringBuiler先有一个简单的认识。String是不可变字符串，StringBuffer和StringBuilder是长度可变的字符串，区别是前者是线程安全的，后者是线程不安全的，同样后者的效率也会更高。 StringBuffer 上的主要操作是 append 和 insert 方法，可重载这些方法，以接受任意类型的数据。每个方法都能有效地将给定的数据转换成字符串，然后将该字符串的字符追加或插入到字符串缓冲区中。append 方法始终将这些字符添加到缓冲区的末端；而 insert 方法则在指定的点添加字符。 例如，如果 z 引用一个当前内容为 “start” 的字符串缓冲区对象，则此方法调用 z.append(“le”) 会使字符串缓冲区包含 “startle”，而 z.insert(4, “le”) 将更改字符串缓冲区，使之包含 “starlet”。 通常，如果 sb 引用 StringBuilder 的一个实例，则 sb.append(x) 和 sb.insert(sb.length(), x) 具有相同的效果。 还有delete删除方法deleteCharAt(int index)delete(int start ,int end) 构建字符串每次连接字符串，需要重新生成新的String对象，耗时，浪费空间。可以使用StringBuilder 如 许多小段字符串构建成一个字符串， 首先，构建一个空 的字符串构建器。 123456StringBuilder builder = new String StringBuilder();// 每次添加一个部分内容，就需要调用append方法。builder.append(ch); // 添加一个字符builder.append(str); //appends a string// 需要构建字符串时，就调用toString方法，得到一个String对象。String completeString = builder.toString(); StringBuffer是线程安全的，而StringBuilder是非线程安全的。StringBuilder是从JDK 5开始，为StringBuffer类补充的一个单线程的等价类。我们在使用时应优先考虑使用StringBuilder，因为它支持StringBuffer的所有操作，但是因为它不执行同步，不会有线程安全带来额外的系统消耗，所以速度更快。 二、String为什么不可变 虽然String、StringBuffer和StringBuilder都是final类，它们生成的对象都是不可变的，而且它们内部也都是靠char数组实现的，但是不同之处在于，String类中定义的char数组是final的，而StringBuffer和StringBuilder都是继承自AbstractStringBuilder类，它们的内部实现都是靠这个父类完成的，而这个父类中定义的char数组只是一个普通是私有变量，可以用append追加。因为AbstractStringBuilder实现了Appendable接口。 1234567891011121314151617181920- StringBuilder()构造一个空的字符串构建器 。- int length ( ) 返回构建器或缓冲器中的代码单元数量 。 - StringBui1der append(String str)追加一个字符串并返回 this 。- StringBui1der append ( char c )追加一个代码单元并返回 this 。 - StringBui1der appendCodePoint(int cp) 追加一个代码点 并将其转换为一个或两个代码单元并返回 this - void setCharAt (int i , char c) 将第i个代码单元设置为 c 。 - StringBui1der insert(int offset , String str) 在 offset 位置插入一个字符串并返回 this 。 - StringBuilder insert ( int offset , Char c ) 在 offset 位置插入一个代码单元并返回 this 。 - StringBui1der delete (int startindex , int endlndex) 删除偏移量从 startindex 到 endlndex - 1 的代码单元并返回 this 。 - String toString () 返回一个与构建器或缓冲器内容相同的字符串 输入输出读取输入 读取 标准输入流; System.in 主语是主机 12345678910111213// 首先 构造一个Scanner 对象，并且与 \"标准输入流\" System.in关联Scanner in = new Scanner(System.in);//现在 就可以使用 Scanner类的各种方法实现输入操作了 例如 nextLine方法将输入一行System.out.print(\"What is your name? \");String name = in.nextLine(); // 使用 nextLine 方法是因为在输人行中有可能包含空格// 要想读取一个单词 （ 以空白符作为分隔符) 就调用String firstName = in.next();// 要想读取一个整数 ，就调用nextlnt 方法System.out.print(\"How old are you? \");int age = in.nextInt();// 要想读取下一个浮点数 调用nextDouble// 添加库 import java.util.*;// Scanner 类定义在 java . util 包中 当使用的类不是定义在基本 java . lang 包中时 一定要使用import 指示字将相应的包加载进来 1234567891011import java.util.*;public class InputTest { public static void main(String[] args) { Scanner in = new Scanner(System.in);// System.out.println(\"What's your name? \"); String name = in.nextLine();// System.out.println(\"How old are you?\"); int age = in.nextInt(); System.out.println(\"your name: \"+ name + \", your age: \"+age); }} 1234567891011121314// 用给定的输人流创建一个 Scanner 对象- Scanner(InputStream in)// 读取输入的下一行内容- String nextLine()// 读取输入的下一个单词 （ 以空格作为分隔符)- String next()// 读取下一个整型- int nextInt()// 读取并转换下一个表示整数或浮点数的字符序列- double nextDouble()// 检测输人中是否还有其他单词- boolean hasNext ( )- boolean hasNextInt ( )- boolean hasNextDouble ( ) 格式化输出可以使用System.out.print(x) 将数值 x 输出到控制台上 123System.out.printf(\"%8.2f\", x);System.out.printf(\"Hello, %s. Next year, you will be %d\", name, age);System.out.printf(\"%,.2f\", 10000.0/3.0); 3,333.33 每一个以%字符开始的格式说明符都用相应的参数替换。格式说明符尾部的转换符将指示被 格式化的数值类型。f 表示浮点数;s 表示字符串;d表示十进制整数。还可以给出控制格式化输出的各种标志. 可以使用 s 转换符格式化任意的对象。对于任意现了 Formattable 接口的对象都 将调用 formatTo 方法 否则将调用 toString 方法 它可以将对象转换为字符串。 可以使用静态的String.format方法创建一个格式化的字符串，而不打印输出: 1String message = String.format(\"Hello, %s. Next Year you will be %d\", name, age+1); 文件输入与输出想要对文件进行读写，需要一个File对象构造一个Scanner对象，如下： 1Scanner in = new Scanner(Paths.get(\"myfile.txt\"),\"UTF-8\"); 现在介绍前面的任何一个Scanner 方法对文件进行读写。 要想写文件，使用 PrintWriter对象。在构造器中，只需要提供文件名: 1PrintWriter out = new PrintWriter(\"myfile.txt\", \"UTF-8\"); 如文件不存在，则创建文件。 1234- Scanner(File f) // 构造一个从给定文件读取数据的Scanner- Scanner(String data) // 构造一个从给定字符串读取的数据的Scanner- PrintWriter(String fileName) // 构造一个将数据写入文件的PrintWriter。文件名由参数指定- static Path get(String pathname) // 根据给定的路径名构造一个Path 控制流程使用条件 和循环语句。 块作用域块(Block)概念: 复合语句，由一对大括号括起来的若干条简单的Java语句。确定变量的作用域，可嵌套，但嵌套的两个块中不能声明相同变量。C++可重定义一个变量。 条件/ 循环123456789if (condition) { statement1; statement2;}while(condition) { statement1; statement2;} 确定循环for 循环语言， 迭代。 12345678910111213for(int i = 0; i &lt;= 10; i++) {}read_data: while (n == 1) { for (int i = 0; i&lt;n; i++) { for (int j= 0; j&lt;2; j++) { break read_data; } } }标签必须放在希望跳出的最外层循环之前,并且必须紧跟一个冒号 for 语句的第 1 部分通常用于对计数器初始化;第 2 部分给出每次新一轮循环执行前要检测的循环条件; 第 3 部分指示如何更新计数器 大数值基本的整数和浮点数精度不足，可使用java.math的两个类：BigInteger和BigDecimal。可处理包含任意长度数字序列的整数，浮点数。 使用静态的 valueOf 方法可以将普通的数值转换为大数值： 不能使用算术运算符(+和*)处理，需要使用大数值类中的add和multiply方法。 123BigInteger a = BigInteger.valueOf(100);BigInteger c = a.add(b);BigInteger d = c.multiply(b.add(BigInteger.valueOf(2))); Java中没有运算符重载的功能。不能重载/ *等运算符； Java只为字符串的连接重载了+运算符。 12345BigInteger add(BigInteger other);BigInteger divide(BigInteger other);BigInteger mod(BigInteger other);int CompareTo(BigInteger other);static BigInteger valueOf(long x); 数组数组是一种数据结构，用来存储同一类型值的集合。通过以下整型下标可以方位数组中的每一个值。例如，a是一个整形数组，a[i]就是数组的中下标为i的整数。 创建整型数组，所有元素初始化为0；所有boolean初始化为false；对象数组为null 字符串长度用string.length(); 数组用array.length; 使用new和{}初始化数组 在声明数组变量时，需要指出数组类型(数据元素类型紧跟[])和数组变量名。 123456789101112int[] a; //该语句只声明了变量a，没有讲a初始化一个真正的数组，应使用newint[] a = new int[100]; // 数组长度不要求是常量 newint [ n ] 会创建一个长度为 n 的数组int[] a = new int[100];for(int i =0; i&lt;100; i++) { a[i] = i;}// String[] names = new String[10];for(int i =0; i&lt;10; i++) names[i] = \"\";for(int i =0; i&lt;names.length; i++) { System.out.println(a[i]);} 一旦创建了数组，就不能再改变大小了；如果要在运行过程中扩展数组大小。就应该使用另一种数据结构–数组列表。 12345678910111213for each循环使用 //这个循环应该读作 “ 循环 a 中的每一个元素 ” （ for each element in a )for (variable: collection) statementfor(int element : a) System,out.println(element); int[] a = new int[10]; for (int i = 0; i&lt; 10; i++) { a[i] = i; } for (int x : a) { System.out.print(x+\" \"); } System.out.println(Arrays.toString(a)); 数组初始化以及匿名数组 在Java中，提供了一种创建数组对象并同时赋予初始化的简化书写形式； 123456int[] smallPrimes = {1, 2,3, 4,5,6}; 还可以匿名初始化数组:new int[] {1,2, 3,54,6}smallPrimes = new int[] {1, 3, 5};这是下列语句的简写形式：int[] anoymous = {1,2,4}; 这种表示法将创建一个新数组并利用括号中提供的值进行初始化,数组的大小就是初始值的个数。使用这种语法形式可以在不创建新变量的情况下重新初始化一个数组。 数组拷贝 在 Java 中，允许将一个数组变量拷贝给另一个数组变量 这时 两个变量将引用同一个数组: 1234567int[] luckyNumbers = new int{1, 3,34};int[] badLucky = luckyNumbers;badLucky[2] = 21;//如果希望将一个数组的所有值拷贝到一个新的数组中去,使用Arrays.copyOfint[] copiedLuckyNumbers = Arrays.copyOf(luckyNumbers,luckyNumbers.length) // 第 2 个参数是新数组的长度,这个方法通常用来增加数组的大小 ：数组元素是数值型,0;boolean-&gt;false 数组排序 使用Arrays.sort(); Arrays的API使用。 多维数组多维数组将使用多个下标访问数组元素， 适用于表示表格或更加复杂的排列形式 初始化方式一： 在 Java 中 声明一个二维数组相当简单，与一维数组一样,在调用new对多维数组进行初始化之前不能使用它,在这里可以这样初始化 12double[][] balances;balances = new double[NYEARS][NRATES]; 初始化方式二：另外, 如果知道数组元素 就可以不调用 new, 而直接使用简化的书写形式对多维数组进行初始化 。 123456int[][] magicSquare = { {1, 2, 4, 5}, {1, 2, 4, 5}, {1, 2, 4, 5}, {1, 2, 4, 5},}; 一旦数组初始化，可以利用balances[i][j]访问 for each 不能自动处理二维数组的每一个元素，他是按照行处理，也是一维数组。需要使用嵌套循环。 1234567for (double[] row : a) { for (doule index : row) { do }}//要想快速地打印一个二维数组的数据元素列表Arrays.deepToString(a); 不规则数组Java实际上没有多维数组。只有一维数组；多维数组可称为“数组的数组”。在前面的示例中， balances 数组实际上是一个包含10 个元素的数组 ，而每个元素又是一个由 6个浮点数组成的数组。 表达式 balances[i]引用第 i 个子数组，也就是二维表的第 i 行，它本身也是一个数组。由于可以单独地存取数组的某一行，所以可以让两行交换。还可以方便地构造一个不规则数组，即数组的每一行有不同的长度，下面是一个典 型的示例。","link":"/2019/10/22/第一章Java程序设计概述/"},{"title":"算法40讲数组和链表","text":"Array一块连续的内存地址 通过下标访问O(1) Access: O(1)Insert: 平均 O(n)Delete: 平均 O(n) Linked List -&gt; 改善数组 head-&gt; L-&gt; O -&gt; G Doubly Linked Listspace O(n)prepend O(1)append O(1)lookup O(n)insert O(1)delete O(1) 实战题目三分学，七分练，坚持去做！！！ LeetCode 206 反转指针 LeetCode 24 swap linked list LeetCode 141 Linked List Cycle 硬做 0.5s 记录时间 set，记录节点，判重 O(n) 快慢指针 O(n) https://leetcode.com/problems/linked-list-cycle-ii https://leetcode.com/problems/reverse-nodes-in-k-group 链表题目多多记录公式，固定套路； 开拓思维！！！！！多种解法！！去练习，刻意练习！！","link":"/2019/11/19/算法40讲数组和链表/"},{"title":"论文阅读-RandWiredNN","text":"论文阅读-RandWiredNNRandWireNN 基本思想是研究设计stochastic network generator，也就是设计网络构架的机制，它的关注点在网络的连接方式上。论文作者引入了一种网络模型空间的构造方法，即图论中的random graph，之后用grid search搜索出较好的神经网络子集，并在ImageNet的1000-class分类任务上进行验证。 论文： Exploring Randomly Wired Neural Networks for Image Recognition 【pdf】 作者：Saining Xie, Alexander Kirillov, Ross Girshick, Kaiming He 参考文章：https://zhuanlan.zhihu.com/p/62837029 本文阅读有大量参考上述参考文章，也会补充部分个人细节与思考。 摘要RandWireNN 基本思想是研究设计stochastic network generator，也就是设计网络构架的机制，它的关注点在网络的连接方式上。下图是本文中获得的网络结构： 论文作者引入了一种网络模型空间的构造方法，即图论中的random graph，之后用grid search搜索出较好的神经网络子集，并在ImageNet的1000-class分类任务上进行验证。 深度学习模型炼丹师一般手工精心设计网络中不同层之间的连接方式，如CNN的convolution，RNN的recurrent，ResNets的x+F(x)，以及DenseNets的[x, F(x)]等。自动化机器学习/深度学习的研究者则主要关注在巨大的网络模型空间中如何高效地搜索出较好的神经网络，如早期研究人员一般使用random search/gird search，现在演变出reinforcement learning, gradient-based, weight-sharing,以及evolutionary等方法。论文作者引入了一种网络模型空间的构造方法，即图论中的random graph，之后用grid search搜索出较好的神经网络子集，并在ImageNet的1000-class分类任务上进行验证。 方法论文的主要工作包含以下步骤： （1）基于图论的随机图方法生成随机图Random Graph； （2）将Random Graph转换为一个神经网络NN； （3）将多个NN堆叠起来，形成最终的随机连接神经网络RandWireNN； （4）在ImageNet 1000-class任务上验证RandWireNN的表现； （5）重复（1）到（4）步骤。 一、基于图论的随机图方法生成随机图Random Graph作者引入了三种随机图生成方法，即Erdos-Renyi（ER）、Barabasi-Albert（BA）和 Watts-Strogatz（WS）。这三个方法生成随机图的机制比较简单。 ER: N个节点，节点两两之间以P的概率有一条边。该方法包含一个参数P，故以ER(P)表示。 BA: 初始有M个节点（M&lt;N），每增加1个新节点的时候，该节点以一定的概率与已有的所有节点相连（这个概率与已有节点的度有关），重复，直到这个新节点有M条边。重复，直到整个图有N个节点。该方法包含一个参数M，故以BA(M)表示。 WS: 所有N个节点排成一个圈，每个节点与两边邻近的K/2个节点相连。之后按照顺时针方向遍历每一个节点，与当前节点相连的某一条边以概率P与其他某个节点相连。重复K/2次。该方法包含2个参数K和P，故以WS(K,P)表示。 其中模型的节点总数N由论文作者根据网络复杂度（FLOPs）手动指定，因此ER方法的参数搜索空间为P∈[0.0,1.0]，BA方法的参数搜索空间为M∈[1,N]，WS方法的参数搜索空间为K∈[1,N-1] x P∈[0.0,1.0]。图1是三种方法生成的随机图。 图1：随机图 二、把生成的随机图Random Graph转换为一个神经网络NN1. 将随机图转化为DAG由图1可知，生成的每个随机图中的边是没有方向的，且不知道哪个是输入节点，哪个是输出节点。 首先要给每条边指定一个方向，即把生成的随机图Random Graph转换为有向无环图DAG。方法就是给每个节点分配一个索引index（从1到N），若两个节点之间有边，则边的方向从小索引节点到大索引节点。其中ER方法按照随机的方式给N个节点分配索引；BA方法给初始的M个节点分配索引1~M，之后每增加一个节点，其索引加1；WS方法则按照顺时针方向从小到大给N个节点分配索引。 2. 为DAG的边和节点赋上操作边操作：本文中假设每条边都代表着数据流，将一个tensor从一个节点送到另一个节点。 节点操作：每一个节点代表一个一个实际操作。这个操作可以细分为以下三步： Aggregation：（输入操作：聚合）输入数据(从一条或多条边)到节点通过加权求和来聚合在一起；其中权重是正向可学习的。 Transformation：（转换操作：卷积）聚合数据由定义为[relu-convolu- bn]三元组的转换处理。所有节点都使用相同类型的卷积，默认情况下为3×3可分卷积。 Distribution：（输出操作：复制）节点的输出边缘发送转换后的数据的相同副本。 图2：节点的操作 这样设计操作带来的一些特点： Additive aggregation能够维持输入输出channel数不变，防止后面的卷积计算越来越大，来避免仅仅因为增加计算量而提高大型输入节点的重要性程度，而忽略网络连接的作用。 Transformation应具有相同数量的输出和输入通道(除非切换阶段），以确保转换后的数据可以与来自任何其他节点的数据相结合。固定通道计数之后，不管输入和输出的程度如何，都会保持每个节点的FLOPs(浮点操作)和参数计数不变。 不论输入和输出的程度如何，聚集和分布几乎没有参数(加权求和的参数数目很少)。此外，假设每条边都是无参数的，这样一来，图的FLOPs和参数数量与节点的数量大致成正比，且几乎与边的数量无关。 这些属性几乎将FLOPs和参数计数与网络连接解耦，例如，在随机网络实例或不同生成器之间，FLOPs的偏差通常为±2%。这可以在不增加/减少模型复杂性的情况下比较不同的图。因此，任务性能的差异反映了连接模式的属性。 3. 设置cell的输入节点和输出节点给DAG指定唯一的输入节点（input）和唯一的输出节点(output)。DAG本身包含N个节点，那么额外指定一个输入节点与DAG中所有入度为0的节点相连，其负责将输入的图片转发出去；再额外指定一个输出节点与DAG中所有出度为0的节点相连，该节点进行均值计算后将结果输出。 三、将多个NN堆叠起来，形成最终的随机连接神经网络RandWireNN借鉴其他经典深度学习神经网络，作者将多个NN堆叠起来形成最终的随机连接神经网络RandWireNN。图3为一个示例，其包含共5个stages，其中stage1是一个卷积层，stage2可以是一个卷积层或者是一个NN，stage3、stage4和stage5均为NN。不同stage之间：卷积操作的stride为2，故feature map的大小逐渐减半；卷积操作的卷积核的数量x2，故feature map的通道数(即C)也x2 其中每个NN的节点数N（不包含输入和输出节点）设置为32，通道数C设置为78或者109/154。不同的（N,C）对应不同的网络复杂度（FLOPs），这样可以跟不同规模（small/regular/larger）的其他经典网络进行实验比较。 四、在ImageNet 1000-class任务上验证RandWireNN的表现（1）三个随机图生成方法（ER，BA，WS）的比较 如图4所示，WS方法的表现最好，于是接下来作者挑选了WS（4,0.75）与其他网络进行比较。论文还进行了另外一个网络结构鲁棒性测试的实验，即将网络中的某个节点或者某条边随机去掉，然后测试网络的表现，最后发现WS的结构最不稳定。这个研究点很有意思，也许将来会有一种网络生成器能够生成又好又鲁棒的网络结构，类似大脑，也许有种大脑结构最不容易得精神疾病。 首先是与小规模网络(smaller)进行比较，此时N=32，C=78。如表1所示，比MobileNet/ShuffleNet/NASNet/PNAS/DARTS等表现好，比Amoeba-C略差。7 其次与中等规模的网络（regular）进行比较，此时N=32，C=109/154，如表2所示，比ResNet-50/101和ResNeXt-50/101表现好。 最后与大规模网络（larger）进行比较，此时直接使用刚才训练好的中等规模网络，不过输入图像的size由224x224提高到320x320，如表3所示，虽然比其他网络表现略差，但是计算量少了很多。 （3）迁移表现 论文将RandWireNN作为骨干网络，用Faster R-CNN with FPN来检测目标（COCO object detection），如表4所示，比ResNet-50和ResNeXt-50效果好。 总结与思考总结本文贡献： 使用 WS 模型的最佳生成器生成的多个网络性能优于或可与完全手工设计的同类网络和通过各种神经结构搜索方法找到的网络相媲美。 还观察到，对于同一生成器生成的不同随机网络，精度的方差较低，但不同生成器之间存在明显的精度差距。这些观察结果表明，网络生成器的设计很重要。 最后，工作表明，从设计单个网络到设计网络生成器的新过渡是可能的，类似于如何从设计特征过渡到设计学习特征的网络。 思考 本文还是保持了相当部分的手工设计，对于随机图参数的选择方式是否可以改进 比较有意义的是，本文把NAS领域的重点从运算操作的选择转向了网络连接(拓扑)的构造。 参考文章中俞一鹏博士的思考： （1）这篇论文将图论的方法引入，并强调network generator的重要性，很有新意，好比“虎父无犬子”，“龙生龙，凤生凤”。 （2）这个idea也许你也能想到，不过快速实现这个idea的能力更重要； （3）不同stage之间只有一个node连接，这个设计未必合理； （4）迁移学习表现比较的实验过于简单，且其他实验结果也没有很惊艳。 （5）引入的图论方法有点简单，也许有更复杂的图生成方法，比如Bio-inspired类的； （6）只与NASNet进行了表现比较，没有与其他AutoML方法进行对比。","link":"/2019/06/13/论文阅读-RandWiredNN/"},{"title":"","text":"title: 第四章 对象与类date: 2019-11-22 21:17:46tags: [coding]category: [Java学习] 主要介绍如下： 面向对象程序设计 如何创建标准Java类库中的类对象 如何编写自己的类 概述面向对象程序设计 （ 简称 OOP)是当今主流的程序设计范型。Java完全面向对象。 面向对象的程序是由对象组成的，每个对象包含对用户公开的特定功能部分和隐藏的实现部分。 传统的结构化程序设计通过设计一系列的过程 （ 即算法） 来求解问题。一旦确定了这些过程，开始考虑存储数据的方式。算法+数据结构 = 程序。算法是第一位的，数据结构是第二位的。程序员的工作方式, 首先要确定如何操作数据，然后再决定如何组织数据，以便于数据操作。而OOP却调换了这个次序，将数据放在第一位，然后再考虑操作数据的算法。 对于一些规模较小的问题，将其分解为过程的开发方式比较理想。而面向对象更加适用于解决规模较大的问题。 类类 （ class ) 是构造对象的模板或蓝图。我们可以将类想象成制作小甜饼的切割机，将对象想象为小甜饼 。由类构造 （ construct ) 对象的过程称为创建类的实例 （ instance ) . 用 Java 编写的所有代码都位于某个类的内部。标准的 Java 库提供 了几千个类。尽管如此，还是需要在Java 程序中创建一些自己的类，以便描述应用程序所对应的问题域中的对象。 封装 （ encapsulation , 有时称为数据隐藏 ） 是与对象有关的一个重要概念。从形式上看，封装不过是将数据和行为组合在一个包， 并对对象的使用者隐藏了数据的实现方式。对象中的数据称为实例域，操作数据的过程称为方法(method)。对每个特定的类实例(对象)都有一组特定的实例域值。这些值的结合就是这个对象的当前状态。无论何时，向对象发送一个消息，他的状态就可能改变。 实现封装的关键在于绝对不能让类中的方法直接地访问其他类的实例域。仅能通过方法对对象的数据进行交互。 继承，OOP 的另一个原则会让用户自定义 Java 类变得轻而易举，可以通过扩展一个类来建立另外一个新的类，所有的类都源自于一个 “ 神通广大的超类 ”。它就是 Object。这个扩展后的新类具有所扩展的类的全部属性和方法，只需提供适用于这个新类的新方法和数据域 对象要想使用OOP ,清楚对象的三个主要特性： 对象的行为(behavior)， 可以施加哪些操作， 对象的状态(state), 对象的如何相应 对象的标识(identity)。 同一个类的所有对象实例, 对象的行为是可调用的方法定义。 识别类传统的过程化程序设计。必须从顶部的main函数开始编写，在面向对象所谓的顶部，要 首先从设计类开始，然后再往下每个类中添加方法。 识别类的简单规则是在分析问题的过程中寻找名词，而方法对应着动词。 类之间的关系 依赖(‘uses-a’); 聚合(‘has-a’) 继承(‘is-a’) 依赖 （ dependence ) , 即 “ uses - a ” 关系. 如果一个类的方法操纵另一个类的对象 我们就说一个类依赖于另一个类。应该尽可能地将相互依赖的类减至最少，让类之间的耦合度最小。 聚合关系意味着类 A 的对象包含类 B 的对象 继承 （inheritance ) , 即 “ is - a ” 关系 是一种用于表示特殊与一般关系的 使用预定义类使用类。但如 Math类的方法，Math封装了功能，不需要也不必隐藏数据，无数据 对象与对象变量要想使用对象，就必须先构造对象，并指定其初始化状态。然后，对对象应用方法。 Java中，用构造器(constructor)构造新实例。构造器是一种特殊的方法，用来构造并初始化对象。 如Java类库中的包含一个Date类。 构造器的名字应该与类名相同，因此Date类的构造器为Date，要想构造一个Date对象，需要在构造器前面加上new操作符，如 new Date().这个表达式构造了一个新对象 , 这个对象被初始化为当前的日期和时间 如果需要的话 也可以将这个对象传递给一个方法。Date类中有一个toString方法。这个方法将返回字符串 String s = new Date().toString(); 在对象与对象变量之间存在着一个重要的区别; Date deadline; // deadlin doesn’t refer to any object;定义了一个对象变量deadline,它可以引用Date类型的对象，但是，变量deadline不是一个对象，实际上也没有引用对象。不能使用方法。 必须首先初始化变量 deadline , 方法1：deadline = new Date(); 方法2：deadline = birthday; 引用已有的对象。 一定要认识到 ： 一个对象变量并没有实际包含一个对象而仅仅引用一个对象 在Java中，任何对象变量的值都是对存储在另一个地方的一个对象的引用。new操作符的返回值也是一个引用。 语句： Date deadline = new Date () ; 有两个部分：表达式new Date() 构造了一个Date类型的对象，并且它的值是对新创建对象的引用。这个引用存储在变量deadline中。可以显示将对象置为null，表明这个对象变量目前没有引用任何对象。 如果将一个方法应用于一个值为 null的对象上 ，那么就会产生运行时错误。 局部变量不会自动地初始化为 null ，而必须通过调用 new 或将它们设置为 null 进行初始化 很多人错误地认为 Java 对象变量与 C ++ 的引用类似,然而 在 C ++ 中没有空引用 ， 并且引用不能被赋值.可以将 Java 的对象变量看作 C + + 的对象指针 将 Java 的对象变量看作 C + + 的对象指针 如果把一个变量的值賦给另一个变量, 两个变量就指向同一个日期. 在 Java 中的 null 引用对应 C ++中的 NULL 指针. 所有的 Java 对象都存储在堆中,当一个对象包含另一个对象变量时,这个变量依然包含着指向另一个堆对象的指针 LocalDate类Date类中实例有一个状态，即特定的时间点。Date类中： 实际上时间是用距离一个固定时间点的毫秒数表示的。这个点称为纪元epoch。即UTC 1970年1月1日00:00:00, 类库设计者决定将保存时间与给时间点命名分开. 所以标准 Java 类库分别包含了两个类: 一个是用来表示时间点的 Date 类, 另一个是用来表示大家熟悉的日历表示法的 LocalDate 类. 将时间与日历分开是一种很好的面向对象设计. 不要使用构造器来构造 LocalDate 类的对象。实际上 应当使用静态工厂方法 ( factory method ) 代表你调用构造器。LocalDate.now() 构造一个新对象。 一旦有 了一个 LocalDate 对象，可以用方法 get Year 、getMonth Value 和 getDayOfMonth得到年 月和日： 1234567891011LocalDate newss = LocalDate.now();//通常都希望将构造的对象保存在一个对象变量中LocalDate newYearsEve = LocalDate.of(1999, 1, 23);int year = newYearsEve.getYear();int month = newYearsEve.getMonthValue();int day = newYearsEve.getDayOfMonth();// plusDays 方法会得到一个新的 LocalDate , LocalDate aThousandDaysLater = newYearsEve.plusDays(1000); year = aThousandDaysLater.getYear(); month = aThousandDaysLater.getMonthValue(); day = aThousandDaysLater.getDayOfMonth(); plusDays 方法会生成一个新的 LocalDate 对象,然后把这个新对象赋给 aThousandDaysLater变量 原来的对象不做任何改动 我们说 plusDays 方法没有更改调用这个方法的对象. 用户自定义类已经开始编写了一些简单的类；如何设计复杂应用程序的各种主力类(workhorse class)。通常这些类没有main方法，却有自己的实例域和实例方法。想创建一个完整的程序, 若干类组合，只有一个类有main方法。 Employee类在Java中，最简单的类定义形式： 1234567891011class ClassName { field1; field2; ... constructor1, constructor2, ... method1, method2, } 如下面的简单的Employee类，在编写资金管理时，用到 1234567891011121314151617181920212223242526272829303132333435363738394041import java.time.LocalDate;class Employee { // instance field // 可以用 public 标记实例域 但这是一种极为不提倡的做法,public 數据域允许程序中的任何方法对其进行读取和修改。 private String name; private double salary; private LocalDate hireDay; // constructor public Employee(String n, double s, int year, int month, int day) { name = n; salary = s; hireDay = LocalDate.of(year, month, day); } // a method public String getName() { return name; } // more method}public class EmployeeTest { public static void main(String[] args) { // fill the staff array with three Employee objects Employee[] staff = new Employee[3]; staff[0] = new Employee(\"Carl Cracker\", 75000, 1987, 12, 15); staff[1] = new Employee(\"Harry Hacker\", 5000, 1993, 10, 16); staff[2] = new Employee(\"Tony Tester\", 6000, 2001, 11, 12); // raise everyon's salary by 5% for (Employee e : staff) { e.raiseSalary(5); } for (Employee e : staff) { System.out.println(\"name=\"+e.getName()+\" salary=\"+e.getSalary()+\" hireDay=\"+e.getHireDay()); } staff[0].test = \"teswt\"; }} 在类中：Employee 类和带有 public 访问修饰符的 EmployeeTest 类。EmployeeTest 类包含了 main 方法 其中使用了前面介绍的指令。源文件名是 EmployeeTest . java 这是因为文件名必须与 public 类的名字相匹配。在一个 源文件中，只能有一个公有类，但可以有任意数目的非公有类。 接下来当编译这段源代码的时候，编译器将在目录下创建两个类文件: EmployeeTest.class和Employ.class。 将程序中包含main方法的类名提供了给字节码解释器，以便启动这个程序: java EmployeeTest 字节码解释器开始运行EmployeeTest类的main方法中的代码。在这段代码中，先后构成了三个新Employee对象，并显示他们的状态。 多源文件使用实际上习惯于每一个类存在一个单独的源文件中。将Employee类放在Employee.java中，将EmployeeTest放入EmployeeTest.java中。可通过通配符调用Java编译器: javac Employee*.java; 或者 javac EmployeeTest.java 剖析Employee类下面对Employee类剖析。首先从这个类的方法开始。通过查看源代码会发现，这个类包含一个构造器和4个方法，用public标记。 用 public 标记实例域，但这是一种极为不提倡的做法,public 數据域允许程序中的任何方法对其进行读取和修改。 最后，请注意，有两个实例域本身就是对象: name域是String对象，hireDay域是LocalDate类对象。 从构造器开始Employee类构造器 123456// constructorpublic Employee(String n, double s, int year, int month, int day) { name = n; salary = s; hireDay = LocalDate.of(year, month, day);} 构造器与类同名。在构造Employee类的对象时，构造器会运行，将实例域初始化。new Employee(&quot;Carl Cracker&quot;, 75000, 1987, 12, 15); 可有多个构造器。 构造器没有返回值。 构造器与其他的方法有一个重要的不同 。构造器总是伴随着 new 操作符的执行被调用, 而不能对一个已经存在的对象调用构造器来达到重新设置实例域的目的, 将产生编译错误。 请注意 不要在构造器中定义与实例域重名的局部变量，这些变量只能在构造器内部访问。这些变量屏蔽了同名的实例域。因此 ，必须注意在所有的方法中不要命名与实例域同名的变量 注意: Java构造器与C++一致，Java对象都是在堆中构造，构造器总伴随着new一起使用。 12Employee number007(\"Tony Tester\", 6000, 2001, 11, 12);// 在C++可以这样使用，Java不行Employee number007 = new Employee(\"Tony Tester\", 6000, 2001, 11, 12); //Java中使用 隐式参数与显式参数方法用于操作对象以及存取它们的实例域。例如，方法: 123456789public void raiseSalary(double byPercent) { double raise = salary * byPercent / 100; salary += raise;}// 将调用这个方法的对象的salary实例设置为新值。例如number007.raiseSalary(5);// 它的结果将 number007.salary 域的值增加 5 %,具体该调用执行下列指令double raise = number007.salary * 5 /100;number007.salary += raise; raiseSalary用两个参数: 1. 隐式参数(implicit)，是出现在方法名前的Employee类对象，也可称为方法调用的目标或者接收者。 2. 第二个参数是位于方法名后面括号中的数值。这是一个显式参数(explicit)。显式参数是明显列在方法声明中的，例如double byPercent, 隐式参数没有出现在方法声明中。 在每一个方法中，关键字this表示隐式参数，如需要可用下列方法编写raiseSalary方法: 1234public void raiseSalary(double byPercent) { double raise = this.salary * byPercent / 100; this.salary += raise;} 有些程序员更偏爱这样的风格, 因为这样可以将实例域与局部变量明显地区分开来。 1234567void Employee::raiseSalary(double byPercent) { //in C++ not Java 在类外定义方法 ...}class Employee { ... int getName(){return name;} // inline C++} 在Java，所有方法都必须在类的内部定义， 但不代表他们是内联方法。是否将某个方法设置为内联方法是Java虚拟机的任务，即 编译器会监视调用那些简洁、经常被调用的，没有被重载以及可以被优化的方法。 封装的优点最后仔细看下getName方法，getSalary，getHireDay方法。这些都是典型的访问器方法，又称为域访问器。一般需要获取或者设置实例域的值。 一个私有的数据域 一个公有的域访问器的方法 一个公有的域更改器方法: 更改器方法可以执行错误检查 注意：不要编写返回引用可变对象的访问器方法，在Employee类中，就违反了该设计原则: 1234567891011class Employee { private Date hireDay; public Date getHireDay() { return hireDay; //Bad }}// LocalDate类中没有更改器的方法，但Date类中有一个更改器方法setTime,可以设置Employee harry = ..Date d = harry.getHireDay();double tenYears = 10*365...;d.setTime(d.getTime() - (long)tenYears); 因此，如果需要返回一个可变对象的引用，首先应该先克隆(clone)，对象 clone 指存放在另一个位置上的对象副本。 123456class Employee{ private Date hireDay; public Date getHireDay() { return (Date)hireDay.clone(); }} 如果需要返回一个可变数据域的拷贝 ，就应该使用clone 基于类的访问权限方法可以访问所调用对象的私有数据 。一个方法可以访问所属类的所有对象的私有数据。 12345678class Employee{ ... public boolean equals(Employee other) { return name.equals(other.name); }}// 典型的调用方式是if(harry.equals(boss)) .. 这个方法可以访问harry的私有域，它还可以访问boss的私有域，这是合法的。其原因是boss是Employee类对象，而Employee类的方法可以访问Employee类的 任何一个对象的私有域。 注意： C++也是一样的，方法可以访问所属类的私有特性(feature) , 而不仅限访问隐私参数的私有特性。 私有方法在实现一个类时，公有数据很危险，一般私有化。方法一般公有，特殊情况下，如何一个计算代码，划分成若干独立的辅助方法。使用private。 final实例域可以将实例域定义为 final，构建对象时必须初始化这样的域。必须确保在每一个构造器执行之后，这个域的值被设置，并且在后面的操作中，不能够再对它进行修改。例如，可以将 Employee 类中的 name 域声明为 final , 因为在对象构建之后 这个值不会再改变， 被修改 即没有 setName 方法 12345class Employee{ //insance field// private String name; private final String name;} final 修饰符大都应用于基本(primitive)类型域，或者不可变(immutable)类的域。(如果类中的每个方法都不会改变其对象，这种类成为不可变类，如String类。) 对于可变的类 ，使用 final 修饰符可能会对读者造成混乱 123private final StringBuilder evaluations;// 在Employee构造器中会初始化为evaluations = new StringBuilder(); final关键字只是表示存储在evaluations变量中的对象引用不会指示其他的StringBuilder对象，不过这个对象可以更改: 123public void giveGoldStar() { evaluations.append(LocalDate.now() + \": Gold star!\\n\");} 静态域与静态方法前面中，main 方法都被标记为 static 修饰符。可以给类名直接使用，生命周期程序延长，给类名使用。 静态域如果将域定义为static，每个类只有一个这样的域，而每一个对象对于所有的实例域都有自己的一份拷贝。例如，假定需要给每一个雇员赋予一个唯一标识码，这里给Employee类添加一个实例域id和一个静态域nextId。 12345class Employee{ //insance field private int Id; private static int nextId = 1;} 现在 ，每一个雇员对象都有一个自己的 id 域 ，但这个类的所有实例将共享一个 nextld域.换句话说 ， 如果有 1000 个Employee类的对象 ，则有1000个实例域 id 。但是只有一个静态域nextId。即是没有一个雇员对象，静态域nextId也存在，它属于类，不属于任何独立的对象。在绝大多数的面向对象程序设计语言中，静态域被称为类域，沿用了 C ++ 的叫法 并无实际意义 12345678910 public void setId(int ID) {// this.id = ID; this.id = nextId; nextId++; }// 假定为 harry 设定雇员标识码harry.setID(10);harry的id域被设置为静态域nextId的当前值，并且静态域nextId的值加1:harry.id = Employee.nextId;Employee.nextId++; 静态常量静态变量使用得比较少，但静态常量却使用得比较多，如 123public class Math { public static final double PI = 3.1415926;} 在程序中，可通过Math.PI的形式获得这个常量。 静态变量（Static Variable）在计算机编程领域指在程序执行前系统就为之静态分配（也即在运行时中不再改变分配情况）存储空间的一类变量)。与之相对应的是在运行时只暂时存在的自动变量（即局部变量）与以动态分配方式获取存储空间的一些对象，其中自动变量的存储空间在调用栈上分配与释放。 static 声明的变量在C语言中有两方面的特征： 1.变量被放在程序的全局存储区中，这样在下一次调用的时候还可以保持原来的赋值。这一点是它与堆栈变量和堆变量的区别。 2.变量用static告知编译器，自己仅在变量的作用范围内可见。这一点是它与全局变量的区别。 全局变量、静态全局变量、静态局部变量和局部变量的区别变量可以分为：全局变量、静态全局变量、静态局部变量和局部变量。按存储区域分，全局变量、静态全局变量和静态局部变量都存放在内存的静态存储区域，局部变量存放在内存的栈区。按作用域分，全局变量在整个工程文件内都有效；静态全局变量只在定义它的文件内有效；静态局部变量只在定义它的函数内有效，并且程序仅分配一次内存，函数返回后，该变量不会消失；局部变量在定义它的函数内有效，但是函数返回后失效。 static表示“全局”或者“静态”的意思，用来修饰成员变量和成员方法，也可以形成静态static代码块，但是Java语言中没有全局变量的概念。 static是静态修饰符，什么叫静态修饰符呢？大家都知道，在程序中任何变量或者代码都是在编译时由系统自动分配内存来存储的，而所谓静态就是指在编译后所分配的内存会一直存在，直到程序退出内存才会释放这个空间，也就是只要程序在运行，那么这块内存就会一直存在。这样做有什么意义呢？在Java程序里面，所有的东西都是对象，而对象的抽象就是类，对于一个类而言，如果要使用他的成员，那么普通情况下必须先实例化对象后，通过对象的引用才能够访问这些成员，但是用static修饰的成员可以通过类名加“.”进行直接访问。 静态方法静态方法是一种不能向对象实施操作的方法。例如，Math类中的pow方式就是一个静态方法: 1Math.pow(x, a);//计算，在运算时，没有使用任何Math对象，换句话，静态方法没有隐式参数。 Employee 静态方法不能访问Id 实例域 因为它不能操作对象。建议使用类名来调用静态方法。 在下面两种情况使用静态方法: 一个方法不需要访问对象状态，其需要的参数是通过显式参数提供。(Math.pow) 一个方法只需要访问类的静态域。(Employee.getNextId) static属于类且不属于类对象的变量和函数 工厂方法静态方法还有另外一种常见的用途。类似 LocalDate 和 NumberFormat 的类使用静态工厂(factory method)来构造对象。如LocalDate.now()和LocalDate.of()，NumberFormat类如下使用工厂方法生成不同风格的格式化对象： 12NumberFormat currencyFormatter = NumberFormat.getCurrencyInstance();NumberFormat percnetFormatter = NumberFormat.getPercentInstance(); 为什么 NumberFormat 类不利用构造器完成这些操作呢 ？ 这主要有两个原因: 无法命名构造器。构造器的名字必须的对象类型，但这里希望将得到的货币实例和和百分比的实例采用不同的名字。 当使用构造器时，无法改变所构造的对象类型。而Factory方法返回一个DecimalFormat类对象。 main方法需要注意，不需要使用对象调用静态方法(object.method)。例如，不需要构造Math类对象就可使用Math.pow。 同理，main方法也是静态方法。 12345class Application { public static void main(String[] args) { // construct objects here }} main方法不对任何对象进行操作。事实上，在启动程序时还没有任何一个对象。静态main方法将执行并创建程序所需要的对象。 每个类可以有一个main方法。这是一个常用与对类经常进行单元测试的技巧。例如，可以在Employee类中添加 12345678910111213class Employee { private String name; private double salary; public Employee(String n, double s) { this.name = n; this.salary = s; } public static void main(String[] args) { //unit test Employee emp = new Employee(\"s\", 1.1, 1000, 11, 12); emp.raiseSalary(3); }} 如果 Employee 类是一个更大型应用程序的一部分 就可以使用下面这条语句运行程序java Application, Employee 类的 main 方法永远不会执行。 方法参数将参数传递给方法。按值调用(call by value)表示方法接收的是调用者提供的值。而引用调用(call by reference)表示方法接收的是调用者提供的变量地址。一个方法可以修改传递引用所对应的变量值，而不能修改传递值调用所对应的变量值。 Java 程序设计语言总是采用按值调用。也就是说 ，方法得到的是所有参数值的一个拷贝。特别是 ，方法不能修改传递给它的任何参数变量的内容 12double percent = 10;harry.raiseSalary(percent); // 调用完成之后percent依然是10 方法得到是对象引用的拷贝，对象引用及其他的拷贝同时引用同一个对象。 Java参数总结： 一个方法不能修改一个基本数据类型的参数(即数值型或布尔型) 一个方法可以改变一个对象参数的状态 。 一个方法不能让对象参数引用一个新的对象 C++中值调用和引用调用。引用参数标有&amp;符号。例如void tripleValue(double&amp; x)方法实现修改引用参数目的。 对象构造前面已经学习了编写简单的构造器 ， 可以定义对象的初始状态。由于对象构造非 常重要 所以 Java 提供了多种编写构造器的机制。 重载有些类有多个构造器，例如如下构造一个空的StringBuilder对象: 1StringBuilder messages = new StringBuilder(); 或者，可以指定一个初始化字符串: 1StringBuilder todoList = new StringBuilder(\"To do:\\n\"); 这种加重载overloading。同一个类中多个方法有相同名字、不同的参数，编译器必须挑选具体的执行方法，通过用各个方法给出参数类型与特定方法调用所使用的值类型进行匹配来挑选对应的方法。找不到，则编译时错误。Java可以重载任何方法, 要完整描述一个方法，需要指出方法名以及参数类型，叫方法的签名(signature)。例如，String类中有4个称为indexOf的公有方法。签名是 123index(int)index(int, int)index(String, int) 返回参数不是方法签名的一部分，即不能有两个名字相同、参数类型相同却返回不同类型值的方法。 默认域初始化如果在构造器中没有显式地给域赋予初值，则会自动赋予默认值。数值为0，布尔值未false，对象引用为null。 无参数的构造器很多类都包含一个无参数的构造函数，对象由无参数构造函数创建，其状态设置为默认值。如Employee： 12345678class Employee { private String name; public Employee() { name = \" \"; salary = 0.0; hireDay = LocalDate.now(); }} 默认提供public Employee()，如果没有重写，调用默认值；提供了构造器public Employee(String name)；但是未提供无参数构造器；则构造对象时使用这个这个方法这则是不合法。 显示域初始化通过重载类的构造器方法 可以采用多种形式设置类的实例域的初始状态。可以在类定义中直接将一个值赋给任何域 123class Employee { private String name = \"111\";} 在执行构造器之前，先执行赋值操作。当一个类的所有构造器都希望把相同的值赋予某个特定的实例域时 ，这种方式特别有用。 初始值不一定是常量值 。在下面的例子中，可以初始值不一定是常量值 。 在下面的例子中 可以调用方法对域进行初始化。 12345678910class Employee { private static int nextId; private int id = assignId(); ... private static int assignId() { int r = nextId; nextId++; return r; }} 注释：在C++中，不能直接初始化类的实例域。所有的域必须在构造器中设置。但是，有一个特殊的初始化器列表语法，如下： 12Employee::Employee(String n, double s, int y, int m. int d):name(n),salary(s),hireDay(y, m, d){} 参数名在编写很小的构造器时(这是十分常见的),常常在参数命名上出现错误。 通常，参数用单个字符命名: 12345678910public Employee(String n, double s){ name = n; salary = s;}// 可以命名参数 aName, aSalarypublic Employee(String aName, double aSalary) {}// 还一种常用的技巧 ，它基于这样的事实 参数变量用同样的名字将实例域屏蔽起来,如果将参数命名为salary,salary将引用这个参数 而不是实例域,可以采用this.salary的形式访问实例域public Employee(String name, double salary) { // 可以用下划线，Java不常用 this.name = name; this.salary = salary;} 调用另一个构造器关键字this引用方法的隐式参数，还有另一个含义。如构造器的第一个语句如this(…)，这个构造器将调用同一个类的另一个构造器，如 1234567class Employee { public Employee(double s) { // calls Employee( String , double) this(\"Employee #\"+ nextId, s); nextId++; }} 当调用 new Employee ( 60000 ) 时, Employee(double)构造器将调用Employee(String, double)构造器。 采用这种方式使用 this 关键字非常有用，这样对公共的构造器代码部分只编写一次 在 Java 中: this 引用等价于 C + + 的 this 指针,C++不能调用另一个构造器。 初始化模块前面已经讲过两种初始化数据域的方法: 在构造器中设置值 在声明中赋值 Java中还有第三种：成为初始化块(initialization block)。在一个类的声明中，可以包含多个代码块。只要构造类的对象，这些块就会被执行。 123456789101112131415class Empployee{ private static int nextId; private int id; private String name; private double salary; // object initialization 建议始化块放在域定义之后 { id = nextId; nextId++; } public Employee(double s) { this(\"Employee #\"+nextId, s); nextId++; }} 这种机制不是必需的 ，也不常见 通常会直接将初始化代码放在构造器中 由于初始化数据域有多种途径，所以列出构造过程的所有路径可能相当混乱。下面是调用构造器的具体处理步骤： 所有数据域被初始化为默认值 （ 0 false 或 null )。 按照在类声明中出现的次序，一次执行所有域初始化语句和初始化块。 如果构造器第一行调用了第二个构造器 则执行第二个构造器主体。 执行这个构造器的主体。 如果对类的静态域进行初始化的代码比较复杂 ，那么可以使用静态的初始化块 12345// static initialization blockstatic { Random generator = new Random(); nextId = generator.nextId(10000);} 在类第一次加载的时候 ，将会进行静态域的初始化。与实例域一样 除非将它们显式地 ，设置成其他值 ，否则默认的初始值是 0 、false 或 null 。 对象析构与finalize方法C ++ , 有显式的析构器方法。其中放置一些当对象不再使用时需要执行的清理代码。在析构器中，最常见的操作是回收分配给对象的存储空间。Java中自动的垃圾回收器，所以不支持析构器。某些对象使用了内存之外的其他资源 例如 文件或使用了系统资源的另一个对 象的句柄，当资源不再需要时 将其回收和再利用将显得十分重要。可以为任何一个类添加 finalize 方法 。finalize 方法将在垃圾回收器清除对象之前调用。在实际应用中 ， 不要依赖于使用finalize方法，不知道什么时候才能够调用。 如果某个资源需要在使用完毕后立刻被关闭 ，那么就需要由人工来管理。对象用完时 ，可以应用一个 close 方法来完成相应的清理操作。 包Java允许使用包(package)将类组织起来。借助包可以方便的组织自己的代码，并将自己的代码与被人提供代码库分开管理。 标准的 Java 类库分布在多个包中，包括 java.lang 、java.util 和 java.net 等。标准的 Java包具有一个层次结构，如目录嵌套。所有标准JavaJava 包都处于 java和 javax 包层次中。 使用包的主要原因是确保类名的唯一性 假如两个程序员不约而同地建立了 Employee类。只要将这些类放置在不同的包中 就不会产生冲突。 建议将公司的因特网域名 （ 这显然是独一无二的 ） 以逆序的形式作为包名。如 com.org.zju.cst 每一个都拥有独立的类集合。嵌套的包没有任何关系 类的导入一个类可以使用所属包中的所有类 ，以及其他包中的公有类( public class)。采用两种方式访问另一个包中的公有类。第一种方式：每个类名之前添加完整的包名： 1java.time.LocalDate today = new java.time.LocalDate.now() 第二种使用import语句。 可以使用 import 语句导人一个特定的类或者整个包 import 语句应该位于源文件的顶部( 但位于 package 语句的后面 )。 123// 而无须在前面加上包前缀 。还可以导人一个包中的特定类// java . time . * 的语法比较简单 对代码的大小也没有任何负面影响将会使代码的读者更加准确地知道加载了哪些类import java.util.*; 但是，需要注意的是只能使用星号(*) 导入一个包, 而不能使用 import java 或 import java .*.* 导入以java 为前缀的所有包。在大多数情况下, 只导入所需的包, 并不必过多地理睬它们。 12345import java.util.*;import java.sql.*; // 如果java.util和java.sql包中都有特定日期(Date) 增加一个特定的import语句import java.util.Date; //如果都要使用java.util.Date deadline = new java.util.Date();java.sql.Date today = new java.sql.Date(...) 在包中定位类是编译器(compiler)的工作 。类文件中的字节码肯定使用完整的包名来引用其他类 注意：C++，import和#include两者无共同之处，在C++中，必须使用#include将外部特性的声明加载进来，这个因为C++无法查看任何文件的内部，除了正在编译的文件以及在头文件中明确包含的文件，Java编译可以查看其他文件内部。在Java中，通过显示地给出包名，不用import；Import 语句的唯一的好处是简捷，可以使用简短的名字而不是完整的包名来引用一个类 。例如 ，在import java.util.*; 在 C++ 中，与包 机 制 类 似 的 是 命 名 空 间 (namespace)。在Java中，package与import语句类似于C++中的的namespace和using指令。 静态导入import 语句不仅可以导人类，还增加了导人静态方法和静态域的功能。 如果在源文件的顶部，添加一条指令： 1import static java.lang,System.*; 就可以使用System类的静态方法和静态域 ，而不必加类名前缀：","link":"/2019/11/22/第四章-对象与类/"},{"title":"算法40讲笔记","text":"介绍算法的重要性？ 每天的工作任务安排，算法来说，不是队列 来一个处理一个，更不是栈，最新的先处理； 而是优先级队列。很痛苦，但是坚持下来，产出最多，收获最好！！！ 原因一: 编程coding是内功的修炼 原因二: 大公司必备条件 原因三: 硅谷现场手写 原因四: 算法和数据结构实用并有趣的 有效学习算法和数据结构不是记或者背； 第一是要去理解, 明白其结构； 第二是去练习，大量的练习。 &lt;&lt;异类-不一样的成功启示录&gt;&gt; 马尔科姆 任何一个人想要在某个领域达到顶尖的位置: 切碎知识点； 刻意练习； 获得反馈 chunk it up 切碎知识点 大知识体系 分成 小模块； 购买该领域的专业书籍学习；最后整合回来，注意每块之间的脉络联系；一块一块的去学习，去练习 理解记知识：每块知识点之间串起来，关键是找到主干，不断分散，枝叶 deliberate practicing 刻意练习 很重要：大量练习缺陷，不熟悉的地方；会有不舒服，枯燥的地方 坚持住，才能提升自己，和自己作斗争。 练习什么：1. review各个知识点整体框架概念，知识点之间的联系,要在脑子中能够构建起图谱 2. 针对新知识，不断去写，去coding，去理解，去做笔记。 feed back 反馈 及时反馈 主动型反馈(自己去找) 高手代码(Github, Leetcode!!) 第一视角直播 被动式反馈(可遇不可求，高手给你指点) 高手 code review 切题四件套 clarification !!明确题目意思 Possible solutions 可能的解，范围，拉掉的可能性 compare（time/space） optimal（加强） Coding 多写 Test cases 课程总览和算法复杂度Data Structure：Array; Stack/Queue; PriorityQueue(heap); LinkedList(single/double); Tree/Binary Tree; HashTable; Disjoint Set; Trie; BloomFilter; LRU Cache Algorithm:General Coding; In-order/Pre-order/Post-order traversal; Greedy; Recursion/Backtrace; Breadth-first search; Depth-first search; Divide and Conquer; Dynamic Programming; Binary Search; Graph 时间复杂度和 空间复杂度时间复杂度 这是 常识，习惯；若是无法判断；代入特殊值，1，3，5，7 Big O notation O(1): Constant Complexity: Constant 常数复杂度； 一次运算；O(log n): Logarithmic Complexity: 对数复杂度 O(n): Linear Complexity: 线性时间复杂度 和N有关， 程序跑多少次O(n^2): N square Complexity 平方 O(n^3): N square Complexity 立⽅O(2^n): Exponential Growth 指数 O(n!): Factorial 阶乘 多个只看最高阶的复杂度；大概多少次； 如何练习算法！！！三分学习，七分练习；一定开始自己的练习 练习题 + 练习方式 刻意练习 坚持 刻意练习：不要自己会做的题目，要花时间去做要做常见，不会的题目。 练习缺陷 弱点的地方 不舒服 枯燥，难受 运用 目就用切题四件套； 条件反射！！！！！ 明确题目的意思 看到一个题目，数据的范围，错过的可能性，需要注意的地方； 想到可能的解法，找出最佳解法，对比时间，空间复杂度 改进暴力算法！ Coding 多写 测试用例 反馈！！！solution from leetcode discussion from leetcode !!关键的关键！ 现在就动手 练习 - 坚持 - 机会给予有准备的⼈ 每节课的课后作业","link":"/2019/11/18/算法40讲笔记/"},{"title":"动态规划_剪绳子","text":"题目给你一根长度为 n 的绳子，请把绳子剪成整数长度的 m 段（m、n都是整数，n&gt;1并且m&gt;1）,求最大乘积 https://leetcode-cn.com/problems/jian-sheng-zi-lcof/ 解法递归+记忆化存储 方法一：分治 思想， 采用递归， 最大F(n)可以F(n-1)求得，一直递推至F(2) 123456789101112131415161718192021class Solution { public int cuttingRope(int n) { int[] memo = new int[n+1]; return memorize(n, memo); } private int memorize(int n, int[] memo) { if (n ==2) { return 1; } if (memo[n] != 0) { return memo[n]; } int res = -1; for (int i=1; i&lt;n; i++) { res = Math.max(res, Math.max(i*(n-i), i*memorize(n-i, memo))); } memo[n] = res; return memo[n]; }} 数学公式 方法二：数学公式 由算术几何均值不等式 推论一： 将绳子 以相等的长度等分为多段 ，得到的乘积最大。 推论二： 尽可能将绳子以长度 3等分为多段时，乘积最大。 切分规则：最优： 3 。把绳子尽可能切为多个长度为 3 的片段，留下的最后一段绳子的长度可能为 0,1,2三种情况。次优： 2。若最后一段绳子长度为 2 ；则保留，不再拆为 1+1 。最差： 1。若最后一段绳子长度为 1；则应把一份 3 + 1替换为 2 + 2，因为22 &gt;31。 123456789class Solution { public int cuttingRope(int n) { if(n &lt;= 3) return n - 1; int a = n / 3, b = n % 3; if(b == 0) return (int)Math.pow(3, a); if(b == 1) return (int)Math.pow(3, a - 1) * 4; return (int)Math.pow(3, a) * 2; }} 贪心 分治想法， 每一步最优，推论得出 方法二 动态规划（自底向上）同样地，我们也可以使用动态规划，从已知值 F(2)逐步迭代到目标值 F(n)，它是一种自底向上的方法。 当绳子长度n&gt;3时，最后一段绳子长度只有2，3两种情况(证明参考高赞精选贴)，因此： dp[i] = max { 2 dp[i-2], 3 dp[i-3] } 1234567891011class Solution { public int cuttingRope(int n) { if(n&lt;=3) return n-1; int[] dp = new int[n+1]; dp[2] = 2; dp[3] = 3; for(int i = 4; i &lt;= n; i++){ dp[i] = 2 * dp[i-2] &gt; 3 * dp[i-3] ? 2 * dp[i-2] : 3 * dp[i-3]; } return dp[n]; }}","link":"/2020/06/07/贪心-剪绳子/"},{"title":"贪心_剪绳子2","text":"题目https://leetcode-cn.com/problems/jian-sheng-zi-ii-lcof 大数越界问题 解题 贪心 + 快速幂 12345678910111213141516171819202122232425262728293031323334353637public class Solution { public int cuttingRope(int n) { if (n &lt;= 3) { return n - 1; } int a = n / 3; int b = n % 3; if (b == 2) { return (int) (quickPow(3, a) * b % 1000000007); } else { return (int) ((quickPow(3, a - 1) * (b + 3)) % 1000000007); } }// private long quickPow(int x, int n) {// if (n == 0) {// return 1;// }// long y = quickPow(x, n / 2);// return (n &amp; 1) == 1 ? (y * y * x) % 1000000007 : (y * y) % 1000000007;// } private long quickPow(int x, long n) { long res = 1; long tt = x; while (n != 0) { if ((n &amp; 1) == 1) { res *= tt; res %= 1000000007; } tt *= tt; tt %= 1000000007; n /= 2; } return res; }} 123456789101112131415161718class Solution { public int cuttingRope(int n) { if(n == 2) { return 1; } if(n == 3){ return 2; } int mod = (int)1e9 + 7; long res = 1; while(n &gt; 4) { res *= 3; res %= mod; n -= 3; } return (int)(res * n % mod); }} 整数分解https://leetcode-cn.com/problems/integer-break/solution/343-zheng-shu-chai-fen-tan-xin-by-jyd/ 123456789class Solution { public int integerBreak(int n) { if(n &lt;= 3) return n - 1; int a = n / 3, b = n % 3; if(b == 0) return (int)Math.pow(3, a); if(b == 1) return (int)Math.pow(3, a - 1) * 4; return (int)Math.pow(3, a) * 2; }}","link":"/2020/06/07/贪心-剪绳子2/"},{"title":"TensorFlowFederatedTutorial","text":"Federated Learning for Image Classification注意：此colab已经过验证，可以与tensorflow_federated pip包的0.4.0版本一起使用，但Tensorflow Federated项目仍处于预发布版开发阶段，可能无法在master上运行。 在本教程中，我们使用经典的MNIST培训示例来介绍TFF的联合学习（FL）API层，tff.learning-一组更高级别的接口，可用于执行常见类型的联合学习任务，例如联合训练，针对TensorFlow中实现的用户提供的模型。 本教程和Federated Learning API主要面向希望将自己的TensorFlow模型插入TFF，将后者主要视为黑盒子的用户。有关TFF以及如何实现自己的联合学习算法的更深入理解，请参阅FC Core API上的教程 - 自定义联合算法第1部分和第2部分。 有关tff.learning的更多信息，请继续使用Federated Learning for Text Generation，该教程除了涵盖周期性模型之外，还演示加载预训练的序列化Keras模型，以便使用联合学习结合使用Keras进行评估。 Before we start在开始之前，请运行以下命令以确保您的环境设置正确。如果您没有看到问候语，请参阅“安装指南”以获取相关说明。 Preparing the input data让我们从数据开始。联合学习需要联合数据集，即来自多个用户的数据集合。联合数据通常是非i.i.d.，这带来了一系列独特的挑战。 为了便于实验，我们在TFF存储库中添加了一些数据集，包括MNIST的联合版本，其中包含已使用Leaf(62 different classes (10 digits, 26 lowercase, 26 uppercase), images are 28 by 28 pixels (with option to make them all 128 by 128 pixels), 3500 users)重新处理的原始NIST数据集的版本，以便数据由原始作者键入。数字。由于每个作者都有一个独特的风格，这个数据集展示了非i.i.d的类型。联合数据集的预期行为。 这是我们如何加载它。 12#@test {&quot;output&quot;: &quot;ignore&quot;}emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data() load_data（）返回的数据集是tff.simulation.ClientData的实例，这个接口允许您枚举用户集，构造表示特定用户数据的tf.data.Dataset，并查询个别元素的结构。以下是如何使用此界面探索数据集的内容。请记住，虽然此接口允许您迭代客户端ID，但这只是模拟数据的一个功能。您很快就会看到，联合学习框架不使用客户端身份 - 它们的唯一目的是允许您选择数据的子集进行模拟。 1234567891011121314len(emnist_train.client_ids)3383emnist_train.output_types, emnist_train.output_shapes(OrderedDict([(u&apos;label&apos;, tf.int32), (u&apos;pixels&apos;, tf.float32)]), OrderedDict([(u&apos;label&apos;, TensorShape([])), (u&apos;pixels&apos;, TensorShape([28, 28]))])) example_dataset = emnist_train.create_tf_dataset_for_client( emnist_train.client_ids[0])example_element = iter(example_dataset).next()example_element[&apos;label&apos;].numpy()5 12345#@test {&quot;output&quot;: &quot;ignore&quot;}from matplotlib import pyplot as pltplt.imshow(example_element[&apos;pixels&apos;].numpy(), cmap=&apos;gray&apos;, aspect=&apos;equal&apos;)plt.grid(&apos;off&apos;)_ = plt.show() 在模拟中将联邦数据提供给TFF的方法之一就是Python列表，列表中的每个元素都包含单个用户的数据，无论是作为列表还是作为tf.data.Dataset。由于我们已经有一个提供后者的接口，让我们使用它。 这是一个简单的辅助函数，它将构建一组给定用户的数据集列表，作为一轮培训或评估的输入。 123def make_federated_data(client_data, client_ids): return [preprocess(client_data.create_tf_dataset_for_client(x)) for x in client_ids] 现在，我们如何选择客户？ 在典型的联合训练场景中，我们正在处理可能非常大量的用户设备，其中只有一小部分可用于在给定时间点进行训练。例如，当客户端设备是仅在插入电源，关闭计量网络以及以其他方式空闲时参与训练的移动电话时就是这种情况。 当然，我们处于模拟环境中，所有数据都是本地可用的。通常情况下，在运行模拟时，我们只需对客户的随机子集进行抽样，以参与每轮培训，每轮培训通常不同。 也就是说，正如你可以通过研究联邦平均算法的论文所发现的那样，在每轮中随机抽样的客户子集的系统中实现收敛可能需要一段时间，并且必须运行数百轮是不切实际的。这个互动教程。 我们要做的是对客户端集进行一次采样，并在多轮中重复使用相同的集合以加速收敛（故意过度拟合这些少数用户的数据）。我们将其作为练习让读者修改本教程以模拟随机抽样 - 这很容易做到（一旦你这样做，请记住，让模型收敛可能需要一段时间）。 12345678#@test {&quot;output&quot;: &quot;ignore&quot;}NUM_CLIENTS = 3sample_clients = emnist_train.client_ids[0:NUM_CLIENTS]federated_train_data = make_federated_data(emnist_train, sample_clients)len(federated_train_data), federated_train_data[0] Creating a model with Keras如果您使用Keras，您可能已经拥有构建Keras模型的代码。这是一个简单模型的例子，足以满足我们的需求。 1234567891011121314def create_compiled_keras_model(): model = tf.keras.models.Sequential([ tf.keras.layers.Dense( 10, activation=tf.nn.softmax, kernel_initializer='zeros', input_shape=(784,))]) def loss_fn(y_true, y_pred): return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy( y_true, y_pred)) model.compile( loss=loss_fn, optimizer=gradient_descent.SGD(learning_rate=0.02), metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]) return model 关于编译的一个重要说明。当在Federated Averaging算法中使用时，如下所示，优化器只是总优化算法的一半，因为它仅用于计算每个客户端上的本地模型更新。算法的其余部分涉及如何通过客户端对这些更新进行平均，以及如何将它们应用于服务器上的全局模型。特别是，这意味着此处使用的优化器和学习速率的选择可能需要与您在标准i.i.d上训练模型的选择不同。数据集。我们建议从常规SGD开始，可能比平时学习率更低。我们在这里使用的学习率没有经过仔细调整，随意进行实验。 为了使用任何带有TFF的模型，需要将其包装在tff.learning.Model接口的实例中，该接口公开方法以标记模型的正向传递，元数据属性等，类似于Keras，但也引入了额外的元素，例如控制计算联合度量的过程的方法。我们暂时不要担心这个问题;如果你有一个我们上面刚刚定义的编译过的Keras模型，你可以通过调用tff.learning.from_compiled_keras_model让TFF为你包装它，将模型和样本数据批量作为参数传递，如下所示 123def model_fn(): keras_model = create_compiled_keras_model() return tff.learning.from_compiled_keras_model(keras_model, sample_batch) Training the model on federated data现在我们有一个包装为tff.learning.Model的模型用于TFF，我们可以通过调用辅助函数tff.learning.build_federated_averaging_process让TFF构造一个联合平均算法，如下所示。 请记住，参数需要是构造函数（例如上面的model_fn），而不是已经构造的实例，因此模型的构造可以在由TFF控制的上下文中发生（如果您对原因感到好奇）为此，我们建议您阅读有关自定义算法的后续教程。 12#@test {\"output\": \"ignore\"}iterative_process = tff.learning.build_federated_averaging_process(model_fn) 刚刚发生了什么？ TFF构建了一对联合计算并将它们打包成tff.utils.IterativeProcess，其中这些计算可作为一对初始化和下一个属性使用。 简而言之，联合计算是TFF内部语言中可以表达各种联合算法的程序（您可以在自定义算法教程中找到更多相关信息）。在这种情况下，生成并打包到iterative_process中的两个计算实现联合平均。 TFF的目标是以可以在真实的联合学习设置中执行的方式定义计算，但是目前仅实现本地执行模拟运行时。要在模拟器中执行计算，只需像Python函数一样调用它。此默认解释环境不是为高性能而设计的，但它足以满足本教程的需要;我们希望提供更高性能的仿真运行时，以便在未来版本中进行更大规模的研究。 让我们从初始化(initialize)计算开始。与所有联合计算的情况一样，您可以将其视为一种函数。计算不带参数，并返回一个结果-服务器上联合平均过程的状态表示。虽然我们不想深入了解TFF的细节，但看看这种状态看起来可能是有益的。您可以将其可视化如下。 1234#@test {&quot;output&quot;: &quot;ignore&quot;}str(iterative_process.initialize.type_signature)&apos;( -&gt; &lt;model=&lt;trainable=&lt;dense/kernel=float32[784,10],dense/bias=float32[10]&gt;,non_trainable=&lt;&gt;&gt;,optimizer_state=&lt;int64&gt;&gt;@SERVER)&apos; 虽然上面的类型签名可能起初看起来有点神秘，但您可以认识到服务器状态包含一个模型（MNIST的初始模型参数将分发给所有设备），以及optimizer_state（由服务器维护的附加信息，例如用于超参数调度表的轮数等。 Let’s invoke the initialize computation to construct the server state. 1state = iterative_process.initialize() 接下来，这对联合计算中的第二个表示单轮联合平均,next，其包括将服务器状态（包括模型参数）推送到客户端，对其本地数据进行设备上培训，收集和平均模型更新，并在服务器上生成新的更新模型。 从概念上讲，您可以将next视为具有如下功能类型签名。 1SERVER_STATE, FEDERATED_DATA -&gt; SERVER_STATE, TRAINING_METRICS 特别是，应该考虑next（）不是作为在服务器上运行的函数，而是作为整个分散计算的声明性功能表示 - 一些输入由服务器（SERVER_STATE）提供，但每个参与设备提供自己的本地数据集。 让我们进行一轮培训，并将结果可视化。我们可以将上面已经生成的联合数据用于用户样本。 123#@test {&quot;timeout&quot;: 600, &quot;output&quot;: &quot;ignore&quot;}state, metrics = iterative_process.next(state, federated_train_data)print(&apos;round 1, metrics={}&apos;.format(metrics)) 1round 1, metrics=&lt;sparse_categorical_accuracy=0.142909,loss=3.14069&gt; 让我们再进行几轮。如前所述，通常在此时，您将从每轮随机选择的新用户样本中选择一部分模拟数据，以模拟用户不断进出的实际部署，但在此交互式笔记本中，为了演示，我们将重用相同的用户，以便系统快速收敛。 1234#@test {&quot;skip&quot;: true}for round_num in range(2, 11): state, metrics = iterative_process.next(state, federated_train_data) print(&apos;round {:2d}, metrics={}&apos;.format(round_num, metrics)) 123456789round 2, metrics=&lt;sparse_categorical_accuracy=0.166909,loss=2.90004&gt;round 3, metrics=&lt;sparse_categorical_accuracy=0.203273,loss=2.64551&gt;round 4, metrics=&lt;sparse_categorical_accuracy=0.248364,loss=2.41201&gt;round 5, metrics=&lt;sparse_categorical_accuracy=0.291636,loss=2.19657&gt;round 6, metrics=&lt;sparse_categorical_accuracy=0.341818,loss=1.99344&gt;round 7, metrics=&lt;sparse_categorical_accuracy=0.397455,loss=1.81096&gt;round 8, metrics=&lt;sparse_categorical_accuracy=0.446182,loss=1.65356&gt;round 9, metrics=&lt;sparse_categorical_accuracy=0.486182,loss=1.51823&gt;round 10, metrics=&lt;sparse_categorical_accuracy=0.533455,loss=1.39974&gt; 每轮联合训练后训练损失减少，表明模型正在趋同。这些培训指标有一些重要的注意事项，但请参阅本教程后面的评估部分。 Custom Federated Algorithms, Part 1: Introduction to the Federated Core本教程是两部分系列的第一部分，演示如何使用联合核心（FC）在TensorFlow Federated（TFF）中实现自定义类型的联合算法 - 这是一组作为基础的低级接口。我们已经实现了联邦学习（FL）层。 第一部分更具概念性;我们介绍了TFF中使用的一些关键概念和编程抽象，并且我们使用温度传感器的分布式阵列演示了它们在一个非常简单的例子中的用法。在本系列的第二部分中，我们使用我们在此介绍的机制来实现联合训练和评估算法的简单版本。作为后续行动，我们鼓励您研究tff.learning中联邦平均的实施。 在本系列的最后，您应该能够认识到Federated Core的应用程序不一定仅限于学习。我们提供的编程抽象是非常通用的，并且可以用于例如实现分布式数据上的分析和其他定制类型的计算。 虽然本教程是独立设计的，但我们鼓励您首先阅读有关图像分类和文本生成的教程，以便更高层次，更温和地介绍TensorFlow联合框架和联合学习API（tff.learning），它将帮助您将我们在此描述的概念放在上下文中。 Intended Uses[预期用途]简而言之，Federated Core（FC）是一种开发环境，可以紧凑地表达将TensorFlow代码与分布式通信运算符相结合的程序逻辑，例如在联合平均中使用的运算符如：计算分布式和，平均值和其他类型分布式聚合在系统中的一组客户端设备上，向这些设备分发模型和参数等。 你可能知道tf.contrib.distribute，在这一点上要问的一个自然问题可能是：这个框架在哪些方面有所不同？毕竟，两个框架都试图分发TensorFlow计算。 考虑它的一种方法是，虽然tf.contrib.distribute的既定目标是允许用户使用现有模型和培训代码进行最小的更改以实现分布式培训，并且更加关注如何利用分布式基础架构为了提高现有培训代码的效率，TFF联邦核心的目标是让研究人员和从业人员明确控制他们将在系统中使用的分布式通信的具体模式。 FC的重点是提供一种灵活且可扩展的语言来表达分布式数据流算法，而不是一组具体的已实现的分布式训练功能。 TFF FC API的主要目标受众之一是研究人员和从业人员，他们可能希望尝试新的联合学习算法，并评估影响分布式系统中数据流的编排方式的微妙设计选择的后果，不受系统实现细节的困扰。 FC API的目标抽象级别大致对应于伪代码，可用于描述研究出版物中联合学习算法的机制 - 系统中存在哪些数据以及如何转换，但不会降低到单个点对点网络消息交换。 TFF作为一个整体是针对数据被分发的场景，并且必须保持这样，例如出于隐私原因，并且在集中位置收集所有数据可能不是可行的选择。与其中所有数据可以在数据中心的集中位置累积的情况相比，这暗示了需要更高程度的显式控制的机器学习算法的实现。 Before we startBefore we dive into the code, please try to run the following “Hello World” example to make sure your environment is correctly setup. If it doesn’t work, please refer to the Installation guide for instructions. 1234567#@test {\"skip\": true}# NOTE: If you are running a Jupyter notebook, and installing a locally built# pip package, you may need to edit the following to point to the '.whl' file# on your local filesystem.!pip install -q tensorflow_federated 1234567891011121314151617from __future__ import absolute_import, division, print_functionimport numpy as npfrom six.moves import rangeimport tensorflow as tffrom tensorflow_federated import python as tfftf.enable_resource_variables()@tff.federated_computationdef hello_world(): return 'Hello, World!'hello_world()Hello, World! Federated dataTFF的一个显着特点是它允许您在联合数据上紧凑地表达基于TensorFlow的计算。我们将在本教程中使用术语联合数据来引用分布式系统中一组设备上托管的数据项集合。例如，在移动设备上运行的应用程序可以收集数据并将其存储在本地，而无需上传到集中位置。或者，分布式传感器阵列可以收集并存储其位置处的温度读数。 像上面例子中那样的联合数据在TFF中被视为一等公民first-class citizens，即它们可能作为参数和函数结果出现，并且它们具有类型。为了强化这一概念，我们将联合数据集称为联合值或联合类型的值 。federated values; values of federated types; 需要理解的重点是，我们将所有设备上的整个数据项集合（例如，分布式阵列中所有传感器的整个集合温度读数）建模为单个联合值。 例如，以下是如何在TFF中定义由一组客户端设备托管的 federated float的类型。可以将在一组分布式传感器中实现的温度读数的集合建模为该联合类型的值。 1federated_float_on_clients = tff.FederatedType(tf.float32, tff.CLIENTS) 更一般地，TFF中的(federated type)联合类型是通过指定其成员成分的类型T（驻留在各个设备上的数据项）以及托管此类(federated values)联合值的设备组G（以及第三个，我们将在短期内提及的可选信息。我们将托管联合值的设备组G称为值的位置。因此，tff.CLIENTS是展示位置的示例。 12345str(federated_float_on_clients.member)&apos;float32&apos;str(federated_float_on_clients.placement)&apos;CLIENTS&apos; 具有成员成分T和位置G的联合类型可以紧凑地表示为{T} @G，如下所示 123str(federated_float_on_clients)&apos;{float32}@CLIENTS&apos; 这个简洁符号中的花括号{}提醒您，成员组成部分（不同设备上的数据项）可能会有所不同，正如您所期望的那样，例如温度传感器读数，因此客户端作为一个组的拥有联合多个 T型项目 —— 一起构成联合值的T型项目。 值得注意的是，联合值的成员组成部分通常对程序员不透明，即联合值不应被视为用系统中设备的标识符作为键的简单dict - 这些值旨在仅由抽象地表示集体转换，是通过各种分布式通信协议（例如聚合）的联合操作 federated operators。如果这听起来过于抽象，请不要担心 - 我们将很快回到这一点，我们将用具体的例子来说明它。 these values are intended to be collectively transformed only by federated operators that abstractly represent various kinds of distributed communication protocols (such as aggregation). TFF中的联合类型有两种形式：联合值的成员组成部分可能不同（如上所述），以及已知它们全部相等的类型。这由tff.FederatedType构造函数中的第三个可选的all_equal参数控制（默认为False） 1federated_float_on_clients.all_equal 具有放置G的federated type联合类型，其中已知所有T型成员成分相等，可以紧凑地表示为T @ G（与{T} @G相对，即，花括号去掉以反映多组成员组成部分由单个项目组成的事实。 12str(tff.FederatedType(tf.float32, tff.CLIENTS, all_equal=True))&apos;float32@CLIENTS&apos; 在实际场景中可能出现的这种类型的联合值的一个示例是超参数（诸如学习速率，规范等），其已经由服务器分发到参与联合训练的一组设备。 另一个示例是用于在服务器处预训练的机器学习模型的一组参数，然后将其广播到一组客户端设备，其中它们可以针对每个用户进行个性化。 例如，假设我们有一对float32参数a和b用于简单的一维线性回归模型。我们可以构建（非联合）类型的此类模型以在TFF中使用，如下所示。打印类型字符串中的角度括号&lt;&gt;是命名或未命名元组的紧凑TFF表示法。 1234simple_regression_model_type = ( tff.NamedTupleType([(&apos;a&apos;, tf.float32), (&apos;b&apos;, tf.float32)]))str(simple_regression_model_type) 1&apos;&lt;a=float32,b=float32&gt;&apos; 请注意，我们仅指定上面的dtypes。还支持非标量类型。在上面的代码中，tf.float32是更通用的tff.TensorType（dtype = tf.float32，shape = []）的快捷符号。 将此模型广播给客户端时，生成的联合值的类型可以表示如下。 1234str(tff.FederatedType( simple_regression_model_type, tff.CLIENTS, all_equal=True)) &apos;&lt;a=float32,b=float32&gt;@CLIENTS&apos; 对于上面使用联合浮点数的对称性，我们将这种类型称为联合元组。更一般地说，我们经常使用术语联合XYZ来指代成员组成部分类似于XYZ的联合值。因此，我们将讨论联合元组，联合序列，联合模型等内容。 现在，回到float32 @ CLIENTS - 虽然它似乎在多个设备上复制，但它实际上是一个float32，因为所有成员都是相同的。一般来说，您可能会想到任何所有相等的联合类型，即T @ G形式之一，与非联合类型T同构，因为在这两种情况下，实际上只有一个（尽管可能是复制的）项目T型。 鉴于T和T @ G之间的同构，您可能想知道后者类型可能起什么作用（如果有的话）。继续阅读。. PlacementsDesign Overview在上一节中，我们介绍了放置的概念 - 可能共同托管联合值的系统参与者组，我们已经演示了如何使用tff.CLIENTS作为放置的示例规范。 为了解释为什么放置Placements的概念是如此基本以至于我们需要将它结合到TFF类型系统中，回想一下我们在本教程开头提到的关于TFF的一些预期用途的内容。 虽然在本教程中，您将只看到在模拟环境中本地执行的TFF代码，但我们的目标是让TFF能够编写您可以部署的代码，以便在分布式系统中的物理设备组上执行，可能包括移动或嵌入式设备运行Android。这些设备中的每一个将接收一组单独的指令以在本地执行，这取决于它在系统中扮演的角色（最终用户设备，集中协调器，多层体系结构中的中间层等）。重要的是能够推断哪些设备子集执行什么代码，以及数据的不同部分可能在物理上实现的位置。 在处理例如移动设备上的应用数据时，这尤其重要。由于数据是私有的并且可能是敏感数据，因此我们需要能够静态验证此数据永远不会离开设备（并证明有关数据处理方式的事实）。 placement specifications放置规范是旨在支持此功能的机制之一。 TFF被设计为一个以数据为中心的编程环境，因此，与一些专注于操作和可能运行这些操作的现有框架不同，TFF专注于数据，数据的实现以及数据的转换方式。因此，placement放置被建模为TFF中数据的属性，而不是作为数据操作的属性。实际上，正如您将在下一节中看到的那样，一些TFF操作跨越各个位置，并且“在网络中”运行，可以这么说，而不是由单个机器或一组机器执行。 将某个值的类型表示为T @ G或{T} @G（而不仅仅是T）使数据放置决策明确，并且与TFF编写的程序的静态分析一起，它可以作为提供的基础敏感的设备数据的正式隐私保证。 然而，在这一点上需要注意的一件重要事情是，虽然我们鼓励TFF用户明确表示托管数据的参与设备组（展示位置），但程序员永远不会处理个人参与者的原始数据或身份。 （注意：虽然它远远超出了本教程的范围，但我们应该提到上面有一个值得注意的例外，一个tff.federated_collect运算符用作低级原语，仅用于特殊情况。在不建议可以避免的情况下，因为它可能限制未来可能的应用程序。例如，如果在静态分析过程中，我们确定计算使用这种低级机制，我们可能会禁止它访问某些数据类型。） 在TFF代码的主体内，按照设计，无法枚举构成由tff.CLIENTS表示的组的设备，或探测组中特定设备的存在。在Federated Core API，没有任何设备或客户端标识的概念。架构抽象的基础集或我们提供的支持模拟的核心运行时基础架构中。您编写的所有计算逻辑将表示为整个客户端组上的操作。 回想一下我们之前提到的关于联邦类型的值与Python dict不同的内容，因为它不能简单地枚举它们的成员组成部分。将您的TFF程序逻辑操作的值视为与展示位置（组）相关联，而不是与单个参与者相关联。 Placements 展示位置也被设计为TFF中的一等公民，并且可以作为展示位置类型的参数和结果显示（由API中的tff.PlacementType表示）。将来，我们计划提供各种运算符来转换或组合展示位置，但这超出了本教程的范围。目前，在TFF中将place放置为不透明的原始内置类型就足够了，类似于int和bool是Python中不透明的内置类型，tff.CLIENTS是这种类型的常量文字，与1不同是int类型的常量文字。 Specifying Placements[具体]TFF提供了两个基本的放置文字，即tff.CLIENTS和tff.SERVER，可以很容易地表达丰富多样的实际场景，这些场景自然地建模为客户端 - 服务器架构，具有多个客户端设备（移动电话，嵌入式设备，分布式数据库） ，传感器等）由单个中央服务器协调器编排。 TFF还支持自定义放置，多个客户端组，多层和其他更通用的分布式体系结构，但讨论它们超出了本教程的范围。 TFF没有规定tff.CLIENTS或tff.SERVER实际代表什么。 特别是，tff.SERVER可能是单个物理设备（单个组的成员），但它也可能是运行状态机复制的容错集群中的一组副本 - 我们不做任何特殊的体系结构假设。相反，我们使用前一节中提到的all_equal位来表示我们通常只处理服务器上的单个数据项。 同样，某些应用程序中的tff.CLIENTS可能代表系统中的所有客户端 - 在联邦学习的上下文中我们有时称之为人口，但是例如，在联合平均的生产实现中，它可能代表一个群组 - 一个子集选择参加特定轮次培训的客户。抽象定义的展示位置在其出现的计算被部署用于执行时（或者在模拟环境中简单地像Python函数一样调用，如本教程中所示）具有具体含义。在我们的本地模拟中，客户端组由作为输入提供的联合数据确定。 Federated computationsDeclaring federated computationsTFF被设计为支持模块化开发的强类型函数编程环境。 TFF中的基本组合单元是联合计算federated computation - 可以接受federated values联合值作为输入并将联合值作为输出返回的逻辑部分。以下是如何定义计算，计算上一个示例中传感器阵列报告的温度平均值。 123@tff.federated_computation(tff.FederatedType(tf.float32, tff.CLIENTS))def get_average_temperature(sensor_readings): return tff.federated_mean(sensor_readings) 看看上面的代码，此时您可能会问 - 是不是已经有装饰器构造来定义TensorFlow中的可组合单元，如tf.contrib.eager.defun，如果是这样，为什么要引入另一个，以及如何它不同？ 简短的回答是，tff.federated_computation包装器生成的代码既不是TensorFlow，也不是Python - 它是内部平台独立粘合语言中的分布式系统规范。在这一点上，这无疑听起来很神秘，但请将联邦计算的这种直观解释作为分布式系统的抽象规范。我们马上解释一下。 首先，让我们来看一下这个定义。 TFF计算通常被建模为函数 - 有或没有参数，但具有明确定义的类型签名。您可以通过查询其type_signature属性来打印计算的类型签名，如下所示。 123str(get_average_temperature.type_signature)&apos;({float32}@CLIENTS -&gt; float32@SERVER)&apos; 类型签名告诉我们，计算接受客户端设备上不同传感器读数的集合，并在服务器上返回单个平均值。 在我们进一步讨论之前，让我们反思一下 - 这个计算的输入和输出位于不同的地方（在CLIENTS和SERVER上）。回想一下我们在前一节中关于TFF操作如何跨越位置并在网络中运行的放置，以及我们刚才所说的关于联邦计算表示分布式系统的抽象规范的内容。我们只定义了一个这样的计算 - 一个简单的分布式系统，其中数据在客户端设备上消耗，并且聚合结果出现在服务器上。 在许多实际场景中，表示顶级任务的计算将倾向于接受其输入并在服务器上报告其输出 - 这反映了计算可能由在服务器上发起和终止的查询触发的想法。 但是，FC API没有强加这个假设，我们内部使用的许多构建块（包括API中可能找到的许多tff.federated _…运算符）都有不同位置的输入和输出，所以一般来说，你应该不要将联合计算视为在服务器上运行或由服务器执行的事物。服务器只是联合计算中的一种参与者。在考虑这种计算的机制时，最好始终默认为全局网络范围的视角，而不是单个集中协调器的视角。 通常，功能类型签名分别紧凑地表示为输入和输出的类型T和U的（T→U）。形式参数的类型（在本例中为sensor_readings）被指定为装饰器的参数。您无需指定结果的类型 - 它是自动确定的。 尽管TFF确实提供了有限形式的多态性，但强烈建议程序员明确它们使用的数据类型，因为这样可以更容易地理解，调试和正式验证代码的属性。在某些情况下，明确指定类型是一项要求（例如，多态计算当前不能直接执行）。 Executing federated computations[执行]为了支持开发和调试，TFF允许您直接调用以Python函数的方式定义的计算，如下所示。如果计算需要将all_equal位设置为False的联合类型的值，则可以将其作为Python中的普通列表提供，对于all_equal位设置为True的联合类型，您可以直接提供（单个）成员组成。这也是结果报告给您的方式。 123get_average_temperature([68.5, 70.3, 69.8])69.533333 在仿真模式下运行这样的计算时，您可以充当具有系统范围视图的外部观察者，他可以在网络中的任何位置提供输入和消耗输出，就像这里的情况一样 - 您提供了客户端值在输入，并消耗服务器结果。 现在，让我们回到我们之前关于tff.federated_computation装饰器的注释，该装饰器以粘合语言发出代码。虽然TFF计算的逻辑可以表示为Python中的普通函数（你只需要像我们上面所做的那样使用tff.federated_computation来装饰它们），你可以直接使用Python参数调用它们，就像这里的任何其他Python函数一样。笔记本，在幕后，正如我们前面提到的，TFF计算实际上不是Python。 我们的意思是当Python解释器遇到用tff.federated_computation修饰的函数时，它会跟踪此函数体中的语句一次（在定义时），然后构造计算逻辑的序列化表示以供将来使用 - 是否用于执行，或作为子组件合并到另一个计算中。 您可以通过添加print语句来验证这一点，如下所示： 123456789@tff.federated_computation(tff.FederatedType(tf.float32, tff.CLIENTS))def get_average_temperature(sensor_readings): print ('Getting traced, the argument is \"{}\".'.format( type(sensor_readings).__name__)) return tff.federated_mean(sensor_readings)Getting traced, the argument is \"ValueImpl\". 您可以考虑定义联合计算的Python代码，类似于您在非热切的上下文中构建TensorFlow图的Python代码的方式（如果您不熟悉TensorFlow的非急切使用，请考虑您的Python代码定义了稍后要执行的操作图，但实际上并未实际运行它们。 TensorFlow中非热切的图形构建代码是Python，但由此代码构造的TensorFlow图是独立于平台且可序列化的。 同样，TFF计算是在Python中定义的，但是它们体内的Python语句（例如我们刚才显示的示例中的tff.federated_mean）被编译成一个可移植的，与平台无关的可序列化表示。 作为开发人员，您不需要关心此表示的详细信息，因为您永远不需要直接使用它，但您应该意识到它的存在，事实上TFF计算从根本上不急于求成，并且无法捕获任意Python状态。包含在TFF计算主体中的Python代码在定义时执行，此时在序列化之前跟踪用tff.federated_computation修饰的Python函数的主体。它不会在调用时再次回溯（除非函数是多态的;有关详细信息，请参阅文档页面）。 您可能想知道为什么我们选择引入专用的内部非Python表示。一个原因是最终，TFF计算旨在可部署到真实的物理环境，并托管在可能无法使用Python的移动或嵌入式设备上。 另一个原因是TFF计算表达了分布式系统的全局行为，而不是表达个体参与者的本地行为的Python程序。您可以在上面的简单示例中看到，使用特殊运算符tff.federated_mean接受客户端设备上的数据，但将结果存储在服务器上。 ???运算符tff.federated_mean不能轻易地在Python中建模为普通运算符，因为它不在本地执行 - 如前所述，它代表一个协调多个系统参与者行为的分布式系统。我们将这些运算符称为联合运算符，以区别于Python中的普通（本地）运算符。 TFF类型系统以及TFF语言支持的基本操作集因此与Python中的操作明显不同，因此需要使用专用表示。 Composing federated computations[组成]如上所述，联邦计算及其组成部分最好被理解为分布式系统的模型，您可以将组合联合计算视为从更简单的系统组成更复杂的分布式系统。您可以将tff.federated_mean运算符视为一种带有类型签名的内置模板联合计算（{T} @CLIENTS - &gt; T @ SERVER）（实际上，就像您编写的计算一样，此运算符也具有复杂性结构 - 在引擎盖下我们将其分解为更简单的操作符）。 组合联合计算也是如此。计算get_average_temperature可以在用tff.federated_computation修饰的另一个Python函数的主体中调用 - 这样做会使它嵌入父类的主体中，就像之前tff.federated_mean嵌入它自己的主体一样。 要注意的一个重要限制是用tff.federated_computation修饰的Python函数体必须只包含联合运算符，即它们不能直接包含TensorFlow操作。例如，您不能直接使用tf.contrib.nest接口添加一对联合值。 TensorFlow代码必须限制为使用下一节中讨论的tff.tf_computation修饰的代码块。只有在以这种方式包装时，才能在tff.federated_computation的主体中调用包装的TensorFlow代码。 这种分离的原因是技术性的（很难欺operator如tf.add与非张量一起工作）以及架构。联合计算的语言（即，使用tff.federated_computation修饰的Python函数的序列化主体构造的逻辑）被设计为用作独立于平台的粘合语言。这种粘合语言目前用于从TensorFlow代码的嵌入部分构建分布式系统（仅限于tff.tf_computation块）。在充足的时间内，我们预计需要嵌入其他非TensorFlow逻辑的部分，例如可能代表输入管道的关系数据库查询，所有这些都使用相同的粘合语言（tff.federated_computation块）连接在一起。 TensorFlow logicDeclaring TensorFlow computations[申明]TFF设计用于TensorFlow。因此，您将在TFF中编写的大部分代码可能是普通的（即本地执行的）TensorFlow代码。为了在TFF中使用这样的代码，如上所述，它只需要用tff.tf_computation进行修饰。 For example, here’s how we could implement a function that takes a number and adds 0.5 to it. 123@tff.tf_computation(tf.float32)def add_half(x): return tf.add(x, 0.5) 再一次，看看这个，你可能想知道为什么我们应该定义另一个装饰器tff.tf_computation而不是简单地使用现有的机制，如tf.contrib.eager.defun。与前一节不同，这里我们处理一个普通的TensorFlow代码块。 这有几个原因，其全部处理超出了本教程的范围，但值得命名的主要有两个： 为了嵌入在联邦计算体中使用TensorFlow代码实现的可重用构建块，它们需要满足某些属性 - 例如在定义时获取跟踪和序列化，具有类型签名等。这通常需要某种形式的装饰器。 此外，TFF需要能够接受数据流（表示为tf.data.Datasets）的计算能力，例如机器学习应用程序中的训练示例批处理流，作为输入或输出。 TensorFlow目前不存在此功能; tff.tf_computation装饰器为它提供部分（现在仍然是实验性的）支持。 通常，我们建议尽可能使用TensorFlow的组合本机机制，例如tf.contrib.eager.defun，因为可以预期TFF的装饰器与渴望函数交互的确切方式。 现在，回到上面的示例代码片段，我们刚刚定义的计算add_half可以像任何其他TFF计算一样由TFF处理。特别是，它具有TFF类型签名。 123str(add_half.type_signature)&apos;(float32 -&gt; float32)&apos; 请注意，此类型签名没有展示位置。 TensorFlow计算不能使用或返回联合类型。 您现在还可以将add_half用作其他计算中的构建块。例如，以下是如何使用tff.federated_map运算符将add_half逐点应用于客户端设备上联合浮点数的所有成员组成部分。 1234567@tff.federated_computation(tff.FederatedType(tf.float32, tff.CLIENTS))def add_half_on_clients(x): return tff.federated_map(add_half, x) str(add_half_on_clients.type_signature) &apos;({float32}@CLIENTS -&gt; {float32}@CLIENTS)&apos; Executing TensorFlow computations[执行]用tff.tf_computation定义的计算的执行遵循与我们为tff.federated_computation描述的规则相同的规则。它们可以在Python中作为普通的callable调用，如下所示。 123add_half_on_clients([1.0, 3.0, 2.0])[1.5, 3.5, 2.5] 再一次，值得注意的是，以这种方式调用计算add_half_on_clients模拟了一个分散的过程。数据在客户端上使用，并在客户端上返回。实际上，该计算使每个客户端执行本地动作。在这个系统中没有明确提到的tff.SERVER（即使在实践中，编排这样的处理可能涉及一个）。将这种定义的计算视为概念上类似于MapReduce中的Map阶段。 另外，请记住，我们在上一节中所说的关于TFF计算在定义时序列化的内容对于tff.tf_computation代码也是如此 - add_half_on_clients的Python主体在定义时被跟踪一次。在后续调用中，TFF使用其序列化表示。 用tff.federated_computation修饰的Python方法与用tff.tf_computation修饰的方法之间的唯一区别是后者被序列化为TensorFlow图形（而前者不允许包含直接嵌入其中的TensorFlow代码）。 在引擎盖下，用tff.tf_computation修饰的每个方法暂时禁用急切执行，以便捕获计算的结构。虽然急切执行是在本地禁用的，但欢迎使用急切的TensorFlow，AutoGraph，TensorFlow 2.0构造等，只要您以一种能够正确序列化的方式编写计算逻辑。 For example, the following code will fail: 123456789101112131415#@test {&quot;output&quot;: &quot;ignore&quot;}try: # Eager mode constant_10 = tf.constant(10.) @tff.tf_computation(tf.float32) def add_ten(x): return x + constant_10except Exception as err: print (err) Tensor(&quot;Const_1:0&quot;, shape=(), dtype=float32) must be from the same graph as Tensor(&quot;arg:0&quot;, shape=(), dtype=float32). 上面的失败是因为constant_10已经在图形之外构造了tff.tf_computation在序列化过程中在add_ten体内部构造。 另一方面，调用在tff.tf_computation中调用时修改当前图形的python函数很好： 12345678910def get_constant_10(): return tf.constant(10.)@tff.tf_computation(tf.float32)def add_ten(x): return x + get_constant_10()add_ten(5.0)15.0 请注意，TensorFlow中的序列化机制正在发展，我们期望TFF如何序列化计算的细节也在不断发展。 Working with tf.data.Datasets如前所述，tff.tf_computations的一个独特功能是它们允许您使用代码抽象定义的tf.data.Datasets作为形式参数。需要使用tff.SequenceType构造函数声明要在TensorFlow中表示为数据集的参数。 例如，类型规范tff.SequenceType（tf.float32）定义了TFF中的float元素的抽象序列。序列可以包含张量或复杂的嵌套结构（我们稍后会看到这些结构的例子）。一系列T型项目的简明表示是T *。 1234float32_sequence = tff.SequenceType(tf.float32)str(float32_sequence)&apos;float32*&apos; 假设在我们的温度传感器示例中，每个传感器不仅保持一个温度读数，而且保持多个。以下是如何在TensorFlow中定义TFF计算，该计算使用tf.data.Dataset.reduce运算符计算单个本地数据集中的平均温度。 12345678@tff.tf_computation(tff.SequenceType(tf.float32))def get_local_temperature_average(local_temperatures): sum_and_count = ( local_temperatures.reduce((0.0, 0), lambda x, y: (x[0] + y, x[1] + 1))) return sum_and_count[0] / tf.to_float(sum_and_count[1])str(get_local_temperature_average.type_signature)'(float32* -&gt; float32)' 在用tff.tf_computation修饰的方法体中，TFF序列类型的形式参数简单地表示为行为类似于tf.data.Dataset的对象，即支持相同的属性和方法（它们目前不作为子类实现）该类型 - 这可能随着TensorFlow中数据集的支持的发展而改变）。 您可以按如下方式轻松验证。 123456@tff.tf_computation(tff.SequenceType(tf.int32))def foo(x): return x.reduce(np.int32(0), lambda x, y: x + y)foo([1, 2, 3])6 请记住，与普通的tf.data.Datasets不同，这些类似数据集的对象是占位符。它们不包含任何元素，因为它们表示抽象的序列类型参数，在具体上下文中使用时绑定到具体数据。对于抽象定义的占位符数据集的支持在这一点上仍然有限，并且在TFF的早期，您可能会遇到某些限制，但我们在本教程中不需要担心它们（请参阅文档页面）详情）。 当在本地执行接受模拟模式中的序列的计算时，例如在本教程中，您可以将序列作为Python列表提供，如下所示（以及其他方式，例如，作为tf.data.Dataset in eager模式，但是现在，我们会保持简单）。 123get_local_temperature_average([68.5, 70.3, 69.8])69.533333 与所有其他TFF类型一样，上面定义的序列可以使用tff.NamedTupleType构造函数来定义嵌套结构。例如，以下是如何声明接受A，B序列的计算的计算，并返回其产品的总和。我们在计算主体中包含跟踪语句，以便您可以看到TFF类型签名如何转换为数据集的output_types和output_shapes。 12345678910111213@tff.tf_computation(tff.SequenceType([(&apos;A&apos;, tf.int32), (&apos;B&apos;, tf.int32)]))def foo(ds): print (&apos;output_types = {}, shapes = {}&apos;.format( ds.output_types, ds.output_shapes)) return ds.reduce(np.int32(0), lambda total, x: total + x[&apos;A&apos;] * x[&apos;B&apos;]) output_types = OrderedDict([(&apos;A&apos;, tf.int32), (&apos;B&apos;, tf.int32)]), shapes = OrderedDict([(&apos;A&apos;, TensorShape([])), (&apos;B&apos;, TensorShape([]))])str(foo.type_signature)&apos;(&lt;A=int32,B=int32&gt;* -&gt; int32)&apos;foo([{&apos;A&apos;: 2, &apos;B&apos;: 3}, {&apos;A&apos;: 4, &apos;B&apos;: 5}])26 使用tf.data.Datasets作为形式参数的支持仍然有些限制和发展，尽管在本教程中使用的简单场景中起作用。 Putting it all together现在，让我们再次尝试在联合设置中使用TensorFlow计算。假设我们有一组传感器，每个传感器都有一个本地的温度读数序列。我们可以通过平均传感器的局部平均值来计算全球温度平均值，如下所示。 12345@tff.federated_computation( tff.FederatedType(tff.SequenceType(tf.float32), tff.CLIENTS))def get_global_temperature_average(sensor_readings): return tff.federated_mean( tff.federated_map(get_local_temperature_average, sensor_readings)) 请注意，这不是来自所有客户的所有本地温度读数的简单平均值，因为这将需要根据它们在本地维护的读数的数量来衡量来自不同客户的贡献。我们将其作为练习让读者更新上述代码; tff.federated_mean运算符接受权重作为可选的第二个参数（预期是联合浮点数）。 另请注意，get_global_temperature_average的输入现在变为联合int序列。联合序列是我们通常在联合学习中表示设备上数据的方式，序列元素通常表示数据批次（您很快就会看到这样的示例）。 123str(get_global_temperature_average.type_signature)&apos;({float32*}@CLIENTS -&gt; float32@SERVER)&apos; 以下是我们如何在Python中本地执行数据样本的计算。请注意，我们提供输入的方式现在是列表形式。外部列表遍历由tff.CLIENTS表示的组中的设备，内部的迭代遍历每个设备的本地序列中的元素。 123get_global_temperature_average([[68.0, 70.0], [71.0], [68.0, 72.0, 70.0]])70.0 他总结了本教程的第一部分……我们鼓励你继续第二部分。","link":"/2019/05/09/TensorFlowFederatedTutorial/"},{"title":"Python入门","text":"Python入门概念定位一种高级的，动态类型的多范型编程语言，写法近似伪代码，面向对象。哲学：“优雅”、“明确”、“简单”，简单优雅，尽量写容易看明白的代码，尽量写少的代码。 缺点 运行速度慢，和C程序相比非常慢。Python为解释型语言 源代码不能加密安装安装Python 3.6，mac默认安转Python2.7，可使用Homebrew，在命令行homebrew install python3，进行安装第一个Python程序使用sublime3开发，新建文件hello.py 输入 1print('hello, world') hello, world 输出 12name = input()print(name) 运行cd到当前目录下， 使用 python3 filename.py 基础数据类型和变量 整数: 程序中的表示方法和数学上的写法一致,例如:1,100,-8080,0 浮点数: 1.23e9 字符串: 以单引号’或双引号”括起来的任意文本 'I\\'m \\&quot;OK\\&quot;!' 布尔值: True、False,注意大小写，使用 and, not和or,代表与或非,如：True or True,True and False,not True 空值: 特殊的值, 使用None表示 变量 变量在程序中就是用一个变量名表示，同一个变量可以反复赋值，而且可以是不同类型的变量。 这种变量本身类型不固定的语言称之为动态语言，与之对应的是静态语言。静态语言在定义变量时必须指定变量类型，如果赋值的时候类型不匹配，就会报错 最后，理解变量在计算机内存中的表示也非常重要。 1a = \"abc\" Python解释器干了两件事情： 在内存中创建了一个’ABC’的字符串； 在内存中创建了一个名为a的变量，并把它指向’ABC’。即python中变量名可以理解为一个指针。 12345a = 'ABC'b = aa = 'XYZ'print('a:', a)print('b:', b) a: XYZ b: ABC 执行过程 执行a = ‘ABC’，解释器创建了字符串’ABC’和变量a，并把a指向’ABC’。 执行b = a，解释器创建了变量b，并把b指向a指向的字符串’ABC’。 执行a = ‘XYZ’，解释器创建了字符串’XYZ’，并把a的指向改为’XYZ’，但b并没有更改。 所以，最后打印变量b的结果自然是’ABC’了。 常量 常量就是不能变的变量,Python根本没有任何机制保证PI不会被改变，所以，用全部大写的变量名表示常量只是一个习惯上的用法在Python中，有两种除法，一种除法是/ 110 / 3 3.3333333333333335 /除法计算结果是浮点数，即使是两个整数恰好整除，结果也是浮点数：还有一种除法是//，称为地板除，两个整数的除法仍然是整数： 110 // 3 3 余数运算，可以得到两个整数相除的余数： 110 % 3 1 小结 Python支持多种数据类型，在计算机内部，可以把任何数据都看成一个“对象”，而变量就是在程序中用来指向这些数据对象的，对变量赋值就是把数据和变量给关联起来。 对变量赋值x = y是把变量x指向真正的对象，该对象是变量y所指向的。随后对变量y的赋值不影响变量x的指向。 注意：Python的整数没有大小限制，而某些语言的整数根据其存储长度是有大小限制的，例如Java对32位整数的范围限制在-2147483648-2147483647。 Python的浮点数也没有大小限制，但是超出一定范围就直接表示为inf（无限大）。 字符串和编码字符编码在计算机内存中，统一使用Unicode编码，即两个字节表示一个字符。当需要保存到硬盘或者需要传输的时候，就转换为UTF-8编码。 Python的字符串字符串是以Unicode编码的，也就是说，Python的字符串支持多语言。对于单个字符的编码，Python提供了ord()函数获取字符的整数表示，chr()函数把编码转换为对应的字符 格式化采用的格式化方式和C语言是一致的，用%实现，举例如下： 1'Hello, %s' % 'world' &apos;Hello, world&apos; 1'Hi, %s, you have $%d.' % ('Michael', 1000000) &apos;Hi, Michael, you have $1000000.&apos; %运算符就是用来格式化字符串的。在字符串内部，%s表示用字符串替换，%d表示用整数替换，有几个%?占位符，后面就跟几个变量或者值，顺序要对应好。如果只有一个%?，括号可以省略。 使用list和tuplelistPython内置的一种数据类型是列表：list。list是一种有序的集合，可以随时添加和删除其中的元素。 12classmates = ['Michael', 'Bob', 'Tracy']classmates [&apos;Michael&apos;, &apos;Bob&apos;, &apos;Tracy&apos;] 取最后一个元素，除了计算索引位置外，还可以用-1做索引，直接获取最后一个元素。 list是一个可变的有序表，所以，可以往list中追加，插入，删除，修改等。list是一个可变的有序表，所以，可以往list中追加元素到末尾： 12345678classmates.append('Adam')print('Append: Adam:', classmates)classmates.insert(1, 'Jack')print(\"insert Jack\", classmates)classmates.pop(2) # 替换某个数字print(\"pop\", classmates)classmates[-1] = 'last'print(\"classmates[-1]\", classmates) Append: Adam: [&apos;Michael&apos;, &apos;Jack&apos;, &apos;Tracy&apos;, &apos;Adam&apos;, &apos;last&apos;, &apos;Adam&apos;] insert Jack [&apos;Michael&apos;, &apos;Jack&apos;, &apos;Jack&apos;, &apos;Tracy&apos;, &apos;Adam&apos;, &apos;last&apos;, &apos;Adam&apos;] pop [&apos;Michael&apos;, &apos;Jack&apos;, &apos;Tracy&apos;, &apos;Adam&apos;, &apos;last&apos;, &apos;Adam&apos;] classmates[-1] [&apos;Michael&apos;, &apos;Jack&apos;, &apos;Tracy&apos;, &apos;Adam&apos;, &apos;last&apos;, &apos;last&apos;] 123# list里面的元素的数据类型也可以不同，比如：L = ['Apple', 123, True]L [&apos;Apple&apos;, 123, True] tuple另一种有序列表叫元组：tuple。tuple和list非常类似，但是tuple一旦初始化就不能修改它也没有append()，insert()这样的方法。其他获取元素的方法和list是一样的，你可以正常地使用classmates[0]，classmates[-1]，但不能赋值成另外的元素。 12classmates = ('Michael', 'Bob', 'Tracy')print(\"classmates:\", classmates) classmates: (&apos;Michael&apos;, &apos;Bob&apos;, &apos;Tracy&apos;) 因为tuple不可变，所以代码更安全。如果可能，能用tuple代替list就尽量用tuple。 tuple的陷阱：当你定义一个tuple时，在定义的时候，tuple的元素就必须被确定下来，比如： 1234t = (1, 2)print(t)x = () # 如果要定义一个空的tuple，可以写成()：x (1, 2) () 但是，要定义一个只有1个元素的tuple，如果你这么定义： 12t = (1)print(t) 1 定义的不是tuple，是1这个数！这是因为括号()既可以表示tuple，又可以表示数学公式中的小括号，这就产生了歧义，因此，Python规定，这种情况下，按小括号进行计算，计算结果自然是1。 所以，只有1个元素的tuple定义时必须加一个逗号,，来消除歧义： 12t = (1,) # Python在显示只有1个元素的tuple时，也会加一个逗号,，以免你误解成数学计算意义上的括号。t (1,) 最后来看一个“可变的”tuple： 1234t = ('a', 'b', ['A', 'B'])t[2][0] = \"X\"t[2][1] = \"Y\"print(t) (&apos;a&apos;, &apos;b&apos;, [&apos;X&apos;, &apos;Y&apos;]) 这个tuple定义的时候有3个元素，分别是’a’，’b’和一个list。不是说tuple一旦定义后就不可变了吗？怎么后来又变了？ 变量作为指针；表面上看，tuple的元素确实变了，但其实变的不是tuple的元素，而是list的元素。tuple一开始指向的list并没有改成别的list，所以，tuple所谓的“不变”是说，tuple的每个元素，指向永远不变。即指向’a’，就不能改成指向’b’，指向一个list，就不能改成指向其他对象，但指向的这个list本身是可变的！ 条件判断在Python程序中，用if语句实现： 12345678910age = 12if age &gt;= 18 and age &lt;= 60: print('your age is', age) print('adult')elif age &gt;= 60: print('your age is', age) print('old')else: print('your age is', age) print('teenage') your age is 12 teenage 根据Python的缩进规则,注意不要少写了冒号:, if语句执行有个特点，它是从上往下判断，如果在某个判断上是True，把该判断对应的语句执行后，就忽略掉剩下的elif和else再议 input这是因为input()返回的数据类型是str，str不能直接和整数比较，必须先把str转换成整数。Python提供了int()函数来完成这件事情： 123456s = input('birth: ')birth = int(s) # int() 转成数字型if birth &lt; 2000: print('00前')else: print('00后') birth: 1943 00前 循环for x in ...循环就是把每个元素代入变量x，然后执行缩进块的语句。第二种循环是while循环，只要条件满足，就不断循环，条件不满足时退出循环 1234567sum = 0for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: print(x) sum = sum + xprint(sum)list(range(5))# list 调用 1 2 3 4 5 6 7 8 9 10 55 [0, 1, 2, 3, 4] 123456sum = 0n = 99while n &gt; 0: sum = sum + n n = n - 2print(sum) 2500 使用dict和setdictPython内置了字典：dict的支持，dict全称dictionary，在其他语言中也称为map，使用键-值（key-value）存储，具有极快的查找速度。 1234d={\"Maichel\": 89, \"Adele\": \"ss\", \"John\":\"dd\"}print(d[\"Maichel\"])print(d[\"Adele\"])print(d[\"John\"]) 89 ss dd 要避免key不存在的错误，有两种办法，一是通过in判断key是否存在 二是通过dict提供的get()方法，如果key不存在，可以返回None，或者自己指定的value 12print('Thomas' in d)d.get('Thomas', -1) False -1 123#要删除一个key，用pop(key)方法，对应的value也会从dict中删除：d.pop('John')print(d) {&apos;Maichel&apos;: 89, &apos;Adele&apos;: &apos;ss&apos;} 使用key-value存储结构的dict在Python中非常有用，选择不可变对象作为key很重要，最常用的key是字符串。这是因为dict根据key来计算value的存储位置，如果每次计算相同的key得出的结果不同，那dict内部就完全混乱了。这个通过key计算位置的算法称为哈希算法（Hash）。要保证hash的正确性，作为key的对象就不能变。在Python中，字符串、整数等都是不可变的，因此，可以放心地作为key。而list是可变的，就不能作为key。 setset和dict类似，也是一组key的集合，但不存储value。由于key不能重复，所以，在set中，没有重复的key。要创建一个set，需要提供一个list作为输入集合： 12s = set([1, 2, 3])s {1, 2, 3} 12345sets = set([2, 4, 5])sets.add(4) # 通过add(key)方法可以添加元素到set中，可以重复添加，但不会有效果sets.add(1)sets.remove(5) # 通过remove(key)方法可以删除元素sets {1, 2, 4} 12345# set可以看成数学意义上的无序和无重复元素的集合，因此，两个set可以做数学意义上的交集、并集等操作：s1 = set([1, 2, 3])s2 = set([2, 3, 4])print(s1 &amp; s2)print(s1 | s2) {2, 3} {1, 2, 3, 4} 再议不可变对象str是不变对象，而list是可变对象。对于可变对象，比如list，对list进行操作，list内部的内容是会变化的，比如： 123a = ['c', 'b', 'a']a.sort()print(a) [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;] 而对于不可变对象，比如str，对str进行操作： 1234a = 'abc'b = a.replace('a', 'A')print(b)print(a) Abc abc 要始终牢记的是，a是变量，而’abc’才是字符串对象！有些时候，我们经常说，对象a的内容是’abc’，但其实是指，a本身是一个变量，它指向的对象的内容才是’abc’： a——-&gt; ‘abc’; 当我们调用a.replace(‘a’, ‘A’)时，实际上调用方法replace是作用在字符串对象’abc’上的，而这个方法虽然名字叫replace，但却没有改变字符串’abc’的内容。相反，replace方法创建了一个新字符串’Abc’并返回，如果我们用变量b指向该新字符串，就容易理解了，变量a仍指向原有的字符串’abc’，但变量b却指向新字符串’Abc’了; 所以，对于不变对象来说，调用对象自身的任意方法，也不会改变该对象自身的内容。相反，这些方法会创建新的对象并返回，这样，就保证了不可变对象本身永远是不可变的。 函数调用函数要调用一个函数，需要知道函数的名称和参数,可以从Python的官方网站查看文档，另外从也可以在交互式命令行通过`help(abs)`查看`abs函数`的帮助信息。内置的abs(),绝对值函数； 12345print(abs(-4))print(int('3'))print(float('12.34'))print(str(12.34))print(bool(0)) 4 3 12.34 12.34 False 定义函数 在Python中，定义一个函数要使用def语句，依次写出函数名、括号、括号中的参数和冒号:，然后，在缩进块中编写函数体，函数的返回值用return语句返回。 请注意，函数体内部的语句在执行时，一旦执行到return时，函数就执行完毕，并将结果返回。因此，函数内部通过条件判断和循环可以实现非常复杂的逻辑。如果没有return语句，函数执行完毕后也会返回结果，只是结果为None。return None可以简写为return。 如果你已经把my_abs()的函数定义保存为abstest.py文件了，那么，可以在该文件的当前目录下启动Python解释器，用from abstest import my_abs来导入my_abs()函数，注意abstest是文件名（不含.py扩展名） 空函数 如果想定义一个什么事也不做的空函数，可以用pass语句： pass语句什么都不做，那有什么用？实际上pass可以用来作为占位符，比如现在还没想好怎么写函数的代码，就可以先放一个pass，让代码能运行起来。 12def op(): pass 参数检查 对参数类型做检查，只允许整数和浮点数类型的参数。数据类型检查可以用内置函数isinstance()实现： 1234567891011def my_abs(x): if not isinstance(x, (int, float)): raise TypeError(\"bad operand type\") if x &lt; 0: x = -x else: x = x return xmy_abs(-3)my_abs('a') --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-68-e53a6b326d38&gt; in &lt;module&gt;() 9 10 my_abs(-3) ---&gt; 11 my_abs(&apos;a&apos;) &lt;ipython-input-68-e53a6b326d38&gt; in my_abs(x) 1 def my_abs(x): 2 if not isinstance(x, (int, float)): ----&gt; 3 raise TypeError(&quot;bad operand type&quot;) 4 if x &lt; 0: 5 x = -x TypeError: bad operand type 返回多个值比如在游戏中经常需要从一个点移动到另一个点，给出坐标、位移和角度，就可以计算出新的新的坐标： 1234567import mathdef move(x, y, step, angle): nx = x+step*math.cos(angle) ny = y - step*math.sin(angle) return nx, nyx, y = move(100, 100, 60, math.pi / 6)print(x, y) 151.96152422706632 70.0 但其实这只是一种假象，Python函数返回的仍然是单一值;原来返回值是一个tuple！但是，在语法上，返回一个tuple可以省略括号，而多个变量可以同时接收一个tuple，按位置赋给对应的值，所以，Python的函数返回多值其实就是返回一个tuple，但写起来更方便。 小结 定义函数时，需要确定函数名和参数个数； 如果有必要，可以先对参数的数据类型做检查； 函数体内部可以用return随时返回函数结果； 函数执行完毕也没有return语句时，自动return None。 函数可以同时返回多个值，但其实就是一个tuple。 函数的参数 定义函数的时候，我们把参数的名字和位置确定下来，函数的接口定义就完成了. 除了正常定义的必选参数外，还可以使用默认参数、可变参数和关键字参数，使得函数定义出来的接口，不但能处理复杂的参数，还可以简化调用者的代码。 位置参数 修改后的power(x, n)函数有两个参数：x和n，这两个参数都是位置参数，调用函数时，传入的两个值按照位置顺序依次赋给参数x和n。 1234567def power(x, n): sum = 1 while n &gt; 0: sum = sum * x n = n - 1 return sumpower(2, 3) 8 默认参数 由于我们经常计算x2，所以，完全可以把第二个参数n的默认值设定为2 设置默认参数时，有几点要注意： 是必选参数在前，默认参数在后，否则Python的解释器会报错 是如何设置默认参数:当函数有多个参数时，把变化大的参数放前面，变化小的参数放后面。变化小的参数就可以作为默认参数 定义默认参数要牢记一点：默认参数必须指向不变对象！ 12345678def calc(numbers): sum = 0 for n in numbers: sum = sum +n*n return sum# 但是调用的时候，需要先组装出一个list或tuple：print(calc([1,2,3]))print(calc((1, 2, 3))) 14 14 可变参数 1. 可变参数就是传入的参数个数是可变的，可以是1个、2个到任意个，还可以是0个。 2. 给定一组数字a，b，c……，请计算a2 + b2 + c2 + ……， 要定义出这个函数，我们必须确定输入的参数。由于参数个数不确定，我们首先想到可以把a，b，c……作为一个list或tuple传进来，这样，函数可以定义如下： 12345678# 函数的参数改为可变参数：def calc(*numbers): sum = 0 for n in numbers: sum = sum + n * n return sumprint(calc(1, 2, 3))print(calc(1, 2, 3, 4)) 14 30 定义可变参数和定义一个list或tuple参数相比，仅仅在参数前面加了一个*号。 在函数内部，参数numbers接收到的是一个tuple，因此，函数代码完全不变。但是，调用该函数时，可以传入任意个参数，包括0个参数 如果已经有一个list或者tuple，要调用一个可变参数怎么办？可以这样做： 123nums = [1, 2, 3]calc(nums[0], nums[1], nums[2]) # 过于繁琐calc(*nums) # 允许你在list或tuple前面加一个*号，把list或tuple的元素变成可变参数传进去 14 关键字参数 可变参数允许你传入0个或任意个参数，这些可变参数在函数调用时自动组装为一个tuple。 而关键字参数允许你传入0个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个dict。请看示例： 123456789def person(name, age, **kw): return (\"name:\", name, \"age:\", age, \"others:\",kw)# 函数person除了必选参数name和age外，还接受关键字参数kw。在调用该函数时，可以只传入必选参数：print(person(\"Maichel\", 12))print(person('Bob', 35, city='Beijing'))print(person(\"AoA\", 30, job=\"Engineer\", city=\"HZ\")) # 它可以扩展函数的功能,extra={\"Job\":\"Engineer\", \"City\": \"HZ\"}print(person(\"COC\", 22, **extra)) (&apos;name:&apos;, &apos;Maichel&apos;, &apos;age:&apos;, 12, &apos;others:&apos;, {}) (&apos;name:&apos;, &apos;Bob&apos;, &apos;age:&apos;, 35, &apos;others:&apos;, {&apos;city&apos;: &apos;Beijing&apos;}) (&apos;name:&apos;, &apos;AoA&apos;, &apos;age:&apos;, 30, &apos;others:&apos;, {&apos;job&apos;: &apos;Engineer&apos;, &apos;city&apos;: &apos;HZ&apos;}) (&apos;name:&apos;, &apos;COC&apos;, &apos;age:&apos;, 22, &apos;others:&apos;, {&apos;Job&apos;: &apos;Engineer&apos;, &apos;City&apos;: &apos;HZ&apos;}) **extra表示把extra这个dict的所有key-value用关键字参数传入到函数的**kw参数，kw将获得一个dict，注意kw获得的dict是extra的一份拷贝，对kw的改动不会影响到函数外的extra 命名关键字参数 1. 对于关键字参数，函数的调用者可以传入任意不受限制的关键字参数。至于到底传入了哪些，就需要在函数内部通过kw检查。 2. 仍以person()函数为例，我们希望检查是否有city和job参数 12345678def person(name, age, **kw): if 'city' in kw: pass if 'job' in kw: pass print('name:', name, 'age:', age, 'other:', kw)# 但是调用者仍可以传入不受限制的关键字参数：person('Jack', 24, city='Beijing', addr='Chaoyang', zipcode=123456) name: Jack age: 24 other: {&apos;city&apos;: &apos;Beijing&apos;, &apos;addr&apos;: &apos;Chaoyang&apos;, &apos;zipcode&apos;: 123456} 12345678# 如果要限制关键字参数的名字，就可以用命名关键字参数，例如，只接收city和job作为关键字参数。这种方式定义的函数如下：def person(name, age, *, job, city): print(name, age, city, job)# 和关键字参数**kw不同，命名关键字参数需要一个特殊分隔符*，*后面的参数被视为命名关键字参数。person('Jack', 24, city='Beijing', job='Engineer')#如果函数定义中已经有了一个可变参数，后面跟着的命名关键字参数就不再需要一个特殊分隔符*了：def person2(name, age, *args, city, job): print(name, age, args, city, job) Jack 24 Beijing Engineer 123456# 命名关键字参数必须传入参数名，这和位置参数不同。如果没有传入参数名，调用将报错：person('Jack', 24, 'Beijing', 'Engineer')# 由于调用时缺少参数名city和job，Python解释器把这4个参数均视为位置参数，但person()函数仅接受2个位置参数。# 命名关键字参数可以有缺省值，从而简化调用def person(name, age, *, city='Beijing', job): print(name, age, city, job) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-89-d3e3a315b65a&gt; in &lt;module&gt;() 1 # 命名关键字参数必须传入参数名，这和位置参数不同。如果没有传入参数名，调用将报错： ----&gt; 2 person(&apos;Jack&apos;, 24, &apos;Beijing&apos;, &apos;Engineer&apos;) TypeError: person() takes 2 positional arguments but 4 were given 参数组合 1. 定义函数，可以用必选参数、默认参数、可变参数、关键字参数和命名关键字参数，这5种参数都可以组合使用。 2. 但是请注意，参数定义的顺序必须是：必选参数、默认参数、可变参数、命名关键字参数和关键字参数。 123456789def f1(a, b, c=0, *args, **kw): print('a =', a, 'b =', b, 'c =', c, 'args =', args, 'kw =', kw)def f2(a, b, c=0, *, d, **kw): print('a =', a, 'b =', b, 'c =', c, 'd =', d, 'kw =', kw)print(f1(1, 2))print(f1(1, 2, c=3))print(f1(1, 2, 3, 'a', 'b'))f1(1, 2, 3, 'a', 'b', x=99)f2(1, 2, d=99, ext=None) a = 1 b = 2 c = 0 args = () kw = {} None a = 1 b = 2 c = 3 args = () kw = {} None a = 1 b = 2 c = 3 args = (&apos;a&apos;, &apos;b&apos;) kw = {} None a = 1 b = 2 c = 3 args = (&apos;a&apos;, &apos;b&apos;) kw = {&apos;x&apos;: 99} a = 1 b = 2 c = 0 d = 99 kw = {&apos;ext&apos;: None} 小结 1. Python的函数具有非常灵活的参数形态，既可以实现简单的调用，又可以传入非常复杂的参数。 2. 默认参数一定要用不可变对象，如果是可变对象，程序运行时会有逻辑错误！ 3. 要注意定义可变参数和关键字参数的语法： 1. `*args`是可变参数，args接收的是一个tuple； 2. `**kw`是关键字参数，kw接收的是一个dict。 4. 以及调用函数时如何传入可变参数和关键字参数的语法： 1. 可变参数既可以直接传入：`func(1, 2, 3)`，又可以先组装`list或tuple`，再通过`*args`传入：`func(*(1, 2, 3))`； 2. 关键字参数既可以直接传入：func(a=1, b=2)，又可以先组装dict，再通过`**kw`传入：`func(**{&apos;a&apos;: 1, &apos;b&apos;: 2})`。 3. 使用`*args`和`**kw`是Python的习惯写法，当然也可以用其他参数名，但最好使用习惯用法。 4. 命名的关键字参数是为了限制调用者可以传入的参数名，同时可以提供默认值。 5. 定义命名的关键字参数在没有可变参数的情况下不要忘了写分隔符*，否则定义的将是位置参数。 递归函数1. 在函数内部，可以调用其他函数。如果一个函数在内部调用自身本身，这个函数就是递归函数。 2. 递归函数注意 递归公式与递归结束条件 高级特性切片取前3个元素，用一行代码就可以完成切片： 12L=[\"AA\",\"vvv\", \"dddd\",\"ddd\"]print(L[0:3]) #左闭右开[) [&apos;AA&apos;, &apos;vvv&apos;, &apos;dddd&apos;] 前4个数，每两个取一个： 12345L=[\"AA\",\"vvv\", \"dddd\",\"ddd\"]print(L[0:3])print(L[-2:])#记住倒数第一个元素的索引是-1。print(L[-2:-1])print(L[-1:-2]) [&apos;AA&apos;, &apos;vvv&apos;, &apos;dddd&apos;] [&apos;dddd&apos;, &apos;ddd&apos;] [&apos;dddd&apos;] [] 反向： 12L=[\"AA\",\"vvv\", \"dddd\",\"ddd\"]L[::-1]# 反向取值 [&apos;ddd&apos;, &apos;dddd&apos;, &apos;vvv&apos;, &apos;AA&apos;] tuple也是一种list，唯一区别是tuple不可变。因此，tuple也可以用切片操作，只是操作的结果仍是tuple： 1(0, 1, 2, 3, 4)[:3] (0, 1, 2) 字符串’xxx’也可以看成是一种list，每个元素就是一个字符。因此，字符串也可以用切片操作，只是操作结果仍是字符串 1'ABCDEF'[::-2] &apos;FDB&apos; 迭代如果给定一个list或tuple，我们可以通过for循环来遍历这个list或tuple，这种遍历我们称为迭代（Iteration）。Python的for循环不仅可以用在list或tuple上，还可以作用在其他可迭代对象上.只要是可迭代对象，无论有无下标，都可以迭代，如dict，str等 1234567dict = {\"a\":1, \"b\":2, \"c\":{\"hello\":4, \"sky\": \"full\"}}for k,v in dict.items(): print(k,v)for key in dict: print(key)for value in dict.values(): print(value) a 1 b 2 c {&apos;hello&apos;: 4, &apos;sky&apos;: &apos;full&apos;} a b c 1 2 {&apos;hello&apos;: 4, &apos;sky&apos;: &apos;full&apos;} 当我们使用for循环时，只要作用于一个可迭代对象，for循环就可以正常运行，而我们不太关心该对象究竟是list还是其他数据类型。 那么，如何判断一个对象是可迭代对象呢？方法是通过collections模块的Iterable类型判断: 123456789from collections import Iterableprint(isinstance(\"ABC\", Iterable))print(isinstance(123, Iterable))# 要对list实现类似Java那样的下标循环，Python内置的enumerate函数可以把一个list变成索引-元素对，# 这样就可以在for循环中同时迭代索引和元素本身：for i, value in enumerate([\"a\",\"d\", \"f\"]): print(i, value)for (x, y) in [(1, 1,), (2, 3),(4, 5)]: print(x, y) True False 0 a 1 d 2 f 1 1 2 3 4 5 列表生成式列表生成式即List Comprehensions，是Python内置的非常简单却强大的可以用来创建list的生成式。生成list [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]可以用list(range(1, 11))：可以用: 123# 写列表生成式时，把要生成的元素x * x放到(前面放输出结果)，后面跟for循环，就可以把list创建出来，十分有用，多写几次，很快就可以熟悉这种语法。# for循环(后面放内容，判断条件)还可以加上if判断，这样我们就可以筛选出仅偶数的平方[x*x for x in range(1, 11) if x%2 == 0] [4, 16, 36, 64, 100] 生成器如果列表元素可以按照某种算法推算出来，那我们可以在循环的过程中不断推算出后续的元素。这样就不必创建完整的list，从而节省大量的空间。一边循环一边计算的机制，称为生成器：generator。第一种方法很简单，只要把一个列表生成式的[]改成()，就创建了一个generator： 123456L = [x*x for x in range(10)] print(L)L2 = (x*x for x in range(10))# 可以直接打印出list的每一个元素，要一个一个打印出来，可以通过next()函数获得generator的下一个返回值：print(L2)next(L2)next(L2) # ，generator保存的是算法，每次调用next(g)，就计算出g的下一个元素的值，直到计算到最后一个元素，没有更多的元素时，抛出StopIteration的错误。 [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] &lt;generator object &lt;genexpr&gt; at 0x7f82a028aa98&gt; 1 1234# 正确的方法是使用for循环，因为generator也是可迭代对象：L = (x*x for x in range(10))for item in L: print(item) 0 1 4 9 16 25 36 49 64 81 定义generator的另一种方法。如果一个函数定义中包含yield关键字，那么这个函数就不再是一个普通函数，而是一个generator： 123456789101112def fib(max): n, a, b = 0, 0, 1 while n &lt; max: print(b) a, b = b, a+b n = n + 1 return 'done'# 其中a, b = b, a+b # 相当于 t = (b, a+b) t是一个元组，元素不变# a = t[0] # b = t[1]fib(5) 1 1 2 3 5 &apos;done&apos; 123456789101112# 可以看出，fib函数实际上是定义了斐波拉契数列的推算规则，可以从第一个元素开始，推算出后续任意的元素，这种逻辑其实非常类似generator。# 上面的函数和generator仅一步之遥。要把fib函数变成generator，只需要把print(b)改为yield b就可以了def fib(max): n, a, b = 0, 0, 1 while n &lt; max: yield b a, b = b, a+b n = n+1 return 'done'# 如果一个函数定义中包含yield关键字，那么这个函数就不再是一个普通函数，而是一个generator：f = fib(6)f &lt;generator object fib at 0x7fdad92d8a98&gt; generator和函数的执行流程不一样。函数是顺序执行，遇到return语句或者最后一行函数语句就返回。而变成generator的函数，在每次调用next()的时候执行，遇到yield语句返回，再次执行时从上次返回的yield语句处继续执行。 123456789101112def odd(): print('step 1') yield 1 print('step 2') yield(3) print('step 3') yield(5)o = odd()next(o)next(o)next(o)next(o) step 1 step 2 step 3 --------------------------------------------------------------------------- StopIteration Traceback (most recent call last) &lt;ipython-input-4-35e8ec08e813&gt; in &lt;module&gt;() 10 next(o) 11 next(o) ---&gt; 12 next(o) StopIteration: 可以看到，odd不是普通函数，而是generator，在执行过程中，遇到yield就中断，下次又继续执行。执行3次yield后，已经没有yield可以执行了，所以，第4次调用next(o)就报错。 回到fib的例子，我们在循环过程中不断调用yield，就会不断中断。当然要给循环设置一个条件来退出循环，不然就会产生一个无限数列出来。 同样的，把函数改成generator后，我们基本上从来不会用next()来获取下一个返回值，而是直接使用for循环来迭代： 12for n in fib(6): print(n) 1 1 2 3 5 8 但是用for循环调用generator时，发现拿不到generator的return语句的返回值。如果想要拿到返回值，必须捕获StopIteration错误，返回值包含在StopIteration的value中： 123456789g = fib(6)# 格式 generatorwhile True: try: x = next(g) print(\"g:\",x) except StopIteration as e: print(\"Generator return value:\",e.value) break g: 1 g: 1 g: 2 g: 3 g: 5 g: 8 Generator return value: done 123456789101112131415161718192021222324def triangles():# 杨辉triangles currentLine = [1] while True: yield currentLine next1 = [0] + currentLine next2 = currentLine + [0] i = 0 # i 表示list各项的下标，用于将两个list各项数值相加。list长度不变 currentLine = [] while i &lt; len(next1): currentLine.append(next1[i] + next2[i]) i = i+1# 方法二def triangles (): List = [1] while True: yield List List =[1] + [value + List[index - 1] for index, value in enumerate(List) if index - 1 &gt;= 0] + [1]n = 0for i in triangles(): print(i) n = n+1 if n == 10: break [1] [1, 1] [1, 2, 1] [1, 3, 3, 1] [1, 4, 6, 4, 1] [1, 5, 10, 10, 5, 1] [1, 6, 15, 20, 15, 6, 1] [1, 7, 21, 35, 35, 21, 7, 1] [1, 8, 28, 56, 70, 56, 28, 8, 1] [1, 9, 36, 84, 126, 126, 84, 36, 9, 1] 迭代器我们已经知道，可以直接作用于for循环的数据类型有以下几种： 1. 集合数据类型，如`list、tuple、dict、set、str`等； 2. `generator`，包括生成器和带yield的`generator function`。 这些可以直接作用于for循环的对象统称为可迭代对象：Iterable。可以使用isinstance()判断一个对象是否是Iterable对象。生成器都是Iterator对象，但list、dict、str虽然是Iterable，却不是Iterator。 把list、dict、str等Iterable变成Iterator可以使用iter()函数 123456from collections import Iterablefrom collections import Iteratorprint(isinstance([], Iterable))print(isinstance(12, Iterable))print(isinstance([], Iterator))print(isinstance(iter((x for x in [1, 2, 3])), Iterator)) True False False True 为什么list、dict、str等数据类型不是Iterator？ 这是因为Python的Iterator对象表示的是一个数据流，Iterator对象可以被next()函数调用并不断返回下一个数据，直到没有数据时抛出StopIteration错误。可以把这个数据流看做是一个有序序列，但我们却不能提前知道序列的长度，只能不断通过next()函数实现按需计算下一个数据，所以Iterator的计算是惰性的，只有在需要返回下一个数据时它才会计算。 Iterator甚至可以表示一个无限大的数据流，例如全体自然数。而使用list是永远不可能存储全体自然数的。 小结 凡是可作用于for循环的对象都是Iterable类型； 凡是可作用于next()函数的对象都是Iterator类型，它们表示一个惰性计算的序列； 集合数据类型如list、dict、str等是Iterable但不是Iterator，不过可以通过iter()函数获得一个Iterator对象。 Pythonfor循环本质上就是通过不断调用next()函数实现的 函数式编程 函数是Python内建支持的一种封装，我们通过把大段代码拆成函数，通过一层一层的函数调用，就可以把复杂任务分解成简单的任务，这种分解可以称之为面向过程的程序设计。函数就是面向过程的程序设计的基本单元。 函数式编程（请注意多了一个“式”字）——Functional Programming，虽然也可以归结到面向过程的程序设计，但其思想更接近数学计算。函数式编程就是一种抽象程度很高的编程范式，纯粹的函数式编程语言编写的函数没有变量，因此，任意一个函数，只要输入是确定的，输出就是确定的，这种纯函数我们称之为没有副作用。而允许使用变量的程序设计语言，由于函数内部的变量状态不确定，同样的输入，可能得到不同的输出，因此，这种函数是有副作用的。 函数式编程的一个特点就是，允许把函数本身作为参数传入另一个函数，还允许返回一个函数！Python对函数式编程提供部分支持。由于Python允许使用变量，因此，Python不是纯函数式编程语言。 高阶函数高阶函数英文叫Higher-order function。什么是高阶函数？我们以实际代码为例子，一步一步深入概念。 变量可以指向函数: 函数本身也可以赋值给变量，即：变量可以指向函数 12345678print(abs)# build-in function# 可见，abs(-10)是函数调用，而abs是函数本身。# 要获得函数调用结果，我们可以把结果赋值给变量：x = abs(-10)print(x)# 如果一个变量指向了一个函数，那么，可否通过该变量来调用这个函数？用代码验证一下：f = absprint(f(-12)) &lt;built-in function abs&gt; 10 12 函数名也是变量: 函数名其实就是指向函数的变量,对于abs()这个函数，完全可以把函数名abs看成变量，它指向一个可以计算绝对值的函数！如果把abs指向其他对象，会有什么情况发生？abs = 10, abs(-10)报错 传入函数: 既然变量可以指向函数，函数的参数能接收变量，那么一个函数就可以接收另一个函数作为参数，这种函数就称之为高阶函数。 1234def add(x, y , f): return f(x) + f(y)add(-1, -4, f)# 把函数作为参数传入，这样的函数称为高阶函数，函数式编程就是指这种高度抽象的编程范式。 5 map/reduce Python内建了map()和reduce()函数。 我们先看map。map()函数接收两个参数，一个是函数，一个是Iterable，map将传入的函数依次作用到序列的每个元素，并把结果作为新的Iterator返回。 map()传入的第一个参数是f，即函数对象本身。由于结果r是一个Iterator，Iterator是惰性序列，因此通过list()函数让它把整个序列都计算出来并返回一个list。 1list(map(str, [1, 2, 3, 4, 5, 6, 7, 8, 9])) [&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;4&apos;, &apos;5&apos;, &apos;6&apos;, &apos;7&apos;, &apos;8&apos;, &apos;9&apos;] reduce把一个函数作用在一个序列[x1, x2, x3, ...]上，这个函数必须接收两个参数，reduce把结果继续和序列的下一个元素做累积计算，其效果就是：reduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4) 12345from functools import reducedef add(x, y): return 10*x +yprint('1',reduce(add, [2, 3, 4, 5]))#from functools import reduce 1 2345 12345678# 我们就可以写出把str转换为int的函数from functools import reducedef add(x, y): return 10*x + ydef char2num(c): digits = {'0':0, '1':1, '2':2, '3':3, '4':4, '5':5, '6':6, '7':7, '8':8, '9':9} return digits[c]reduce(add, map(char2num, '124363')) 124363 123456789101112131415from functools import reduceDIGITS = {'0':0, '1':1, '2':2, '3':3, '4':4, '5':5, '6':6, '7':7, '8':8, '9':9}def char2int(s): def add(x, y): return 10*x + y def char2num(s): return DIGITS[s] return reduce(add, map(char2num, s))char2int('45609876543')# =&gt;用lambda函数进一步简化成：def char2int2(s): def char2num(c): return DIGITS[c] return reduce(lambda x, y : 10*x+y, map(char2num, s))char2int2('456098765431') 456098765431 filter Python内建的filter()函数用于过滤序列。 和map()类似，filter()也接收一个函数和一个序列。和map()不同的是，filter()把传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素。 filter()把传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素。 123456def is_odd(n): return n % 2 == 1list(filter(is_odd, [1, 2, 4, 5, 6, 9, 10, 15]))# 可见用filter()这个高阶函数，关键在于正确实现一个“筛选”函数。#注意到filter()函数返回的是一个Iterator，也就是一个惰性序列，所以要强迫filter()完成计算结果，需要用list()函数获得所有结果并返回list。 [1, 5, 9, 15] 1234567891011121314151617181920# 先构造一个从3开始的奇数序列：def _odd_iter(): #这是一个生成器，并且是一个无限序列。 n = 1 while True: n = n+2 yield ndef _not_divisiable(n):# 定义一个筛选函数： return lambda x: x%n &gt; 0def primes():#定义一个生成器，不断返回下一个素数： yield 2 it = _odd_iter()# 初始序列 while True: n = next(it) # 返回序列的第一个数 yield n it = filter(_not_divisiable(n), it)for n in primes(): if n &lt; 10: print(n) else: break 2 3 5 7 sortedsorted()函数也是一个高阶函数，它还可以接收一个key函数来实现自定义的排序，例如按绝对值大小排序： 12print(sorted([2, -4, 5, -9], key=abs))print(sorted(['alsd', 'Affne', 'ieur', 'Zeir'],key=str.lower, reverse=True)) [2, -4, 5, -9] [&apos;Zeir&apos;, &apos;ieur&apos;, &apos;alsd&apos;, &apos;Affne&apos;] 返回函数函数作为返回值高阶函数除了可以接受函数作为参数外，还可以把函数作为结果值返回。 1234567891011121314151617def sum_calc(*args): sum = 0 for n in args: sum = sum+n return sum# 如果不需要立刻求和，而是在后面的代码中，根据需要再计算怎么办？可以不返回求和的结果，而是返回求和的函数：def lazy_sum(*args): def sum_calc(): sum = 0 for n in args: sum = sum + n return sum return sum return sum_calcf = lazy_sum(1, 4, 5, 9)print(f)print(f()) &lt;function lazy_sum.&lt;locals&gt;.sum_calc at 0x7fdad89a6488&gt; 1 在这个例子中，我们在函数lazy_sum中又定义了函数sum，并且，内部函数sum可以引用外部函数lazy_sum的参数和局部变量，当lazy_sum返回函数sum时，相关参数和变量都保存在返回的函数中，这种称为“闭包（Closure）”的程序结构拥有极大的威力。 1234# 请再注意一点，当我们调用lazy_sum()时，每次调用都会返回一个新的函数，即使传入相同的参数：f1 = lazy_sum(1, 2, 3, 5, 6)f2 = lazy_sum(1, 2, 3, 5, 6)f1 == f2# f1()和f2()的调用结果互不影响 False 闭包注意到返回的函数在其定义内部引用了局部变量args，所以，当一个函数A返回了一个函数B后，其内部的局部变量还被新函数B引用，所以，闭包用起来简单，实现起来可不容易。 1234567891011def count(): fs = [] for i in range(1, 4): def f(): return i*i fs.append(f) return fsf1, f2, f3 = count()print(f1())print(f2())print(f3()) 9 9 9 全部都是9！原因就在于返回的函数引用了变量i，但它并非立刻执行。等到3个函数都返回时，它们所引用的变量i已经变成了3，因此最终结果为9。 返回闭包时牢记一点：返回函数不要引用任何循环变量，或者后续会发生变化的变量。 如果一定要引用循环变量怎么办？方法是再创建一个函数，用该函数的参数绑定循环变量当前的值，无论该循环变量后续如何更改，已绑定到函数参数的值不变： 12345678910111213def count(): def f(j): def g(): return j*j return g fs =[] for i in range(1, 4): fs.append(f(i)) return fsf1, f2, f3 = count()print(f1())print(f2())print(f3()) 1 4 9 匿名函数关键字lambda表示匿名函数，冒号前面的x表示函数参数。 匿名函数有个限制，就是只能有一个表达式，不用写return，返回值就是该表达式的结果。 用匿名函数有个好处，因为函数没有名字，不必担心函数名冲突。 1list(map(lambda x:x*x, [1, 2, 3, 4, 5, 6, 7, 8, 9])) [1, 4, 9, 16, 25, 36, 49, 64, 81] 1234# 匿名函数也是一个函数对象，也可以把匿名函数赋值给一个变量，再利用变量来调用该函数：f = lambda x: x*xprint(f)print(f(5)) &lt;function &lt;lambda&gt; at 0x7fdad89a6950&gt; 25 装饰器由于函数也是一个对象，而且函数对象可以被赋值给变量，所以，通过变量也能调用该函数。质上，decorator就是一个返回函数的高阶函数。所以，我们要定义一个能打印日志的decorator. 1234567def now(): print('2018-10-10')f = nowf()# 函数对象有一个__name__属性，可以拿到函数的名字：print(now.__name__)print(f.__name__) 2018-10-10 now now 假设我们要增强now()函数的功能，比如，在函数调用前后自动打印日志，但又不希望修改now()函数的定义，这种在代码运行期间动态增加功能的方式，称之为“装饰器”（Decorator）。 本质上，decorator就是一个返回函数的高阶函数。所以，我们要定义一个能打印日志的decorator，可以定义如下： 12345678910111213def log(func):#高阶函数 返回打印函数 def wrapper(*args, **kw):# 打印函数 打印日志，返回传入的函数 print(\"wrapper call %s()\" % func.__name__) return func(*args, **kw) return wrapper# 观察上面的log，因为它是一个decorator，所以接受一个函数作为参数，并返回一个函数。# 我们要借助Python的@语法，把decorator置于函数的定义处：@logdef now(): print(\"2018-10-1\") now() wrapper call now() 2018-10-1 分析： 调用now()函数，不仅会运行now()函数本身，还会在运行now()函数前打印一行日志： 把@log放到now()函数的定义处，相当于执行了语句：now = log(now) 由于log()是一个decorator，返回一个函数，所以，原来的now()函数仍然存在，只是现在同名的now变量指向了新的函数， 于是调用now()将执行新函数，即在log()函数中返回的wrapper()函数。 wrapper()函数的参数定义是(*args, **kw)，因此，wrapper()函数可以接受任意参数的调用。 在wrapper()函数内，首先打印日志，再紧接着调用原始函数。 如果decorator本身需要传入参数，那就需要编写一个返回decorator的高阶函数，写出来会更复杂。比如，要自定义log的文本 12345678910111213def log(text): def decorator(func): def wrapper(*args, **kw): print(\"%s %s()\" % (text, func.__name__)) return func(*args, **kw) return wrapper return decorator@log('execute')def now(): print('2018-10-10')now() excute now() 2018-10-10 分析和两层嵌套的decorator相比，3层嵌套的效果是这样的：now = log(‘execute’)(now) 我们来剖析上面的语句，首先执行log('execute')，返回的是decorator函数，再调用返回的函数，参数是now函数，返回值最终是wrapper函数。 以上两种decorator的定义都没有问题，但还差最后一步。因为我们讲了函数也是对象，它有name等属性，但你去看经过decorator装饰之后的函数，它们的name已经从原来的’now’变成了’wrapper’。 因为返回的那个wrapper()函数名字就是’wrapper’，所以，需要把原始函数的name等属性复制到wrapper()函数中，否则，有些依赖函数签名的代码执行就会出错。不需要编写wrapper.__name__ = func.__name__这样的代码，Python内置的functools.wraps就是干这个事的，所以，一个完整的decorator的写法如下： 12345678910111213141516171819202122232425import functoolsdef log(func): @functools.wraps(func) def wrapper(*args, **kw): print(\"wrapper %s()\"%func.__name__) return func(*args, **kw) return wrapper@logdef now(): print('2018003')now()# 针对带参数def log2(text): def decorator(func): @functools.wraps(func) def wrapper(*args, **kw): print(\"%s %s()\"%(text, func.__name__)) return func(*args, **kw) return wrapper return decorator@log2('fff')def now2(): print('fjnef')now() wrapper now() 2018003 wrapper now() 2018003 小结 在面向对象（OOP）的设计模式中，decorator被称为装饰模式。OOP的装饰模式需要通过继承和组合来实现，而Python除了能支持OOP的decorator外，直接从语法层次支持decorator。Python的decorator可以用函数实现，也可以用类实现。 decorator可以增强函数的功能，定义起来虽然有点复杂，但使用起来非常灵活和方便。 请编写一个decorator，能在函数调用的前后打印出’begin call’和’end call’的日志。 1234567891011121314151617181920212223242526272829303132333435363738394041424344import functools, timedef log(func): @functools.wraps(func) def wrapper(*args, **kw): print(\"begin call\") result = func(*args, **kw) print(\"end call\") return result return wrapper# @log# def fn(x, y):# time.sleep(0.5)# return x + y# fn(2, 3)def logger(text): # 有三种方法可以进行判断: # if(hasattr(text,'__call__')): # if(type(text)!=type('')): # if(callable(text)): if(callable(text)): @functools.wraps(text) def wrapper(*args, **kw): print(\"%s!!\"%(text.__name__)) return text(*args, **kw) return wrapper def decorator(func): @functools.wraps(text) def wrapper(*args, **kw): print(\"%s say: %s\"% (func.__name__, text)) return func(*args, **kw) return wrapper return decorator@loggerdef fn(x, y): return x+y@logger(\"use execute\")def fn2(x, y): return x*yprint(fn(1, 2))print(fn2(3, 4)) fn!! 3 fn2 say: use execute 12 偏函数Python的functools模块提供了很多有用的功能，其中一个就是偏函数（Partial function）。要注意，这里的偏函数和数学意义上的偏函数不一样。当函数的参数个数太多，需要简化时，使用functools.partial可以创建一个新的函数，这个新函数可以固定住原函数的部分参数，从而在调用时更简单。 123456print(int('12345'))print(int(\"12345\", base = 8))print(int(\"12345\", 16))print(int('101010', base=2))#字符串转成数字101010，1010101按照二进制转成10进制显示# functools.partial就是帮助我们创建一个偏函数的，不需要我们自己定义int2()，可以直接使用下面的代码创建一个新的函数int2：# 简单总结functools.partial的作用就是，把一个函数的某些参数给固定住（也就是设置默认值），返回一个新的函数，调用这个新函数会更简单。 12345 5349 74565 42 1234import functoolsint2 = functools.partial(int, base=2)int2('100010')#当函数的参数个数太多，需要简化时，使用functools.partial可以创建一个新的函数，这个新函数可以固定住原函数的部分参数，从而在调用时更简单。 34 模块 为了编写可维护的代码，我们把很多函数分组，分别放到不同的文件里，这样，每个文件包含的代码就相对较少，很多编程语言都采用这种组织代码的方式。在Python中，一个.py文件就称之为一个模块（Module）。 模块是一组Python代码的集合，可以使用其他模块，也可以被其他模块使用。 优势： 最大的好处是大大提高了代码的可维护性。其次，编写代码不必从零开始。当一个模块编写完毕，就可以被其他地方引用。我们在编写程序的时候，也经常引用其他模块，包括Python内置的模块和来自第三方的模块。 使用模块还可以避免函数名和变量名冲突。相同名字的函数和变量完全可以分别存在不同的模块中，因此，我们自己在编写模块时，不必考虑名字会与其他模块冲突。但是也要注意，尽量不要与内置函数名字冲突。 创建自己的模块时，要注意： 模块名要遵循Python变量命名规范，不要使用中文、特殊字符； 模块名不要和系统模块名冲突，最好先查看系统是否已存在该模块，检查方法是在Python交互环境执行import abc，若成功则说明系统存在此模块。 引入了包以后，只要顶层的包名不与别人冲突，那所有模块都不会与别人冲突。现在，abc.py模块的名字就变成了mycompany.abc，类似的，xyz.py的模块名变成了mycompany.xyz。请注意，每一个包目录下面都会有一个__init__.py的文件，这个文件是必须存在的，否则，Python就把这个目录当成普通目录，而不是一个包。__init__.py可以是空文件，也可以有Python代码，因为__init__.py本身就是一个模块，而它的模块名就是mycompany。类似的，可以有多级目录，组成多级层次的包结构。比如如下的目录结构： 12345678mycompany ├─ web │ ├─ __init__.py │ ├─ utils.py │ └─ www.py ├─ __init__.py ├─ abc.py └─ xyz.py 使用就是导入该模块，例如import os 12345678910111213141516171819#!/usr/bin/env python3 # 第1行和第2行是标准注释，第1行注释可以让这个hello.py文件直接在Unix/Linux/Mac上运行，#_*_ coding:utf-8 _*_ # 第2行注释表示.py文件本身使用标准UTF-8编码；'a test module'# 第4行是一个字符串，表示模块的文档注释，任何模块代码的第一个字符串都被视为模块的文档注释；__author__ = \"fallenk liu\" #第6行使用__author__变量把作者写进去，import sys # 导入sys模块后，我们就有了变量sys指向该模块，利用sys这个变量，就可以访问sys模块的所有功能。def test(): args = sys.argv # sys模块有一个argv变量，用list存储了命令行的所有参数。argv至少有一个元素，因为第一个参数永远是该.py文件的名称， if len(args) == 1: print(\"Hello world\") elif len(args) == 2: print('Hello, %s!' % args[1]) else: print(\"Too many arguments\")if __name__ == \"__main__\": # 当我们在命令行运行hello模块文件时，Python解释器把一个特殊变量__name__置为__main__， test()# 而如果在其他地方导入该hello模块时，if判断将失败， #因此，这种if测试可以让一个模块通过命令行运行时执行一些额外的代码，最常见的就是运行测试 Too many arguments 作用域 仅仅在模块内部使用。在Python中，是通过_前缀来实现的 类似__xxx__这样的变量是特殊变量，可以被直接引用，但是有特殊用途，比如上面的__author__，__name__就是特殊变量，hello模块定义的文档注释也可以用特殊变量__doc__访问，我们自己的变量一般不要用这种变量名； 类似_xxx和__xxx这样的函数或变量就是非公开的（private），不应该被直接引用，比如_abc，__abc等； 安装第三方模块 安装第三方模块，是通过包管理工具pip完成的 注意：Mac或Linux上有可能并存Python 3.x和Python 2.x，因此对应的pip命令是pip3。安装常用模块 使用Anaconda, 即可在终端输入import numpy 模块搜索路径: 默认情况下，Python解释器会搜索当前目录、所有已安装的内置模块和第三方模块，搜索路径存放在sys模块的path变量中 12import syssys.path [&apos;&apos;, &apos;/opt/conda/lib/python36.zip&apos;, &apos;/opt/conda/lib/python3.6&apos;, &apos;/opt/conda/lib/python3.6/lib-dynload&apos;, &apos;/opt/conda/lib/python3.6/site-packages&apos;, &apos;/opt/conda/lib/python3.6/site-packages/Mako-1.0.7-py3.6.egg&apos;, &apos;/opt/conda/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg&apos;, &apos;/opt/conda/lib/python3.6/site-packages/IPython/extensions&apos;, &apos;/home/jovyan/.ipython&apos;] 面向对象编程面向对象编程——Object Oriented Programming，简称OOP，是一种程序设计思想。OOP把对象作为程序的基本单元，一个对象包含了数据和操作数据的函数。 面向过程的程序设计把计算机程序视为一系列的命令集合，即一组函数的顺序执行。为了简化程序设计，面向过程把函数继续切分为子函数，即把大块函数通过切割成小块函数来降低系统的复杂度。 而面向对象的程序设计把计算机程序视为一组对象的集合，而每个对象都可以接收其他对象发过来的消息，并处理这些消息，计算机程序的执行就是一系列消息在各个对象之间传递。 在Python中，所有数据类型都可以视为对象，当然也可以自定义对象。自定义的对象数据类型就是面向对象中的类（Class）的概念。我们以一个例子来说明面向过程和面向对象在程序流程上的不同之处。 123456# 假设我们要处理学生的成绩表，为了表示一个学生的成绩，面向过程的程序可以用一个dict表示：std1 = { 'name': 'Michael', 'score': 98 }std2 = { 'name': 'Bob', 'score': 81 }# 而处理学生成绩可以通过函数实现，比如打印学生的成绩：def print_score(std): print(\"%s %s\"%(std[\"name\"], std[\"score\"])) 如果采用面向对象的程序设计思想，我们首选思考的不是程序的执行流程，而是Student这种数据类型应该被视为一个对象，这个对象拥有name和score这两个属性（Property）。如果要打印一个学生的成绩，首先必须创建出这个学生对应的对象，然后，给对象发一个print_score消息，让对象自己把自己的数据打印出来。 123456789101112class Student(object): def __init__(self, name, score): self.name = name self.score = score def print_score(self): print(\"%s is %s\"%(self.name, self.score)) # 给对象发消息实际上就是调用对象对应的关联函数，我们称之为对象的方法（Method）。面向对象的程序写出来就像这样：bart = Student(\"bart Simspn\", 19)lisa = Student(\"lisa Simspn\", 30)bart.print_score()lisa.print_score() bart Simspn is 19 lisa Simspn is 30 总结 面向对象的设计思想是从自然界中来的，因为在自然界中，类（Class）和实例（Instance）的概念是很自然的。Class是一种抽象概念，比如我们定义的Class——Student，是指学生这个概念，而实例（Instance）则是一个个具体的Student，比如，Bart Simpson和Lisa Simpson是两个具体的Student。 所以，面向对象的设计思想是抽象出Class，根据Class创建Instance。 面向对象的抽象程度又比函数要高，因为一个Class既包含数据，又包含操作数据的方法。 类和实例面向对象最重要的概念就是类（Class）和实例（Instance），必须牢记类是抽象的模板，比如Student类，而实例是根据类创建出来的一个个具体的“对象”，每个对象都拥有相同的方法，但各自的数据可能不同。 以Student类为例，在Python中，定义类是通过class关键字： class后面紧接着是类名，即Student，类名通常是大写开头的单词，紧接着是(object)，表示该类是从哪个类继承下来的，继承的概念我们后面再讲，通常，如果没有合适的继承类，就使用object类，这是所有类最终都会继承的类。 定义好了Student类，就可以根据Student类创建出Student的实例，创建实例是通过类名+()实现的 123456class Student(object): passbart = Student()print(Student)print(bart)# 可以看到，变量bart指向的就是一个Student的实例，后面的0x10a67a590是内存地址，每个object的地址都不一样，而Student本身则是一个类。 &lt;class &apos;__main__.Student&apos;&gt; &lt;__main__.Student object at 0x7fdad8998eb8&gt; 1234class Studetn(object): def __init__(self, name, score): self.name = name self.score = score 由于类可以起到模板的作用，因此，可以在创建实例的时候，把一些我们认为必须绑定的属性强制填写进去。通过定义一个特殊的init方法，在创建实例的时候，就把name，score等属性绑上去： 特殊方法“init”前后分别有两个下划线！！！ 注意到init方法的第一个参数永远是self，表示创建的实例本身， 因此，在init方法内部，就可以把各种属性绑定到self，因为self就指向创建的实例本身。 有了init方法，在创建实例的时候，就不能传入空的参数了，必须传入与init方法匹配的参数，但self不需要传，Python解释器自己会把实例变量传进去： 和普通的函数相比，在类中定义的函数只有一点不同，就是第一个参数永远是实例变量self，并且，调用时，不用传递该参数。除此之外，类的方法和普通函数没有什么区别，所以，你仍然可以用默认参数、可变参数、关键字参数和命名关键字参数。 数据封装 面向对象编程的一个重要特点就是数据封装。在上面的Student类中，每个实例就拥有各自的name和score这些数据。我们可以通过函数来访问这些数据. 封装的另一个好处是可以给Student类增加新的方法，比如get_grade 小结 类是创建实例的模板，而实例则是一个一个具体的对象，各个实例拥有的数据都互相独立，互不影响； 方法就是与实例绑定的函数，和普通函数不同，方法可以直接访问实例的数据； 通过在实例上调用方法，我们就直接操作了对象内部的数据，但无需知道方法内部的实现细节。 和静态语言不同，Python允许对实例变量绑定任何数据，也就是说，对于两个实例变量，虽然它们都是同一个类的不同实例，但拥有的变量名称都可能不同： 访问限制 在Class内部，可以有属性和方法，而外部代码可以通过直接调用实例变量的方法来操作数据，这样，就隐藏了内部的复杂逻辑 如果要让内部属性不被外部访问，可以把属性的名称前加上两个下划线__，在Python中，实例的变量名如果以__开头，就变成了一个私有变量（private），只有内部可以访问，外部不能访问，所以，我们把Student类改一改： 12345678910111213class Student(object): def __init__(self, name, score): self.__name = name self.__score = score def print_score(self): print(\"%s is %s\" %(self.__name, self.__score)) def get_score(self): return self.__score def set_score(self, score): if score &gt;= 0 and score &lt;= 100: self.__score = score else: raise ValueError(\"bad value\") 继承和多态在OOP程序设计中，当我们定义一个class的时候，可以从某个现有的class继承，新的class称为子类（Subclass），而被继承的class称为基类、父类或超类（Base class、Super class）。 最大的好处是子类获得了父类的全部功能。 当子类和父类都存在相同的run()方法时，我们说，子类的run()覆盖了父类的run()，在代码运行的时候，总是会调用子类的run()。这样，我们就获得了继承的另一个好处：多态。多态：多态的好处就是，当我们需要传入Dog、Cat、Tortoise……时，我们只需要接收Animal类型就可以了，因为Dog、Cat、Tortoise……都是Animal类型，然后，按照Animal类型进行操作即可。由于Animal类型有run()方法，因此，传入的任意类型，只要是Animal类或者子类，就会自动调用实际类型的run()方法，这就是多态的意思。 对于一个变量，我们只需要知道它是Animal类型，无需确切地知道它的子类型，就可以放心地调用run()方法，而具体调用的run()方法是作用在Animal、Dog、Cat还是Tortoise对象上，由运行时该对象的确切类型决定，这就是多态真正的威力：调用方只管调用，不管细节，而当我们新增一种Animal的子类时，只要确保run()方法编写正确，不用管原来的代码是如何调用的。这就是著名的“开闭”原则： 对扩展开放：允许新增Animal子类； 对修改封闭：不需要修改依赖Animal类型的run_twice()等函数。 静态语言 vs 动态语言 对于静态语言（例如Java）来说，如果需要传入Animal类型，则传入的对象必须是Animal类型或者它的子类，否则，将无法调用run()方法。 对于Python这样的动态语言来说，则不一定需要传入Animal类型。我们只需要保证传入的对象有一个run()方法就可以了。 123class Timer(object): def run(self): print(\"start...\") 这就是动态语言的“鸭子类型”，它并不要求严格的继承体系，一个对象只要“看起来像鸭子，走起路来像鸭子”，那它就可以被看做是鸭子。 Python的“file-like object“就是一种鸭子类型。对真正的文件对象，它有一个read()方法，返回其内容。但是，许多对象，只要有read()方法，都被视为“file-like object“。许多函数接收的参数就是“file-like object“，你不一定要传入真正的文件对象，完全可以传入任何实现了read()方法的对象。 小结继承可以把父类的所有功能都直接拿过来，这样就不必重零做起，子类只需要新增自己特有的方法，也可以把父类不适合的方法覆盖重写。 动态语言的鸭子类型特点决定了继承不像静态语言那样是必须的。 获取对象信息当我们拿到一个对象的引用时，如何知道这个对象是什么类型、有哪些方法呢？ 使用type() 首先，我们来判断对象类型，使用type()函数： 基本类型都可以用type()判断： 1234567print(type(123))print(type(None))# 如果一个变量指向函数或者类，也可以用type()判断：print(type(abs))def fn(): passprint(type(fn))# type()返回对应的class类型 &lt;class &apos;int&apos;&gt; &lt;class &apos;NoneType&apos;&gt; &lt;class &apos;builtin_function_or_method&apos;&gt; &lt;class &apos;function&apos;&gt; 1234**使用isinstance()**1. 对于class的继承关系来说，使用type()就很不方便。我们要判断class的类型，可以使用isinstance()函数。2. `isinstance()`判断的是一个对象是否是该类型本身，或者位于该类型的父继承链上。3. 总是优先使用isinstance()判断类型，可以将指定类型及其子类“一网打尽”。 12print(isinstance('a', str))print(isinstance([1, 2, 3], (list, tuple))) True True 使用dir() 如果要获得一个对象的所有属性和方法，可以使用dir()函数，它返回一个包含字符串的list，比如，获得一个str对象的所有属性和方法： 1dir(\"ABC\") 类似__xxx__的属性和方法在Python中都是有特殊用途的，比如__len__方法返回长度。在Python中，如果你调用len()函数试图获取一个对象的长度，实际上，在len()函数内部，它自动去调用该对象的__len__()方法，所以，下面的代码是等价的 12print(len('ABC'))print(\"ABC\".__len__()) 3 3 12345678# 我们自己写的类，如果也想用len(myObj)的话，就自己写一个__len__()方法：class MyDog(object): def __len__(self): return 100dog = MyDog()len(dog)# 我们自己写的类，如果也想用len(myObj)的话，就自己写一个__len__()方法：\"ABC\".lower() &apos;abc&apos; 123456789101112131415161718192021class MyObject(object): def __init__(self): self.x = 9 def power(self): return self.x*self.xobj = MyObject()# dir(obj)# 配合getattr()、setattr()以及hasattr()，我们可以直接操作一个对象的状态：hasattr(obj, 'x') # 有属性'x'吗？obj.xhasattr(obj, 'y')# 有属性'y'吗？setattr(obj, 'y', 19) # 设置一个属性'y'obj.y# 可以传入一个default参数，如果属性不存在，就返回默认值：getattr(obj, 'z', 404) # 获取属性'z'，如果不存在，返回默认值404# 也可以获得对象的方法：hasattr(obj, 'power') # 有属性'power'吗？getattr(obj, 'power') # 获取属性'power'fn = getattr(obj, 'power') # 获取属性'power'并赋值到变量fnfnfn() 81 小结通过内置的一系列函数，我们可以对任意一个Python对象进行剖析，拿到其内部的数据。要注意的是，只有在不知道对象信息的时候，我们才会去获取对象信息。如果可以直接写： 1sum = obj.x + obj.y # 不要 sum = getattr(obj, 'x') + getattr(obj, 'y') 实例属性和类属性 由于Python是动态语言，根据类创建的实例可以任意绑定属性。 给实例绑定属性的方法是通过实例变量，或者通过self变量： 12345class Student(object): def __init__(self, name): self.name = names = Student('Bob')s.score = 90 12345# Student类本身需要绑定一个属性呢？可以直接在class中定义属性，这种属性是类属性，归Student类所有：class Student(object): name = 'Student'# 从上面的例子可以看出，在编写程序的时候，千万不要对实例属性和类属性使用相同的名字，# 因为相同名称的实例属性将屏蔽掉类属性，但是当你删除实例属性后，再使用相同的名称，访问到的将是类属性。 小结实例属性属于各个实例所有，互不干扰； 类属性属于类所有，所有实例共享一个属性； 不要对实例属性和类属性使用相同的名字，否则将产生难以发现的错误。 面向对象高阶编程 数据封装、继承和多态只是面向对象程序设计中最基础的3个概念。在Python中，面向对象还有很多高级特性，允许我们写出非常强大的功能。 多重继承、定制类、元类等概念 使用slots正常情况下，当我们定义了一个class,创建了一个class实例，我们可以给实例绑定任何属性和方法，即动态语言的灵活性： 123456789101112131415# 先定义classclass Student(object): pass# 然后，尝试给实例绑定一个属性：s = Student()s.name = 'Maichel'print(s.name)# 尝试给实例绑定一个方法：def set_age(self, age): self.age = agefrom types import MethodTypes.set_age = MethodType(set_age, s)# 给实例绑定一个方法s.set_age(23) # 调用实例方法print(s.age)# 但是，给一个实例绑定的方法，对另一个实例是不起作用的 Maichel 23 1234567891011# 为了给所有实例都绑定方法，可以给class绑定方法：def set_score(self, score): self.score = scoreStudent.set_score = set_score# 给class绑定方法后，所有实例均可调用：s.set_score(10)print(s.score)s2 = Student()s2.set_score(12)print(s2.score)# 通常情况下，上面的set_score方法可以直接定义在class中，但动态绑定允许我们在程序运行的过程中动态给class加上功能，这在静态语言中很难实现。 10 12 使用slots: 限制实例的属性，只允许对Student实例添加name和age属性。 为了达到限制的目的，Python允许在定义class的时候，定义一个特殊的slots变量，来限制该class实例能添加的属性： 123456789class Student(object): __slots__ = (\"name\", \"age\", \"score\")# 用tuple定义允许绑定的属性名称s = Student()s.age = 10print(s.age)s.score = 20print(s.words)# 由于'score'没有被放到__slots__中，所以不能绑定score属性，试图绑定score将得到AttributeError的错误。# 使用__slots__要注意，__slots__定义的属性仅对当前类实例起作用，对继承的子类是不起作用的： 10 --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-166-35f3a467de63&gt; in &lt;module&gt;() 5 print(s.age) 6 s.score = 20 ----&gt; 7 print(s.words) 8 # 由于&apos;score&apos;没有被放到__slots__中，所以不能绑定score属性，试图绑定score将得到AttributeError的错误。 9 # 使用__slots__要注意，__slots__定义的属性仅对当前类实例起作用，对继承的子类是不起作用的： AttributeError: &apos;Student&apos; object has no attribute &apos;words&apos; 123456class GrandStudent(Student): passs2 = GrandStudent()s2.score = 30print(s2.score)# 除非在子类中也定义__slots__，这样，子类实例允许定义的属性就是自身的__slots__加上父类的__slots__。 30 使用@property在绑定属性时，如果我们直接把属性暴露出去，虽然写起来很简单，但是，没办法检查参数，导致可以把成绩随便改： 12s = Student()s.score = 9999 123456789101112131415161718# 这显然不合逻辑。为了限制score的范围，可以通过一个set_score()方法来设置成绩，# 再通过一个get_score()来获取成绩，这样，在set_score()方法里，就可以检查参数：class Student(object): def get_score(self): return self._score# def set_score(self, score):# self._score = score def set_score(self, value): if not isinstance(value, int): raise ValueError(\"score must be an integer!\") if value &gt; 100 or value &lt; 0: raise ValueError(\"score must between 0-100!\") self._score = value# 现在，对任意的Student实例进行操作，就不能随心所欲地设置score了：s = Student()s.set_score(60)s.get_score()s.set_score(9999) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-172-9bcb4485b1da&gt; in &lt;module&gt;() 16 s.set_score(60) 17 s.get_score() ---&gt; 18 s.set_score(9999) &lt;ipython-input-172-9bcb4485b1da&gt; in set_score(self, value) 10 raise ValueError(&quot;score must be an integer!&quot;) 11 if value &gt; 100 or value &lt; 0: ---&gt; 12 raise ValueError(&quot;score must between 0-100!&quot;) 13 self._score = value 14 # 现在，对任意的Student实例进行操作，就不能随心所欲地设置score了： ValueError: score must between 0-100! 但是，上面的调用方法又略显复杂，没有直接用属性这么直接简单。 有没有既能检查参数，又可以用类似属性这样简单的方式来访问类的变量呢？ 装饰器（decorator）可以给函数动态加上功能,对于类的方法，装饰器一样起作用。Python内置的@property装饰器就是负责把一个方法变成属性调用的： 1234567891011class Student(object): @property def score(self): return self._score @score.setter def score(self, value): if not isinstance(value, int): raise ValueError(\"score must be integer!\") if value &lt; 0 or value &gt; 100: raise ValueError(\"score must between 0-100!\") self._score = value 把一个getter方法变成属性，只需要加上@property就可以了，此时，@property本身又创建了另一个装饰器@score.setter ，负责把一个setter方法变成属性赋值，于是，我们就拥有一个可控的属性操作： 1234s = Student()s.score = 50 # OK,实际转化为s.set_score(50)s.score# OK, 实际转为s.get_score()s.score = 9999 --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-175-d69d8b502813&gt; in &lt;module&gt;() 2 s.score = 50 # OK,实际转化为s.set_score(50) 3 s.score# OK, 实际转为s.get_score() ----&gt; 4 s.score = 9999 &lt;ipython-input-173-7a33d5731e5a&gt; in score(self, value) 8 raise ValueError(&quot;score must be integer!&quot;) 9 if value &lt; 0 or value &gt; 100: ---&gt; 10 raise ValueError(&quot;score must between 0-100!&quot;) 11 self._score = value ValueError: score must between 0-100! 注意到这个神奇的@property ，我们在对实例属性操作的时候，就知道该属性很可能不是直接暴露的，而是通过getter和setter方法来实现的。 还可以定义只读属性，只定义getter方法，不定义setter方法就是一个只读属性： 12345678910111213141516class Student(object): @property def birth(self): return self._birth @birth.setter def birth(self, value): if not isinstance(value, int): raise ValueError(\"birth must be integer!\") self._birth = value @property def age(self): return 2018 - self._births = Student()s.birth = 1999s.births.age 19 上面的birth是可读写属性，而age就是一个只读属性，因为age可以根据birth和当前时间计算出来。 小结@property广泛应用在类的定义中，可以让调用者写出简短的代码，同时保证对参数进行必要的检查，这样，程序运行时就减少了出错的可能性。 多重继承继承是面向对象编程的一个重要的方式，因为通过继承，子类就可以扩展父类的功能。 回忆一下Animal类层次的设计，假设我们要实现以下4种动物： Dog - 狗狗； Bat - 蝙蝠； Parrot - 鹦鹉； Ostrich - 鸵鸟。按照分类有多种不同的分类,正确的做法是采用多重继承。首先，主要的类层次仍按照哺乳类和鸟类设计： 1234567891011121314151617181920212223242526class Animal(object): passclass Mammal(Animal): passclass Bird(Animal): passclass Dog(Mammal): passclass Bat(Mammal): passclass Parrot(Bird): passclass Ostrich(Bird): pass# 现在，我们要给动物再加上Runnable和Flyable的功能，只需要先定义好Runnable和Flyable的类：class Runnable(object): def run(self): print(\"Running ...\")class Flyable(object): def fly(self): print(\"Flying ...\")# 对于需要Runnable功能的动物，就多继承一个Runnable，例如Dog：通过多重继承，一个子类就可以同时获得多个父类的所有功能。class Dog(Mammal, Runnable): passclass Ostrich(Bird, Flyable): pass MixIn 在设计类的继承关系时，通常，主线都是单一继承下来的，例如，Ostrich继承自Bird。但是，如果需要“混入”额外的功能，通过多重继承就可以实现，比如，让Ostrich除了继承自Bird外，再同时继承Runnable。这种设计通常称之为MixIn。 为了更好地看出继承关系，我们把Runnable和Flyable改为RunnableMixIn和FlyableMixIn。类似的，你还可以定义出肉食动物CarnivorousMixIn和植食动物HerbivoresMixIn ，让某个动物同时拥有好几个MixIn： 12class Dog(Mammal, RunnableMixIn, CarnivorousMixIn): pass MixIn的目的就是给一个类增加多个功能，这样，在设计类的时候，我们优先考虑通过多重继承来组合多个MixIn的功能，而不是设计多层次的复杂的继承关系。 Python自带的很多库也使用了MixIn。举个例子，Python自带了TCPServer和UDPServer这两类网络服务，而要同时服务多个用户就必须使用多进程或多线程模型，这两种模型由ForkingMixIn和ThreadingMixIn提供。通过组合，我们就可以创造出合适的服务来。 比如，编写一个多进程模式的TCP服务，定义如下： 123456789class MyTCPServer(TCPServer, ForkingMixIn): pass# 编写一个多线程模式的UDP服务，定义如下：class MyUDP(UDPServer, ThreadingMixIn): pass# 如果你打算搞一个更先进的协程模型，可以编写一个CoroutineMixIn：class MYTCPServer(TCPServer, CoroutineMixIn): pass# 这样一来，我们不需要复杂而庞大的继承链，只要选择组合不同的类的功能，就可以快速构造出所需的子类。 小结由于Python允许使用多重继承，因此，MixIn就是一种常见的设计。 只允许单一继承的语言（如Java）不能使用MixIn的设计。 定制类看到类似__slots__这种形如__xxx__的变量或者函数名就要注意，这些在Python中是有特殊用途的。 __slots__我们已经知道怎么用了，__len__()方法我们也知道是为了能让class作用于len()函数。 除此之外，Python的class中还有许多这样有特殊用途的函数，可以帮助我们定制类。 __str__ 我们先定义一个Student类，打印一个实例： 1234class Student(object): def __init__(self, name): self.__name = nameprint(Student(\"Maichel\")) &lt;__main__.Student object at 0x7fdad89f06d8&gt; 12345678# 打印出一堆&lt;__main__.Student object at 0x7fdad89f06d8&gt;，不好看。# 能打印得好只需要,定义好__str__()方法，返回一个好看的字符串就可以了：class Student(object): def __init__(self, name): self._name = name def __str__(self): return \"Student object (name %s)\"% self._nameprint(Student(\"Maichels\")) Student object (name Maichels) 12345# 这样打印出来的实例，不但好看，而且容易看出实例内部重要的数据。# 但是发现直接敲变量不用print，打印出来的实例还是不好看：s = Student(\"Maicherls\")s &lt;__main__.Student at 0x7fdad89f0ac8&gt; 这是因为直接显示变量调用的不是__str__() ，而是__repr__() ，两者的区别是__str__()返回用户看到的字符串，而__repr__()返回程序开发者看到的字符串，也就是说，__repr__()是为调试服务的。 解决办法是再定义一个__repr__()。但是通常__str__()和__repr__()代码都是一样的，所以，有个偷懒的写法： 123456class Student(object): def __init__(self, name): self.name = name def __str__(self): return 'Student object (name=%s)' % self.name __repr__ = __str__ iter 如果一个类想被用于for ... in循环，类似list或tuple那样，就必须实现一个__iter__()方法，该方法返回一个迭代对象，然后，Python的for循环就会不断调用该迭代对象的__next__()方法拿到循环的下一个值，直到遇到StopIteration错误时退出循环。 我们以斐波那契数列为例，写一个Fib类，可以作用于for循环： 123456789101112class Fib(object): def __init__(self): self.a, self.b = 0, 1# 初始化两个计数器a，b def __iter__(self): return self# 实例本身就是迭代对象，故返回自己 def __next__(self): self.a, self.b = self.b, self.a+self.b if self.a &gt; 20: # 退出循环的条件 raise StopIteration() return self.a# 返回下一个值for n in Fib(): print(n) 1 1 2 3 5 8 13 getitem Fib实例虽然能作用于for循环，看起来和list有点像，但是，把它当成list来使用还是不行，比如，取第5个元素,会报错；要表现得像list那样按照下标取出元素，需要实现__getitem__()方法： 12345678910111213141516171819202122class Fib(object): def __getitem__(self, n): if isinstance(n, int): # n是索引 a, b = 1, 1 for x in range(n): a, b = b, a + b return a if isinstance(n, slice): # n是切片 start = n.start stop = n.stop if start is None: start = 0 a, b = 1, 1 L = [] for x in range(stop): if x &gt;= start: L.append(a) a, b = b, a + b return Lf = Fib()f[0]f[9] 55 getattr 错误信息很清楚地告诉我们，没有找到score这个attribute。要避免这个错误，除了可以加上一个score属性外，那就是写一个__getattr__()方法，动态返回一个属性。修改如下： 12345678class Student(object): def __init__(self): self.name = 'Michael' def __getattr__(self, attr): if attr=='score': return 99 此外，注意到任意调用如s.abc都会返回None，这是因为我们定义的__getattr__默认返回就是None。要让class只响应特定的几个属性，我们就要按照约定，抛出AttributeError的错误： 123456class Student(object): def __getattr__(self, attr): if attr=='age': return lambda: 25 raise AttributeError('\\'Student\\' object has no attribute \\'%s\\'' % attr) 这实际上可以把一个类的所有属性和方法调用全部动态化处理了，不需要任何特殊手段。 这种完全动态调用的特性有什么实际作用呢？作用就是，可以针对完全动态的情况作调用。利用完全动态的__getattr__，我们可以写出一个链式调用： 123456789class Chain(): def __init__(self, path=''): self._path = path def __getattr__(self, path): return Chain(\"%s/%s\"% (self._path, path)) def __str__(self): return self._path __repr__ = __str__Chain().status.user.timeline.list /status/user/timeline/list __call__ 一个对象实例可以有自己的属性和方法，当我们调用实例方法时，我们用instance.method()来调用。能不能直接在实例本身上调用呢？在Python中，答案是肯定的。任何类，只需要定义一个__call__()方法，就可以直接对实例进行调用。请看示例： 1234567class Student(object): def __init__(self, name): self.name = name def __call__(self): print(\"My name is %s\"%self.name) s = Student('Michael')s() My name is Michael __call__()还可以定义参数。对实例进行直接调用就好比对一个函数进行调用一样，所以你完全可以把对象看成函数，把函数看成对象，因为这两者之间本来就没啥根本的区别。 如果你把对象看成函数，那么函数本身其实也可以在运行期动态创建出来，因为类的实例都是运行期创建出来的，这么一来，我们就模糊了对象和函数的界限。 那么，怎么判断一个变量是对象还是函数呢？其实，更多的时候，我们需要判断一个对象是否能被调用，能被调用的对象就是一个Callable对象，比如函数和我们上面定义的带有__call__()的类实例： 12callable(Student(\"name\"))callable([1, 2, 4]) False 1234567891011121314class Chain(object): def __init__(self,path='GET '): self._path = path def __getattr__(self,path): return Chain('%s/%s' % (self._path,path)) def __call__(self,args): return Chain('%s/%s' % (self._path,args)) def __str__(self): return self._path __repr__ = __str__print(Chain().users('michael').repos) GET /users/michael/repos 使用枚举类 定义常量时,大写整数定义 为这样的枚举类型定义一个class类型，然后，每个常量都是class的一个唯一实例。Python提供了Enum类来实现这个功能： 123456from enum import EnumMonth = Enum(\"Month\",(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"))# 这样我们就获得了Month类型的枚举类，可以直接使用Month.Jan来引用一个常量，或者枚举它的所有成员：for name, member in Month.__members__.items(): print(name,\"=&gt;\",member,\",\", member.value)# value属性则是自动赋给成员的int常量，默认从1开始计数。 Jan =&gt; Month.Jan , 1 Feb =&gt; Month.Feb , 2 Mar =&gt; Month.Mar , 3 Apr =&gt; Month.Apr , 4 May =&gt; Month.May , 5 Jun =&gt; Month.Jun , 6 Jul =&gt; Month.Jul , 7 Aug =&gt; Month.Aug , 8 Sep =&gt; Month.Sep , 9 Oct =&gt; Month.Oct , 10 Nov =&gt; Month.Nov , 11 Dec =&gt; Month.Dec , 12 123456789101112131415161718# 如果需要更精确地控制枚举类型，可以从Enum派生出自定义类：from enum import Enum,unique@uniqueclass Weekday(Enum): Sun = 0 Mon = 1 Tue = 2 Wen = 3 Thu = 4 Fri = 5 Sat = 6day1 = Weekday.Monprint(day1)print(Weekday.Tue)print(Weekday(1))print(Weekday['Fri'])for name, member in Weekday.__members__.items(): print(name,'=&gt;',member,',',member.value) Weekday.Mon Weekday.Tue Weekday.Mon Weekday.Fri Sun =&gt; Weekday.Sun , 0 Mon =&gt; Weekday.Mon , 1 Tue =&gt; Weekday.Tue , 2 Wen =&gt; Weekday.Wen , 3 Thu =&gt; Weekday.Thu , 4 Fri =&gt; Weekday.Fri , 5 Sat =&gt; Weekday.Sat , 6 使用元类type() 动态语言和静态语言最大的不同，就是函数和类的定义，不是编译时定义的，而是运行时动态创建的。 比方说我们要定义一个Hello的class，就写一个hello.py模块： 123456789101112class Hello(object): def hello(self, name=\"world\"): return (\"Hello, %s\"% name)# 当Python解释器载入hello模块时，就会依次执行该模块的所有语句，执行结果就是动态创建出一个Hello的class对象，测试如下：h = Hello()print(h.hello(\"worldss\"))print(type(Hello))print(type(h))# type()函数可以查看一个类型或变量的类型，Hello是一个class，它的类型就是type，而h是一个实例，它的类型就是class Hello。# 我们说class的定义是运行时动态创建的，而创建class的方法就是使用type()函数# type()函数既可以返回一个对象的类型，又可以创建出新的类型，# 比如，我们可以通过type()函数创建出Hello类，而无需通过class Hello(object)...的定义： Hello, worldss &lt;class &apos;type&apos;&gt; &lt;class &apos;__main__.Hello&apos;&gt; 1234567def fn(self, name='world'): # 先定义函数 print('Hello, %s.' % name)Hello = type('Hello',(object,), dict(hello=fn))# 创建Hello classh = Hello()print(h.hello())print(type(Hello))print(type(h)) Hello, world. None &lt;class &apos;type&apos;&gt; &lt;class &apos;__main__.Hello&apos;&gt; 总结 要创建一个class对象，type()函数依次传入3个参数： class的名称； 继承的父类集合，注意Python支持多重继承，如果只有一个父类，别忘了tuple的单元素写法； class的方法名称与函数绑定，这里我们把函数fn绑定到方法名hello上。 通过type()函数创建的类和直接写class是完全一样的，因为Python解释器遇到class定义时，仅仅是扫描一下class定义的语法，然后调用type()函数创建出class。 正常情况下，我们都用class Xxx…来定义类，但是，type()函数也允许我们动态创建出类来，也就是说，动态语言本身支持运行期动态创建类，这和静态语言有非常大的不同，要在静态语言运行期创建类，必须构造源代码字符串再调用编译器，或者借助一些工具生成字节码实现，本质上都是动态编译，会非常复杂。 metaclass除了使用type()动态创建类以外，要控制类的创建行为，还可以使用metaclass。 metaclass，直译为元类，简单的解释就是： 当我们定义了类以后，就可以根据这个类创建出实例，所以：先定义类，然后创建实例 但是如果我们想创建出类呢？那就必须根据metaclass创建出类，所以：先定义metaclass，然后创建类。 连接起来就是：先定义metaclass，就可以创建类，最后创建实例。 所以，metaclass允许你创建类或者修改类。换句话说，你可以把类看成是metaclass创建出来的“实例”。 metaclass是Python面向对象里最难理解，也是最难使用的魔术代码。正常情况下，你不会碰到需要使用metaclass的情况，所以，以下内容看不懂也没关系，因为基本上你不会用到。 我们先看一个简单的例子，这个metaclass可以给我们自定义的MyList增加一个add方法： 定义ListMetaclass，按照默认习惯，metaclass的类名总是以Metaclass结尾，以便清楚地表示这是一个metaclass： 12345678# metaclass是类的模板，所以必须从`type`类型派生：class ListMetaclass(type): def __new__(cls, name, bases, attrs): attrs['add'] = lambda self, value: self.append(value) return type.__new__(cls, name, bases, attrs)# 有了ListMetaclass，我们在定义类的时候还要指示使用ListMetaclass来定制类，传入关键字参数metaclass：class MyList(list, metaclass=ListMetaclass): pass 分析：当我们传入关键字参数metaclass时，魔术就生效了，它指示Python解释器在创建MyList时，要通过ListMetaclass.new()来创建，在此，我们可以修改类的定义，比如，加上新的方法，然后，返回修改后的定义。__new__()方法接收到的参数依次是： 当前准备创建的类的对象； 类的名字； 类继承的父类集合； 类的方法集合。 测试一下MyList是否可以调用add()方法： 123L = MyList()L.add(1)print(L) [1] 动态修改有什么意义？直接在MyList定义中写上add()方法不是更简单吗？正常情况下，确实应该直接写，通过metaclass修改纯属变态。 但是，总会遇到需要通过metaclass修改类定义的。ORM就是一个典型的例子。 ORM全称“Object Relational Mapping”，即对象-关系映射，就是把关系数据库的一行映射为一个对象，也就是一个类对应一个表，这样，写代码更简单，不用直接操作SQL语句。 要编写一个ORM框架，所有的类都只能动态定义，因为只有使用者才能根据表的结构定义出对应的类来。 错误、调试和测试BUG: 程序编写有问题造成 用户输入造成 完全无法在程序运行过程中预测,如IO读写中断，网络抓取数据中断 解决 Python内置了一套异常处理机制，来帮助我们进行错误处理。 此外，我们也需要跟踪程序的执行，查看变量的值是否正确，这个过程称为调试。Python的pdb可以让我们以单步方式执行代码。 最后，编写测试也很重要。有了良好的测试，就可以在程序修改后反复运行，确保程序输出符合我们编写的测试。 错误处理在程序运行的过程中，如果发生了错误，可以事先约定返回一个错误代码，这样，就可以知道是否有错，以及出错的原因。在操作系统提供的调用中，返回错误码非常常见。比如打开文件的函数open()，成功时返回文件描述符（就是一个整数），出错时返回-1。 用错误码来表示是否出错十分不便，因为函数本身应该返回的正常结果和错误码混在一起，造成调用者必须用大量的代码来判断是否出错：sub-&gt;parent。 一旦出错，还要一级一级上报，直到某个函数可以处理该错误（比如，给用户输出一个错误信息）。 所以高级语言通常都内置了一套try...except...finally...的错误处理机制，Python也不例外。 12345678910# try的例子try: print(\"try...\") r = 10/0 print(\"result %s\"%r)except ZeroDivisionError as e: print(\"except\", e)finally: print(\"finally\")print(\"end\") try... except division by zero finally end 当我们认为某些代码可能会出错时，就可以用try来运行这段代码，如果执行出错，则后续代码不会继续执行， 而是直接跳转至错误处理代码，即except语句块，执行完except后，如果有finally语句块，则执行finally语句块，至此，执行完毕。 从输出可以看到，当错误发生时，后续语句print(‘result:’, r)不会被执行，except由于捕获到ZeroDivisionError，因此被执行。 最后，finally语句被执行。然后，程序继续按照流程往下走。 由于没有错误发生，所以except语句块不会被执行，但是finally如果有，则一定会被执行（可以没有finally语句）。 你还可以猜测，错误应该有很多种类，如果发生了不同类型的错误，应该由不同的except语句块处理。没错，可以有多个except来捕获不同类型的错误： 1234567891011121314151617try: print('try...')# r = 10 / int('a') r = 10/int('2') print('result:', r)except ValueError as e: print(\"ValueError:\",e)except ZeroDivisionError as e: print(\"ZeroDivisionError\",e)else: print(\"else no error\")finally: print(\"finally\")print(\"END\")#int()函数可能会抛出ValueError，所以我们用一个except捕获ValueError，用另一个except捕获ZeroDivisionError。# 此外，如果没有错误发生，可以在except语句块后面加一个else，当没有错误发生时，会自动执行else语句. try... result: 5.0 else no error finally END 另外： Python的错误其实也是class，所有的错误类型都继承自BaseException，所以在使用except时需要注意的是，它不但捕获该类型的错误，还把其子类也“一网打尽”。 所有的错误都是从BaseException派生出来的 使用try...except捕获错误还有一个巨大的好处，就是可以跨越多层调用，比如函数main()调用foo() ，foo()调用bar() ，结果bar()出错了，这时，只要main()捕获到了，就可以处理： 12345678910111213def foo(s): return 10 / int(s)def bar(s): return foo(s) * 2def main(): try: bar('0') except Exception as e: print(\"Error:\",e) finally: print(\"finally\")main() Error: division by zero finally 1234567891011121314# 调用栈# 如果错误没有被捕获，它就会一直往上抛，最后被Python解释器捕获，打印一个错误信息，然后程序退出。来看看err.py# err.py:def foo(s): return 10 / int(s)def bar(s): return foo(s) * 2def main(): bar('0')main()# 出错的时候，一定要分析错误的调用栈信息，才能定位错误的位置。 --------------------------------------------------------------------------- ZeroDivisionError Traceback (most recent call last) &lt;ipython-input-241-6643cba01984&gt; in &lt;module&gt;() 11 bar(&apos;0&apos;) 12 ---&gt; 13 main() 14 # 出错的时候，一定要分析错误的调用栈信息，才能定位错误的位置。 &lt;ipython-input-241-6643cba01984&gt; in main() 9 10 def main(): ---&gt; 11 bar(&apos;0&apos;) 12 13 main() &lt;ipython-input-241-6643cba01984&gt; in bar(s) 6 7 def bar(s): ----&gt; 8 return foo(s) * 2 9 10 def main(): &lt;ipython-input-241-6643cba01984&gt; in foo(s) 3 # err.py: 4 def foo(s): ----&gt; 5 return 10 / int(s) 6 7 def bar(s): ZeroDivisionError: division by zero 12345678910111213141516# 记录错误# 如果不捕获错误，自然可以让Python解释器来打印出错误堆栈，但程序也被结束了。# 既然我们能捕获错误，就可以把错误堆栈打印出来，然后分析错误原因，同时，让程序继续执行下去。# Python内置的logging模块可以非常容易地记录错误信息：import loggingdef foo(s): return 10 / int(s)def bar(s): return foo(s) * 2def main(): try: bar('0') except Exception as e: logging.exception(e)main()print('END') ERROR:root:division by zero Traceback (most recent call last): File &quot;&lt;ipython-input-242-53643402f45a&gt;&quot;, line 12, in main bar(&apos;0&apos;) File &quot;&lt;ipython-input-242-53643402f45a&gt;&quot;, line 9, in bar return foo(s) * 2 File &quot;&lt;ipython-input-242-53643402f45a&gt;&quot;, line 7, in foo return 10 / int(s) ZeroDivisionError: division by zero END 通过配置，logging还可以把错误记录到日志文件里，方便事后排查。 抛出错误 因为错误是class，捕获一个错误就是捕获到该class的一个实例。因此，错误并不是凭空产生的，而是有意创建并抛出的。 Python的内置函数会抛出很多类型的错误，我们自己编写的函数也可以抛出错误。 如果要抛出错误，首先根据需要，可以定义一个错误的class，选择好继承关系，然后，用raise语句抛出一个错误的实例： 123456789class FooError(ValueError): passdef foo(s): n = int(s) if n==0: raise FooError(\"invalid value: %s\"% s) return 10/nfoo('0') --------------------------------------------------------------------------- FooError Traceback (most recent call last) &lt;ipython-input-243-fe4ac86bdc8f&gt; in &lt;module&gt;() 13 raise FooError(&quot;invalid value: %s&quot;% s) 14 return 10/n ---&gt; 15 foo(&apos;0&apos;) &lt;ipython-input-243-fe4ac86bdc8f&gt; in foo(s) 11 n = int(s) 12 if n==0: ---&gt; 13 raise FooError(&quot;invalid value: %s&quot;% s) 14 return 10/n 15 foo(&apos;0&apos;) FooError: invalid value: 0 只有在必要的时候才定义我们自己的错误类型。如果可以选择Python已有的内置的错误类型（比如ValueError，TypeError），尽量使用Python内置的错误类型。 最后，我们来看另一种错误处理的方式： 12345678910111213def foo(s): n = int(s) if n==0: raise ValueError(\"invalid value %s\"%n) return 10/ndef bar(s): try: foo(s) except ValueError as e: print('ValueError!') raisebar('0') ValueError! --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-244-8a2347aaf190&gt; in &lt;module&gt;() 11 print(&apos;ValueError!&apos;) 12 raise ---&gt; 13 bar(&apos;0&apos;) &lt;ipython-input-244-8a2347aaf190&gt; in bar(s) 7 def bar(s): 8 try: ----&gt; 9 foo(s) 10 except ValueError as e: 11 print(&apos;ValueError!&apos;) &lt;ipython-input-244-8a2347aaf190&gt; in foo(s) 2 n = int(s) 3 if n==0: ----&gt; 4 raise ValueError(&quot;invalid value %s&quot;%n) 5 return 10/n 6 ValueError: invalid value 0 在bar()函数中，我们明明已经捕获了错误，但是，打印一个ValueError!后，又把错误通过raise语句抛出去了。 其实这种错误处理方式相当常见。捕获错误目的只是记录一下，便于后续追踪。但是，由于当前函数不知道应该怎么处理该错误，所以，最恰当的方式是继续往上抛，让顶层调用者去处理。好比一个员工处理不了一个问题时，就把问题抛给他的老板，如果他的老板也处理不了，就一直往上抛，最终会抛给CEO去处理。 raise语句如果不带参数，就会把当前错误原样抛出。此外，在except中raise一个Error，还可以把一种类型的错误转化成另一种类型： 1234try: 10 / 0except ZeroDivisionError: raise ValueError('input error!') --------------------------------------------------------------------------- ZeroDivisionError Traceback (most recent call last) &lt;ipython-input-245-80c7eb55a8db&gt; in &lt;module&gt;() 1 try: ----&gt; 2 10 / 0 3 except ZeroDivisionError: ZeroDivisionError: division by zero During handling of the above exception, another exception occurred: ValueError Traceback (most recent call last) &lt;ipython-input-245-80c7eb55a8db&gt; in &lt;module&gt;() 2 10 / 0 3 except ZeroDivisionError: ----&gt; 4 raise ValueError(&apos;input error!&apos;) ValueError: input error! 调试程序能一次写完并正常运行的概率很小，基本不超过1%。总会有各种各样的bug需要修正。有的bug很简单，看看错误信息就知道，有的bug很复杂，我们需要知道出错时，哪些变量的值是正确的，哪些变量的值是错误的，因此，需要一整套调试程序的手段来修复bug。 第一种方法简单直接粗暴有效，就是用print()把可能有问题的变量打印出来看看;第二种断言 12345678910111213def foo(s): n = int(s)# print(\"...n=%s\"%n) assert n!=0, \"n is zero\" #assert的意思是，表达式n != 0应该是True，否则，根据程序运行的逻辑，后面的代码肯定会出错。 return 10/ndef bar(): return foo('0')bar()# 用print()最大的坏处是将来还得删掉它，想想程序里到处都是print()，运行结果也会包含很多垃圾信息。所以，我们又有第二种方法。# 断言# 凡是用print()来辅助查看的地方，都可以用断言（assert）来替代：# 程序中如果到处充斥着assert，和print()相比也好不到哪去。不过，启动Python解释器时可以用-O参数来关闭assert：python -O err.py# 关闭后，你可以把所有的assert语句当成pass来看。 10.0 logging 把print()替换为logging是第3种方式，和assert比，logging不会抛出错误，而且可以输出到文件; logging.info()就可以输出一段文本。运行，发现除了ZeroDivisionError，没有任何信息。 123456import logginglogging.basicConfig(level=logging.INFO)# 在import logging之后添加一行配置再试试：s = '1'n = int(s)logging.info(\"n=%d\"%n)print(10/n) 10.0 这就是logging的好处，它允许你指定记录信息的级别，有debug，info，warning，error等几个级别，当我们指定level=INFO时，logging.debug就不起作用了。同理，指定level=WARNING后，debug和info就不起作用了。这样一来，你可以放心地输出不同级别的信息，也不用删除，最后统一控制输出哪个级别的信息。 logging的另一个好处是通过简单的配置，一条语句可以同时输出到不同的地方，比如console和文件。 pdb 第4种方式是启动Python的调试器pdb，让程序以单步方式运行，可以随时查看运行状态。我们先准备好程序： 1234567s = &apos;0&apos;n = int(s)print(10 / n)# 然后启动$ python -m pdb err.py&gt; /Users/michael/Github/learn-python3/samples/debug/err.py(2)&lt;module&gt;()-&gt; s = &apos;0&apos; 以参数-m pdb启动后，pdb定位到下一步要执行的代码-&gt; s = '0' 。输入命令l来查看代码: 12345(Pdb) l 1 # err.py 2 -&gt; s = &apos;0&apos; 3 n = int(s) 4 print(10 / n) 输入命令n可以单步执行代码： 123456(Pdb) n&gt; /Users/michael/Github/learn-python3/samples/debug/err.py(3)&lt;module&gt;()-&gt; n = int(s)(Pdb) n&gt; /Users/michael/Github/learn-python3/samples/debug/err.py(4)&lt;module&gt;()-&gt; print(10 / n) 任何时候都可以输入命令p 变量名来查看变量： 1234(Pdb) p s&apos;0&apos;(Pdb) p n0 输入命令q结束调试，退出程序：(Pdb) q pdb.set_trace() 这个方法也是用pdb，但是不需要单步执行，我们只需要import pdb，然后，在可能出错的地方放一个pdb.set_trace()，就可以设置一个断点： 1234567# err.pyimport pdbs = &apos;0&apos;n = int(s)pdb.set_trace() # 运行到这里会自动暂停print(10 / n) 运行代码，程序会自动在pdb.set_trace()暂停并进入pdb调试环境，可以用命令p查看变量，或者用命令c继续运行： 12345678910$ python err.py &gt; /Users/michael/Github/learn-python3/samples/debug/err.py(7)&lt;module&gt;()-&gt; print(10 / n)(Pdb) p n0(Pdb) cTraceback (most recent call last): File &quot;err.py&quot;, line 7, in &lt;module&gt; print(10 / n)ZeroDivisionError: division by zero 单元测试“测试驱动开发”（TDD：Test-Driven Development），单元测试就不陌生。 单元测试是用来对一个模块、一个函数或者一个类来进行正确性检验的测试工作。比如对函数abs()，我们可以编写出以下几个测试用例： 输入正数，比如1、1.2、0.99，期待返回值与输入相同； 输入负数，比如-1、-1.2、-0.99，期待返回值与输入相反； 输入0，期待返回0； 输入非数值类型，比如None、[]、{}，期待抛出TypeError。 把上面的测试用例放到一个测试模块里，就是一个完整的单元测试。 如果单元测试通过，说明我们测试的这个函数能够正常工作。如果单元测试不通过，要么函数有bug，要么测试条件输入不正确，总之，需要修复使单元测试能够通过。 小结 单元测试可以有效地测试某个程序模块的行为，是未来重构代码的信心保证。 单元测试的测试用例要覆盖常用的输入组合、边界条件和异常。 单元测试代码要非常简单，如果测试代码太复杂，那么测试代码本身就可能有bug。 单元测试通过了并不意味着程序就没有bug了，但是不通过程序肯定有bug。 文档测试经常阅读Python的官方文档，可以看到很多文档都有示例代码。可以把这些示例代码在Python的交互式环境下输入并执行，结果与文档中的示例代码显示的一致。 这些代码与其他说明可以写在注释中，然后，由一些工具来自动生成文档。既然这些代码本身就可以粘贴出来直接运行，那么，可不可以自动执行写在注释中的这些代码呢？ 答案是肯定的。 当我们编写注释时，如果写上这样的注释： 1234567891011121314def abs(n): ''' Function to get absolute value of number. Example: &gt;&gt;&gt; abs(1) 1 &gt;&gt;&gt; abs(-1) 1 &gt;&gt;&gt; abs(0) 0 ''' return n if n &gt;= 0 else (-n) 无疑更明确地告诉函数的调用者该函数的期望输入和输出。 并且，Python内置的“文档测试”（doctest）模块可以直接提取注释中的代码并执行测试。 doctest严格按照Python交互式命令行的输入和输出来判断测试结果是否正确。只有测试异常的时候，可以用…表示中间一大段烦人的输出。 IO编程 IO在计算机中指Input/Output，也就是输入和输出。由于程序和运行时数据是在内存中驻留，由CPU这个超快的计算核心来执行，涉及到数据交换的地方，通常是磁盘、网络等，就需要IO接口。 IO编程中，Stream（流）是一个很重要的概念，可以把流想象成一个水管，数据就是水管里的水，但是只能单向流动。Input Stream就是数据从外面（磁盘、网络）流进内存，Output Stream就是数据从内存流到外面去。对于浏览网页来说，浏览器和新浪服务器之间至少需要建立两根水管，才可以既能发数据，又能收数据。 存在速度严重不匹配：同步和异步的区别就在于是否等待IO执行的结果。文件读写 读写文件是最常见的IO操作。Python内置了读写文件的函数，用法和C是兼容的。 读写文件前，我们先必须了解一下，在磁盘上读写文件的功能都是由操作系统提供的，现代操作系统不允许普通的程序直接操作磁盘，所以，读写文件就是请求操作系统打开一个文件对象（通常称为文件描述符），然后，通过操作系统提供的接口从这个文件对象中读取数据（读文件），或者把数据写入这个文件对象（写文件）。 读文件 要以读文件的模式打开一个文件对象， 使用Python内置的open()函数，传入文件名和标示符：f = open('/Users/michael/test.txt', 'r') 标示符’r’表示读，这样，我们就成功地打开了一个文件。 如果文件不存在，open()函数就会抛出一个IOError的错误，并且给出错误码和详细的信息告诉你文件不存在 如果文件打开成功，接下来，调用read()方法可以一次读取文件的全部内容，Python把内容读到内存，用一个str对象表示： 12&gt;&gt;&gt; f.read()&apos;Hello, world!&apos; 最后一步是调用close()方法关闭文件。文件使用完毕后必须关闭，因为文件对象会占用操作系统的资源，并且操作系统同一时间能打开的文件数量也是有限的：&gt;&gt;&gt; f.close() 由于文件读写时都有可能产生IOError，一旦出错，后面的f.close()就不会调用。 所以，为了保证无论是否出错都能正确地关闭文件，我们可以使用try ... finally来实现： 123456try: f = open(&apos;/path/to/file&apos;, &apos;r&apos;) print(f.read())finally: if f: f.close() 改进：但是每次都这么写实在太繁琐，所以，Python引入了with语句来自动帮我们调用close()方法： 12with open(&apos;/path/to/file&apos;, &apos;r&apos;) as f:print(f.read()) 这和前面的try … finally是一样的，但是代码更佳简洁，并且不必调用f.close()方法。 调用read()会一次性读取文件的全部内容，如果文件有10G，内存就爆了，所以，要保险起见，可以反复调用read(size)方法，每次最多读取size个字节的内容。 另外，调用readline()可以每次读取一行内容，调用readlines()一次读取所有内容并按行返回list。因此，要根据需要决定怎么调用。如果文件很小，read()一次性读取最方便；如果不能确定文件大小，反复调用read(size)比较保险；如果是配置文件，调用readlines()最方便：12for line in f.readlines():print(line.strip()) # 把末尾的&apos;\\n&apos;删掉 file-like Object 像open()函数返回的这种有个read()方法的对象，在Python中统称为file-like Object。除了file外，还可以是内存的字节流，网络流，自定义流等等。file-like Object不要求从特定类继承，只要写个read()方法就行。 StringIO就是在内存中创建的file-like Object，常用作临时缓冲。 二进制文件 前面讲的默认都是读取文本文件，并且是UTF-8编码的文本文件。要读取二进制文件，比如图片、视频等等，用'rb'模式打开文件即可：123&gt;&gt;&gt; f = open(&apos;/Users/michael/test.jpg&apos;, &apos;rb&apos;)&gt;&gt;&gt; f.read()b&apos;\\xff\\xd8\\xff\\xe1\\x00\\x18Exif\\x00\\x00...&apos; # 十六进制表示的字节 字符编码 要读取非UTF-8编码的文本文件，需要给open()函数传入encoding参数，例如，读取GBK编码的文件：123&gt;&gt;&gt; f = open(&apos;/Users/michael/gbk.txt&apos;, &apos;r&apos;, encoding=&apos;gbk&apos;, errors=&apos;ignore&apos;)&gt;&gt;&gt; f.read()&apos;测试&apos; 写文件 写文件和读文件是一样的，唯一区别是调用open()函数时，传入标识符'w'或者'wb'表示写文本文件或写二进制文件。 StringIO和BytesIOStringIO 很多时候，数据读写不一定是文件，也可以在内存中读写。 StringIO顾名思义就是在内存中读写str。 要把str写入StringIO，我们需要先创建一个StringIO，然后，像文件一样写入即可12345678910&gt;&gt;&gt; from io import StringIO&gt;&gt;&gt; f = StringIO()&gt;&gt;&gt; f.write(&apos;hello&apos;)5&gt;&gt;&gt; f.write(&apos; &apos;)1&gt;&gt;&gt; f.write(&apos;world!&apos;)6&gt;&gt;&gt; print(f.getvalue())hello world! BytesIO StringIO操作的只能是str，如果要操作二进制数据，就需要使用BytesIO。 BytesIO实现了在内存中读写bytes，我们创建一个BytesIO，然后写入一些bytes：123456&gt;&gt;&gt; from io import BytesIO&gt;&gt;&gt; f = BytesIO()&gt;&gt;&gt; f.write(&apos;中文&apos;.encode(&apos;utf-8&apos;))6&gt;&gt;&gt; print(f.getvalue())b&apos;\\xe4\\xb8\\xad\\xe6\\x96\\x87&apos; 操作文件和目录如果我们要操作文件、目录，可以在命令行下面输入操作系统提供的各种命令来完成。比如dir、cp等命令。 如果要在Python程序中执行这些目录和文件的操作怎么办？其实操作系统提供的命令只是简单地调用了操作系统提供的接口函数，Python内置的os模块也可以直接调用操作系统提供的接口函数。 打开Python交互式命令行，我们来看看如何使用os模块的基本功能：123&gt;&gt;&gt; import os&gt;&gt;&gt; os.name # 操作系统类型&apos;posix&apos; 环境变量在操作系统中定义的环境变量，全部保存在os.environ这个变量中，可以直接查看:12&gt;&gt;&gt; os.environenviron({&apos;VERSIONER_PYTHON_PREFER_32_BIT&apos;: &apos;no&apos;, &apos;TERM_PROGRAM_VERSION&apos;: &apos;326&apos;, &apos;LOGNAME&apos;: &apos;michael&apos;, &apos;USER&apos;: &apos;michael&apos;, &apos;PATH&apos;: &apos;/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/opt/X11/bin:/usr/local/mysql/bin&apos;, ...}) 操作文件和目录的函数一部分放在os模块中，一部分放在os.path模块中，这一点要注意一下。查看、创建和删除目录可以这么调用：12345678910# 查看当前目录的绝对路径:&gt;&gt;&gt; os.path.abspath(&apos;.&apos;)&apos;/Users/michael&apos;# 在某个目录下创建一个新目录，首先把新目录的完整路径表示出来:&gt;&gt;&gt; os.path.join(&apos;/Users/michael&apos;, &apos;testdir&apos;)&apos;/Users/michael/testdir&apos;# 然后创建一个目录:&gt;&gt;&gt; os.mkdir(&apos;/Users/michael/testdir&apos;)# 删掉一个目录:&gt;&gt;&gt; os.rmdir(&apos;/Users/michael/testdir&apos;) 序列化在程序运行的过程中，所有的变量都是在内存中,比如，定义一个dict：d = dict(name='Bob', age=20, score=88)可以随时修改变量，比如把name改成'Bill'，但是一旦程序结束，变量所占用的内存就被操作系统全部回收。如果没有把修改后的'Bill'存储到磁盘上，下次重新运行程序，变量又被初始化为'Bob'。 我们把变量从内存中变成可存储或传输的过程称之为序列化，在Python中叫pickling，在其他语言中也被称之为serialization，marshalling，flattening等等，都是一个意思。序列化之后，就可以把序列化后的内容写入磁盘，或者通过网络传输到别的机器上。 反过来，把变量内容从序列化的对象重新读到内存里称之为反序列化，即unpickling。 Python提供了pickle模块来实现序列化。 首先，我们尝试把一个对象序列化并写入文件： 123import pickled = dict(name='Bob', age=20, score=88)pickle.dumps(d) b&apos;\\x80\\x03}q\\x00(X\\x04\\x00\\x00\\x00nameq\\x01X\\x03\\x00\\x00\\x00Bobq\\x02X\\x03\\x00\\x00\\x00ageq\\x03K\\x14X\\x05\\x00\\x00\\x00scoreq\\x04KXu.&apos; pickle.dumps()方法把任意对象序列化成一个bytes，然后，就可以把这个bytes写入文件。或者用另一个方法pickle.dump()直接把对象序列化后写入一个file-like Object：123&gt;&gt;&gt; f = open(&apos;dump.txt&apos;, &apos;wb&apos;)&gt;&gt;&gt; pickle.dump(d, f)&gt;&gt;&gt; f.close() 看看写入的dump.txt文件，一堆乱七八糟的内容，这些都是Python保存的对象内部信息。当我们要把对象从磁盘读到内存时，可以先把内容读到一个bytes，然后用pickle.loads()方法反序列化出对象，也可以直接用pickle.load()方法从一个file-like Object中直接反序列化出对象。我们打开另一个Python命令行来反序列化刚才保存的对象：12345&gt;&gt;&gt; f = open(&apos;dump.txt&apos;, &apos;rb&apos;)&gt;&gt;&gt; d = pickle.load(f)&gt;&gt;&gt; f.close()&gt;&gt;&gt; d{&apos;age&apos;: 20, &apos;score&apos;: 88, &apos;name&apos;: &apos;Bob&apos;} JSON如果我们要在不同的编程语言之间传递对象，就必须把对象序列化为标准格式，比如XML，但更好的方法是序列化为JSON，因为JSON表示出来就是一个字符串，可以被所有语言读取，也可以方便地存储到磁盘或者通过网络传输。JSON不仅是标准格式，并且比XML更快，而且可以直接在Web页面中读取，非常方便。JSON标准规定JSON编码是UTF-8 JSON表示的对象就是标准的JavaScript语言的对象。Python内置的json模块提供了非常完善的Python对象到JSON格式的转换。我们先看看如何把Python对象变成一个JSON： 123import jsond = dict(name=\"bob\", age=19, score=100)json.dumps(d) &apos;{&quot;name&quot;: &quot;bob&quot;, &quot;age&quot;: 19, &quot;score&quot;: 100}&apos; dumps()方法返回一个str，内容就是标准的JSON。类似的，dump()方法可以直接把JSON写入一个file-like Object。 要把JSON反序列化为Python对象，用loads()或者对应的load()方法，前者把JSON的字符串反序列化，后者从file-like Object中读取字符串并反序列化： 12json_str = '{\"age\": 20, \"score\": 88, \"name\": \"Bob\"}'json.loads(json_str) {&apos;age&apos;: 20, &apos;score&apos;: 88, &apos;name&apos;: &apos;Bob&apos;} JSON进阶Python的dict对象可以直接序列化为JSON的{}，不过，很多时候，我们更喜欢用class表示对象，比如定义Student类，然后序列化： 12345678import jsonclass Student(object): def __init__(self, name, age, score): self.name = name self.age = age self.score = scores = Student('Bob', 18, 100)print(json.dumps(s)) {&quot;name&quot;: &quot;Bob&quot;, &quot;age&quot;: 18, &quot;score&quot;: 100} 错误的原因是Student对象不是一个可序列化为JSON的对象。 看dumps()方法的参数列表，可以发现，除了第一个必须的obj参数外，dumps()方法还提供了一大堆的可选参数： 这些可选参数就是让我们来定制JSON序列化。前面的代码之所以无法把Student类实例序列化为JSON，是因为默认情况下，dumps()方法不知道如何将Student实例变为一个JSON的{}对象。 可选参数default就是把任意一个对象变成一个可序列为JSON的对象，我们只需要为Student专门写一个转换函数，再把函数传进去即可： 这样，Student实例首先被student2dict()函数转换成dict，然后再被顺利序列化为JSON： 1234567def student2dict(std): return { 'name': std.name, 'age': std.age, 'score': std.score }print(json.dumps(s, default=student2dict)) {&quot;name&quot;: &quot;Bob&quot;, &quot;age&quot;: 18, &quot;score&quot;: 100} 12345# 不过，下次如果遇到一个Teacher类的实例，照样无法序列化为JSON。我们可以偷个懒，把任意class的实例变为dict：print(json.dumps(s, default=lambda obj: obj.__dict__))# 因为通常class的实例都有一个__dict__属性，它就是一个dict，用来存储实例变量。也有少数例外，比如定义了__slots__的class。#同样的道理，如果我们要把JSON反序列化为一个Student对象实例，loads()方法首先转换出一个dict对象，# 然后，我们传入的object_hook函数负责把dict转换为Student实例： {&quot;name&quot;: &quot;Bob&quot;, &quot;age&quot;: 18, &quot;score&quot;: 100} 1234def dict2student(d): return Student(d['name'], d['age'], d['score'])json_str = '{\"age\": 20, \"score\": 88, \"name\": \"Bob\"}'print(json.loads(json_str, object_hook=dict2student)) &lt;__main__.Student object at 0x7fdad92e2a20&gt; 小结Python语言特定的序列化模块是pickle，但如果要把序列化搞得更通用、更符合Web标准，就可以使用json模块。 json模块的dumps()和loads()函数是定义得非常好的接口的典范。当我们使用时，只需要传入一个必须的参数。但是，当默认的序列化或反序列机制不满足我们的要求时，我们又可以传入更多的参数来定制序列化或反序列化的规则，既做到了接口简单易用，又做到了充分的扩展性和灵活性 进程和线程总结一下就是，多任务的实现有3种方式： 多进程模式； 多线程模式； 多进程+多线程模式。 同时执行多个任务通常各个任务之间并不是没有关联的，而是需要相互通信和协调，有时，任务1必须暂停等待任务2完成后才能继续执行，有时，任务3和任务4又不能同时执行，所以，多进程和多线程的程序的复杂度要远远高于我们前面写的单进程单线程的程序。Python既支持多进程，又支持多线程，我们会讨论如何编写这两种多任务程序。 小结 线程是最小的执行单元，而进程由至少一个线程组成。如何调度进程和线程，完全由操作系统决定，程序自己不能决定什么时候执行，执行多长时间。 多进程和多线程的程序涉及到同步、数据共享的问题，编写起来更复杂。 正则表达式 字符串是编程时涉及到的最多的一种数据结构，对字符串进行操作的需求几乎无处不在。比如判断一个字符串是否是合法的Email地址，虽然可以编程提取@前后的子串，再分别判断是否是单词和域名，但这样做不但麻烦，而且代码难以复用。 正则表达式是一种用来匹配字符串的强有力的武器。它的设计思想是用一种描述性的语言来给字符串定义一个规则，凡是符合规则的字符串，我们就认为它“匹配”了，否则，该字符串就是不合法的。 所以我们判断一个字符串是否是合法的Email的方法是： 创建一个匹配Email的正则表达式； 用该正则表达式去匹配用户的输入来判断是否合法。 因为正则表达式也是用字符串表示的，所以，我们要首先了解如何用字符来描述字符。 在正则表达式中，如果直接给出字符，就是精确匹配。用\\d可以匹配一个数字，\\w可以匹配一个字母或数字，所以： '00\\d'可以匹配'007‘，但无法匹配'00A'； '\\d\\d\\d'可以匹配'010'； '\\w\\w\\d'可以匹配'py3'； .可以匹配任意字符，所以： 'py.'可以匹配'pyc' 、 'pyo' 、'py!'等等。 要匹配变长的字符，在正则表达式中，用*表示任意个字符（包括0个），用+表示至少一个字符，用?表示0个或1个字符，用{n}表示n个字符，用{n,m}表示n-m个字符： 来看一个复杂的例子：\\d{3}\\s+\\d{3,8}。我们来从左到右解读一下： \\d{3}表示匹配3个数字，例如'010'； \\s可以匹配一个空格（也包括Tab等空白符），所以\\s+表示至少有一个空格，例如匹配' '，' '等； \\d{3,8}表示3-8个数字，例如'1234567'综合起来，上面的正则表达式可以匹配以任意个空格隔开的带区号的电话号码。 如果要匹配'010-12345'这样的号码呢？由于'-'是特殊字符，在正则表达式中，要用'\\'转义，所以，上面的正则是\\d{3}\\-\\d{3,8}。 但是，仍然无法匹配'010 - 12345'，因为带有空格。所以我们需要更复杂的匹配方式。 进阶 要做更精确地匹配，可以用[]表示范围，比如： - `[0-9a-zA-Z\\_]`可以匹配一个数字、字母或者下划线； - `[0-9a-zA-Z\\_]+`可以匹配至少由一个数字、字母或者下划线组成的字符串，比如`&apos;a100&apos;，&apos;0_Z&apos;，&apos;Py3000&apos;`等等； - `[a-zA-Z\\_][0-9a-zA-Z\\_]*`可以匹配由字母或下划线开头，后接任意个由一个数字、字母或者下划线组成的字符串，也就是Python合法的变量； - `[a-zA-Z\\_][0-9a-zA-Z\\_]{0, 19}`更精确地限制了变量的长度是1-20个字符（前面1个字符+后面最多19个字符）。 A|B可以匹配A或B，所以(P|p)ython可以匹配’Python’或者’python’。 ^表示行的开头，^\\d表示必须以数字开头。 $表示行的结束，\\d$表示必须以数字结束。 你可能注意到了，py也可以匹配'python'，但是加上^py$就变成了整行匹配，就只能匹配'py'了。 re模块 Python提供re模块，包含所有正则表达式的功能。由于Python的字符串本身也用\\转义，所以要特别注意： 1234567s = 'ABC\\\\-001' # Python的字符串# 对应的正则表达式字符串变成：# 'ABC\\-001'# 因此我们强烈建议使用Python的r前缀，就不用考虑转义的问题了：s = r'ABC\\-001' # Python的字符串# 对应的正则表达式字符串不变：# 'ABC\\-001' 12345678910# 先看看如何判断正则表达式是否匹配：import reprint(re.match(r'^\\d{3}\\-\\d{3,8}$', '010-12345'))print(re.match(r'^\\d{3}\\-\\d{3, 8}$', '010 12345'))# match()方法判断是否匹配，如果匹配成功，返回一个Match对象，否则返回None。常见的判断方法就是：test = '用户输入的字符串'if re.match(r'正则表达式', test): print('ok')else: print('failed') &lt;_sre.SRE_Match object; span=(0, 9), match=&apos;010-12345&apos;&gt; None failed 切分字符串用正则表达式切分字符串比用固定的字符更灵活 12345import reprint('a b c'.split(' '))print(re.split(r'\\s+', 'a b c'))print(re.split(r'[\\s\\,]+', 'a,b, c d'))print(re.split(r'[\\s\\,\\;]+', 'a,b;; c d')) [&apos;a&apos;, &apos;b&apos;, &apos;&apos;, &apos;&apos;, &apos;c&apos;] [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;] [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;] [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;] 分组除了简单地判断是否匹配之外，正则表达式还有提取子串的强大功能。用()表示的就是要提取的分组（Group）。比如： ^(\\d{3})-(\\d{3,8})$分别定义了两个组，可以直接从匹配的字符串中提取出区号和本地号码： 12345678m = re.match(r'^(\\d{3})-(\\d{3,8})$', '010-12345')print(m)print(m.group(0))print(m.group(1))print(m.group(2))# 如果正则表达式中定义了组，就可以在Match对象上用group()方法提取出子串来。# 注意到group(0)永远是原始字符串，group(1)、group(2)……表示第1、2、……个子串。# 提取子串非常有用。 &lt;_sre.SRE_Match object; span=(0, 9), match=&apos;010-12345&apos;&gt; 010-12345 010 12345 贪婪匹配 最后需要特别指出的是，正则匹配默认是贪婪匹配，也就是匹配尽可能多的字符。举例如下，匹配出数字后面的0： 1234re.match(r'^(\\d+)(0*)$', '102300').group()# 由于\\d+采用贪婪匹配，直接把后面的0全部匹配了，结果0*只能匹配空字符串了。# 必须让\\d+采用非贪婪匹配（也就是尽可能少匹配），才能把后面的0匹配出来，加个?就可以让\\d+采用非贪婪匹配：re.match(r'^(\\d+?)(0*)$', '102300').groups() (&apos;1023&apos;, &apos;00&apos;) 编译 当我们在Python中使用正则表达式时，re模块内部会干两件事情： 编译正则表达式，如果正则表达式的字符串本身不合法，会报错； 用编译后的正则表达式去匹配字符串。 如果一个正则表达式要重复使用几千次，出于效率的考虑，我们可以预编译该正则表达式，接下来重复使用时就不需要编译这个步骤了，直接匹配： 1234567import re# 编译:re_telephone = re.compile(r'^(\\d{3})-(\\d{3,8})$')# 使用：print(re_telephone.match('010-12345').groups())print(re_telephone.match('010-8086').groups())# 编译后生成Regular Expression对象，由于该对象自己包含了正则表达式，所以调用对应的方法时不用给出正则字符串。 (&apos;010&apos;, &apos;12345&apos;) (&apos;010&apos;, &apos;8086&apos;) 常用内建模块Python之所以自称“batteries included”，就是因为内置了许多非常有用的模块，无需额外安装和配置，即可直接使用。 datetimedatetime是Python处理日期和时间的标准库。 获取当前日期和时间我们先看如何获取当前日期和时间： 1234from datetime import datetimenow = datetime.now()print(now)print(type(now)) 2018-05-08 12:22:23.845776 &lt;class &apos;datetime.datetime&apos;&gt; 注意到datetime是模块，datetime模块还包含一个datetime类，通过from datetime import datetime导入的才是datetime这个类。 如果仅导入import datetime，则必须引用全名datetime.datetime。 datetime.now()返回当前日期和时间，其类型是datetime。 获取指定日期和时间要指定某个日期和时间，我们直接用参数构造一个datetime： 123from datetime import datetimedt = datetime(2018, 4, 10, 12,30)print(dt) 2018-04-10 12:30:00 datetime转换为timestamp在计算机中，时间实际上是用数字表示的。我们把1970年1月1日 00:00:00 UTC+00:00时区的时刻称为epoch time，记为0（1970年以前的时间timestamp为负数），当前时间就是相对于epoch time的秒数，称为timestamp。即timestamp = 0 = 1970-1-1 00:00:00 UTC+0:00对应的北京时间是：timestamp = 0 = 1970-1-1 08:00:00 UTC+8:00 把一个datetime类型转换为timestamp只需要简单调用timestamp()方法： 123from datetime import datetimedt = datetime(2018, 4, 19, 12, 20) # 用指定日期时间创建datetimedt.timestamp() # 把datetime转换为timestamp 1524140400.0 注意Python的timestamp是一个浮点数。如果有小数位，小数位表示毫秒数。 某些编程语言（如Java和JavaScript）的timestamp使用整数表示毫秒数，这种情况下只需要把timestamp除以1000就得到Python的浮点表示方法。 timestamp转换为datetime要把timestamp转换为datetime，使用datetime提供的fromtimestamp()方法： 123from datetime import datetimet = 1524140400.0print(datetime.fromtimestamp(t)) 2018-04-19 12:20:00 注意到timestamp是一个浮点数，它没有时区的概念，而datetime是有时区的。上述转换是在timestamp和本地时间做转换。 本地时间是指当前操作系统设定的时区。例如北京时区是东8区，则本地时间实际上是实际上就是UTC+8:00时区的时间：timestamp也可以直接被转换到UTC标准时区的时间： 1234from datetime import datetimet = 1524140400.0print(datetime.fromtimestamp(t))# 本地print(datetime.utcfromtimestamp(t))# UTC 2018-04-19 12:20:00 2018-04-19 12:20:00 str转换为datetime很多时候，用户输入的日期和时间是字符串，要处理日期和时间，首先必须把str转换为datetime。转换方法是通过datetime.strptime()实现，需要一个日期和时间的格式化字符串： 123from datetime import datetimecday = datetime.strptime('2018-2-2 19:01:03', '%Y-%m-%d %H:%M:%S')print(cday) 2018-02-02 19:01:03 datetime转换为str如果已经有了datetime对象，要把它格式化为字符串显示给用户，就需要转换为str，转换方法是通过strftime()实现的，同样需要一个日期和时间的格式化字符串： 123from datetime import datetimenow = datetime.now()print(now.strftime(\"%a, %b %d %H:%M\")) Tue, May 08 12:46 本地时间转换为UTC时间时区转换常用第三方模块除了内建的模块外，Python还有大量的第三方模块。 基本上，所有的第三方模块都会在PyPI - the Python Package Index上注册，只要找到对应的模块名字，即可用pip安装。 此外，在安装第三方模块一节中，我们强烈推荐安装Anaconda，安装后，数十个常用的第三方模块就已经就绪，不用pip手动安装。 本章介绍常用的第三方模块。 PillowPIL：Python Imaging Library，已经是Python平台事实上的图像处理标准库了。PIL功能非常强大，但API却非常简单易用。 由于PIL仅支持到Python 2.7，加上年久失修，于是一群志愿者在PIL的基础上创建了兼容的版本，名字叫Pillow，支持最新Python 3.x，又加入了许多新特性，因此，我们可以直接安装使用Pillow。 安装Pillow如果安装了Anaconda，Pillow就已经可用了。否则，需要在命令行下通过pip安装：pip3 install pillow 操作图像来看看最常见的图像缩放操作，只需三四行代码： 1234567891011from PIL import Image# 打开一个jpg图像文件，注意是当前路径:im = Image.open('test.jpg')# 获得图像尺寸:w, h = im.sizeprint('Original image size: %sx%s' % (w, h))# 缩放到50%:im.thumbnail((w//2, h//2))print('Resize image to: %sx%s' % (w//2, h//2))# 把缩放后的图像用jpeg格式保存:im.save('thumbnail.jpg', 'jpeg') 123456789# 其他功能如切片、旋转、滤镜、输出文字、调色板等一应俱全。# 比如，模糊效果也只需几行代码：from PIL import Image, ImageFilter# 打开一个jpg图像文件，注意是当前路径:im = Image.open('test.jpg')# 应用模糊滤镜:im2 = im.filter(ImageFilter.BLUR)im2.save('blur.jpg', 'jpeg') requests 我们已经讲解了Python内置的urllib模块，用于访问网络资源。但是，它用起来比较麻烦，而且，缺少很多实用的高级功能。 更好的方案是使用requests。它是一个Python第三方库，处理URL资源特别方便。安装requests如果安装了Anaconda，requests就已经可用了。否则，需要在命令行下通过pip安装：pip install requests使用requests要通过GET访问一个页面，只需要几行代码：1234567&gt;&gt;&gt; import requests&gt;&gt;&gt; r = requests.get(&apos;https://www.douban.com/&apos;) # 豆瓣首页&gt;&gt;&gt; r.status_code200&gt;&gt;&gt; r.textr.text&apos;&lt;!DOCTYPE HTML&gt;\\n&lt;html&gt;\\n&lt;head&gt;\\n&lt;meta name=&quot;description&quot; content=&quot;提供图书、电影、音乐唱片的推荐、评论和...&apos; 对于带参数的URL，传入一个dict作为params参数：12345&gt;&gt;&gt; r = requests.get(&apos;https://www.douban.com/search&apos;, params={&apos;q&apos;: &apos;python&apos;, &apos;cat&apos;: &apos;1001&apos;})&gt;&gt;&gt; r.url # 实际请求的URL&apos;https://www.douban.com/search?q=python&amp;cat=1001&apos;&gt;&gt;&gt; r.encoding&apos;utf-8&apos; 无论响应是文本还是二进制内容，我们都可以用content属性获得bytes对象：12&gt;&gt;&gt; r.contentb&apos;&lt;!DOCTYPE html&gt;\\n&lt;html&gt;\\n&lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot;&gt;\\n...&apos; requests的方便之处还在于，对于特定类型的响应，例如JSON，可以直接获取：123&gt;&gt;&gt; r = requests.get(&apos;https://query.yahooapis.com/v1/public/yql?q=select%20*%20from%20weather.forecast%20where%20woeid%20%3D%202151330&amp;format=json&apos;)&gt;&gt;&gt; r.json(){&apos;query&apos;: {&apos;count&apos;: 1, &apos;created&apos;: &apos;2017-11-17T07:14:12Z&apos;, ... 需要传入HTTP Header时，我们传入一个dict作为headers参数：123&gt;&gt;&gt; r = requests.get(&apos;https://www.douban.com/&apos;, headers={&apos;User-Agent&apos;: &apos;Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit&apos;})&gt;&gt;&gt; r.text&apos;&lt;!DOCTYPE html&gt;\\n&lt;html&gt;\\n&lt;head&gt;\\n&lt;meta charset=&quot;UTF-8&quot;&gt;\\n &lt;title&gt;豆瓣(手机版)&lt;/title&gt;...&apos; 要发送POST请求，只需要把get()方法变成post()，然后传入data参数作为POST请求的数据：1&gt;&gt;&gt; r = requests.post(&apos;https://accounts.douban.com/login&apos;, data={&apos;form_email&apos;: &apos;abc@example.com&apos;, &apos;form_password&apos;: &apos;123456&apos;}) requests默认使用application/x-www-form-urlencoded对POST数据编码。如果要传递JSON数据，可以直接传入json参数：12params = {&apos;key&apos;: &apos;value&apos;}r = requests.post(url, json=params) # 内部自动序列化为JSON 类似的，上传文件需要更复杂的编码格式，但是requests把它简化成files参数：12&gt;&gt;&gt; upload_files = {&apos;file&apos;: open(&apos;report.xls&apos;, &apos;rb&apos;)}&gt;&gt;&gt; r = requests.post(url, files=upload_files) 在读取文件时，注意务必使用’rb’即二进制模式读取，这样获取的bytes长度才是文件的长度。 把post()方法替换为put()，delete()等，就可以以PUT或DELETE方式请求资源。 除了能轻松获取响应内容外，requests对获取HTTP响应的其他信息也非常简单。例如，获取响应头：1234&gt;&gt;&gt; r.headers{Content-Type&apos;: &apos;text/html; charset=utf-8&apos;, &apos;Transfer-Encoding&apos;: &apos;chunked&apos;, &apos;Content-Encoding&apos;: &apos;gzip&apos;, ...}&gt;&gt;&gt; r.headers[&apos;Content-Type&apos;]&apos;text/html; charset=utf-8&apos; virtualenv在开发Python应用程序的时候，系统安装的Python3只有一个版本：3.6。所有第三方的包都会被pip安装到Python3的site-packages目录下。 如果我们要同时开发多个应用程序，那这些应用程序都会共用一个Python，就是安装在系统的Python 3。如果应用A需要jinja 2.7，而应用B需要jinja 2.6怎么办？ 这种情况下，每个应用可能需要各自拥有一套“独立”的Python运行环境。virtualenv就是用来为一个应用创建一套“隔离”的Python运行环境。 首先，我们用pip安装virtualenv： pip3 install virtualenv 然后，假定我们要开发一个新的项目，需要一套独立的Python运行环境，可以这么做： 第一步，创建目录： 123Mac:~ michael$ mkdir myprojectMac:~ michael$ cd myproject/Mac:myproject michael$ 第二步，创建一个独立的Python运行环境，命名为venv： 12345Mac:myproject michael$ virtualenv --no-site-packages venvUsing base prefix &apos;/usr/local/.../Python.framework/Versions/3.4&apos;New python executable in venv/bin/python3.4Also creating executable in venv/bin/pythonInstalling setuptools, pip, wheel...done. 命令virtualenv就可以创建一个独立的Python运行环境，我们还加上了参数--no-site-packages ，这样，已经安装到系统Python环境中的所有第三方包都不会复制过来，这样，我们就得到了一个不带任何第三方包的“干净”的Python运行环境。 新建的Python环境被放到当前目录下的venv目录。有了venv这个Python环境，可以用source进入该环境：12Mac:myproject michael$ source venv/bin/activate(venv)Mac:myproject michael$ 注意到命令提示符变了，有个(venv)前缀，表示当前环境是一个名为venv的Python环境。 下面正常安装各种第三方包，并运行python命令：12345(venv)Mac:myproject michael$ pip install jinja2...Successfully installed jinja2-2.7.3 markupsafe-0.23(venv)Mac:myproject michael$ python myapp.py... 在venv环境下，用pip安装的包都被安装到venv这个环境下，系统Python环境不受任何影响。也就是说，venv环境是专门针对myproject这个应用创建的。 退出当前的venv环境，使用deactivate命令：12(venv)Mac:myproject michael$ deactivate Mac:myproject michael$ 此时就回到了正常的环境，现在pip或python均是在系统Python环境下执行。 完全可以针对每个应用创建独立的Python运行环境，这样就可以对每个应用的Python环境进行隔离。 virtualenv是如何创建“独立”的Python运行环境的呢？原理很简单，就是把系统Python复制一份到virtualenv的环境，用命令source venv/bin/activate进入一个virtualenv环境时，virtualenv会修改相关环境变量，让命令python和pip均指向当前的virtualenv环境。 图形界面Python支持多种图形界面的第三方库，包括：Tk,wxWidgets,Qt,GTK等等。 但是Python自带的库是支持Tk的Tkinter，使用Tkinter，无需安装任何包，就可以直接使用。本章简单介绍如何使用Tkinter进行GUI编程。Tkinter我们来梳理一下概念： 我们编写的Python代码会调用内置的Tkinter，Tkinter封装了访问Tk的接口； Tk是一个图形库，支持多个操作系统，使用Tcl语言开发； Tk会调用操作系统提供的本地GUI接口，完成最终的GUI。 所以，我们的代码只需要调用Tkinter提供的接口就可以了。 第一个GUI程序 使用Tkinter十分简单，我们来编写一个GUI版本的“Hello, world!”。 第一步是导入Tkinter包的所有内容, 第二步是从Frame派生一个Application类，这是所有Widget的父容器 在GUI中，每个Button、Label、输入框等，都是一个Widget。Frame则是可以容纳其他Widget的Widget，所有的Widget组合起来就是一棵树。2.pack()方法把Widget加入到父容器中，并实现布局。pack()是最简单的布局，grid()可以实现更复杂的布局。3.在createWidgets()方法中，我们创建一个Label和一个Button，当Button被点击时，触发self.quit()使程序退出。 第三步，实例化Application，并启动消息循环,GUI程序的主线程负责监听来自操作系统的消息，并依次处理每一条消息。因此，如果消息处理非常耗时，就需要在新线程中处理。 12345678910111213141516from tkinter import*class Application(Frame): def __init__(self, master=None): Frame.__init__(self, master) self.pack() self.createWidgets() def createWidgtes(self): self.helloLabel = Label(self, text=\"Hello Label\") self.helloLabel.pack() self.quitButton = Button(self, text=\"Quit\", command=self.quit) self.quitButton.pack()app=Application()# 设置窗口标题:app.master.title(\"Hello world\")# 主消息循环:app.mainloop() 输入文本 我们再对这个GUI程序改进一下，加入一个文本框，让用户可以输入文本，然后点按钮后，弹出消息对话框。 当用户点击按钮时，触发hello()，通过self.nameInput.get()获得用户输入的文本后，使用tkMessageBox.showinfo()可以弹出消息对话框。","link":"/2018/09/28/Python入门/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","link":"/tags/LeetCode/"},{"name":"coding","slug":"coding","link":"/tags/coding/"},{"name":"important","slug":"important","link":"/tags/important/"},{"name":"C++","slug":"C","link":"/tags/C/"},{"name":"编程","slug":"编程","link":"/tags/编程/"},{"name":"入门","slug":"入门","link":"/tags/入门/"},{"name":"Federated Learning","slug":"Federated-Learning","link":"/tags/Federated-Learning/"},{"name":"AI","slug":"AI","link":"/tags/AI/"},{"name":"深度学习","slug":"深度学习","link":"/tags/深度学习/"},{"name":"paper reading","slug":"paper-reading","link":"/tags/paper-reading/"},{"name":"论文阅读","slug":"论文阅读","link":"/tags/论文阅读/"},{"name":"golang","slug":"golang","link":"/tags/golang/"},{"name":"encryption","slug":"encryption","link":"/tags/encryption/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"刷题","slug":"刷题","link":"/tags/刷题/"},{"name":"双指针","slug":"双指针","link":"/tags/双指针/"},{"name":"排序","slug":"排序","link":"/tags/排序/"},{"name":"Mac终端命令","slug":"Mac终端命令","link":"/tags/Mac终端命令/"},{"name":"神经网络搜索","slug":"神经网络搜索","link":"/tags/神经网络搜索/"},{"name":"data","slug":"data","link":"/tags/data/"},{"name":"python转换","slug":"python转换","link":"/tags/python转换/"},{"name":"intro","slug":"intro","link":"/tags/intro/"},{"name":"Cryptography","slug":"Cryptography","link":"/tags/Cryptography/"},{"name":"Swagger","slug":"Swagger","link":"/tags/Swagger/"},{"name":"联合学习","slug":"联合学习","link":"/tags/联合学习/"},{"name":"代码","slug":"代码","link":"/tags/代码/"},{"name":"Goa入门","slug":"Goa入门","link":"/tags/Goa入门/"},{"name":"云端学习笔记","slug":"云端学习笔记","link":"/tags/云端学习笔记/"},{"name":"提升自我","slug":"提升自我","link":"/tags/提升自我/"},{"name":"paper","slug":"paper","link":"/tags/paper/"},{"name":"BigData","slug":"BigData","link":"/tags/BigData/"},{"name":"pytorch学习","slug":"pytorch学习","link":"/tags/pytorch学习/"},{"name":"禅","slug":"禅","link":"/tags/禅/"},{"name":"自我修炼","slug":"自我修炼","link":"/tags/自我修炼/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"github","slug":"github","link":"/tags/github/"},{"name":"npm","slug":"npm","link":"/tags/npm/"},{"name":"基础","slug":"基础","link":"/tags/基础/"},{"name":"配置文件","slug":"配置文件","link":"/tags/配置文件/"},{"name":"入门指引","slug":"入门指引","link":"/tags/入门指引/"},{"name":"大数据，spark","slug":"大数据，spark","link":"/tags/大数据，spark/"},{"name":"paperReading","slug":"paperReading","link":"/tags/paperReading/"},{"name":"编程技巧，","slug":"编程技巧，","link":"/tags/编程技巧，/"},{"name":"Leetcode","slug":"Leetcode","link":"/tags/Leetcode/"}],"categories":[{"name":"LeetCode","slug":"LeetCode","link":"/categories/LeetCode/"},{"name":"Blcokchain","slug":"Blcokchain","link":"/categories/Blcokchain/"},{"name":"C++","slug":"C","link":"/categories/C/"},{"name":"Docker容器","slug":"Docker容器","link":"/categories/Docker容器/"},{"name":"Federated Learning","slug":"Federated-Learning","link":"/categories/Federated-Learning/"},{"name":"人工智能","slug":"人工智能","link":"/categories/人工智能/"},{"name":"论文阅读","slug":"论文阅读","link":"/categories/论文阅读/"},{"name":"golang学习","slug":"golang学习","link":"/categories/golang学习/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"工具学习","slug":"工具学习","link":"/categories/工具学习/"},{"name":"游戏开发","slug":"游戏开发","link":"/categories/游戏开发/"},{"name":"Python学习","slug":"Python学习","link":"/categories/Python学习/"},{"name":"TensorFlow","slug":"TensorFlow","link":"/categories/TensorFlow/"},{"name":"iOS编程","slug":"iOS编程","link":"/categories/iOS编程/"},{"name":"Tensorflow","slug":"Tensorflow","link":"/categories/Tensorflow/"},{"name":"pytorch","slug":"pytorch","link":"/categories/pytorch/"},{"name":"大数据学习","slug":"大数据学习","link":"/categories/大数据学习/"},{"name":"LeeCode","slug":"LeeCode","link":"/categories/LeeCode/"},{"name":"Leetcode","slug":"Leetcode","link":"/categories/Leetcode/"}]}