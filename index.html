<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Fallenk&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="不忘初心，努力学习，砥砺前进，自我反省">
<meta name="keywords" content="经历，心得，笔记，目标">
<meta property="og:type" content="website">
<meta property="og:title" content="Fallenk&#39;s Blog">
<meta property="og:url" content="https://fallenk.github.io/index.html">
<meta property="og:site_name" content="Fallenk&#39;s Blog">
<meta property="og:description" content="不忘初心，努力学习，砥砺前进，自我反省">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Fallenk&#39;s Blog">
<meta name="twitter:description" content="不忘初心，努力学习，砥砺前进，自我反省">
  
    <link rel="alternate" href="/atom.xml" title="Fallenk&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://fallenk.github.io"></form>
      </div>
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">首页</a>
        
          <a class="main-nav-link" href="/archives">归档</a>
        
          <a class="main-nav-link" href="/about">关于</a>
        
      </nav>
      
    </div>
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Fallenk&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">经历，心得，笔记，目标</a>
        </h2>
      
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-pytorch学习2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/25/pytorch学习2/" class="article-date">
  <time datetime="2019-08-25T09:01:13.000Z" itemprop="datePublished">2019-08-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/pytorch/">pytorch</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/25/pytorch学习2/">pytorch学习2</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <a id="more"></a>
<h2 id="卷积神经网络-CNN-Convolutional-Neural-Network"><a href="#卷积神经网络-CNN-Convolutional-Neural-Network" class="headerlink" title="卷积神经网络 CNN (Convolutional Neural Network)"></a>卷积神经网络 CNN (Convolutional Neural Network)</h2><h3 id="卷积-和-神经网络"><a href="#卷积-和-神经网络" class="headerlink" title="卷积 和 神经网络"></a>卷积 和 神经网络</h3><p>卷积神经网络是如何运作的吧, 举一个识别图片的例子, 我们知道神经网络是由<strong>一连串的神经层组成,每一层神经层里面有存在有很多的神经元</strong>. 这些<strong>神经元就是神经网络识别事物的关键</strong>. <strong>每一种神经网络都会有输入输出值, 当输入值是图片的时候, 实际上输入神经网络的并不是那些色彩缤纷的图案,而是一堆堆的数字</strong>. 就比如说这个. 当神经网络需要处理这么多输入信息的时候, 也就是卷积神经网络就可以发挥它的优势的时候了. 那什么是卷积神经网络呢?</p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/19.png" alt="image-20190819103552681"></p>
<p>先把卷积神经网络这个词拆开来看. “卷积” 和 “神经网络”. 卷积也就是说神经网络<strong>不再是对每个像素的输入信息做处理了,而是图片上每一小块像素区域进行处理</strong>, 这种做法加强了图片信息的连续性. 使得神经网络能看到图形, 而非一个点. 这种做法同时也加深了神经网络对图片的理解. 具体来说, 卷积神经网络<strong>有一个批量过滤器, 持续不断的在图片上滚动收集图片里的信息,每一次收集的时候都只是收集一小块像素区域, 然后把收集来的信息进行整理</strong>, 这时候整理出来的信息有了一些实际上的呈现, 比如这时的神经网络能看到一些边缘的图片信息, 然后在以同样的步骤, 用类似的批量过滤器扫过产生的这些边缘信息, 神经网络从这些边缘信息里面总结出更高层的信息结构,比如说总结的边缘能够画出眼睛,鼻子等等. 再经过一次过滤, 脸部的信息也从这些眼睛鼻子的信息中被总结出来. 最后我们再把这些信息套入几层普通的全连接神经层进行分类, 这样就能得到输入的图片能被分为哪一类的结果了.</p>
<p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190819103829081.png" alt="image-20190819103829081"></p>
<p>我们截取一段 google 介绍卷积神经网络的视频, 具体说说图片是如何被卷积的. 下面是一张猫的图片, 图片有长, 宽, 高 三个参数. 对! 图片是有高度的! 这里的高指的是计算机用于产生颜色使用的信息. 如果是黑白照片的话, 高的单位就只有1, 如果是彩色照片, 就可能有红绿蓝三种颜色的信息, 这时的高度为3. 我们以彩色照片为例子. <strong>过滤器就是影像中不断移动的东西, 他不断在图片收集小批小批的像素块, 收集完所有信息后, 输出的值</strong>, 我们可以理解成是一个高度更高,长和宽更小的”图片”. 这个图片里就能包含一些边缘信息. 然后以同样的步骤再进行多次卷积, 将图片的长宽再压缩, 高度再增加, 就有了对输入图片更深的理解. 将压缩,增高的信息嵌套在普通的分类神经层上,我们就能对这种图片进行分类了.</p>
<h3 id="池化-pooling"><a href="#池化-pooling" class="headerlink" title="池化(pooling)"></a>池化(pooling)</h3><p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/21.png" alt="image-20190819104018442"></p>
<p>研究发现, 在每一次卷积的时候, 神经层可能会无意地丢失一些信息. 这时, 池化 (pooling) 就可以很好地解决这一问题. 而且池化是一个筛选过滤的过程, 能将 layer 中有用的信息筛选出来, 给下一个层分析. 同时也减轻了神经网络的计算负担 (<a href="http://cs231n.github.io/convolutional-networks/#pool" target="_blank" rel="noopener">具体细节参考</a>). 也就是说在卷集的时候, 我们不压缩长宽, 尽量地保留更多信息, 压缩的工作就交给池化了,这样的一项附加工作能够很有效的提高准确性. 有了这些技术,我们就可以搭建一个属于我们自己的卷积神经网络啦.</p>
<h2 id="流行的-CNN-结构"><a href="#流行的-CNN-结构" class="headerlink" title="流行的 CNN 结构"></a>流行的 CNN 结构</h2><p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190819104114779.png" alt="image-20190819104114779"></p>
<p>比较流行的一种搭建结构是这样, 从下到上的顺序, 首先是输入的图片(image), 经过一层卷积层 (convolution), 然后在用池化(pooling)方式处理卷积的信息, 这里使用的是 max pooling 的方式. 然后在经过一次同样的处理, 把得到的第二次处理的信息传入两层全连接的神经层 (fully connected),这也是一般的两层神经网络层,最后在接上一个分类器(classifier)进行分类预测.</p>
<h2 id="CNN-卷积神经网络"><a href="#CNN-卷积神经网络" class="headerlink" title="CNN 卷积神经网络"></a>CNN 卷积神经网络</h2><h3 id="1-MNIST手写数据"><a href="#1-MNIST手写数据" class="headerlink" title="1. MNIST手写数据"></a>1. MNIST手写数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torchvision  <span class="comment"># 数据库 模块 下载 处理数据</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Hyper parameters</span></span><br><span class="line">EPOCH = <span class="number">1</span> <span class="comment"># 训练整批数据的多少次，</span></span><br><span class="line">BATCH_ZISE = <span class="number">50</span></span><br><span class="line">LR = <span class="number">0.001</span></span><br><span class="line">DOWNLOAD_MNIST = <span class="keyword">True</span> <span class="comment"># </span></span><br><span class="line"><span class="comment"># Mnist 手写数字</span></span><br><span class="line"><span class="comment"># 1. use torchvision download mnist dataset</span></span><br><span class="line">train_data = torchvision.datasets.MNIST(</span><br><span class="line">	root=<span class="string">'./mnist/'</span> <span class="comment"># 保存位置</span></span><br><span class="line">  train=<span class="keyword">True</span>, <span class="comment"># this is training data</span></span><br><span class="line">  transform=torchvision.transforms.ToTensor(), <span class="comment"># 转换PIL.Image or numpy.ndarray 或 torch.FloaterTensor (C x H x W), 训练的时候 normalize成[0.0, 1.0]区间</span></span><br><span class="line">  download=DOWNLOAD_MNIST,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 2. package data with dataloader  </span></span><br><span class="line"><span class="comment"># 批训练 50 example, 1 channel, 28*28 (50, 1, 28, 28)</span></span><br><span class="line">train_loader = Data.Dataloader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot one example </span></span><br><span class="line">print(train_data.train_data.size())  <span class="comment"># torch.Size([60000, 28, 28])</span></span><br><span class="line">print(train_data.data.size())  <span class="comment"># torch.Size([60000, 28, 28])</span></span><br><span class="line">print(train_loader.dataset.data.size()) <span class="comment"># (6000, 28, 28)</span></span><br><span class="line"></span><br><span class="line">print(train_data.train_labels.size()) <span class="comment"># (60000,)</span></span><br><span class="line">print(train_data.targets.size())</span><br><span class="line">print(train_loader.dataset.targets.size())</span><br><span class="line"></span><br><span class="line">plt.imshow(train_data.train_data[<span class="number">0</span>].numpy(), cmap=<span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">"%i"</span> % train_data.train_data.labels[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.imshow(train_data.data[<span class="number">0</span>].numpy(), cmap=<span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">"%i"</span> % train_data.targets[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同样, 我们除了训练数据, 还给一些测试数据, 测试看看它有没有训练好.</span></span><br><span class="line">test_data = torchvision.datasets.MNIST(root=<span class="string">'./mnist'</span>, train=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># 为了节约时间, 我们测试时只测试前2000个</span></span><br><span class="line"><span class="comment"># shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)</span></span><br><span class="line">test_x = torch.unsqueeze(test_data.test_data, dim=<span class="number">1</span>).type(torch.FloatTensor)[:<span class="number">2000</span>]/<span class="number">255.</span></span><br><span class="line">test_y = test_data.test_labels[:<span class="number">2000</span>]</span><br><span class="line"><span class="comment"># tensor.nump(); torch.from_numpy()</span></span><br><span class="line"><span class="comment"># minst data(numpy or PIL image) -&gt;train_data(tensor) -&gt;train_loader(tensor with batchsize)</span></span><br></pre></td></tr></table></figure>
<h3 id="2-CNN模型"><a href="#2-CNN模型" class="headerlink" title="2. CNN模型"></a>2. CNN模型</h3><p>和以前一样, 我们用一个 class 来建立 CNN 模型.这个 CNN 整体流程是 卷积(<code>Conv2d</code>) -&gt; 激励函数(<code>ReLU</code>) -&gt; 池化, 向下采样 (<code>MaxPooling</code>) -&gt; 再来一遍 -&gt; 展平多维的卷积成的特征图 -&gt; 接入全连接层 (<code>Linear</code>) -&gt; 输出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>：</span></span><br><span class="line"><span class="class">	<span class="title">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    super(CNN, self).__init__()</span><br><span class="line">    self.conv1 = nn.Sequential(  <span class="comment"># input shape (1, 28, 28)</span></span><br><span class="line">    	nn.Conv2d(</span><br><span class="line">      	in_channels=<span class="number">1</span>, <span class="comment"># input height</span></span><br><span class="line">        out_channels=<span class="number">16</span>, <span class="comment"># n_filters</span></span><br><span class="line">        kernel_size = <span class="number">5</span>, <span class="comment"># filter size</span></span><br><span class="line">        stride=<span class="number">1</span>, <span class="comment"># filter movement/step</span></span><br><span class="line">        padding=<span class="number">2</span>, <span class="comment"># 如想conv2d出来的图片长宽没有变化，padding=(kernel_size-1)/2 当 stride=1</span></span><br><span class="line">      )， <span class="comment"># out shape (16, 28, 28)</span></span><br><span class="line">      nn.ReLU(), <span class="comment"># activation</span></span><br><span class="line">      nn.MaxPool2d(<span class="number">2</span>), <span class="comment"># output (16, 14, 14)</span></span><br><span class="line">    )</span><br><span class="line">    self.conv2 = nn.Sequential( <span class="comment"># input shape (16, 14, 14)</span></span><br><span class="line">    	nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">      nn.ReLU(), <span class="comment"># activation</span></span><br><span class="line">      nn.MaxPool2d(<span class="number">2</span>) <span class="comment"># output shape (32, 7, 7)</span></span><br><span class="line">    )</span><br><span class="line">    self.out = nn.Linear(<span class="number">32</span>*<span class="number">7</span>*<span class="number">7</span>, <span class="number">10</span>) <span class="comment"># fully connected layer, output 10 classes</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    x = self.conv1(x)</span><br><span class="line">    x = self.conv2(x)</span><br><span class="line">    x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>) <span class="comment"># 展平多维平面图 成 (batch_size, 32*7*7)</span></span><br><span class="line">    output = self.out(x)</span><br><span class="line">    <span class="keyword">return</span> output, x</span><br><span class="line">cnn = CNN()</span><br><span class="line">print(cnn) <span class="comment"># net architecture</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">CNN (</span></span><br><span class="line"><span class="string">  (conv1): Sequential (</span></span><br><span class="line"><span class="string">    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span></span><br><span class="line"><span class="string">    (1): ReLU ()</span></span><br><span class="line"><span class="string">    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (conv2): Sequential (</span></span><br><span class="line"><span class="string">    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span></span><br><span class="line"><span class="string">    (1): ReLU ()</span></span><br><span class="line"><span class="string">    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (out): Linear (1568 -&gt; 10)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h3 id="3-训练"><a href="#3-训练" class="headerlink" title="3. 训练"></a>3. 训练</h3><p>下面我们开始训练, 将 <code>x</code> <code>y</code> 都用 <code>Variable</code> 包起来, 然后放入 <code>cnn</code> 中计算 <code>output</code>, 最后再计算误差. 下面代码省略了计算精确度 <code>accuracy</code> 的部分, 如果想细看 <code>accuracy</code> 代码的同学, 请去往我的 <a href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/401_CNN.py" target="_blank" rel="noopener">github</a> 看全部代码.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(cnn.parameters(), lr=LR) <span class="comment"># optimize all cnn parameters</span></span><br><span class="line">loss_func = nn.CrossEntropyLoss() <span class="comment"># the target label is not one-hotted</span></span><br><span class="line"><span class="comment"># training and testing</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCH):</span><br><span class="line">  <span class="keyword">for</span> step, (b_x, b_y) <span class="keyword">in</span> enumerate(train_loader): <span class="comment"># 分配batch data, normalize x when iterate train_loader</span></span><br><span class="line">    output = cnn(b_x)[<span class="number">0</span>]</span><br><span class="line">    loss = loss_func(output, b_y)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        test_output, last_layer = cnn(test_x)</span><br><span class="line">        pred_y = torch.max(test_output, <span class="number">1</span>)[<span class="number">1</span>].data.squeeze()</span><br><span class="line">        accuracy = (pred_y == test_y).sum().item() / float(test_y.size(<span class="number">0</span>))</span><br><span class="line">        print(<span class="string">'Epoch: '</span>, epoch, <span class="string">'| train loss: %.4f'</span> % loss.data, <span class="string">'| test accuracy: %.2f'</span> % accuracy)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0306 | test accuracy: 0.97</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0147 | test accuracy: 0.98</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0427 | test accuracy: 0.98</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0078 | test accuracy: 0.98</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># print 10 predictions from test data</span></span><br><span class="line">test_output, _ = cnn(test_x[:<span class="number">10</span>])</span><br><span class="line">pred_y = torch.max(test_output, <span class="number">1</span>)[<span class="number">1</span>].data.numpy().squeeze()</span><br><span class="line">print(pred_y, <span class="string">'prediction number'</span>)</span><br><span class="line">print(test_y[:<span class="number">10</span>].numpy(), <span class="string">'real number'</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[5 0 4 1 9 2 1 3 1 4] prediction number</span></span><br><span class="line"><span class="string">[5 0 4 1 9 2 1 3 1 4] real number</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h2 id="什么是循环神经网络-RNN-Recurrent-Neural-Network"><a href="#什么是循环神经网络-RNN-Recurrent-Neural-Network" class="headerlink" title="什么是循环神经网络 RNN (Recurrent Neural Network)"></a>什么是循环神经网络 RNN (Recurrent Neural Network)</h2><p>在语言分析, 序列化数据中穿梭自如的循环神经网络 RNN</p>
<h3 id="RNN-的用途"><a href="#RNN-的用途" class="headerlink" title="RNN 的用途"></a>RNN 的用途</h3><p>只想着斯蒂芬乔布斯这个名字 , 请你再把他逆序念出来. 斯布乔(*#&amp;, 有点难吧. 这就说明, <strong>对于预测, 顺序排列是多么重要</strong>. 我们可以预测下一个按照一定顺序排列的字, 但是打乱顺序, 我们就没办法分析自己到底在说什么了.</p>
<h2 id="序列数据"><a href="#序列数据" class="headerlink" title="序列数据"></a>序列数据</h2><p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190822214243233.png" alt="image-20190822214243233"></p>
<p>我们想象现在有一组序列数据 data 0,1,2,3. 在当预测 result0 的时候,我们基于的是 data0, 同样在预测其他数据的时候, 我们也都只单单基于单个的数据. 每次使用的神经网络都是同一个 NN. 不过这些数据是有关联 顺序的 , 就像在厨房做菜, 酱料 A要比酱料 B 早放, 不然就串味了. 所以普通的神经网络结构并不能让 NN 了解这些数据之间的关联.</p>
<h3 id="处理序列数据的神经网络"><a href="#处理序列数据的神经网络" class="headerlink" title="处理序列数据的神经网络"></a>处理序列数据的神经网络</h3><p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190822214343843.png" alt="image-20190822214343843"></p>
<p>那我们如何让数据间的关联也被 NN 加以分析呢? 想想我们人类是怎么分析各种事物的关联吧, 最基本的方式,<strong>就是记住之前发生的事情. 那我们让神经网络也具备这种记住之前发生的事的能力</strong>. 再分析 Data0 的时候, 我们把分析结果存入记忆. 然后当分析 data1的时候, NN会产生新的记忆, 但是新记忆和老记忆是没有联系的. 我们就简单的把老记忆调用过来, 一起分析. 如果继续分析更多的有序数据 , RNN就会把之前的记忆都累积起来, 一起分析.</p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/35.png" alt="image-20190822214449342"></p>
<p>我们再重复一遍刚才的流程, 不过这次是以加入一些数学方面的东西. <strong>每次 RNN 运算完之后都会产生一个对于当前状态的描述 , state</strong>. 我们用简写 <code>S(t)</code> 代替, 然后这个 RNN开始分析 x(t+1) , 他会根据 x(t+1)产生s(t+1), 不过此时 <strong>y(t+1) 是由 s(t) 和 s(t+1) 共同创造</strong>的. 所以我们通常看到的 RNN 也可以表达成这种样子.</p>
<h3 id="RNN-的应用"><a href="#RNN-的应用" class="headerlink" title="RNN 的应用"></a>RNN 的应用</h3><p>RNN 的形式不单单这有这样一种, 他的结构形式很自由. 如果<strong>用于分类问题, 比如说一个人说了一句话, 这句话带的感情色彩是积极的还是消极的. 那我们就可以用只有最后一个时间点输出判断结果的RNN.</strong></p>
<p>又或者这是图片描述 RNN, 我们只需要一个 X 来代替输入的图片, 然后生成对图片描述的一段话.</p>
<p>或者是语言翻译的 RNN, 给出一段英文, 然后再翻译成中文.</p>
<h2 id="什么是-LSTM-循环神经网络"><a href="#什么是-LSTM-循环神经网络" class="headerlink" title="什么是 LSTM 循环神经网络"></a>什么是 LSTM 循环神经网络</h2><h3 id="RNN-的弊端"><a href="#RNN-的弊端" class="headerlink" title="RNN 的弊端"></a>RNN 的弊端</h3><p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/36.png" alt="image-20190822215802638"></p>
<p>之前我们说过, <a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/2-3-RNN/" target="_blank" rel="noopener">RNN</a> 是在有顺序的数据上进行学习的. 为了记住这些数据, RNN 会像人一样产生对先前发生事件的记忆. 不过一般形式的 RNN 就像一个老爷爷, 有时候比较健忘. 为什么会这样呢?</p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/37.png" alt="image-20190822215849509"></p>
<p>想像现在有这样一个 RNN, 他的输入值是一句话: ‘我今天要做红烧排骨, 首先要准备排骨, 然后…., 最后美味的一道菜就出锅了’. 现在请 RNN 来分析, 我今天做的到底是什么菜呢. RNN可能会给出“辣子鸡”这个答案. 由于判断失误, RNN就要开始学习 这个长序列 X 和 ‘红烧排骨’ 的关系 , 而RNN需要的关键信息 ”红烧排骨”却出现在句子开头,</p>
<p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190822215958596.png" alt="image-20190822215958596"></p>
<p>再来看看 RNN是怎样学习的吧. <strong>红烧排骨这个信息原</strong>的记忆要进过长途跋涉才能抵达最后一个时间点. <strong>然后我们得到误差, 而且在 反向传递 得到的误差的时候, 他在每一步都会 乘以一个自己的参数 W</strong>. 如果这个 W 是一个小于1 的数, 比如0.9. 这个0.9 不断乘以误差, 误差传到初始时间点也会是一个接近于零的数, 所以对于初始时刻, 误差相当于就消失了. 我们把这个问题叫做梯度消失或者梯度弥散 Gradient vanishing. 反之如果 W 是一个大于1 的数, 比如1.1 不断累乘, 则到最后变成了无穷大的数, RNN被这无穷大的数撑死了, 这种情况我们叫做梯度爆炸, Gradient exploding. 这就是普通 RNN 没有办法回忆起久远记忆的原因.</p>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190822220343752.png" alt="image-20190822220343752"></p>
<p>LSTM 就是为了解决这个问题而诞生的. LSTM 和普通 RNN 相比, 多出了三个控制器. (<strong>输入控制, 输出控制, 忘记控制</strong>). 现在, LSTM RNN 内部的情况是这样.</p>
<p>他多了一个 控制全局的记忆, 我们用粗线代替. 为了方便理解, 我们把粗线想象成电影或游戏当中的 主线剧情. 而原本的 RNN 体系就是 分线剧情. 三个控制器都是在原始的 RNN 体系上, 我们先看 输入方面 , 如果此时的分线剧情对于剧终结果十分重要, 输入控制就会将这个分线剧情按重要程度 写入主线剧情 进行分析. 再看 忘记方面, 如果此时的分线剧情更改了我们对之前剧情的想法, 那么忘记控制就会将之前的某些主线剧情忘记, 按比例替换成现在的新剧情. 所以 主线剧情的更新就取决于输入 和忘记 控制. 最后的输出方面, 输出控制会基于目前的主线剧情和分线剧情判断要输出的到底是什么.基于这些控制机制, LSTM 就像延缓记忆衰退的良药, 可以带来更好的结果.</p>
<h2 id="什么是自编码-Autoencoder"><a href="#什么是自编码-Autoencoder" class="headerlink" title="什么是自编码 (Autoencoder)"></a>什么是自编码 (Autoencoder)</h2><p>自编码 autoencoder 是一种什么码呢. 他是不是 条形码? 二维码? 打码? 其中的一种呢? NONONONO. 和他们统统没有关系. <strong>自编码是一种神经网络的形式.</strong>如果你一定要把他们扯上关系, 我想也只能这样解释啦.</p>
<h3 id="压缩与解压"><a href="#压缩与解压" class="headerlink" title="压缩与解压"></a>压缩与解压</h3><p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/40.png" alt="image-20190824165233675"></p>
<p>有一个神经网络, 它在做的事情是 <strong>接收一张图片, 然后 给它打码, 最后 再从打码后的图片中还原</strong>. 太抽象啦? 行, 我们再具体点.</p>
<p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190824171847757.png" alt="image-20190824171847757"></p>
<p>假设刚刚那个神经网络是这样, 对应上刚刚的图片,看出图片其实是<strong>经过了压缩,再解压的这一道工序. 当压缩的时候, 原有的图片质量被缩减, 解压时用信息量小却包含了所有关键信息的文件恢复出原本的图片</strong>. 为什么要这样做呢?</p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/42.png" alt="image-20190825164048242"></p>
<p>原来有时神经网络要接受大量的输入信息, 比如输入信息是高清图片时, 输入信息量可能达到上千万, 让神经网络直接从上千万个信息源中学习是一件很吃力的工作. 所以, 何不压缩一下, <strong>提取出原图片中的最具代表性的信息, 缩减输入信息量</strong>, 再<strong>把缩减过后的信息放进神经网络学习</strong>. 这样学习起来就简单轻松了. 所以, 自编码就能在这时发挥作用. 通过<strong>将原数据白色的X 压缩, 解压 成黑色的X</strong>, 然后<strong>通过对比黑白 X</strong> ,<strong>求出预测误差, 进行反向传递, 逐步提升自编码的准确性</strong>. <strong>训练好的自编码中间这一部分就是能总结原数据的精髓</strong>. 可以看出, 从头到尾, 我们只用到了输入数据 X, 并没有用到 X 对应的数据标签, 所以也可以说自编码是一种<strong>非监督学习</strong>. 到了真正使用自编码的时候. 通常只会用到自编码前半部分.</p>
<h3 id="编码器-Encoder"><a href="#编码器-Encoder" class="headerlink" title="编码器 Encoder"></a>编码器 Encoder</h3><p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/43.png" alt="image-20190825164326962"></p>
<p>这 部分也叫作 encoder 编码器. <strong>编码器能得到原数据的精髓</strong>, 然后我们<strong>只需要再创建一个小的神经网络学习这个精髓的数据</strong>,不仅减少了神经网络的负担, 而且同样能达到很好的效果.</p>
<p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190825164413250.png" alt="image-20190825164413250"></p>
<p>这是一个<strong>通过自编码整理出来的数据</strong>, 他能<strong>从原数据中总结出每种类型数据的特征</strong>, 如果把这些特征类型都放在一张二维的图片上, 每种类型都已经被很好的用原数据的精髓区分开来. 如果你了解 PCA 主成分分析, 再提取主要特征时, 自编码和它一样,甚至超越了 PCA. 换句话说, <strong>自编码 可以像 PCA 一样 给特征属性降维</strong>.</p>
<h3 id="解码器-Decoder"><a href="#解码器-Decoder" class="headerlink" title="解码器 Decoder"></a>解码器 Decoder</h3><p>至于解码器 Decoder, 我们也能那它来做点事情. 我们知道, <strong>解码器在训练的时候是要将精髓信息解压成原始信息</strong>, 那么这就<strong>提供了一个解压器的作用, 甚至我们可以认为是一个生成器</strong> (类似于<a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/2-6-GAN/" target="_blank" rel="noopener">GAN</a>). 那做这件事的一种特殊自编码叫做 variational autoencoders, 你能在<a href="http://kvfrans.com/variational-autoencoders-explained/" target="_blank" rel="noopener">这里</a>找到他的具体说明.</p>
<p>有一个例子就是让它能模仿并生成手写数字.</p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/45.png" alt="image-20190825164602297"></p>
<h2 id="AutoEncoder-自编码-非监督学习"><a href="#AutoEncoder-自编码-非监督学习" class="headerlink" title="AutoEncoder (自编码/非监督学习)"></a>AutoEncoder (自编码/非监督学习)</h2><p>神经网络也能进行非监督学习, <strong>只需要训练数据, 不需要标签数据</strong>. 自编码就是这样一种形式. 自编码能<strong>自动分类数据, 而且也能嵌套在半监督学习的上面</strong>, <strong>用少量的有标签样本和大量的无标签样本学习</strong>. 如果对自编码还没有太多概念, 强烈推荐我的这个<a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/2-5-autoencoder/" target="_blank" rel="noopener">动画短片</a>, 让你秒懂自编码.</p>
<p>这次我们还用 MNIST 手写数字数据来压缩再解压图片.</p>
<p><img src="https://morvanzhou.github.io/static/results/torch/4-4-1.gif" alt="AutoEncoder (/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/4-4-3.gif )"></p>
<p>然后用压缩的特征进行非监督分类.</p>
<p><img src="https://morvanzhou.github.io/static/results/torch/4-4-2.gif" alt="AutoEncoder (/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/4-4-2.gif )"></p>
<h3 id="1-训练数据"><a href="#1-训练数据" class="headerlink" title="1. 训练数据"></a>1. 训练数据</h3><p>自编码<strong>只用训练集</strong>就好了, 而且<strong>只需要训练 training data 的 image</strong>, 不用训练 labels.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="comment"># hyper parameter</span></span><br><span class="line">EPOCH = <span class="number">10</span></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line">LR = <span class="number">0.005</span></span><br><span class="line">DOWNLOAD_MNIST = TRUE </span><br><span class="line">N_NEST_IMG = <span class="number">5</span> <span class="comment"># 显示5张图片</span></span><br><span class="line"><span class="comment"># 下载数据</span></span><br><span class="line">train_data = torchvision.dataset.MNIST(</span><br><span class="line">	root=<span class="string">"./mnist"</span>,</span><br><span class="line">  train=<span class="keyword">True</span>, <span class="comment"># training data</span></span><br><span class="line">  transform=torchvision.transforms.ToTensor(), <span class="comment"># convert a PIL.Image or numpy.ndarray to torch.FloatTensor() of shape(C, H, W) and normalize in the range  [0.0, 1.0]</span></span><br><span class="line">  download=DOWNLOAD_MNIST,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 转换</span></span><br><span class="line">train_loader = Data.Dataloader(dataset=train_data, batchsize=BATCH_SIZE,shuffle=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/46.png" alt="image-20190825172404642"></p>
<p>这就是一张我们要训练的手写数字 4.</p>
<h3 id="2-AutoEncoder"><a href="#2-AutoEncoder" class="headerlink" title="2. AutoEncoder"></a>2. AutoEncoder</h3><p>AutoEncoder 形式很简单, 分别是 <code>encoder</code> 和 <code>decoder</code>, <strong>压缩和解压</strong>, 压缩后得到压缩的特征值, 再从压缩的特征值解压成原图片.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AutoEncoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    super(AutoEncoder, self).__init__()</span><br><span class="line">    <span class="comment"># 压缩</span></span><br><span class="line">    self.encoder = nn.Sequential(</span><br><span class="line">    	nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">128</span>)</span><br><span class="line">      nn.Tanh(),</span><br><span class="line">      nn.Linear(<span class="number">128</span>, <span class="number">64</span>),</span><br><span class="line">      nn.Tanh(),</span><br><span class="line">      nn.Linear(<span class="number">64</span>, <span class="number">12</span>),</span><br><span class="line">      nn.Tanh(),</span><br><span class="line">      nn.Linear(<span class="number">12</span>, <span class="number">3</span>), <span class="comment"># 压缩成3个特征，进行 3D图像可视化</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 解压</span></span><br><span class="line">    self.decoder = nn.Sequential(</span><br><span class="line">    	nn.Linear(<span class="number">3</span>, <span class="number">12</span>),</span><br><span class="line">      nn.Tanh(),</span><br><span class="line">      nn.Linear(<span class="number">12</span>, <span class="number">64</span>),</span><br><span class="line">      nn.Tanh(),</span><br><span class="line">      nn.Linear(<span class="number">64</span>, <span class="number">128</span>),</span><br><span class="line">      nn.Tanh(),</span><br><span class="line">      nn.Linear(<span class="number">128</span>, <span class="number">28</span>*<span class="number">28</span>),</span><br><span class="line">      nn.sigmoid(),  <span class="comment"># 激励函数让输出在(0,1)</span></span><br><span class="line">    )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">      encoded = self.encoder(x)</span><br><span class="line">      decoded = self.decoder(encoded)</span><br><span class="line">      <span class="keyword">return</span> encoded, decoded</span><br><span class="line">autoEncoder = AutoEncoder()</span><br></pre></td></tr></table></figure>
<h3 id="3-训练-1"><a href="#3-训练-1" class="headerlink" title="3. 训练"></a>3. 训练</h3><p><strong>训练, 并可视化训练的过程</strong>. 我们可以有效的利用 <code>encoder</code> 和 <code>decoder</code> 来做很多事, 比如这里我们<strong>用 <code>decoder</code> 的信息输出看和原图片的对比</strong>, 还<strong>能用 <code>encoder</code> 来看经过压缩后, 神经网络对原图片的理解.</strong> <code>encoder</code> 能将不同图片数据大概的分离开来. 这样就是一个无监督学习的过程.</p>
<p><img src="https://morvanzhou.github.io/static/results/torch/4-4-1.gif" alt="AutoEncoder (/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/4-4-1.gif )"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(autoEncoder.parameters(), lr=LR)</span><br><span class="line">loss_func = nn.MSELoss()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCH):</span><br><span class="line">  <span class="keyword">for</span> step, (x, b_label) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">    b_x = x.view(<span class="number">-1</span>, <span class="number">28</span>*<span class="number">28</span>) <span class="comment"># batch x, shape (batch, 28*28)</span></span><br><span class="line">    b_y = x.view(<span class="number">-1</span>, <span class="number">28</span>*<span class="number">28</span>) <span class="comment"># batch x, shape (batch, 28*28)</span></span><br><span class="line">    </span><br><span class="line">    encoded, decoded = autoAncoder(b_x)</span><br><span class="line">    loss = loss_func(b_y, decoded) <span class="comment"># mean square error </span></span><br><span class="line">    optimizer.zero_grad() <span class="comment"># clear gradients for this training step</span></span><br><span class="line">    loss.backward() <span class="comment"># backpropagation, compute gradients</span></span><br><span class="line">    optimizer.step() <span class="comment"># apply gradients</span></span><br></pre></td></tr></table></figure>
<p><img src="https://morvanzhou.github.io/static/results/torch/4-4-4.png" alt="AutoEncoder (/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/4-4-4.png )"></p>
<h3 id="画3D图"><a href="#画3D图" class="headerlink" title="画3D图"></a>画3D图</h3><p>如上；3D 的可视化图挺有趣的, 还能挪动观看, 更加直观, 好理解.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 要观看的数据</span></span><br><span class="line">view_data = train_data.data[:<span class="number">200</span>].view(<span class="number">-1</span>, <span class="number">28</span>*<span class="number">28</span>).type(torch.FloatTensor)/<span class="number">255.</span></span><br><span class="line">encoded_data, _ = autoencoder(view_data) <span class="comment"># 提取压缩的特征值</span></span><br><span class="line">fig = plt.figure(<span class="number">2</span>)</span><br><span class="line">ax = Axes3D(fig)    <span class="comment"># 3D 图</span></span><br><span class="line"><span class="comment"># x, y, z 的数据值</span></span><br><span class="line">X = encoded_data.data[:, <span class="number">0</span>].numpy()</span><br><span class="line">Y = encoded_data.data[:, <span class="number">1</span>].numpy()</span><br><span class="line">Z = encoded_data.data[:, <span class="number">2</span>].numpy()</span><br><span class="line">values = train_data.train_labels[:<span class="number">200</span>].numpy()  <span class="comment"># 标签值</span></span><br><span class="line"><span class="keyword">for</span> x, y, z, s <span class="keyword">in</span> zip(X, Y, Z, values):</span><br><span class="line">  c = cm.rainbow(int(<span class="number">255</span>*s/<span class="number">9</span>)) <span class="comment"># 上色</span></span><br><span class="line">  ax.text(x, y, z, s, backgroundcolor=c)  <span class="comment"># 标位子</span></span><br><span class="line">  </span><br><span class="line">ax.set_xlim(X.min(), X.max())</span><br><span class="line">ax.set_ylim(Y.min(), Y.max())</span><br><span class="line">ax.set_zlim(Z.min(), Z.max())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="什么是批标准化-Batch-Normalization"><a href="#什么是批标准化-Batch-Normalization" class="headerlink" title="什么是批标准化 (Batch Normalization)"></a>什么是批标准化 (Batch Normalization)</h2><h3 id="普通数据标准化"><a href="#普通数据标准化" class="headerlink" title="普通数据标准化"></a>普通数据标准化</h3><p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/23.png" alt="image-20190819202503666"></p>
<p>Batch Normalization, 批标准化, 和普通的数据标准化类似, <strong>是将分散的数据统一的一种做法, 也是优化神经网络的一种方法</strong>. 在之前 Normalization 的简介视频中我们一提到, 具有统一规格的数据, 能让机器学习更容易学习到数据之中的规律.</p>
<h3 id="每层都做标准化"><a href="#每层都做标准化" class="headerlink" title="每层都做标准化"></a>每层都做标准化</h3><p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190819202632570.png" alt="image-20190819202632570"></p>
<p><strong>在神经网络中, 数据分布对训练会产生影响</strong>. 比如某个神经元 x 的值为1, 某个 Weights 的初始值为 0.1, 这样后一层神经元计算结果就是 Wx = 0.1; 又或者 x = 20, 这样 Wx 的结果就为 2. 现在还不能看出什么问题, 但是, 当我们加上一层激励函数, 激活这个 Wx 值的时候, 问题就来了. 如果使用 像 tanh 的激励函数, Wx 的激活值就变成了 ~0.1 和 ~1, 接近于 1 的部已经处在了 激励函数的饱和阶段, 也就是如果 x 无论再怎么扩大, tanh 激励函数输出值也还是 接近1. 换句话说<strong>, 神经网络在初始阶段已经不对那些比较大的 x 特征范围 敏感了</strong>. 这样很糟糕, 想象我轻轻拍自己的感觉和重重打自己的感觉居然没什么差别, 这就证明我的感官系统失效了. 当然我们是可以用之前提到的对数据做 normalization 预处理, 使得输入的 x 变化范围不会太大, 让输入值经过激励函数的敏感部分. 但刚刚这个不敏感问题不仅仅发生在神经网络的输入层, 而且在隐藏层中也经常会发生.</p>
<p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190819202806837.png" alt="image-20190819202806837"></p>
<p>只是时候 x 换到了隐藏层当中, 我们能不能对隐藏层的输入结果进行像之前那样的normalization 处理呢? 答案是可以的, 因为大牛们发明了一种技术, 叫做 batch normalization, 正是处理这种情况.</p>
<h3 id="BN-添加位置"><a href="#BN-添加位置" class="headerlink" title="BN 添加位置"></a>BN 添加位置</h3><p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/26.png" alt="image-20190819202842876"></p>
<p>Batch normalization 的 batch 是批数据, 把数据分成小批小批进行 stochastic gradient descent. 而且在每批数据进行前向传递 forward propagation 的时候, 对每一层都进行 normalization 的处理,</p>
<h3 id="BN-效果"><a href="#BN-效果" class="headerlink" title="BN 效果"></a>BN 效果</h3><p>Batch normalization 也可以被看做一个层面. 在一层层的添加神经网络的时候, 我们先有数据 X, 再添加全连接层, 全连接层的计算结果会经过 激励函数 成为下一层的输入, 接着重复之前的操作. <strong>Batch Normalization (BN) 就被添加在每一个全连接和激励函数之间</strong>.</p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/27.png" alt="image-20190819203006098"></p>
<p>之前说过, 计算结果在进入激励函数前的值很重要, 如果我们不单单看一个值, 我们可以说, 计算结果值的分布对于激励函数很重要. 对于数据值大多分布在这个区间的数据, 才能进行更有效的传递. <strong>对比这两个在激活之前的值的分布. 上者没有进行 normalization, 下者进行了 normalization, 这样当然是下者能够更有效地利用 tanh 进行非线性化的过程.</strong></p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/28.png" alt="image-20190819203050813"></p>
<p>没有 normalize 的数据 使用 tanh 激活以后, 激活值大部分都分布到了饱和阶段, 也就是大部分的激活值不是-1, 就是1, 而 normalize 以后, 大部分的激活值在每个分布区间都还有存在. 再将这个激活后的分布传递到下一层神经网络进行后续计算, 每个区间都有分布的这一种对于神经网络就会更加有价值. Batch normalization 不仅仅 normalize 了一下数据, 他还进行了反 normalize 的手续. 为什么要这样呢?</p>
<h3 id="BN-算法"><a href="#BN-算法" class="headerlink" title="BN 算法"></a>BN 算法</h3><p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/29.png" alt="image-20190819203125107"></p>
<p>我们引入一些 batch normalization 的公式. 这三步就是我们在刚刚一直说的 normalization 工序, 但是公式的后面还有一个反向操作, <strong>将 normalize 后的数据再扩展和平移</strong>. 原来这是为了让神经网络自己去学着<strong>使用和修改这个扩展参数 gamma, 和 平移参数 β,</strong> 这样神经网络就能自己慢慢琢磨出前面的 normalization 操作到底有没有起到优化的作用, 如果没有起到作用, 我就使用 gamma 和 belt 来抵消一些 normalization 的操作.</p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/30.png" alt="image-20190819203221565"></p>
<p>最后我们来看看一张神经网络训练到最后, 代表了每层输出值的结果的分布图. 这样我们<strong>就能一眼看出 Batch normalization 的功效啦. 让每一层的值在有效的范围内传递下去</strong>.</p>
<p>BN就是通过<strong>一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布</strong>， 就是每次处理之后，用BN拉回N（0,1），这样就避免了梯度爆炸和梯度消失</p>
<h2 id="Batch-Normalization-批标准化"><a href="#Batch-Normalization-批标准化" class="headerlink" title="Batch Normalization 批标准化"></a>Batch Normalization 批标准化</h2><p>批标准化通俗来说就是对每一层神经网络进行标准化 (normalize) 处理, 我们知道对输入数据进行标准化能让机器学习有效率地学习。 如果把每一层后看成这种接受输入数据的模式, 那我们何不 “批标准化” 所有的层呢</p>
<p>那我们就看看下面的两个动图, 这就是在每层神经网络有无 batch normalization 的区别啦.</p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/32.gif" alt="Batch Normalization æ¹æ åå"></p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/31.gif" alt="Batch Normalization æ¹æ åå"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://fallenk.github.io/2019/08/25/pytorch学习2/" data-id="cjzqqxth900343i8oowbqrkg4" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/pytorch学习/">pytorch学习</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-yaml入门" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/23/yaml入门/" class="article-date">
  <time datetime="2019-08-23T03:11:47.000Z" itemprop="datePublished">2019-08-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/yaml/">yaml</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/23/yaml入门/">yaml入门</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="YAML-语言教程"><a href="#YAML-语言教程" class="headerlink" title="YAML 语言教程"></a>YAML 语言教程</h1><p>配置文件的学习，yaml专门用来写配置文件，本文介绍 YAML 的语法，以 <a href="https://github.com/nodeca/js-yaml" target="_blank" rel="noopener">JS-YAML</a> 的实现为例。你可以去<a href="http://nodeca.github.io/js-yaml/" target="_blank" rel="noopener">在线 Demo</a> 验证下面的例子</p>
        
          <p class="article-more-link">
            <a href="/2019/08/23/yaml入门/#more">继续阅读全文 »</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://fallenk.github.io/2019/08/23/yaml入门/" data-id="cjzqqxti3003i3i8ob2ujb51u" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/配置文件/">配置文件</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-pytorch学习" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/15/pytorch学习/" class="article-date">
  <time datetime="2019-08-15T11:49:17.000Z" itemprop="datePublished">2019-08-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/pytorch/">pytorch</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/15/pytorch学习/">pytorch学习</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Introduction-to-Neural-Network"><a href="#Introduction-to-Neural-Network" class="headerlink" title="Introduction to Neural Network"></a>Introduction to Neural Network</h1>
        
          <p class="article-more-link">
            <a href="/2019/08/15/pytorch学习/#more">继续阅读全文 »</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://fallenk.github.io/2019/08/15/pytorch学习/" data-id="cjzqqxtgq002t3i8oytm174j7" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/pytorch学习/">pytorch学习</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-Golang总结" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/15/Golang总结/" class="article-date">
  <time datetime="2019-08-15T02:00:23.000Z" itemprop="datePublished">2019-08-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/golang学习/">golang学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/15/Golang总结/">Golang总结</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="编译问题"><a href="#编译问题" class="headerlink" title="编译问题"></a>编译问题</h2><p>go build；go install； go getValue</p>
        
          <p class="article-more-link">
            <a href="/2019/08/15/Golang总结/#more">继续阅读全文 »</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://fallenk.github.io/2019/08/15/Golang总结/" data-id="cjzqqxtdj000z3i8otp6ky9r3" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/golang/">golang</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-FLandEncryption" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/07/FLandEncryption/" class="article-date">
  <time datetime="2019-08-07T02:53:08.000Z" itemprop="datePublished">2019-08-07</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Federated-Learning/">Federated Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/07/FLandEncryption/">FLandEncryption</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Papers-and-Code"><a href="#Papers-and-Code" class="headerlink" title="Papers and Code"></a>Papers and Code</h2>
        
          <p class="article-more-link">
            <a href="/2019/08/07/FLandEncryption/#more">继续阅读全文 »</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://fallenk.github.io/2019/08/07/FLandEncryption/" data-id="cjzqqxtdb000r3i8oeohz0kwy" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/encryption/">encryption</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-Feddataset" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/05/Feddataset/" class="article-date">
  <time datetime="2019-08-05T13:18:05.000Z" itemprop="datePublished">2019-08-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Federated-Learning/">Federated Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/05/Feddataset/">Feddataset</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Investigate current larger federated learning projects’ datasets.</p>
<p>motivated by both image classification and language modeling tasks, </p>
<p>2000 models, presented CIFAR10; a large language modeling task</p>
<p>intial study: three model families on two datasets;  </p>
        
          <p class="article-more-link">
            <a href="/2019/08/05/Feddataset/#more">继续阅读全文 »</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://fallenk.github.io/2019/08/05/Feddataset/" data-id="cjzqqxtdg000w3i8ohaaeyp3v" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Federated-Learning/">Federated Learning</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-spring-bootSummary" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/19/spring-bootSummary/" class="article-date">
  <time datetime="2019-06-19T12:06:01.000Z" itemprop="datePublished">2019-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/19/spring-bootSummary/">spring-bootSummary</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Spring-boot"><a href="#Spring-boot" class="headerlink" title="Spring-boot"></a>Spring-boot</h1><ol>
<li><p>快速搭建SSM工程</p></li></ol>
        
          <p class="article-more-link">
            <a href="/2019/06/19/spring-bootSummary/#more">继续阅读全文 »</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://fallenk.github.io/2019/06/19/spring-bootSummary/" data-id="cjzqqxthl003b3i8oygvw0px3" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Java/">Java</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-论文阅读-RandWiredNN" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/13/论文阅读-RandWiredNN/" class="article-date">
  <time datetime="2019-06-13T09:06:02.000Z" itemprop="datePublished">2019-06-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/论文阅读/">论文阅读</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/13/论文阅读-RandWiredNN/">论文阅读-RandWiredNN</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="论文阅读-RandWiredNN"><a href="#论文阅读-RandWiredNN" class="headerlink" title="论文阅读-RandWiredNN"></a>论文阅读-RandWiredNN</h1><p>RandWireNN 基本思想是<strong>研究设计stochastic network generator</strong>，也就是<strong>设计网络构架的机制</strong>，它的关注点<strong>在网络的连接方式上</strong>。论文作者引入了一种网络模型空间的构造方法，即<strong>图论中的random graph，之后用grid search搜索出较好的神经网络子集</strong>，并在ImageNet的1000-class分类任务上进行<strong>验证</strong>。</p>
<p>论文： Exploring Randomly Wired Neural Networks for Image Recognition 【<a href="https://arxiv.org/abs/1904.01569" target="_blank" rel="noopener">pdf</a>】</p>
<p>作者：<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xie%2C+S" target="_blank" rel="noopener">Saining Xie</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kirillov%2C+A" target="_blank" rel="noopener">Alexander Kirillov</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Girshick%2C+R" target="_blank" rel="noopener">Ross Girshick</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He%2C+K" target="_blank" rel="noopener">Kaiming He</a></p>
<p>参考文章：<a href="https://zhuanlan.zhihu.com/p/62837029" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/62837029</a></p>
        
          <p class="article-more-link">
            <a href="/2019/06/13/论文阅读-RandWiredNN/#more">继续阅读全文 »</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://fallenk.github.io/2019/06/13/论文阅读-RandWiredNN/" data-id="cjzqqxtl600593i8o9qwpomj9" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/神经网络搜索/">神经网络搜索</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-NeuralArchitectureSearch" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/03/NeuralArchitectureSearch/" class="article-date">
  <time datetime="2019-06-03T02:25:21.000Z" itemprop="datePublished">2019-06-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/论文阅读/">论文阅读</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/03/NeuralArchitectureSearch/">综述-NeuralArchitectureSearch</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="综述-Nerual-Architecture-Search"><a href="#综述-Nerual-Architecture-Search" class="headerlink" title="综述- Nerual Architecture Search"></a>综述- Nerual Architecture Search</h1><p>神经网络架构搜索，顾名思义，就是让<strong>机器自己去学习如何构架一个神经网络</strong>，得到比人类专家手工设计的网络更好的效果。这个思路是非常一脉相承的，就像机器学习到深度学习，也是<strong>完成了人类专家手工提取特征到由机器自己学习特征</strong>这样的步骤转换。</p>
        
          <p class="article-more-link">
            <a href="/2019/06/03/NeuralArchitectureSearch/#more">继续阅读全文 »</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://fallenk.github.io/2019/06/03/NeuralArchitectureSearch/" data-id="cjzqqxte3001c3i8os7bi99di" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/神经网络搜索/">神经网络搜索</a></li></ul>

    </footer>
  </div>
  
</article>
 


  
    <article id="post-彩票假设paper阅读笔记" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/05/29/彩票假设paper阅读笔记/" class="article-date">
  <time datetime="2019-05-29T13:26:20.000Z" itemprop="datePublished">2019-05-29</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/论文阅读/">论文阅读</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/29/彩票假设paper阅读笔记/">彩票假设paper阅读笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>The Lottery Ticket Hypothesis: Finding sparse, trainable neural networks</p>
<p>抽奖彩票假说: 寻找稀疏，可训练的神经网络</p>
        
          <p class="article-more-link">
            <a href="/2019/05/29/彩票假设paper阅读笔记/#more">继续阅读全文 »</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://fallenk.github.io/2019/05/29/彩票假设paper阅读笔记/" data-id="cjzqqxtk7004i3i8orsv6i8gr" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/paperReading/">paperReading</a></li></ul>

    </footer>
  </div>
  
</article>
 


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/2/">下一页&raquo;</a>
  </nav>
</section>
           
    <aside id="sidebar">
  
    
  <div class="widget-wrap">
     
        <h3 class="follow-title ">Follow me</h3>
     
    <div class="widget follow">
      
              <a class="github" aria-hidden="true" href="https://github.com/fallenk" target="_blank" title="Github"></a>
      
      
            <a class="weibo" aria-hidden="true"  href="http://weibo.com/laohoubin" target="_blank" title="微博"></a>
      
      
              <a class="zhihu" aria-hidden="true"  href="http://www.zhihu.com/people/fallenk" target="_blank" title="知乎"></a>
      
      
            <a class="email" aria-hidden="true"  href="mailto:fallenk_liu@yeah.com" target="_blank" title="邮箱"></a>
      
    </div>
  </div>


  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title categories">分类</h3>
    <div class="widget" id="categories">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Blcokchain/">Blcokchain</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Cryptography-Intro/">Cryptography Intro</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Docker容器/">Docker容器</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Federated-Learning/">Federated Learning</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/LeetCode/">LeetCode</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python学习/">Python学习</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Swagger学习/">Swagger学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/TensorFlow/">TensorFlow</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tensorflow/">Tensorflow</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/golang学习/">golang学习</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/iOS编程/">iOS编程</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/pytorch/">pytorch</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/yaml/">yaml</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/人工智能/">人工智能</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/健康生活/">健康生活</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/博客建设/">博客建设</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/名人杂谈/">名人杂谈</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据学习/">大数据学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/工具学习/">工具学习</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/游戏开发/">游戏开发</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/物联网/">物联网</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/论文阅读/">论文阅读</a><span class="category-list-count">11</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title tagcloud">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/AI/" style="font-size: 14px;">AI</a> <a href="/tags/BigData/" style="font-size: 14px;">BigData</a> <a href="/tags/CV，-论文阅读/" style="font-size: 14px;">CV， 论文阅读</a> <a href="/tags/Cryptography/" style="font-size: 14px;">Cryptography</a> <a href="/tags/Federated-Learning/" style="font-size: 14px;">Federated Learning</a> <a href="/tags/Goa入门/" style="font-size: 14px;">Goa入门</a> <a href="/tags/Java/" style="font-size: 14px;">Java</a> <a href="/tags/Mac终端命令/" style="font-size: 14px;">Mac终端命令</a> <a href="/tags/Swagger/" style="font-size: 14px;">Swagger</a> <a href="/tags/coding/" style="font-size: 25px;">coding</a> <a href="/tags/encryption/" style="font-size: 14px;">encryption</a> <a href="/tags/github/" style="font-size: 14px;">github</a> <a href="/tags/golang/" style="font-size: 14px;">golang</a> <a href="/tags/hexo/" style="font-size: 16.2px;">hexo</a> <a href="/tags/important/" style="font-size: 14px;">important</a> <a href="/tags/intro/" style="font-size: 14px;">intro</a> <a href="/tags/npm/" style="font-size: 14px;">npm</a> <a href="/tags/paper/" style="font-size: 14px;">paper</a> <a href="/tags/paper-reading/" style="font-size: 14px;">paper reading</a> <a href="/tags/paperReading/" style="font-size: 16.2px;">paperReading</a> <a href="/tags/python转换/" style="font-size: 14px;">python转换</a> <a href="/tags/pytorch学习/" style="font-size: 16.2px;">pytorch学习</a> <a href="/tags/云端学习笔记/" style="font-size: 14px;">云端学习笔记</a> <a href="/tags/人工智能/" style="font-size: 14px;">人工智能</a> <a href="/tags/代码/" style="font-size: 16.2px;">代码</a> <a href="/tags/入门/" style="font-size: 22.8px;">入门</a> <a href="/tags/入门指引/" style="font-size: 14px;">入门指引</a> <a href="/tags/刷题/" style="font-size: 16.2px;">刷题</a> <a href="/tags/双指针/" style="font-size: 14px;">双指针</a> <a href="/tags/基础/" style="font-size: 16.2px;">基础</a> <a href="/tags/大数据，spark/" style="font-size: 14px;">大数据，spark</a> <a href="/tags/排序/" style="font-size: 14px;">排序</a> <a href="/tags/提升自我/" style="font-size: 14px;">提升自我</a> <a href="/tags/智能架构/" style="font-size: 14px;">智能架构</a> <a href="/tags/机器学习/" style="font-size: 14px;">机器学习</a> <a href="/tags/深度学习/" style="font-size: 14px;">深度学习</a> <a href="/tags/神经网络搜索/" style="font-size: 16.2px;">神经网络搜索</a> <a href="/tags/禅/" style="font-size: 14px;">禅</a> <a href="/tags/编程/" style="font-size: 25px;">编程</a> <a href="/tags/编程技巧，/" style="font-size: 14px;">编程技巧，</a> <a href="/tags/考试/" style="font-size: 22.8px;">考试</a> <a href="/tags/联合学习/" style="font-size: 18.4px;">联合学习</a> <a href="/tags/自我修炼/" style="font-size: 16.2px;">自我修炼</a> <a href="/tags/论文阅读/" style="font-size: 20.6px;">论文阅读</a> <a href="/tags/调养，生活/" style="font-size: 14px;">调养，生活</a> <a href="/tags/财务自由/" style="font-size: 14px;">财务自由</a> <a href="/tags/配置文件/" style="font-size: 14px;">配置文件</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title recent-posts">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/08/25/pytorch学习2/">pytorch学习2</a>
          </li>
        
          <li>
            <a href="/2019/08/23/yaml入门/">yaml入门</a>
          </li>
        
          <li>
            <a href="/2019/08/15/pytorch学习/">pytorch学习</a>
          </li>
        
          <li>
            <a href="/2019/08/15/Golang总结/">Golang总结</a>
          </li>
        
          <li>
            <a href="/2019/08/07/FLandEncryption/">FLandEncryption</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title archive">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">April 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a><span class="archive-list-count">7</span></li></ul>
    </div>
  </div>


  
    
<div class="widget-wrap">
    <h3 class="widget-title">Links</h3>
    <div class="widget">
        <ul>
            
            <li>
                <a href="http://blog.giscafer.com">giscafer&#39;s blog</a>
            </li>
            
            <li>
                <a href="http://www.gis520.com">GIS520社区</a>
            </li>
            
        </ul>
    </div>
</div>

  
    <!--微信公众号二维码-->

  <div class="widget-wrap">
    <h3 class="follow-title ">WeChat</h3>
    <div class="widget wechat-widget">
        <img src="http://blog.giscafer.com/static/images/qrcode_giscafer.jpg" alt="扫码关注" width="250"/>
    </div>
  </div>


  
</aside>

      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-left">
      &copy; 2014 - 2019 Fallenk Liu&nbsp;|&nbsp;
      主题 <a href="https://github.com/giscafer/hexo-theme-cafe/" target="_blank">Cafe</a>
    </div>
     <div id="footer-right">
      联系方式&nbsp;|&nbsp;fallenk_liu@yeah.com
    </div>
  </div>
</footer>
 <script src="/jquery/jquery.min.js"></script>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    <img class="back-to-top-btn" src="/images/fly-to-top.png"/>
<script>
// Elevator script included on the page, already.
window.onload = function() {
  var elevator = new Elevator({
    selector:'.back-to-top-btn',
    element: document.querySelector('.back-to-top-btn'),
    duration: 1000 // milliseconds
  });
}
</script>
      

  
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "e2fb4051c49842688ce669e634bc983f",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
    

  







<!-- author:forvoid begin -->
<!-- author:forvoid begin -->

<!-- author:forvoid end -->

<!-- author:forvoid end -->


  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      })
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      })
    </script>
    <script type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


 <script src="/js/is.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/elevator.js"></script>
  </div>
</body>
</html>