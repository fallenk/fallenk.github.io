<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Pytorch快速入门 | Fallenk&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="简介原文连接： https://zhuanlan.zhihu.com/p/26854386本文为快速学习入门Pytorch。学习之道：明白目的；pytorchd的结构框架；pytorch的使用。">
<meta name="keywords" content="编程,入门">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch快速入门">
<meta property="og:url" content="https://fallenk.github.io/2018/12/05/Pytorch快速入门/index.html">
<meta property="og:site_name" content="Fallenk&#39;s Blog">
<meta property="og:description" content="简介原文连接： https://zhuanlan.zhihu.com/p/26854386本文为快速学习入门Pytorch。学习之道：明白目的；pytorchd的结构框架；pytorch的使用。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://fallenk.github.io/2018/12/05/Pytorch快速入门/torch1.png">
<meta property="og:image" content="https://fallenk.github.io/2018/12/05/Pytorch快速入门/torch2.png">
<meta property="og:image" content="https://fallenk.github.io/2018/12/05/Pytorch快速入门/torch3.png">
<meta property="og:image" content="https://fallenk.github.io/2018/12/05/Pytorch快速入门/torch4.png">
<meta property="og:image" content="https://fallenk.github.io/2018/12/05/Pytorch快速入门/torch5.png">
<meta property="og:image" content="https://fallenk.github.io/2018/12/05/Pytorch快速入门/torch6.png">
<meta property="og:image" content="https://fallenk.github.io/2018/12/05/Pytorch快速入门/torch7.png">
<meta property="og:image" content="https://fallenk.github.io/2018/12/05/Pytorch快速入门/torch8.png">
<meta property="og:image" content="https://fallenk.github.io/2018/12/05/Pytorch快速入门/torch9.png">
<meta property="og:image" content="https://fallenk.github.io/2018/12/05/Pytorch快速入门/torch10.png">
<meta property="og:image" content="https://fallenk.github.io/2018/12/05/Pytorch快速入门/torch11.png">
<meta property="og:image" content="https://fallenk.github.io/2018/12/05/Pytorch快速入门/torch12.png">
<meta property="og:image" content="https://fallenk.github.io/2018/12/05/Pytorch快速入门/torch13.png">
<meta property="og:image" content="https://fallenk.github.io/2018/12/05/Pytorch快速入门/torch14.png">
<meta property="og:image" content="https://fallenk.github.io/2018/12/05/Pytorch快速入门/torch15.png">
<meta property="og:image" content="https://fallenk.github.io/2018/12/05/Pytorch快速入门/torch16.png">
<meta property="og:updated_time" content="2018-12-28T06:09:18.461Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Pytorch快速入门">
<meta name="twitter:description" content="简介原文连接： https://zhuanlan.zhihu.com/p/26854386本文为快速学习入门Pytorch。学习之道：明白目的；pytorchd的结构框架；pytorch的使用。">
<meta name="twitter:image" content="https://fallenk.github.io/2018/12/05/Pytorch快速入门/torch1.png">
  
    <link rel="alternate" href="/atom.xml" title="Fallenk&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://fallenk.github.io"></form>
      </div>
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">首页</a>
        
          <a class="main-nav-link" href="/archives">归档</a>
        
          <a class="main-nav-link" href="/about">关于</a>
        
      </nav>
      
    </div>
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Fallenk&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">经历，心得，笔记，目标</a>
        </h2>
      
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Pytorch快速入门" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/05/Pytorch快速入门/" class="article-date">
  <time datetime="2018-12-05T12:17:44.000Z" itemprop="datePublished">2018-12-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Python学习/">Python学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Pytorch快速入门
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>原文连接： <a href="https://zhuanlan.zhihu.com/p/26854386" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26854386</a><br>本文为快速学习入门Pytorch。<br>学习之道：明白目的；pytorchd的结构框架；pytorch的使用。<br><a id="more"></a></p>
<ul>
<li>目的：An open source deep learning platform that provides a seamless path from research prototyping to production deployment.</li>
<li>结构：平台，对象，操作，框架</li>
<li>使用：引用，使用</li>
</ul>
<p>大纲：总体框架；输入是什么？ 怎么执行？ 输出是什么？</p>
<ol>
<li>解释论文 the chain structure;  =&gt; 源代码hook</li>
<li>解释论文 send the chain structure; =&gt; worker </li>
<li>解释论文 MPC tensor =&gt; </li>
</ol>
<p>dir() 返回参数的属性、方法列表</p>
<ul>
<li>self在定义时需要定义，但是在调用时会自动传入。</li>
<li>self的名字并不是规定死的，但是最好还是按照约定是用self</li>
<li>self总是指调用时的类的实例。</li>
</ul>
<p>梯度就是函数在某一点的梯度是这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。<br>正向传播: input <em> weights = out; out 与 target对比得到loss函数(误差)；<br>反向传播: 对误差更新权值，重新计算输出；即 计算误差对梯度的求导grad, w = w -学习率</em>grad, 再算输出</p>
<h1 id="10分钟快速入门PyTorch-0"><a href="#10分钟快速入门PyTorch-0" class="headerlink" title="10分钟快速入门PyTorch (0)"></a>10分钟快速入门PyTorch (0)</h1><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p>根据URL： <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener">https://pytorch.org/get-started/locally/</a><br>安装： <code>pip3 install torch torchvision</code></p>
<h2 id="pytorch基础"><a href="#pytorch基础" class="headerlink" title="pytorch基础"></a>pytorch基础</h2><p>介绍一下pytorch处理的对象以及操作。</p>
<h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p>首先介绍里面最基本的操作对象，tensor。<br>使用<code>jupytert notebook</code>:</p>
<p><img src="/2018/12/05/Pytorch快速入门/torch1.png" alt=""></p>
<p>tensor就是张量的英文，表示<strong>多维的矩阵</strong>，比如一维就是向量，二维就是一般的矩阵等等，<strong>pytorch里面处理的单位就是一个一个的tensor</strong>.可以显示的得到其大小</p>
<p><img src="/2018/12/05/Pytorch快速入门/torch2.png" alt=""></p>
<p>这个和numpy很相似，同时tensor和numpy.array之间也可以相互转换.<br><img src="/2018/12/05/Pytorch快速入门/torch3.png" alt=""><br>tensor的运算也很简单，一般的四则运算都是支持的</p>
<h2 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h2><p>pytorch和numpy不一样的地方就来了，就是其提供了<strong>自动求导功能</strong>，也就是可以<strong>自动给你你要的参数的梯度</strong>，这个操作由另外一个基本元素提供，Variable<br><img src="/2018/12/05/Pytorch快速入门/torch4.png" alt=""><br>本质上Variable和Tensor没有区别，不过<strong>Variable会放入一个计算图，然后进行前向传播，反向传播以及自动求导</strong>.<br>一个Variable里面包含着三个属性，data，grad和creator，其中<strong>creator表示得到这个Variabel的操作</strong>，比如乘法或者加法等等，<strong>grad表示方向传播的梯度</strong>，<strong>data表示取出这个Variabel里面的数据</strong><br><img src="/2018/12/05/Pytorch快速入门/torch5.png" alt=""><br>这就是一个简单的计算图的例子</p>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>前面讲了两个操作对象，最后讲一下pytorch里面的<strong>模型建立</strong>，<strong>模型的建立主要依赖于torch.nn</strong>，torch.nn包含这个所有神经网络的层的结构。<br><img src="/2018/12/05/Pytorch快速入门/torch6.png" alt=""></p>
<p>这就是构建所有神经网络的模板，不管你想<strong>构建卷积神经网络还是循环神经网络或者是生成对抗网络都依赖于这个结构</strong>.<br><a href="https://github.com/L1aoXingyu/pytorch-beginner" target="_blank" rel="noopener">代码网址</a></p>
<h1 id="10分钟快速入门PyTorch-1"><a href="#10分钟快速入门PyTorch-1" class="headerlink" title="10分钟快速入门PyTorch (1)"></a>10分钟快速入门PyTorch (1)</h1><p>以上基本的介绍了pytorch里面的<strong>操作单元，Tensor，以及计算图中的操作单位Variable</strong>，相信大家都已经熟悉了，下面这一部分我们就从两个最基本的机器学习，<strong>线性回归以及logistic回归</strong>来开始建立我们的计算图进行运算。</p>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>对于线性回归，相信大家都很熟悉了，各种机器学习的书第一个要讲的内容必定有线性回归，这里简单的回顾一下什么是简单的一元线性回归。即<strong>给出一系列的点，找一条直线，使得这条直线与这些点的距离之和最小</strong>。<br><img src="/2018/12/05/Pytorch快速入门/torch7.png" alt=""><br>上面这张图就简单地描绘出了线性回归的基本原理，下面我们重点讲讲如何用pytorch写一个简单的线性回归。</p>
<h2 id="code"><a href="#code" class="headerlink" title="code"></a>code</h2><h3 id="1-data"><a href="#1-data" class="headerlink" title="1. data"></a>1. data</h3><p>首先我们需要给出一系列的点作为线性回归的数据，使用numpy来存储这些点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x_train = np.array([[<span class="number">3.3</span>], [<span class="number">4.4</span>], [<span class="number">5.5</span>], [<span class="number">6.71</span>], [<span class="number">6.93</span>], [<span class="number">4.168</span>],</span><br><span class="line">                    [<span class="number">9.779</span>], [<span class="number">6.182</span>], [<span class="number">7.59</span>], [<span class="number">2.167</span>], [<span class="number">7.042</span>],</span><br><span class="line">                    [<span class="number">10.791</span>], [<span class="number">5.313</span>], [<span class="number">7.997</span>], [<span class="number">3.1</span>]], dtype=np.float32)</span><br><span class="line"></span><br><span class="line">y_train = np.array([[<span class="number">1.7</span>], [<span class="number">2.76</span>], [<span class="number">2.09</span>], [<span class="number">3.19</span>], [<span class="number">1.694</span>], [<span class="number">1.573</span>],</span><br><span class="line">                    [<span class="number">3.366</span>], [<span class="number">2.596</span>], [<span class="number">2.53</span>], [<span class="number">1.221</span>], [<span class="number">2.827</span>],</span><br><span class="line">                    [<span class="number">3.465</span>], [<span class="number">1.65</span>], [<span class="number">2.904</span>], [<span class="number">1.3</span>]], dtype=np.float32)</span><br></pre></td></tr></table></figure>
<p><img src="/2018/12/05/Pytorch快速入门/torch8.png" alt=""></p>
<p>还记得pytorch里面的基本处理单元吗？Tensor，我们需要将numpy转换成Tensor，如果你还记得上一节的内容，那么你就一定记得这个函数，<code>torch.from_numpy()</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_train = torch.from_numpy(x_train)</span><br><span class="line">y_train = torch.from_numpy(y_train)</span><br></pre></td></tr></table></figure>
<p>这样我们的数据就转换成了Tensor。</p>
<h3 id="2-model"><a href="#2-model" class="headerlink" title="2. model"></a>2. model</h3><p>上一节讲了基本的模型框架，按照这个框架就可以写出一个线性回归模型了.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">class LinearRegression(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(LinearRegression, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(1, 1)  # input and output is 1 dimension</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        out = self.linear(x)</span><br><span class="line">        return out</span><br><span class="line">model = LinearRegression()</span><br></pre></td></tr></table></figure>
<p>这里的nn.Linear表示的是 y=w*x+b，里面的两个参数都是1，表示的是x是1维，y也是1维。当然这里是可以根据你想要的输入输出维度来更改的，之前使用的别的框架的同学应该很熟悉。</p>
<p>然后需要定义loss和optimizer，就是误差和优化函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=1e-4)</span><br></pre></td></tr></table></figure>
<p>这里使用的是最小二乘loss，之后我们做分类问题更多的使用的是<code>cross entropy loss</code>，交叉熵。优化函数使用的是随机梯度下降，注意需要将model的参数<code>model.parameters()</code>传进去让这个函数知道他要优化的参数是那些。</p>
<h3 id="3-train"><a href="#3-train" class="headerlink" title="3. train"></a>3. train</h3><p>接着开始训练</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = 1000</span><br><span class="line">for epoch in range(num_epochs):</span><br><span class="line">    inputs = Variable(x_train)</span><br><span class="line">    target = Variable(y_train)</span><br><span class="line"></span><br><span class="line">    # forward</span><br><span class="line">    out = model(inputs) # 前向传播</span><br><span class="line">    loss = criterion(out, target) # 计算loss</span><br><span class="line">    # backward</span><br><span class="line">    optimizer.zero_grad() # 梯度归零</span><br><span class="line">    loss.backward() # 方向传播</span><br><span class="line">    optimizer.step() # 更新参数</span><br><span class="line"></span><br><span class="line">    if (epoch+1) % 20 == 0:</span><br><span class="line">        print(&apos;Epoch[&#123;&#125;/&#123;&#125;], loss: &#123;:.6f&#125;&apos;.format(epoch+1,</span><br><span class="line">                                                  num_epochs,                                             loss.data[0]))</span><br></pre></td></tr></table></figure>
<p>第一个循环表示每个epoch，接着开始前向传播，然后计算loss，然后反向传播，接着优化参数，特别注意的是在每次反向传播的时候需要将参数的梯度归零，即<code>optimzier.zero_grad()</code></p>
<h3 id="4-validation"><a href="#4-validation" class="headerlink" title="4. validation"></a>4. validation</h3><p>训练完成之后我们就可以开始测试模型了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.eval()</span><br><span class="line">predict = model(Variable(x_train))</span><br><span class="line">predict = predict.data.numpy()</span><br></pre></td></tr></table></figure>
<p>特别注意的是需要用 model.eval()，让model变成测试模式，这主要是对dropout和batch normalization的操作在训练和测试的时候是不一样的.<br>最后可以得到这个结果<br><img src="/2018/12/05/Pytorch快速入门/torch9.png" alt=""><br>以及loss的结果<br><img src="/2018/12/05/Pytorch快速入门/torch10.png" alt=""><br>ok，在这篇文章中我们使用pytorch实现了简单的线性回归模型，掌握了pytorch的一些基本操作，下一节我们将使用logistic回归对MNIST手写字体数据集做识别。</p>
<h1 id="10分钟快速入门PyTorch-2"><a href="#10分钟快速入门PyTorch-2" class="headerlink" title="10分钟快速入门PyTorch (2)"></a>10分钟快速入门PyTorch (2)</h1><p>上一节介绍了简单的线性回归，如何在pytorch里面用最小二乘来拟合一些离散的点，这一节我们将开始简单的logistic回归，介绍图像分类问题，使用的数据是手写字体数据集MNIST。</p>
<h2 id="logistic回归"><a href="#logistic回归" class="headerlink" title="logistic回归"></a>logistic回归</h2><p>logistic回归简单来说和线性回归是一样的，要做的运算同样是 <code>y = w * x + b</code>，logistic回归简单的是<strong>做二分类问题</strong>，使用<strong>sigmoid函数将所有的正数和负数都变成0-1之间的数</strong>，这样就可以用这个数来确定到底属于哪一类，可以简单的认为概率大于0.5即为第二类，小于0.5为第一类。<br><img src="/2018/12/05/Pytorch快速入门/torch11.png" alt=""><br>这就是sigmoid的图形<br><img src="/2018/12/05/Pytorch快速入门/torch12.png" alt=""><br>而我们这里要做的是多分类问题，对于每一个数据，我们输出的维数是分类的总数，比如10分类，我们输出的就是一个10维的向量，然后我们使用另外一个激活函数，softmax<br><img src="/2018/12/05/Pytorch快速入门/torch13.png" alt=""><br>这就是softmax函数作用的机制，其实简单的理解就是<strong>确定这10个数每个数对应的概率有多大，因为这10个数有正有负，所以通过指数函数将他们全部变成正数，然后求和，然后这10个数每个数都除以这个和，这样就得到了每个类别的概率</strong>。</p>
<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><h3 id="data"><a href="#data" class="headerlink" title="data"></a>data</h3><p>首先导入torch里面专门做图形处理的一个库，torchvision，根据官方安装指南，你在安装pytorch的时候torchvision也会安装。</p>
<p>我们需要使用的是<code>torchvision.transforms</code>和<code>torchvision.datasets</code>以及<code>torch.utils.data.DataLoader</code></p>
<p>首先DataLoader是导入图片的操作，里面有一些参数，比如batch_size和shuffle等，默认load进去的图片类型是PIL.Image.open的类型，如果你不知道PIL，简单来说就是一种读取图片的库.</p>
<p>torchvision.transforms里面的操作是对导入的图片做处理，比如可以随机取(50, 50)这样的窗框大小，或者随机翻转，或者去中间的(50, 50)的窗框大小部分等等，但是里面必须要用的是transforms.ToTensor()，这可以将PIL的图片类型转换成tensor，这样pytorch才可以对其做处理.</p>
<p>torchvision.datasets里面有很多数据类型，里面有官网处理好的数据，比如我们要使用的MNIST数据集，可以通过torchvision.datasets.MNIST()来得到，还有一个常使用的是torchvision.datasets.ImageFolder()，这个可以让我们按文件夹来取图片，和keras里面的flow_from_directory()类似，具体的可以去看看官方文档的介绍。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 定义超参数</span><br><span class="line">batch_size = 32</span><br><span class="line">learning_rate = 1e-3</span><br><span class="line">num_epoches = 100</span><br><span class="line"></span><br><span class="line"># 下载训练集 MNIST 手写数字训练集</span><br><span class="line">train_dataset = datasets.MNIST(root=&apos;./data&apos;, train=True,</span><br><span class="line">                               transform=transforms.ToTensor(),</span><br><span class="line">                               download=True)</span><br><span class="line"></span><br><span class="line">test_dataset = datasets.MNIST(root=&apos;./data&apos;, train=False,</span><br><span class="line">                              transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)</span><br></pre></td></tr></table></figure>
<h2 id="model"><a href="#model" class="headerlink" title="model"></a>model</h2><p>之前讲过模型定义的框架，废话不多说，直接上代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class Logstic_Regression(nn.Module):</span><br><span class="line">    def __init__(self, in_dim, n_class):</span><br><span class="line">        super(Logstic_Regression, self).__init__()</span><br><span class="line">        self.logstic = nn.Linear(in_dim, n_class)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        out = self.logstic(x)</span><br><span class="line">        return out</span><br><span class="line"></span><br><span class="line">model = Logstic_Regression(28*28, 10)  # 图片大小是28x28</span><br></pre></td></tr></table></figure>
<p>我们需要向这个模型传入参数，第一个参数定义为数据的维度，第二维数是我们分类的数目。</p>
<p>接着我们可以在gpu上跑模型，怎么做呢？<br>首先可以判断一下你是否能在gpu上跑<br><code>torh.cuda.is_available()</code><br>如果返回True就说明有gpu支持</p>
<p>接着你只需要一个简单的命令就可以了<br><code>model = model.cuda()</code><br>或者<br><code>model.cuda()</code><br>都可以</p>
<p>然后需要定义loss和optimizer<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=learning_rate)</span><br></pre></td></tr></table></figure></p>
<p>这里我们使用的loss是交叉熵，是一种处理分类问题的loss，optimizer我们还是使用随机梯度下降</p>
<h2 id="train"><a href="#train" class="headerlink" title="train"></a>train</h2><p>接着就可以开始训练了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">for epoch in range(num_epoches):</span><br><span class="line">    print(&apos;epoch &#123;&#125;&apos;.format(epoch+1))</span><br><span class="line">    print(&apos;*&apos;*10)</span><br><span class="line">    running_loss = 0.0</span><br><span class="line">    running_acc = 0.0</span><br><span class="line">    for i, data in enumerate(train_loader, 1):</span><br><span class="line">        img, label = data</span><br><span class="line">        img = img.view(img.size(0), -1)  # 将图片展开成 28x28</span><br><span class="line">        if use_gpu:</span><br><span class="line">            img = Variable(img).cuda()</span><br><span class="line">            label = Variable(label).cuda()</span><br><span class="line">        else:</span><br><span class="line">            img = Variable(img)</span><br><span class="line">            label = Variable(label)</span><br><span class="line">        # 向前传播</span><br><span class="line">        out = model(img)</span><br><span class="line">        loss = criterion(out, label)</span><br><span class="line">        running_loss += loss.data[0] * label.size(0)</span><br><span class="line">        _, pred = torch.max(out, 1)</span><br><span class="line">        num_correct = (pred == label).sum()</span><br><span class="line">        running_acc += num_correct.data[0]</span><br><span class="line">        # 向后传播</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure>
<p>注意我们如果将模型放到了gpu上，相应的我们的Variable也要放到gpu上，也很简单</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">img = Variable(img).cuda()</span><br><span class="line">label = Variable(label).cuda()</span><br></pre></td></tr></table></figure>
<p>然后可以测试模型，过程与训练类似，只是注意要将模型改成测试模式<br><code>model.eval()</code><br>这是跑完100 epoch的结果</p>
<p><img src="/2018/12/05/Pytorch快速入门/torch14.png" alt=""><br>具体的结果多久打印一次，如何打印可以自己在for循环里面去设计</p>
<p>这一部分我们就讲解了如何用logistic回归去做一个简单的图片分类问题，知道了如何在gpu上跑模型，下一节我们将介绍如何写简单的卷积神经网络，不了解卷积网络的同学可以先去我的专栏看看之前卷积网络的介绍。</p>
<h1 id="10分钟快速入门PyTorch-3"><a href="#10分钟快速入门PyTorch-3" class="headerlink" title="10分钟快速入门PyTorch (3)"></a>10分钟快速入门PyTorch (3)</h1><p>前面两节讲了最基本的机器学习算法，线性回归和logistic回归，这一节将介绍传统机器学习里面最后一个算法-神经网络，这也是深度学习的基石，所谓的深度学习，也可以理解为很深层的神经网络。说起这里，有一个小段子，神经网络曾经被打入了冷宫，因为SVM派的崛起，SVM不了解的同学可以去google一下，中文叫支持向量机，因为其有着完备的数学解释，并且之前神经网络运算复杂等问题，导致神经网络停步不前，这个时候任何以神经网络为题目的论文都发不出去，反向传播算法的鼻祖hinton为了解决这个问题，于是就想到了用深度学习为题目。</p>
<p>段子说完，接下来开始我们的简单神经网络。</p>
<h2 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h2><p>其实简单的神经网络说起来很简单，先放图为敬</p>
<p><img src="/2018/12/05/Pytorch快速入门/torch15.png" alt=""><br>通过图片就能很简答的看出来，其实<strong>每一层网络所做的就是 <code>y=W*X+b</code></strong>，只不过W的维数由X和输出维数决定，比如X是10维向量，想要输出的维数，也就是中间层的神经元个数为20，那么W的维数就是20x10，b的维数就是20x1，这样输出的y的维数就为20。</p>
<p>中间层的维数可以自己设计，而最后一层输出的维数就是你的分类数目，比如我们等会儿要做的MNIST数据集是10个数字的分类，那么最后输出层的神经元就为10。</p>
<h2 id="Code-1"><a href="#Code-1" class="headerlink" title="Code"></a>Code</h2><p>有了前面两节的经验，这一节的代码就很简单了，数据的导入和之前一样<br>定义模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Neuralnetwork</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_dim, n_hidden_1, n_hidden_2, out_dim)</span>:</span></span><br><span class="line">        super(Neuralnetwork, self).__init__()</span><br><span class="line">        self.layer1 = nn.Linear(in_dim, n_hidden_1)</span><br><span class="line">        self.layer2 = nn.Linear(n_hidden_1, n_hidden_2)</span><br><span class="line">        self.layer3 = nn.Linear(n_hidden_2, out_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = Neuralnetwork(<span class="number">28</span>*<span class="number">28</span>, <span class="number">300</span>, <span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model = model.cuda()</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=learning_rate)</span><br></pre></td></tr></table></figure>
<p>上面定义了三层神经网络，输入是28x28，因为图片大小是28x28，中间两个隐藏层大小分别是300和100，最后是个10分类问题，所以输出层为10.<br>训练过程与之前完全一样<br>这是50次之后的输出结果，可以和上一节logistic回归比较一下.</p>
<p>可以发现准确率大大提高，其实logistic回归可以看成简单的一层网络，从这里我们就可以看出为什么多层网络比单层网络的效果要好，这也是为什么深度学习要叫深度的原因。</p>
<p><img src="/2018/12/05/Pytorch快速入门/torch16.png" alt=""><br>下一节我们将正式进入到深度学习，第一个模型将是计算机视觉领域的王牌模型，卷积神经网络。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://fallenk.github.io/2018/12/05/Pytorch快速入门/" data-id="cjzgps69c002swz8ou2pvkhvs" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/入门/">入门</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/编程/">编程</a></li></ul>

    </footer>
  </div>
  
    
 <script src="/jquery/jquery.min.js"></script>
  <div id="random_posts">
    <h2>推荐文章</h2>
    <div class="random_posts_ul">
      <script>
          var random_count =4
          var site = {BASE_URI:'/'};
          function load_random_posts(obj) {
              var arr=site.posts;
              if (!obj) return;
              // var count = $(obj).attr('data-count') || 6;
              for (var i, tmp, n = arr.length; n; i = Math.floor(Math.random() * n), tmp = arr[--n], arr[n] = arr[i], arr[i] = tmp);
              arr = arr.slice(0, random_count);
              var html = '<ul>';
            
              for(var j=0;j<arr.length;j++){
                var item=arr[j];
                html += '<li><strong>' + 
                item.date + ':&nbsp;&nbsp;<a href="' + (site.BASE_URI+item.uri) + '">' + 
                (item.title || item.uri) + '</a></strong>';
                if(item.excerpt){
                  html +='<div class="post-excerpt">'+item.excerpt+'</div>';
                }
                html +='</li>';
                
              }
              $(obj).html(html + '</ul>');
          }
          $('.random_posts_ul').each(function () {
              var c = this;
              if (!site.posts || !site.posts.length){
                  $.getJSON(site.BASE_URI + 'js/posts.js',function(json){site.posts = json;load_random_posts(c)});
              } 
               else{
                load_random_posts(c);
              }
          });
      </script>
    </div>
  </div>

    
<nav id="article-nav">
  
    <a href="/2018/12/07/A-generic-framwork-for-privacy-preserving-deep-learning/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">上一篇</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2018/11/30/goa快速入门/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">下一篇</strong>
      <div class="article-nav-title">goa快速入门</div>
    </a>
  
</nav>

  
</article>
 
     
  <div class="comments" id="comments">
    
     
       
      <div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div>
    
       
      
      
  </div>
 
  

</section>
           
    <aside id="sidebar">
  
    

  
    
    <div class="widget-wrap">
    
      <div class="widget" id="toc-widget-fixed">
      
        <strong class="toc-title">文章目录</strong>
        <div class="toc-widget-list">
              <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#简介"><span class="toc-number">1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10分钟快速入门PyTorch-0"><span class="toc-number">2.</span> <span class="toc-text">10分钟快速入门PyTorch (0)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#环境配置"><span class="toc-number">2.1.</span> <span class="toc-text">环境配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch基础"><span class="toc-number">2.2.</span> <span class="toc-text">pytorch基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor"><span class="toc-number">2.3.</span> <span class="toc-text">Tensor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Variable"><span class="toc-number">2.4.</span> <span class="toc-text">Variable</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络"><span class="toc-number">2.5.</span> <span class="toc-text">神经网络</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10分钟快速入门PyTorch-1"><span class="toc-number">3.</span> <span class="toc-text">10分钟快速入门PyTorch (1)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#线性回归"><span class="toc-number">3.1.</span> <span class="toc-text">线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#code"><span class="toc-number">3.2.</span> <span class="toc-text">code</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-data"><span class="toc-number">3.2.1.</span> <span class="toc-text">1. data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-model"><span class="toc-number">3.2.2.</span> <span class="toc-text">2. model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-train"><span class="toc-number">3.2.3.</span> <span class="toc-text">3. train</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-validation"><span class="toc-number">3.2.4.</span> <span class="toc-text">4. validation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10分钟快速入门PyTorch-2"><span class="toc-number">4.</span> <span class="toc-text">10分钟快速入门PyTorch (2)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#logistic回归"><span class="toc-number">4.1.</span> <span class="toc-text">logistic回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Code"><span class="toc-number">4.2.</span> <span class="toc-text">Code</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#data"><span class="toc-number">4.2.1.</span> <span class="toc-text">data</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#model"><span class="toc-number">4.3.</span> <span class="toc-text">model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#train"><span class="toc-number">4.4.</span> <span class="toc-text">train</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10分钟快速入门PyTorch-3"><span class="toc-number">5.</span> <span class="toc-text">10分钟快速入门PyTorch (3)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Neural-Network"><span class="toc-number">5.1.</span> <span class="toc-text">Neural Network</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Code-1"><span class="toc-number">5.2.</span> <span class="toc-text">Code</span></a></li></ol></li></ol>
          </div>
      </div>
    </div>

  
    

  
    
  
    
  
    

  
    
  
    <!--微信公众号二维码-->


  
</aside>

      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-left">
      &copy; 2014 - 2019 Fallenk Liu&nbsp;|&nbsp;
      主题 <a href="https://github.com/giscafer/hexo-theme-cafe/" target="_blank">Cafe</a>
    </div>
     <div id="footer-right">
      联系方式&nbsp;|&nbsp;fallenk_liu@yeah.com
    </div>
  </div>
</footer>
 <script src="/jquery/jquery.min.js"></script>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    <img class="back-to-top-btn" src="/images/fly-to-top.png"/>
<script>
// Elevator script included on the page, already.
window.onload = function() {
  var elevator = new Elevator({
    selector:'.back-to-top-btn',
    element: document.querySelector('.back-to-top-btn'),
    duration: 1000 // milliseconds
  });
}
</script>
      

  
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "e2fb4051c49842688ce669e634bc983f",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
    

  







<!-- author:forvoid begin -->
<!-- author:forvoid begin -->

<!-- author:forvoid end -->

<!-- author:forvoid end -->


  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      })
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      })
    </script>
    <script type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


 <script src="/js/is.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/elevator.js"></script>
  </div>
</body>
</html>