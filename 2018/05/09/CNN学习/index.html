<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>CNN学习 | Fallenk&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="CNN学习快速学习背景知识在前面的文章中，我们介绍了全连接神经网络，以及它的训练和使用。我们用它来识别了手写数字，然而，这种结构的网络对于图像识别任务来说并不是很合适。本文将要介绍一种更适合图像、语音识别任务的神经网络结构——卷积神经网络(Convolutional Neural Network, CNN)。说卷积神经网络是最重要的一种神经网络也不为过，它在最近几年大放异彩，几乎所有图像、语音识别">
<meta name="keywords" content="AI,深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="CNN学习">
<meta property="og:url" content="https://fallenk.github.io/2018/05/09/CNN学习/index.html">
<meta property="og:site_name" content="Fallenk&#39;s Blog">
<meta property="og:description" content="CNN学习快速学习背景知识在前面的文章中，我们介绍了全连接神经网络，以及它的训练和使用。我们用它来识别了手写数字，然而，这种结构的网络对于图像识别任务来说并不是很合适。本文将要介绍一种更适合图像、语音识别任务的神经网络结构——卷积神经网络(Convolutional Neural Network, CNN)。说卷积神经网络是最重要的一种神经网络也不为过，它在最近几年大放异彩，几乎所有图像、语音识别">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://img-blog.csdn.net/20180113174854023">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-0ac9923bebd3c9dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640">
<meta property="og:image" content="https://img-blog.csdn.net/20180113174953099">
<meta property="og:image" content="https://img-blog.csdn.net/20180115081038308">
<meta property="og:image" content="https://img-blog.csdn.net/20180115081038308">
<meta property="og:image" content="https://img-blog.csdn.net/20180115081038308">
<meta property="og:image" content="https://img-blog.csdn.net/20180115081038308">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-ad98d6b22f1a66ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/360">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-a36210f89c7164a7.png">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-548b82ccd7977294.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640">
<meta property="og:image" content="https://img-blog.csdn.net/20180115081320003">
<meta property="og:image" content="https://img-blog.csdn.net/20180115081402212">
<meta property="og:image" content="https://img-blog.csdn.net/20180115081425922">
<meta property="og:image" content="https://img-blog.csdn.net/20180115081453443">
<meta property="og:image" content="https://img-blog.csdn.net/20180115081551153">
<meta property="og:image" content="https://img-blog.csdn.net/20180115081629010">
<meta property="og:image" content="https://img-blog.csdn.net/20180115081659514">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-318017ad134effc5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640">
<meta property="og:image" content="https://img-blog.csdn.net/20180115081754383">
<meta property="og:image" content="https://img-blog.csdn.net/20180115081826274">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-b05427072f4c548d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-19110dee0c54c0b2.gif">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-7f362ea9350761d9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-7f362ea9350761d9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-f5fa1e904cb0287e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-7919cabd375b4cfd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640">
<meta property="og:image" content="https://img-blog.csdn.net/20180115081908059">
<meta property="og:image" content="https://img-blog.csdn.net/20180115081942388">
<meta property="og:image" content="https://img-blog.csdn.net/20180115082001142">
<meta property="og:image" content="https://img-blog.csdn.net/20180115082049200">
<meta property="og:image" content="https://img-blog.csdn.net/20180115082111660">
<meta property="og:image" content="https://img-blog.csdn.net/20180115082001142">
<meta property="og:image" content="https://img-blog.csdn.net/20180115082227146">
<meta property="og:image" content="https://img-blog.csdn.net/20180115082300868">
<meta property="og:image" content="https://img-blog.csdn.net/20180115082341336">
<meta property="og:image" content="https://img-blog.csdn.net/20180115082420974">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-958f31b01695b085.gif">
<meta property="og:image" content="https://img-blog.csdn.net/20180115082547691">
<meta property="og:image" content="https://img-blog.csdn.net/20180115082558146">
<meta property="og:image" content="https://img-blog.csdn.net/20180115082608199">
<meta property="og:image" content="https://img-blog.csdn.net/20180115082618338">
<meta property="og:image" content="https://img-blog.csdn.net/20180115082642970">
<meta property="og:image" content="https://img-blog.csdn.net/20180115082721477">
<meta property="og:image" content="https://img-blog.csdn.net/20180115082743120">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-8d30f15073885d7b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640">
<meta property="og:image" content="https://img-blog.csdn.net/20180115082809330">
<meta property="og:image" content="https://img-blog.csdn.net/20180115082833025">
<meta property="og:image" content="https://img-blog.csdn.net/20180115082900300">
<meta property="og:image" content="https://img-blog.csdn.net/20180115082934277">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-03bfc7683ad2e3ad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640">
<meta property="og:image" content="https://img-blog.csdn.net/20180115083129939">
<meta property="og:image" content="https://img-blog.csdn.net/20180115083208546">
<meta property="og:image" content="https://img-blog.csdn.net/20180115083326036">
<meta property="og:image" content="https://img-blog.csdn.net/20180115083434404">
<meta property="og:image" content="https://img-blog.csdn.net/20180115083849542">
<meta property="og:image" content="https://img-blog.csdn.net/20180115083949912">
<meta property="og:image" content="https://img-blog.csdn.net/20180115083849542">
<meta property="og:image" content="https://img-blog.csdn.net/20180115083949912">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-52295dad2641037f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640">
<meta property="og:image" content="https://img-blog.csdn.net/20180115084140183">
<meta property="og:image" content="https://img-blog.csdn.net/20180115084218924">
<meta property="og:image" content="https://img-blog.csdn.net/20180115084246597">
<meta property="og:image" content="https://img-blog.csdn.net/20180115084308647">
<meta property="og:image" content="https://img-blog.csdn.net/20180115084354638">
<meta property="og:image" content="https://img-blog.csdn.net/20180115084218924">
<meta property="og:image" content="https://img-blog.csdn.net/20180115084427681">
<meta property="og:image" content="https://img-blog.csdn.net/20180115084218924">
<meta property="og:image" content="https://img-blog.csdn.net/20180115084518548">
<meta property="og:image" content="https://img-blog.csdn.net/20180115084615779">
<meta property="og:image" content="https://img-blog.csdn.net/20180115084218924">
<meta property="og:image" content="https://img-blog.csdn.net/20180115084640172">
<meta property="og:image" content="https://img-blog.csdn.net/20180115084720056">
<meta property="og:image" content="https://img-blog.csdn.net/20180115084748345">
<meta property="og:image" content="https://img-blog.csdn.net/20180115084818865">
<meta property="og:image" content="https://img-blog.csdn.net/20180115084748345">
<meta property="og:image" content="https://img-blog.csdn.net/20180115084835449">
<meta property="og:image" content="https://img-blog.csdn.net/20180115084543714">
<meta property="og:image" content="https://img-blog.csdn.net/20180115084955559">
<meta property="og:image" content="https://img-blog.csdn.net/20180115084218924">
<meta property="og:image" content="https://img-blog.csdn.net/20180115085015737">
<meta property="og:image" content="https://img-blog.csdn.net/20180115085035952">
<meta property="og:image" content="https://img-blog.csdn.net/20180115085144383">
<meta property="og:image" content="https://img-blog.csdn.net/20180115085220743">
<meta property="og:image" content="https://img-blog.csdn.net/20180115085257382">
<meta property="og:image" content="https://img-blog.csdn.net/20180115085352749">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-2fb37b0a3ff0e1f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640">
<meta property="og:image" content="https://img-blog.csdn.net/20180115085414018">
<meta property="og:image" content="https://img-blog.csdn.net/20180115085432922">
<meta property="og:image" content="https://img-blog.csdn.net/20180115085443988">
<meta property="og:image" content="https://img-blog.csdn.net/20180115085503736">
<meta property="og:image" content="https://img-blog.csdn.net/20180115085532360">
<meta property="og:image" content="https://img-blog.csdn.net/20180115085606788">
<meta property="og:image" content="https://img-blog.csdn.net/20180115085625646">
<meta property="og:image" content="https://img-blog.csdn.net/20180115085643812">
<meta property="og:image" content="https://img-blog.csdn.net/20180115085709269">
<meta property="og:image" content="https://img-blog.csdn.net/20180115085733597">
<meta property="og:image" content="https://img-blog.csdn.net/20180115085812725">
<meta property="og:image" content="https://img-blog.csdn.net/20180115085828281">
<meta property="og:image" content="https://img-blog.csdn.net/20180115085840820">
<meta property="og:image" content="https://img-blog.csdn.net/20180115085914035">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-754f37eb7603e99f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640">
<meta property="og:image" content="https://img-blog.csdn.net/20180115085942571">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090033391">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090033391">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090033391">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090057475">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090106537">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090033391">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-af2da9701a03dc3c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090134108">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090247722">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090310191">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090310191">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090134108">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090134108">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090402786">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-afe6d3a863b7cbcc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090523967">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090537795">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090801070">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090625906">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090911701">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090625906">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090933516">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090625906">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091038992">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090933516">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090625906">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091038992">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091118338">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091159303">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091213472">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091225598">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091213472">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091213472">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091159303">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091343304">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091405511">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091428661">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091454609">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091213472">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091518934">
<meta property="og:image" content="https://img-blog.csdn.net/20180115090911701">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091600148">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-aeba8c8666a22e72.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091648804">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091709205">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091731785">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091826316">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091731785">
<meta property="og:image" content="https://img-blog.csdn.net/20180115091826316">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-a30c883f19db53b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640">
<meta property="og:image" content="https://img-blog.csdn.net/20180115092007360">
<meta property="og:image" content="https://img-blog.csdn.net/20180115092036622">
<meta property="og:image" content="https://img-blog.csdn.net/20180115092107090">
<meta property="og:image" content="https://img-blog.csdn.net/20180115092007360">
<meta property="og:image" content="https://img-blog.csdn.net/20180115092036622">
<meta property="og:image" content="https://img-blog.csdn.net/20180115092036622">
<meta property="og:image" content="https://img-blog.csdn.net/20180115092210308">
<meta property="og:image" content="https://img-blog.csdn.net/20180115092233932">
<meta property="og:image" content="https://img-blog.csdn.net/20180115092252830">
<meta property="og:image" content="https://img-blog.csdn.net/20180115092322505">
<meta property="og:image" content="https://img-blog.csdn.net/20180115092425761">
<meta property="og:image" content="https://img-blog.csdn.net/20180115092433497">
<meta property="og:image" content="https://img-blog.csdn.net/20180115092440786">
<meta property="og:image" content="https://img-blog.csdn.net/20180115092449384">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-af77e98c09fad84c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-a30c883f19db53b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-c3a6772cb07b416a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640">
<meta property="og:image" content="https://img-blog.csdn.net/20180115093011283">
<meta property="og:image" content="https://img-blog.csdn.net/20180115093045140">
<meta property="og:image" content="https://img-blog.csdn.net/20180115093100178">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-c7071f47ea5f8f9a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/480">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2256672-31b42c6c9daa16a4.png">
<meta property="og:updated_time" content="2018-05-10T13:23:29.432Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CNN学习">
<meta name="twitter:description" content="CNN学习快速学习背景知识在前面的文章中，我们介绍了全连接神经网络，以及它的训练和使用。我们用它来识别了手写数字，然而，这种结构的网络对于图像识别任务来说并不是很合适。本文将要介绍一种更适合图像、语音识别任务的神经网络结构——卷积神经网络(Convolutional Neural Network, CNN)。说卷积神经网络是最重要的一种神经网络也不为过，它在最近几年大放异彩，几乎所有图像、语音识别">
<meta name="twitter:image" content="https://img-blog.csdn.net/20180113174854023">
  
    <link rel="alternate" href="/atom.xml" title="Fallenk&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://fallenk.github.io"></form>
      </div>
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">首页</a>
        
          <a class="main-nav-link" href="/archives">归档</a>
        
          <a class="main-nav-link" href="/about">关于</a>
        
      </nav>
      
    </div>
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Fallenk&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">经历，心得，笔记，目标</a>
        </h2>
      
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-CNN学习" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/05/09/CNN学习/" class="article-date">
  <time datetime="2018-05-09T13:58:41.000Z" itemprop="datePublished">2018-05-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/人工智能/">人工智能</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      CNN学习
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h1 id="CNN学习快速学习"><a href="#CNN学习快速学习" class="headerlink" title="CNN学习快速学习"></a>CNN学习快速学习</h1><h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><p>在前面的文章中，我们介绍了<a href="https://www.zybuluo.com/hanbingtao/note/485480" target="_blank" rel="noopener">全连接神经网络</a>，以及它的训练和使用。我们用它来识别了手写数字，然而，这种结构的网络对于图像识别任务来说并不是很合适。本文将要介绍一种更适合图像、语音识别任务的神经网络结构——卷积神经网络<code>(Convolutional Neural Network, CNN)</code>。说卷积神经网络是最重要的一种神经网络也不为过，它在最近几年大放异彩，几乎所有图像、语音识别领域的重要突破都是卷积神经网络取得的，比如谷歌的GoogleNet、微软的ResNet等，打败李世石的AlphaGo也用到了这种网络。本文将详细介绍卷积神经网络以及它的训练算法，以及动手实现一个简单的卷积神经网络。<br><a id="more"></a></p>
<h2 id="一个新的激活函数–ReLU"><a href="#一个新的激活函数–ReLU" class="headerlink" title="一个新的激活函数–ReLU"></a>一个新的激活函数–ReLU</h2><p>最近几年卷积神经网络中，激活函数往往不选择sigmoid或tanh函数，而是选择relu函数。Relu函数的定义是：<br><img src="https://img-blog.csdn.net/20180113174854023" alt=""><br>Relu函数图像如下图所示:<br><img src="http://upload-images.jianshu.io/upload_images/2256672-0ac9923bebd3c9dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640" alt=""></p>
<p>Relu函数作为激活函数，有下面几大优势：</p>
<ul>
<li><strong>速度快</strong> 和<code>sigmoid函数</code>需要计算指数和倒数相比，<code>relu函数</code>其实就是一个<code>max(0,x)</code>，计算代价小很多。</li>
<li><strong>减轻梯度消失问题</strong> 回忆一下计算梯度的公式<img src="https://img-blog.csdn.net/20180113174953099" alt="">其中，<img src="https://img-blog.csdn.net/20180115081038308" alt="">是sigmoid函数的导数。在使用反向传播算法进行梯度计算时，每经过一层sigmoid神经元，梯度就要乘上一个<img src="https://img-blog.csdn.net/20180115081038308" alt="">。从下图可以看出，<img src="https://img-blog.csdn.net/20180115081038308" alt="">函数最大值是1/4。因此，乘一个<img src="https://img-blog.csdn.net/20180115081038308" alt="">会导致梯度越来越小，这对于深层网络的训练是个很大的问题。而relu函数的导数是1，不会导致梯度变小。当然，激活函数仅仅是导致梯度减小的一个因素，但无论如何在这方面relu的表现强于sigmoid。使用relu激活函数可以让你训练更深的网络。<img src="http://upload-images.jianshu.io/upload_images/2256672-ad98d6b22f1a66ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/360" alt=""></li>
<li><strong>稀疏性</strong> 通过对大脑的研究发现，大脑在工作的时候只有大约5%的神经元是激活的，而采用sigmoid激活函数的人工神经网络，其激活率大约是50%。有论文声称人工神经网络在15%-30%的激活率时是比较理想的。因为relu函数在输入小于0时是完全不激活的，因此可以获得一个更低的激活率。</li>
</ul>
<h2 id="全连接网络-VS-卷积网络"><a href="#全连接网络-VS-卷积网络" class="headerlink" title="全连接网络 VS 卷积网络"></a>全连接网络 VS 卷积网络</h2><p>全连接神经网络之所以不太适合图像识别任务，主要有以下几个方面的问题：</p>
<ul>
<li><strong>参数数量太多</strong> 考虑一个输入1000<em>1000像素的图片(一百万像素，现在已经不能算大图了)，输入层有1000</em>1000=100万节点。假设第一个隐藏层有100个节点(这个数量并不多)，那么仅这一层就有(1000<em>1000+1)</em>100=1亿参数，这实在是太多了！我们看到图像只扩大一点，参数数量就会多很多，因此它的扩展性很差。</li>
<li><strong>没有利用像素之间的位置信息</strong> 对于图像识别任务来说，每个像素和其周围像素的联系是比较紧密的，和离得很远的像素的联系可能就很小了。如果一个神经元和上一层所有神经元相连，那么就相当于对于一个像素来说，把图像的所有像素都等同看待，这不符合前面的假设。当我们完成每个连接权重的学习之后，最终可能会发现，有大量的权重，它们的值都是很小的(也就是这些连接其实无关紧要)。努力学习大量并不重要的权重，这样的学习必将是非常低效的。</li>
<li><strong>网络层数限制</strong> 我们知道网络层数越多其表达能力越强，但是通过梯度下降方法训练深度全连接神经网络很困难，因为全连接神经网络的梯度很难传递超过3层。因此，我们不可能得到一个很深的全连接神经网络，也就限制了它的能力。</li>
</ul>
<p>那么，卷积神经网络又是怎样解决这个问题的呢？主要有三个思路：</p>
<ul>
<li><strong>局部连接</strong> 这个是最容易想到的，每个神经元不再和上一层的所有神经元相连，而只和一小部分神经元相连。这样就减少了很多参数。</li>
<li><strong>权值共享</strong> 一组连接可以共享同一个权重，而不是每个连接有一个不同的权重，这样又减少了很多参数。</li>
<li><strong>下采样</strong> 可以使用Pooling来减少每层的样本数，进一步减少参数数量，同时还可以提升模型的鲁棒性。</li>
</ul>
<p>对于图像识别任务来说，卷积神经网络通过尽可能保留重要的参数，去掉大量不重要的参数，来达到更好的学习效果。</p>
<p>接下来，我们将详述卷积神经网络到底是何方神圣。</p>
<h2 id="卷积神经网络是啥"><a href="#卷积神经网络是啥" class="headerlink" title="卷积神经网络是啥"></a>卷积神经网络是啥</h2><p>首先，我们先获取一个感性认识，下图是一个卷积神经网络的示意图：<img src="http://upload-images.jianshu.io/upload_images/2256672-a36210f89c7164a7.png" alt=""></p>
<h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p>如上图所示，一个卷积神经网络由<strong>若干卷积层</strong>、<strong>Pooling层</strong>、<strong>全连接层</strong>组成。你可以构建各种不同的卷积神经网络，它的常用架构模式为：<br><code>INPUT -&gt; [[CONV]*N -&gt; POOL?]*M -&gt; [FC]*K</code></p>
<p>也就是N个卷积层叠加，然后(可选)叠加一个Pooling层，重复这个结构M次，最后叠加K个全连接层。</p>
<p>对于上图展示的卷积神经网络：<br><code>INPUT -&gt; CONV -&gt; POOL -&gt; CONV -&gt; POOL -&gt; FC -&gt; FC</code><br>按照上面描述：<br><code>INPUT -&gt; [[CONV]*1 -&gt; POOL]*2 -&gt; [FC]*2</code> 即<code>N=1, M=2, K=2</code></p>
<h3 id="三维的层结构"><a href="#三维的层结构" class="headerlink" title="三维的层结构"></a>三维的层结构</h3><p>从图1我们可以发现卷积神经网络的层结构和全连接神经网络的层结构有很大不同。全连接神经网络每层的神经元是按照一维排列的，也就是排成一条线的样子；而卷积神经网络每层的神经元是按照三维排列的，也就是排成一个长方体的样子，有宽度、高度和深度。</p>
<p>对于图1展示的神经网络，我们看到<strong>输入层的宽度和高度</strong>对应于输入图像的宽度和高度，而它的深度为1。接着，第一个卷积层对这幅图像<strong>进行了卷积操作</strong>(后面我们会讲如何计算卷积)，<strong>得到了三个Feature Map</strong>。这里的”3”可能是让很多初学者迷惑的地方，实际上，就是这个卷积层包含三个Filter，也就是三套参数，每个Filter都可以把原始输入图像卷积得到一个Feature Map，三个Filter就可以得到三个Feature Map。至于一个卷积层可以有多少个Filter，那是可以自由设定的。也就是说，卷积层的Filter个数也是一个超参数。我们可以<strong>把Feature Map可以看做是通过卷积变换提取到的图像特征</strong>，三个Filter就对原始图像提取出三组不同的特征，也就是得到了三个Feature Map，也称做三个<code>通道(channel)</code>。</p>
<p>继续观察图1，在第一个卷积层之后，Pooling层对三个Feature Map做了下采样(后面我们会讲如何计算下采样)，得到了三个更小的Feature Map。接着，是第二个<strong>卷积层</strong>，它有5个Filter。每个Fitler都把前面下采样之后的3个<strong>Feature Map卷积在一起，得到一个新的Feature Map。这样，5个Filter就得到了5个Feature Map。接着，是第二个Pooling，继续对5个Feature Map进行下采样</strong>，得到了5个更小的Feature Map。</p>
<p>图1所示网络的最后两层是全连接层。第一个全连接层的每个神经元，和上一层5个Feature Map中的每个神经元相连，第二个全连接层(也就是输出层)的每个神经元，则和第一个全连接层的每个神经元相连，这样得到了整个网络的输出。</p>
<p>至此，我们对卷积神经网络有了最基本的感性认识。接下来，我们将介绍卷积神经网络中各种层的计算和训练。</p>
<h2 id="卷积神经网络输出值的计算"><a href="#卷积神经网络输出值的计算" class="headerlink" title="卷积神经网络输出值的计算"></a>卷积神经网络输出值的计算</h2><h3 id="卷积层输出值的计算"><a href="#卷积层输出值的计算" class="headerlink" title="卷积层输出值的计算"></a>卷积层输出值的计算</h3><p>我们用一个简单的例子来讲述如何计算卷积，然后，我们抽象出卷积层的一些重要概念和计算方法。</p>
<p>假设有一个<code>5*5</code>的图像，使用一个<code>3*3</code>的filter进行卷积，想得到一个<code>3*3</code>的F<code>eature Map</code>，如下所示：<br><img src="http://upload-images.jianshu.io/upload_images/2256672-548b82ccd7977294.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640" alt=""></p>
<p>为了清楚的描述卷积计算过程，我们首先对图像的每个像素进行编号，<img src="https://img-blog.csdn.net/20180115081320003" alt="">用表示图像的第i行第i列元素；对filter的每个权重进行编号，<img src="https://img-blog.csdn.net/20180115081402212" alt="">用表示第m行第n列权重，<img src="https://img-blog.csdn.net/20180115081425922" alt="">用表示filter的偏置项；<img src="https://img-blog.csdn.net/20180115081453443" alt="">对Feature Map的每个元素进行编号，用表示Feature Map的第i行第j列元素；用f表示激活函数(这个例子选择relu函数作为激活函数)。然后，使用下列公式计算卷积：<br><img src="https://img-blog.csdn.net/20180115081551153" alt=""><br>例如，对于Feature Map左上角元素<img src="https://img-blog.csdn.net/20180115081629010" alt="">来说，其卷积计算方法为：<br><img src="https://img-blog.csdn.net/20180115081659514" alt=""><br>计算结果如下图所示:<br><img src="http://upload-images.jianshu.io/upload_images/2256672-318017ad134effc5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640" alt=""><br>接下来，Feature Map的元素<img src="https://img-blog.csdn.net/20180115081754383" alt="">的卷积计算方法为：<img src="https://img-blog.csdn.net/20180115081826274" alt=""><br>计算结果如下图所示：<br><img src="http://upload-images.jianshu.io/upload_images/2256672-b05427072f4c548d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640" alt=""><br>可以依次计算出Feature Map中所有元素的值。下面的动画显示了整个Feature Map的计算过程：<img src="http://upload-images.jianshu.io/upload_images/2256672-19110dee0c54c0b2.gif" alt=""><br>上面的计算过程中，步幅(stride)为1。步幅可以设为大于1的数。例如，当步幅为2时，Feature Map计算如下：<br><img src="http://upload-images.jianshu.io/upload_images/2256672-7f362ea9350761d9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640" alt=""><br><img src="http://upload-images.jianshu.io/upload_images/2256672-7f362ea9350761d9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640" alt=""><br><img src="http://upload-images.jianshu.io/upload_images/2256672-f5fa1e904cb0287e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640" alt=""><br><img src="http://upload-images.jianshu.io/upload_images/2256672-7919cabd375b4cfd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640" alt=""><br>我们注意到，当步幅设置为2的时候，Feature Map就变成2*2了。这说明图像大小、步幅和卷积后的Feature Map大小是有关系的。事实上，它们满足下面的关系：<br><img src="https://img-blog.csdn.net/20180115081908059" alt=""><br>在上面两个公式中，<img src="https://img-blog.csdn.net/20180115081942388" alt="">是卷积后Feature Map的宽度；<img src="https://img-blog.csdn.net/20180115082001142" alt="">是卷积前图像的宽度；F是filter的宽度；P是Zero Padding数量，Zero Padding是指在原始图像周围补几圈0，如果P的值是1，那么就补1圈0；S是步幅；<img src="https://img-blog.csdn.net/20180115082049200" alt="">是卷积后Feature Map的高度；<img src="https://img-blog.csdn.net/20180115082111660" alt="">是卷积前图像的宽度。式2和式3本质上是一样的。<br>以前面的例子来说，图像宽度<img src="https://img-blog.csdn.net/20180115082001142" alt="">=5，filter宽度F=3，Zero Padding的值P=0，步幅S=2，则<br><img src="https://img-blog.csdn.net/20180115082227146" alt=""><br>说明Feature Map宽度是2。同样，我们也可以计算出Feature Map高度也是2。<br>前面我们已经讲了深度为1的卷积层的计算方法，如果深度大于1怎么计算呢？其实也是类似的。如果卷积前的图像深度为D，那么相应的filter的深度也必须为D。我们扩展一下式1，得到了深度大于1的卷积计算公式：<br><img src="https://img-blog.csdn.net/20180115082300868" alt=""><br>在式4中，D是深度；F是filter的大小(宽度或高度，两者相同)；<img src="https://img-blog.csdn.net/20180115082341336" alt="">表示filter的第d层第m行第n列权重；<img src="https://img-blog.csdn.net/20180115082420974" alt="">表示图像的第d层第i行第j列像素；其它的符号含义和式1是相同的，不再赘述。<br>我们前面还曾提到，每个卷积层可以有多个filter。每个filter和原始图像进行卷积后，都可以得到一个Feature Map。因此，卷积后Feature Map的深度(个数)和卷积层的filter个数是相同的。</p>
<p>下面的动画显示了包含两个filter的卷积层的计算。我们可以看到<code>7*7*3</code>输入，经过两个<code>3*3*3filter</code>的卷积(步幅为2)，得到了<code>3*3*2</code>的输出。另外我们也会看到下图的<code>Zero padding</code>是1，也就是在输入元素的周围补了一圈0。<code>Zero padding</code>对于图像边缘部分的特征提取是很有帮助的。<br><img src="http://upload-images.jianshu.io/upload_images/2256672-958f31b01695b085.gif" alt="">以上就是卷积层的计算方法。这里面体现了<strong>局部连接</strong>和<strong>权值共享</strong>：每层神经元只和上一层部分神经元相连(卷积计算规则)，且filter的权值对于上一层所有神经元都是一样的。对于包含两个<code>3*3*3</code>的fitler的卷积层来说，其参数数量仅有<code>(3*3*3+1)*2=56</code>个，且参数数量与上一层神经元个数无关。与<strong>全连接神经网络</strong>相比，其参数数量大大减少了。</p>
<h3 id="用卷积公式来表达卷积层计算-暂时跳过"><a href="#用卷积公式来表达卷积层计算-暂时跳过" class="headerlink" title="用卷积公式来表达卷积层计算(暂时跳过)"></a>用卷积公式来表达卷积层计算(暂时跳过)</h3><p>式4的表达很是繁冗，最好能简化一下。就像利用矩阵可以简化表达全连接神经网络的计算一样，我们利用卷积公式可以简化卷积神经网络的表达。<br>下面我们介绍二维卷积公式。<br>设矩阵A，B，其行、列数分别为<img src="https://img-blog.csdn.net/20180115082547691" alt="">、<img src="https://img-blog.csdn.net/20180115082558146" alt="">、<img src="https://img-blog.csdn.net/20180115082608199" alt="">、<img src="https://img-blog.csdn.net/20180115082618338" alt="">，则二维卷积公式如下：<br><img src="https://img-blog.csdn.net/20180115082642970" alt=""><br>且使得满足条件<img src="https://img-blog.csdn.net/20180115082721477" alt=""><br>我们可以把上式写成<img src="https://img-blog.csdn.net/20180115082743120" alt=""><br>如果我们按照式5来计算卷积，我们可以发现矩阵A实际上是filter，而矩阵B是待卷积的输入，位置关系也有所不同：<br><img src="http://upload-images.jianshu.io/upload_images/2256672-8d30f15073885d7b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640" alt=""><br>从上图可以看到，A左上角的值<img src="https://img-blog.csdn.net/20180115082809330" alt="">与B对应区块中右下角<img src="https://img-blog.csdn.net/20180115082833025" alt="">的值相乘，而不是与左上角<img src="https://img-blog.csdn.net/20180115082900300" alt="">的相乘。因此，数学中的卷积和卷积神经网络中的『卷积』还是有区别的，为了避免混淆，我们把<strong>卷积神经网络</strong>中的『卷积』操作叫做互相关(cross-correlation)操作。<br>卷积和互相关操作是可以转化的。首先，我们把矩阵A翻转180度，然后再交换A和B的位置（即把B放在左边而把A放在右边。卷积满足交换率，这个操作不会导致结果变化），那么卷积就变成了互相关。<br>如果我们不去考虑两者这么一点点的区别，我们可以把式5代入到式4：<br><img src="https://img-blog.csdn.net/20180115082934277" alt=""><br>其中，是卷积层输出的feature map。同式4相比，式6就简单多了。然而，这种简洁写法只适合步长为1的情况。</p>
<h3 id="Pooling层输出值的计算"><a href="#Pooling层输出值的计算" class="headerlink" title="Pooling层输出值的计算"></a>Pooling层输出值的计算</h3><p>Pooling层主要的作用是下采样，通过去掉Feature Map中不重要的样本，进一步减少参数数量。Pooling的方法很多，最常用的是Max Pooling。Max Pooling实际上就是在n<em>n的样本中取最大值，作为采样后的样本值。下图是2</em>2 max pooling：<br><img src="http://upload-images.jianshu.io/upload_images/2256672-03bfc7683ad2e3ad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640" alt=""><br>除了Max Pooing之外，常用的还有Mean Pooling——取各样本的平均值。<br>对于深度为D的Feature Map，各层独立做Pooling，因此Pooling后的深度仍然为D。</p>
<h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p><a href="http://blog.csdn.net/luanpeng825485697/article/details/79009223" target="_blank" rel="noopener">零基础入门深度学习(3) - 神经网络和反向传播算法</a></p>
<h2 id="卷积神经网络的训练"><a href="#卷积神经网络的训练" class="headerlink" title="卷积神经网络的训练"></a>卷积神经网络的训练</h2><p>和<strong>全连接神经网络</strong>相比，卷积神经网络的训练要复杂一些。但训练的原理是一样的：</p>
<ol>
<li>利用链式求导计算损失函数对每个权重的偏导数（梯度），</li>
<li>然后根据梯度下降公式更新权重。</li>
<li>训练算法依然是反向传播算法。</li>
</ol>
<h3 id="神经网络和反向传播算法"><a href="#神经网络和反向传播算法" class="headerlink" title="神经网络和反向传播算法"></a>神经网络和反向传播算法</h3><p>整个算法分为三个步骤：</p>
<ol>
<li>前向计算每个神经元<img src="https://img-blog.csdn.net/20180115083129939" alt="">的输出值（j表示网络的第j个神经元，以下同）；</li>
<li>反向计算每个神经元的误差项<img src="https://img-blog.csdn.net/20180115083208546" alt="">，在有的文献中也叫做敏感度(sensitivity)。它实际上是网络的损失函数对神经元加权输入<img src="https://img-blog.csdn.net/20180115083326036" alt="">的偏导数，即:<img src="https://img-blog.csdn.net/20180115083434404" alt=""></li>
<li>计算每个神经元连接权重的梯度（表示从神经元i连接到神经元j的权重），公式为，其中，表示神经元i的输出。</li>
</ol>
<p>最后，根据梯度下降法则更新每个权重即可。<br>对于卷积神经网络，由于涉及到<strong>局部连接、下采样</strong>的等操作，影响到了第二步<strong>误差项<img src="https://img-blog.csdn.net/20180115083849542" alt="">的具体计算方法</strong>，而<strong>权值共享影响了第三步权重<img src="https://img-blog.csdn.net/20180115083949912" alt="">的梯度的计算方法</strong>。接下来，我们分别介绍卷积层和Pooling层的训练算法。</p>
<h3 id="卷积层的训练"><a href="#卷积层的训练" class="headerlink" title="卷积层的训练"></a>卷积层的训练</h3><p>对于卷积层，我们先来看看上面的第二步，即如何将误差项<img src="https://img-blog.csdn.net/20180115083849542" alt="">传递到上一层；然后再来看看第三步，即如何计算filter每个权值<img src="https://img-blog.csdn.net/20180115083949912" alt="">的梯度。</p>
<h4 id="卷积层误差项的传递"><a href="#卷积层误差项的传递" class="headerlink" title="卷积层误差项的传递"></a>卷积层误差项的传递</h4><h5 id="最简单情况下误差项的传递"><a href="#最简单情况下误差项的传递" class="headerlink" title="最简单情况下误差项的传递"></a>最简单情况下误差项的传递</h5><p>我们先来考虑步长为1、输入的深度为1、filter个数为1的最简单的情况。<br>假设输入的大小为<code>3*3</code>，filter大小为<code>2*2</code>，按步长为1卷积，我们将得到<code>2*2</code>的feature map。如下图所示：<br><img src="http://upload-images.jianshu.io/upload_images/2256672-52295dad2641037f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640" alt=""><br>在上图中，为了描述方便，我们为每个元素都进行了编号。用<img src="https://img-blog.csdn.net/20180115084140183" alt="">表示第<img src="https://img-blog.csdn.net/20180115084218924" alt="">层第i行第j列的误差项；用<img src="https://img-blog.csdn.net/20180115084246597" alt="">表示filter第m行第n列权重，用<img src="https://img-blog.csdn.net/20180115084308647" alt="">表示filter的偏置项；用<img src="https://img-blog.csdn.net/20180115084354638" alt="">表示第<img src="https://img-blog.csdn.net/20180115084218924" alt="">层第i行第j列神经元的输出；用<img src="https://img-blog.csdn.net/20180115084427681" alt="">表示第<img src="https://img-blog.csdn.net/20180115084218924" alt="">行神经元的加权输入；用<img src="https://img-blog.csdn.net/20180115084518548" alt="">表示第l层第i行第j列的误差项；用<img src="https://img-blog.csdn.net/20180115084615779" alt="">表示第<img src="https://img-blog.csdn.net/20180115084218924" alt="">层的激活函数。它们之间的关系如下：<br><img src="https://img-blog.csdn.net/20180115084640172" alt=""><br>上式中，<img src="https://img-blog.csdn.net/20180115084720056" alt="">、<img src="https://img-blog.csdn.net/20180115084748345" alt="">、<img src="https://img-blog.csdn.net/20180115084818865" alt="">都是数组，<img src="https://img-blog.csdn.net/20180115084748345" alt="">是<img src="https://img-blog.csdn.net/20180115084835449" alt="">由组成的数组，conv表示卷积操作。<br>在这里，我们假设第<img src="https://img-blog.csdn.net/20180115084543714" alt="">中的每个<img src="https://img-blog.csdn.net/20180115084955559" alt="">值都已经算好，我们要做的是计算第<img src="https://img-blog.csdn.net/20180115084218924" alt="">层每个神经元的误差项<img src="https://img-blog.csdn.net/20180115085015737" alt="">。<br>根据链式求导法则：<br><img src="https://img-blog.csdn.net/20180115085035952" alt=""><br><img src="https://img-blog.csdn.net/20180115085144383" alt=""><br><img src="https://img-blog.csdn.net/20180115085220743" alt=""><br><img src="https://img-blog.csdn.net/20180115085257382" alt=""><br>从上面三个例子，我们发挥一下想象力，不难发现，计算<img src="https://img-blog.csdn.net/20180115085352749" alt="">，相当于把第l层的sensitive map周围补一圈0，在与180度翻转后的filter进行cross-correlation，就能得到想要结果，如下图所示：<br><img src="http://upload-images.jianshu.io/upload_images/2256672-2fb37b0a3ff0e1f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640" alt=""><br>因为卷积相当于将filter旋转180度的cross-correlation，因此上图的计算可以用卷积公式完美的表达：<br><img src="https://img-blog.csdn.net/20180115085414018" alt=""><br>上式中的<img src="https://img-blog.csdn.net/20180115085432922" alt="">表示第<img src="https://img-blog.csdn.net/20180115085443988" alt="">层的filter的权重数组。也可以把上式的卷积展开，写成求和的形式：<br><img src="https://img-blog.csdn.net/20180115085503736" alt=""><br>现在，我们再求第二项<img src="https://img-blog.csdn.net/20180115085532360" alt="">。因为<img src="https://img-blog.csdn.net/20180115085606788" alt=""><br>所以这一项极其简单，仅求激活函数<img src="https://img-blog.csdn.net/20180115085625646" alt="">的导数就行了。<br><img src="https://img-blog.csdn.net/20180115085643812" alt=""><br>将第一项和第二项组合起来，我们得到最终的公式：<br><img src="https://img-blog.csdn.net/20180115085709269" alt=""><br>也可以将式7写成卷积的形式：<img src="https://img-blog.csdn.net/20180115085733597" alt=""><br>其中，符号表<img src="https://img-blog.csdn.net/20180115085812725" alt="">示element-wise product，即将矩阵中每个对应元素相乘。注意式8中的<img src="https://img-blog.csdn.net/20180115085828281" alt="">、<img src="https://img-blog.csdn.net/20180115085840820" alt="">、<img src="https://img-blog.csdn.net/20180115085914035" alt="">都是矩阵。<br>以上就是步长为1、输入的深度为1、filter个数为1的最简单的情况，卷积层误差项传递的算法。下面我们来推导一下步长为S的情况.</p>
<h4 id="卷积步长为S时的误差传递"><a href="#卷积步长为S时的误差传递" class="headerlink" title="卷积步长为S时的误差传递"></a>卷积步长为S时的误差传递</h4><p>我们先来看看步长为S与步长为1的差别。<br><img src="http://upload-images.jianshu.io/upload_images/2256672-754f37eb7603e99f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640" alt=""><br>如上图，上面是步长为1时的卷积结果，下面是步长为2时的卷积结果。我们可以看出，因为步长为2，得到的feature map跳过了步长为1时相应的部分。因此，当我们反向计算误差项时，我们可以对步长为S的sensitivity map相应的位置进行补0，将其『还原』成步长为1时的sensitivity map，再用式8进行求解。</p>
<h4 id="输入层深度为D时的误差传递"><a href="#输入层深度为D时的误差传递" class="headerlink" title="输入层深度为D时的误差传递"></a>输入层深度为D时的误差传递</h4><p>当输入深度为D时，filter的深度也必须为D，<img src="https://img-blog.csdn.net/20180115085942571" alt="">层的<img src="https://img-blog.csdn.net/20180115090033391" alt="">通道只与filter的通道<img src="https://img-blog.csdn.net/20180115090033391" alt="">的权重进行计算。因此，反向计算误差项时，我们可以使用式8，用filter的第<img src="https://img-blog.csdn.net/20180115090033391" alt="">通道权重对第<img src="https://img-blog.csdn.net/20180115090057475" alt="">层sensitivity map进行卷积，得到第<img src="https://img-blog.csdn.net/20180115090106537" alt="">层<img src="https://img-blog.csdn.net/20180115090033391" alt="">通道的sensitivity map。如下图所示：<br><img src="http://upload-images.jianshu.io/upload_images/2256672-af2da9701a03dc3c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640" alt=""></p>
<h4 id="filter数量为N时的误差传递"><a href="#filter数量为N时的误差传递" class="headerlink" title="filter数量为N时的误差传递"></a>filter数量为N时的误差传递</h4><p>filter数量为N时，输出层的深度也为N，第i个filter卷积产生输出层的第i个feature map。由于第<img src="https://img-blog.csdn.net/20180115090134108" alt="">层每个加权输入<img src="https://img-blog.csdn.net/20180115090247722" alt="">都同时影响了第<img src="https://img-blog.csdn.net/20180115090310191" alt="">层所有feature map的输出值，因此，反向计算误差项时，需要使用全导数公式。也就是，我们先使用第d个filter对第<img src="https://img-blog.csdn.net/20180115090310191" alt="">层相应的第d个sensitivity map进行卷积，得到一组N个<img src="https://img-blog.csdn.net/20180115090134108" alt="">层的偏sensitivity map。依次用每个filter做这种卷积，就得到D组偏sensitivity map。最后在各组之间将N个偏sensitivity map 按元素相加，得到最终的N个<img src="https://img-blog.csdn.net/20180115090134108" alt="">层的sensitivity map：<br><img src="https://img-blog.csdn.net/20180115090402786" alt=""><br>以上就是卷积层误差项传递的算法，如果读者还有所困惑，可以参考后面的代码实现来理解。</p>
<h4 id="卷积层filter权重梯度的计算"><a href="#卷积层filter权重梯度的计算" class="headerlink" title="卷积层filter权重梯度的计算"></a>卷积层filter权重梯度的计算</h4><p>我们要在得到第层sensitivity map的情况下，计算filter的权重的梯度，由于卷积层是权重共享的，因此梯度的计算稍有不同。<br><img src="http://upload-images.jianshu.io/upload_images/2256672-afe6d3a863b7cbcc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640" alt=""><br>如上图所示，<img src="https://img-blog.csdn.net/20180115090523967" alt="">是第<img src="https://img-blog.csdn.net/20180115090537795" alt="">层的输出，<img src="https://img-blog.csdn.net/20180115090801070" alt="">是第l层filter的权重，是第l层的sensitivity map。我们的任务是计算<img src="https://img-blog.csdn.net/20180115090625906" alt="">的梯度，即<img src="https://img-blog.csdn.net/20180115090911701" alt="">。<br>为了计算偏导数，我们需要考察权重<img src="https://img-blog.csdn.net/20180115090625906" alt="">对<img src="https://img-blog.csdn.net/20180115090933516" alt="">的影响。权重项<img src="https://img-blog.csdn.net/20180115090625906" alt="">通过影响<img src="https://img-blog.csdn.net/20180115091038992" alt="">的值，进而影响<img src="https://img-blog.csdn.net/20180115090933516" alt="">。我们仍然通过几个具体的例子来看权重<img src="https://img-blog.csdn.net/20180115090625906" alt="">项对<img src="https://img-blog.csdn.net/20180115091038992" alt="">的影响，然后再从中总结出规律。<br><img src="https://img-blog.csdn.net/20180115091118338" alt=""><br>从上面的公式看出，由于权值共享，权值<img src="https://img-blog.csdn.net/20180115091159303" alt="">对所有<img src="https://img-blog.csdn.net/20180115091213472" alt="">的都有影响。<img src="https://img-blog.csdn.net/20180115091225598" alt="">是每一个的<img src="https://img-blog.csdn.net/20180115091213472" alt="">函数，而每一个<img src="https://img-blog.csdn.net/20180115091213472" alt="">又是的<img src="https://img-blog.csdn.net/20180115091159303" alt="">函数，根据全导数公式，计算<img src="https://img-blog.csdn.net/20180115091343304" alt="">就是要把每个偏导数都加起来：<img src="https://img-blog.csdn.net/20180115091405511" alt=""><br>例2，计算<img src="https://img-blog.csdn.net/20180115091428661" alt="">：<br>通过查看<img src="https://img-blog.csdn.net/20180115091454609" alt="">与<img src="https://img-blog.csdn.net/20180115091213472" alt="">的关系，我们很容易得到：<br><img src="https://img-blog.csdn.net/20180115091518934" alt=""><br>实际上，每个权重项都是类似的，我们不一一举例了。现在，是我们再次发挥想象力的时候，我们发现计算<img src="https://img-blog.csdn.net/20180115090911701" alt="">规律是：<img src="https://img-blog.csdn.net/20180115091600148" alt=""><br>也就是用sensitivity map作为卷积核，在input上进行cross-correlation，如下图所示：<br><img src="http://upload-images.jianshu.io/upload_images/2256672-aeba8c8666a22e72.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640" alt=""><br>最后，我们来看一看偏置项<img src="https://img-blog.csdn.net/20180115091648804" alt="">的梯度。通过查看前面的公式，我们很容易发现：<img src="https://img-blog.csdn.net/20180115091709205" alt=""><br>也就是偏置项的梯度就是sensitivity map所有误差项之和。<br>对于步长为S的卷积层，处理方法与传递<strong>误差项</strong>是一样的，首先将sensitivity map『还原』成步长为1时的sensitivity map，再用上面的方法进行计算。<br>获得了所有的梯度之后，就是根据梯度下降算法来更新每个权重。这在前面的文章中已经反复写过，这里就不再重复了。</p>
<p>至此，我们已经解决了卷积层的训练问题，接下来我们看一看Pooling层的训练。</p>
<h3 id="Pooling层的训练"><a href="#Pooling层的训练" class="headerlink" title="Pooling层的训练"></a>Pooling层的训练</h3><p>无论<code>max pooling</code>还是<code>mean pooling</code>，都没有需要学习的参数。因此，在卷积神经网络的训练中，Pooling层需要做的仅仅是将误差项传递到上一层，而没有梯度的计算。</p>
<h4 id="Max-Pooling误差项的传递"><a href="#Max-Pooling误差项的传递" class="headerlink" title="Max Pooling误差项的传递"></a>Max Pooling误差项的传递</h4><p>如下图，假设第<img src="https://img-blog.csdn.net/20180115091731785" alt="">层大小为<code>4*4</code>，<code>pooling filter</code>大小为<code>2*2</code>，步长为2，这样，<code>max pooling</code>之后，第l层大小为<code>2*2</code>。假设第l层的<img src="https://img-blog.csdn.net/20180115091826316" alt="">值都已经计算完毕，我们现在的任务是计算第<img src="https://img-blog.csdn.net/20180115091731785" alt="">层的<img src="https://img-blog.csdn.net/20180115091826316" alt="">值。<br><img src="http://upload-images.jianshu.io/upload_images/2256672-a30c883f19db53b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640" alt=""><br>我们用<img src="https://img-blog.csdn.net/20180115092007360" alt="">表示第l-1层的加权输入；用<img src="https://img-blog.csdn.net/20180115092036622" alt="">表示第l层的加权输入。我们先来考察一个具体的例子，然后再总结一般性的规律。对于max pooling：<br><img src="https://img-blog.csdn.net/20180115092107090" alt=""><br>也就是说，只有区块中最大<img src="https://img-blog.csdn.net/20180115092007360" alt="">的才会对<img src="https://img-blog.csdn.net/20180115092036622" alt="">的值产生影响。我们假设最大的值是<img src="https://img-blog.csdn.net/20180115092036622" alt="">，则上式相当于：<br><img src="https://img-blog.csdn.net/20180115092210308" alt=""><br>那么，我们不难求得下面几个偏导数：<br><img src="https://img-blog.csdn.net/20180115092233932" alt=""><br>因此：<img src="https://img-blog.csdn.net/20180115092252830" alt="">而：<img src="https://img-blog.csdn.net/20180115092322505" alt=""><br>现在，我们发现了规律：对于max pooling，下一层的误差项的值会原封不动的传递到上一层对应区块中的最大值所对应的神经元，而其他神经元的误差项的值都是0。如下图所示(假设<img src="https://img-blog.csdn.net/20180115092425761" alt="">、<img src="https://img-blog.csdn.net/20180115092433497" alt="">、<img src="https://img-blog.csdn.net/20180115092440786" alt="">、<img src="https://img-blog.csdn.net/20180115092449384" alt="">为所在区块中的最大输出值)：<br><img src="http://upload-images.jianshu.io/upload_images/2256672-af77e98c09fad84c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640" alt=""></p>
<h4 id="Mean-Pooling误差项的传递"><a href="#Mean-Pooling误差项的传递" class="headerlink" title="Mean Pooling误差项的传递"></a>Mean Pooling误差项的传递</h4><p>我们还是用前面屡试不爽的套路，先研究一个特殊的情形，再扩展为一般规律。<br><img src="http://upload-images.jianshu.io/upload_images/2256672-a30c883f19db53b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640" alt=""><br>同理我们发现了规律：对于mean pooling，下一层的误差项的值会平均分配到上一层对应区块中的所有神经元。如下图所示：<img src="http://upload-images.jianshu.io/upload_images/2256672-c3a6772cb07b416a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640" alt=""><br>上面这个算法可以表达为高大上的克罗内克积(Kronecker product)的形式，有兴趣的读者可以研究一下。<br><img src="https://img-blog.csdn.net/20180115093011283" alt=""><br>其中，n是pooling层filter的大小<img src="https://img-blog.csdn.net/20180115093045140" alt="">，<img src="https://img-blog.csdn.net/20180115093100178" alt="">、都是矩阵。</p>
<p>至此，我们已经把卷积层、Pooling层的训练算法介绍完毕，加上上一篇文章讲的全连接层训练算法，您应该已经具备了编写卷积神经网络代码所需要的知识。为了加深对知识的理解，接下来，我们将展示如何实现一个简单的卷积神经网络。</p>
<h2 id="卷积神经网络的实现"><a href="#卷积神经网络的实现" class="headerlink" title="卷积神经网络的实现"></a>卷积神经网络的实现</h2><p>现在，我们亲自动手实现一个卷积神经网络，以便巩固我们所学的知识。</p>
<p>首先，我们要改变一下代码的架构，『层』成为了我们最核心的组件。这是因为卷积神经网络有不同的层，而每种层的算法都在对应的类中实现。</p>
<p>这次，我们用到了在python中编写算法经常会用到的numpy包。为了使用numpy，我们需要先将numpy导入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<h3 id="卷积层的实现"><a href="#卷积层的实现" class="headerlink" title="卷积层的实现"></a>卷积层的实现</h3><h4 id="卷积层初始化"><a href="#卷积层初始化" class="headerlink" title="卷积层初始化"></a>卷积层初始化</h4><p>我们用ConvLayer类来实现一个卷积层。下面的代码是初始化一个卷积层，可以在构造函数中设置卷积层的超参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用ConvLayer类来实现一个卷积层。  </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvLayer</span><span class="params">(object)</span>:</span>  </span><br><span class="line">    <span class="comment"># 下面的代码是初始化一个卷积层，可以在构造函数中设置卷积层的超参数  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_width, input_height,  </span></span></span><br><span class="line"><span class="function"><span class="params">                 channel_number, filter_width,  </span></span></span><br><span class="line"><span class="function"><span class="params">                 filter_height, filter_number,  </span></span></span><br><span class="line"><span class="function"><span class="params">                 zero_padding, stride, activator,  </span></span></span><br><span class="line"><span class="function"><span class="params">                 learning_rate)</span>:</span>  </span><br><span class="line">        self.input_width = input_width   <span class="comment">#  输入宽度  </span></span><br><span class="line">        self.input_height = input_height  <span class="comment"># 输入高度  </span></span><br><span class="line">        self.channel_number = channel_number  <span class="comment"># 通道数=输入的深度=过滤器的深度  </span></span><br><span class="line">        self.filter_width = filter_width  <span class="comment"># 过滤器的宽度  </span></span><br><span class="line">        self.filter_height = filter_height  <span class="comment"># 过滤器的高度  </span></span><br><span class="line">        self.filter_number = filter_number  <span class="comment"># 过滤器的数量。  </span></span><br><span class="line">        self.zero_padding = zero_padding  <span class="comment"># 补0圈数  </span></span><br><span class="line">        self.stride = stride <span class="comment"># 步幅  </span></span><br><span class="line">        self.output_width = int(ConvLayer.calculate_output_size(self.input_width, filter_width, zero_padding,stride))  <span class="comment"># 计算输出宽度  </span></span><br><span class="line">        self.output_height = int(ConvLayer.calculate_output_size(self.input_height, filter_height, zero_padding,stride))  <span class="comment"># 计算输出高度  </span></span><br><span class="line">        self.output_array = np.zeros((self.filter_number,self.output_height, self.output_width)) <span class="comment"># 创建输出三维数组。每个过滤器都产生一个二维数组的输出  </span></span><br><span class="line">        self.filters = []   <span class="comment"># 卷积层的每个过滤器  </span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(filter_number):  </span><br><span class="line">            self.filters.append(Filter(filter_width,filter_height, self.channel_number))  </span><br><span class="line">        self.activator = activator   <span class="comment"># 使用rule激活器  </span></span><br><span class="line">        self.learning_rate = learning_rate  <span class="comment"># 学习速率</span></span><br></pre></td></tr></table></figure>
<p>calculate_output_size函数用来确定卷积层输出的大小，其实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确定卷积层输出的大小  </span></span><br><span class="line"><span class="meta">@staticmethod  </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_output_size</span><span class="params">(input_size,filter_size, zero_padding, stride)</span>:</span>  </span><br><span class="line">    <span class="keyword">return</span> (input_size - filter_size + <span class="number">2</span> * zero_padding) / stride + <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>Filter类保存了卷积层的参数以及梯度，并且实现了用梯度下降算法来更新参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Filter类保存了卷积层的参数以及梯度，并且实现了用梯度下降算法来更新参数。  </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Filter</span><span class="params">(object)</span>:</span>  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, width, height, depth)</span>:</span>  </span><br><span class="line">        self.weights = np.random.uniform(<span class="number">-1e-4</span>, <span class="number">1e-4</span>,(depth, height, width))  <span class="comment"># 随机初始化卷基层权重一个很小的值，  </span></span><br><span class="line">        self.bias = <span class="number">0</span>  <span class="comment"># 初始化偏量为0  </span></span><br><span class="line">        self.weights_grad = np.zeros(self.weights.shape)   <span class="comment"># 初始化权重梯度  </span></span><br><span class="line">        self.bias_grad = <span class="number">0</span>  <span class="comment"># 初始化偏量梯度  </span></span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span>  </span><br><span class="line">        <span class="keyword">return</span> <span class="string">'filter weights:\n%s\nbias:\n%s'</span> % (repr(self.weights), repr(self.bias))  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 读取权重  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_weights</span><span class="params">(self)</span>:</span>  </span><br><span class="line">        <span class="keyword">return</span> self.weights  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 读取偏量  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_bias</span><span class="params">(self)</span>:</span>  </span><br><span class="line">        <span class="keyword">return</span> self.bias  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 更新权重和偏量  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, learning_rate)</span>:</span>  </span><br><span class="line">        self.weights -= learning_rate * self.weights_grad  </span><br><span class="line">        self.bias -= learning_rate * self.bias_grad</span><br></pre></td></tr></table></figure>
<p>我们对参数的初始化采用了常用的策略，即：权重随机初始化为一个很小的值，而偏置项初始化为0。</p>
<p>ReluActivator类实现了relu激活函数，其中，forward方法实现了前向计算，而backward方法则是计算导数。比如，relu函数的实现如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rule激活器  </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReluActivator</span><span class="params">(object)</span>:</span>  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, weighted_input)</span>:</span>  <span class="comment"># 前向计算，计算输出  </span></span><br><span class="line">        <span class="keyword">return</span> max(<span class="number">0</span>, weighted_input)  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, output)</span>:</span>  <span class="comment"># 后向计算，计算导数  </span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">if</span> output &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>IdentityActivator类实现f(x)=x激活函数，其中，forward方法实现了前向计算，而backward方法则是计算导数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># IdentityActivator激活器.f(x)=x  </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IdentityActivator</span><span class="params">(object)</span>:</span>  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, weighted_input)</span>:</span>  <span class="comment"># 前向计算，计算输出  </span></span><br><span class="line">        <span class="keyword">return</span> weighted_input  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, output)</span>:</span>  <span class="comment"># 后向计算，计算导数  </span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h4 id="卷积层前向计算的实现"><a href="#卷积层前向计算的实现" class="headerlink" title="卷积层前向计算的实现"></a>卷积层前向计算的实现</h4><p>ConvLayer类的forward方法实现了卷积层的前向计算（即计算根据输入来计算卷积层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算卷积层的输出。输出结果保存在self.output_array  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_array)</span>:</span>  </span><br><span class="line">        self.input_array = input_array  <span class="comment"># 多个通道的图片，每个通道为一个二维图片  </span></span><br><span class="line">        self.padded_input_array = padding(input_array,self.zero_padding)  <span class="comment"># 先将输入补足0  </span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.filter_number):  <span class="comment">#每个过滤器都产生一个二维数组的输出  </span></span><br><span class="line">            filter = self.filters[i]  </span><br><span class="line">            conv(self.padded_input_array,filter.get_weights(), self.output_array[i],self.stride, filter.get_bias())  </span><br><span class="line">        <span class="comment"># element_wise_op函数实现了对numpy数组进行按元素操作，并将返回值写回到数组中  </span></span><br><span class="line">        element_wise_op(self.output_array,self.activator.forward)</span><br></pre></td></tr></table></figure>
<p>上面的代码里面包含了几个工具函数。element_wise_op函数实现了对numpy数组进行按元素操作，并将返回值写回到数组中，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对numpy数组进行逐个元素的操作。op为函数。element_wise_op函数实现了对numpy数组进行按元素操作，并将返回值写回到数组中  </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">element_wise_op</span><span class="params">(array, op)</span>:</span>  </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> np.nditer(array,op_flags=[<span class="string">'readwrite'</span>]):  </span><br><span class="line">        i[...] = op(i)   <span class="comment"># 将元素i传入op函数，返回值，再修改i</span></span><br></pre></td></tr></table></figure>
<p>conv函数实现了2维和3维数组的卷积，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算一个过滤器的卷积运算，输出一个二维数据。每个通道的输入是图片，但是可能不是一个通道，所以这里自动适配输入为2D和3D的情况。  </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv</span><span class="params">(input_array,kernel_array,output_array,stride, bias)</span>:</span>  </span><br><span class="line">    output_width = output_array.shape[<span class="number">1</span>]   <span class="comment"># 获取输出的宽度。一个过滤器产生的输出一定是一个通道  </span></span><br><span class="line">    output_height = output_array.shape[<span class="number">0</span>] <span class="comment"># 获取输出的高度  </span></span><br><span class="line">    kernel_width = kernel_array.shape[<span class="number">-1</span>]  <span class="comment"># 过滤器的宽度。有可能有多个通道。多通道时shape=[深度、高度、宽度]，单通道时shape=[高度、宽度]  </span></span><br><span class="line">    kernel_height = kernel_array.shape[<span class="number">-2</span>] <span class="comment"># 过滤器的高度。有可能有多个通道。多通道时shape=[深度、高度、宽度]，单通道时shape=[高度、宽度]  </span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(output_height):  </span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(output_width):  </span><br><span class="line">            juanjiqu = get_patch(input_array, i, j, kernel_width,kernel_height, stride)   <span class="comment"># 获取输入的卷积区。（单通道或多通道）  </span></span><br><span class="line">            <span class="comment"># 这里是对每个通道的两个矩阵对应元素相乘求和，再将每个通道的和值求和  </span></span><br><span class="line">            kernel_values= (np.multiply(juanjiqu,kernel_array)).sum() <span class="comment"># 卷积区与过滤器卷积运算。1，一个通道内，卷积区矩阵与过滤器矩阵对应点相乘后，求和值。2、将每个通道的和值再求和。  </span></span><br><span class="line">            output_array[i][j] = kernel_values + bias  <span class="comment">#将卷积结果加上偏量</span></span><br></pre></td></tr></table></figure>
<p>padding函数实现了zero padding操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 为数组增加Zero padding。zp步长，自动适配输入为2D和3D的情况  </span><br><span class="line">def padding(input_array, zp):  </span><br><span class="line">    if zp == 0: # 如果不补0  </span><br><span class="line">        return input_array  </span><br><span class="line">    else:  </span><br><span class="line">        if input_array.ndim == 3:   # 如果输入有多个通道  </span><br><span class="line">            input_width = input_array.shape[2]  # 获取输入的宽度  </span><br><span class="line">            input_height = input_array.shape[1]  # 获取输入的宽度  </span><br><span class="line">            input_depth = input_array.shape[0]  # 获取输入的深度  </span><br><span class="line">            padded_array = np.zeros((input_depth,input_height + 2 * zp,input_width + 2 * zp))  # 先定义一个补0后大小的全0矩阵  </span><br><span class="line">            padded_array[:,zp: zp + input_height,zp: zp + input_width] = input_array # 每个通道上，将中间部分替换成输入，这样就变成了原矩阵周围补0 的形式  </span><br><span class="line">            return padded_array  </span><br><span class="line">        elif input_array.ndim == 2:  # 如果输入只有一个通道  </span><br><span class="line">            input_width = input_array.shape[1] # 获取输入的宽度  </span><br><span class="line">            input_height = input_array.shape[0] # 虎丘输入的高度  </span><br><span class="line">            padded_array = np.zeros((input_height + 2 * zp,input_width + 2 * zp))  # 先定义一个补0后大小的全0矩阵  </span><br><span class="line">            padded_array[zp: zp + input_height,zp: zp + input_width] = input_array  # 将中间部分替换成输入，这样就变成了原矩阵周围补0 的形式  </span><br><span class="line">            return padded_array</span><br></pre></td></tr></table></figure>
<h4 id="卷积层反向传播算法的实现"><a href="#卷积层反向传播算法的实现" class="headerlink" title="卷积层反向传播算法的实现"></a>卷积层反向传播算法的实现</h4><p>现在，是介绍卷积层核心算法的时候了。我们知道反向传播算法需要完成几个任务：</p>
<ol>
<li>将误差项传递到上一层。</li>
<li>计算每个参数的梯度。</li>
<li>更新参数。<br>以下代码都是在ConvLayer类中实现。我们先来看看将误差项传递到上一层的代码实现。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"># 将误差项传递到上一层。sensitivity_array: 本层的误差。activator: 上一层的激活函数  </span><br><span class="line">    def bp_sensitivity_map(self, sensitivity_array,activator):   # 公式9  </span><br><span class="line">        # 根据卷积步长，对原始sensitivity map进行补0扩展，扩展成如果步长为1的输出误差形状。再用公式8求解  </span><br><span class="line">        expanded_error_array = self.expand_sensitivity_map(sensitivity_array)  </span><br><span class="line">        # print(sensitivity_array)  </span><br><span class="line">        # full卷积，对sensitivitiy map进行zero padding  </span><br><span class="line">        # 虽然原始输入的zero padding单元也会获得残差，但这个残差不需要继续向上传递，因此就不计算了  </span><br><span class="line">        expanded_width = expanded_error_array.shape[2]   # 误差的宽度  </span><br><span class="line">        zp = int((self.input_width + self.filter_width - 1 - expanded_width) / 2)   # 计算步长  </span><br><span class="line">        padded_array = padding(expanded_error_array, zp)  #补0操作  </span><br><span class="line">        # 初始化delta_array，用于保存传递到上一层的sensitivity map  </span><br><span class="line">        self.delta_array = self.create_delta_array()  </span><br><span class="line">        # 对于具有多个filter的卷积层来说，最终传递到上一层的sensitivity map相当于所有的filter的sensitivity map之和  </span><br><span class="line">        for i in range(self.filter_number):   # 遍历每一个过滤器。每个过滤器都产生多通道的误差，多个多通道的误差叠加  </span><br><span class="line">            filter = self.filters[i]  </span><br><span class="line">            # 将滤波器每个通道的权重权重翻转180度。  </span><br><span class="line">            flipped_weights=[]  </span><br><span class="line">            for oneweight in filter.get_weights():  # 这一个滤波器下的每个通道都进行180翻转  </span><br><span class="line">                flipped_weights.append(np.rot90(oneweight, 2))  </span><br><span class="line">            flipped_weights = np.array(flipped_weights)  </span><br><span class="line">            # 计算与一个filter对应的delta_array  </span><br><span class="line">            delta_array = self.create_delta_array()  </span><br><span class="line">            for d in range(delta_array.shape[0]):   # 计算每个通道上的误差，存储在delta_array的对应通道上  </span><br><span class="line">                # print(&apos;大小：\n&apos;,flipped_weights[d])  </span><br><span class="line">                conv(padded_array[i], flipped_weights[d],delta_array[d], 1, 0)  </span><br><span class="line">            self.delta_array += delta_array   # 将每个滤波器每个通道产生的误差叠加  </span><br><span class="line">  </span><br><span class="line">        # 将计算结果与激活函数的偏导数做element-wise乘法操作  </span><br><span class="line">        derivative_array = np.array(self.input_array)  # 复制一个矩阵，因为下面的会改变元素的值，所以深复制了一个矩阵  </span><br><span class="line">        element_wise_op(derivative_array,activator.backward)  # 逐个元素求偏导数。  </span><br><span class="line">        self.delta_array *= derivative_array  # 误差乘以偏导数。得到上一层的误差</span><br></pre></td></tr></table></figure>
<p>expand_sensitivity_map方法就是将步长为S的sensitivity map『还原』为步长为1的sensitivity map，代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对步长为S的sensitivitymap相应的位置进行补0，将其『还原』成步长为1时的sensitivitymap，再用式8进行求解  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">expand_sensitivity_map</span><span class="params">(self, sensitivity_array)</span>:</span>  </span><br><span class="line">        depth = sensitivity_array.shape[<span class="number">0</span>]   <span class="comment"># 获取误差项的深度  </span></span><br><span class="line">        <span class="comment"># 确定扩展后sensitivity map的大小，即计算stride为1时sensitivity map的大小  </span></span><br><span class="line">        expanded_width = (self.input_width - self.filter_width + <span class="number">2</span> * self.zero_padding + <span class="number">1</span>)  </span><br><span class="line">        expanded_height = (self.input_height - self.filter_height + <span class="number">2</span> * self.zero_padding + <span class="number">1</span>)  </span><br><span class="line">        <span class="comment"># 构建新的sensitivity_map  </span></span><br><span class="line">        expand_array = np.zeros((depth, expanded_height, expanded_width))  </span><br><span class="line">        <span class="comment"># 从原始sensitivity map拷贝误差值，每有拷贝的位置，就是要填充的0  </span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.output_height):  </span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(self.output_width):  </span><br><span class="line">                i_pos = i * self.stride  </span><br><span class="line">                j_pos = j * self.stride  </span><br><span class="line">                expand_array[:, i_pos, j_pos] = sensitivity_array[:, i, j]  </span><br><span class="line">        <span class="keyword">return</span> expand_array</span><br></pre></td></tr></table></figure>
<p>create_delta_array是创建用来保存传递到上一层的sensitivity map的数组</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 创建用来保存传递到上一层的sensitivity map的数组。（上一层的输出也就是这一层的输入。所以上一层的误差项的维度和这一层的输入的维度相同）  </span><br><span class="line">   def create_delta_array(self):  </span><br><span class="line">       return np.zeros((self.channel_number,self.input_height, self.input_width))</span><br></pre></td></tr></table></figure>
<p>接下来，是计算梯度的代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 计算梯度。根据误差值，计算本层每个过滤器的w和b的梯度  </span><br><span class="line">    def bp_gradient(self, sensitivity_array):  </span><br><span class="line">        # 处理卷积步长，对原始sensitivity map进行扩展  </span><br><span class="line">        expanded_error_array = self.expand_sensitivity_map(sensitivity_array)  </span><br><span class="line">        for i in range(self.filter_number):  # 每个过滤器产生一个输出  </span><br><span class="line">            # 计算每个权重的梯度  </span><br><span class="line">            filter = self.filters[i]  </span><br><span class="line">            for d in range(filter.weights.shape[0]):   # 过滤器的每个通道都要计算梯度  </span><br><span class="line">                conv(self.padded_input_array[d],expanded_error_array[i],filter.weights_grad[d], 1, 0)   #  公式（31、32中间）  </span><br><span class="line">  </span><br><span class="line">            # 计算偏置项的梯度  </span><br><span class="line">            filter.bias_grad = expanded_error_array[i].sum()   # 公式（34）</span><br></pre></td></tr></table></figure>
<p>最后，是按照梯度下降算法更新参数的代码，这部分非常简单</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 按照梯度下降，更新权重  </span><br><span class="line">def update(self):  </span><br><span class="line">    for filter in self.filters:  </span><br><span class="line">        filter.update(self.learning_rate)   # 每个过滤器</span><br></pre></td></tr></table></figure>
<h4 id="卷积层的梯度检查"><a href="#卷积层的梯度检查" class="headerlink" title="卷积层的梯度检查"></a>卷积层的梯度检查</h4><p>为了验证我们的公式推导和代码实现的正确性，我们必须要对卷积层进行梯度检查。下面是代吗实现：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">def init_test():  </span><br><span class="line">    a = np.array(  # 作为输入  </span><br><span class="line">        [[[0, 1, 1, 0, 2],  </span><br><span class="line">          [2, 2, 2, 2, 1],  </span><br><span class="line">          [1, 0, 0, 2, 0],  </span><br><span class="line">          [0, 1, 1, 0, 0],  </span><br><span class="line">          [1, 2, 0, 0, 2]],  </span><br><span class="line">         [[1, 0, 2, 2, 0],  </span><br><span class="line">          [0, 0, 0, 2, 0],  </span><br><span class="line">          [1, 2, 1, 2, 1],  </span><br><span class="line">          [1, 0, 0, 0, 0],  </span><br><span class="line">          [1, 2, 1, 1, 1]],  </span><br><span class="line">         [[2, 1, 2, 0, 0],  </span><br><span class="line">          [1, 0, 0, 1, 0],  </span><br><span class="line">          [0, 2, 1, 0, 1],  </span><br><span class="line">          [0, 1, 2, 2, 2],  </span><br><span class="line">          [2, 1, 0, 0, 1]]])  </span><br><span class="line">    b = np.array(   # 作为输出误差  </span><br><span class="line">        [[[0, 1, 1],  </span><br><span class="line">          [2, 2, 2],  </span><br><span class="line">          [1, 0, 0]],  </span><br><span class="line">         [[1, 0, 2],  </span><br><span class="line">          [0, 0, 0],  </span><br><span class="line">          [1, 2, 1]]])  </span><br><span class="line">    cl = ConvLayer(5, 5, 3, 3, 3, 2, 1, 2, IdentityActivator(), 0.001)  </span><br><span class="line">    cl.filters[0].weights = np.array(   # 初始化第一层卷积层权重  </span><br><span class="line">        [[[-1, 1, 0],  </span><br><span class="line">          [0, 1, 0],  </span><br><span class="line">          [0, 1, 1]],  </span><br><span class="line">         [[-1, -1, 0],  </span><br><span class="line">          [0, 0, 0],  </span><br><span class="line">          [0, -1, 0]],  </span><br><span class="line">         [[0, 0, -1],  </span><br><span class="line">          [0, 1, 0],  </span><br><span class="line">          [1, -1, -1]]], dtype=np.float64)  </span><br><span class="line">    cl.filters[0].bias = 1  # 初始化第一层卷积层偏重  </span><br><span class="line">    cl.filters[1].weights = np.array(   # 初始化第二层卷积层权重  </span><br><span class="line">        [[[1, 1, -1],  </span><br><span class="line">          [-1, -1, 1],  </span><br><span class="line">          [0, -1, 1]],  </span><br><span class="line">         [[0, 1, 0],  </span><br><span class="line">          [-1, 0, -1],  </span><br><span class="line">          [-1, 1, 0]],  </span><br><span class="line">         [[-1, 0, 0],  </span><br><span class="line">          [-1, 0, 1],  </span><br><span class="line">          [-1, 0, 0]]], dtype=np.float64)  </span><br><span class="line">    return a, b, cl  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"># 测试前向传播  </span><br><span class="line">def test():  </span><br><span class="line">    a, b, cl = init_test()  </span><br><span class="line">    cl.forward(a)     # 对输出进行以一次前向预测  </span><br><span class="line">    # print(cl.output_array)  </span><br><span class="line">  </span><br><span class="line"># 测试后向传播  </span><br><span class="line">def test_bp():  </span><br><span class="line">    a, b, cl = init_test()  </span><br><span class="line">    cl.backward(a, b, IdentityActivator())  # 对输出误差后向传播  </span><br><span class="line">    cl.update()  # 跟新权重  </span><br><span class="line">    # print(cl.filters[0])  # 查看说过更新一次后的滤波器权重  </span><br><span class="line">    # print(cl.filters[1])  # 查看说过更新一次后的滤波器权重  </span><br><span class="line">  </span><br><span class="line"># 梯度检查  </span><br><span class="line">def gradient_check():  </span><br><span class="line">    # 设计一个误差函数，取所有节点输出项之和  </span><br><span class="line">    error_function = lambda o: o.sum()  </span><br><span class="line">  </span><br><span class="line">    # 计算forward值  </span><br><span class="line">    a, b, cl = init_test()  </span><br><span class="line">    cl.forward(a)  # 对输入进行一次预测  </span><br><span class="line">  </span><br><span class="line">    # 求取sensitivity map  </span><br><span class="line">    sensitivity_array = np.ones(cl.output_array.shape,dtype=np.float64)  </span><br><span class="line">  </span><br><span class="line">    # 计算梯度  </span><br><span class="line">    cl.backward(a, sensitivity_array,IdentityActivator())  </span><br><span class="line">    # 检查梯度  </span><br><span class="line">    epsilon = 10e-4  </span><br><span class="line">    for d in range(cl.filters[0].weights_grad.shape[0]):  </span><br><span class="line">        for i in range(cl.filters[0].weights_grad.shape[1]):  </span><br><span class="line">            for j in range(cl.filters[0].weights_grad.shape[2]):  </span><br><span class="line">                cl.filters[0].weights[d, i, j] += epsilon  </span><br><span class="line">                cl.forward(a)  </span><br><span class="line">                err1 = error_function(cl.output_array)  </span><br><span class="line">                cl.filters[0].weights[d, i, j] -= 2 * epsilon  </span><br><span class="line">                cl.forward(a)  </span><br><span class="line">                err2 = error_function(cl.output_array)  </span><br><span class="line">                expect_grad = (err1 - err2) / (2 * epsilon)  </span><br><span class="line">                cl.filters[0].weights[d, i, j] += epsilon  </span><br><span class="line">                print(&apos;weights(%d,%d,%d): expected - actural %f - %f&apos; % (d, i, j, expect_grad, cl.filters[0].weights_grad[d, i, j]))</span><br></pre></td></tr></table></figure>
<p>上面代码值得思考的地方在于，传递给卷积层的sensitivity map是全1数组，留给读者自己推导一下为什么是这样（提示：激活函数选择了identity函数：）。<br>运行上面梯度检查的代码，我们得到的输出如下，期望的梯度和实际计算出的梯度一致，这证明我们的算法推导和代码实现确实是正确的。<br><img src="http://upload-images.jianshu.io/upload_images/2256672-c7071f47ea5f8f9a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/480" alt=""> 以上就是卷积层的实现。</p>
<h3 id="Max-Pooling层的实现"><a href="#Max-Pooling层的实现" class="headerlink" title="Max Pooling层的实现"></a>Max Pooling层的实现</h3><p>max pooling层的实现相对简单，我们直接贴出全部代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"># Max Pooling层的实现。就是一个卷积区域取最大值，形成输出。除了Max Pooing之外，常用的还有Mean Pooling——取各样本的平均值。  </span><br><span class="line">class MaxPoolingLayer(object):  </span><br><span class="line">    def __init__(self, input_width, input_height,  </span><br><span class="line">                 channel_number, filter_width,  </span><br><span class="line">                 filter_height, stride):  </span><br><span class="line">        self.input_width = input_width  </span><br><span class="line">        self.input_height = input_height  </span><br><span class="line">        self.channel_number = channel_number  </span><br><span class="line">        self.filter_width = filter_width  </span><br><span class="line">        self.filter_height = filter_height  </span><br><span class="line">        self.stride = stride  </span><br><span class="line">        self.output_width = int((input_width -filter_width) / self.stride + 1)  </span><br><span class="line">        self.output_height = int((input_height -filter_height) / self.stride + 1)  </span><br><span class="line">        self.output_array = np.zeros((self.channel_number,self.output_height, self.output_width))  </span><br><span class="line">  </span><br><span class="line">    # 前向计算。  </span><br><span class="line">    def forward(self, input_array):  </span><br><span class="line">        for d in range(self.channel_number):  </span><br><span class="line">            for i in range(self.output_height):  </span><br><span class="line">                for j in range(self.output_width):  </span><br><span class="line">                    self.output_array[d, i, j] = (get_patch(input_array[d], i, j,self.filter_width,self.filter_height,self.stride).max())   # 获取卷积区后去最大值  </span><br><span class="line">  </span><br><span class="line">    # 后向传播更新w和b  </span><br><span class="line">    def backward(self, input_array, sensitivity_array):  </span><br><span class="line">        self.delta_array = np.zeros(input_array.shape)  </span><br><span class="line">        for d in range(self.channel_number):  </span><br><span class="line">            for i in range(self.output_height):  </span><br><span class="line">                for j in range(self.output_width):  </span><br><span class="line">                    patch_array = get_patch(input_array[d], i, j,self.filter_width,self.filter_height,self.stride)  # 获取卷积区  </span><br><span class="line">                    k, l = get_max_index(patch_array)  # 获取最大值的位置  </span><br><span class="line">                    self.delta_array[d,i * self.stride + k,j * self.stride + l] = sensitivity_array[d, i, j]   # 更新误差</span><br></pre></td></tr></table></figure>
<p>全连接层的实现和上一篇文章类似，在此就不再赘述了。至此，你已经拥有了实现了一个简单的卷积神经网络所需要的基本组件。对于卷积神经网络，现在有很多优秀的开源实现，因此我们并不需要真的自己去实现一个。贴出这些代码的目的是为了让我们更好的了解卷积神经网络的基本原理。</p>
<h2 id="卷积神经网络的应用"><a href="#卷积神经网络的应用" class="headerlink" title="卷积神经网络的应用"></a>卷积神经网络的应用</h2><p>MNIST手写数字识别<br>LeNet-5是实现手写数字识别的卷积神经网络，在MNIST测试集上，它取得了0.8%的错误率。LeNet-5的结构如下：<img src="http://upload-images.jianshu.io/upload_images/2256672-31b42c6c9daa16a4.png" alt=""><br>关于LeNet-5的详细介绍，网上的资料很多，因此就不再重复了。感兴趣的读者可以尝试用我们自己实现的卷积神经网络代码去构造并训练LeNet-5（当然代码会更复杂一些）。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://fallenk.github.io/2018/05/09/CNN学习/" data-id="cjvgkvd810004zi8owltcdn0a" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li></ul>

    </footer>
  </div>
  
    
 <script src="/jquery/jquery.min.js"></script>
  <div id="random_posts">
    <h2>推荐文章</h2>
    <div class="random_posts_ul">
      <script>
          var random_count =4
          var site = {BASE_URI:'/'};
          function load_random_posts(obj) {
              var arr=site.posts;
              if (!obj) return;
              // var count = $(obj).attr('data-count') || 6;
              for (var i, tmp, n = arr.length; n; i = Math.floor(Math.random() * n), tmp = arr[--n], arr[n] = arr[i], arr[i] = tmp);
              arr = arr.slice(0, random_count);
              var html = '<ul>';
            
              for(var j=0;j<arr.length;j++){
                var item=arr[j];
                html += '<li><strong>' + 
                item.date + ':&nbsp;&nbsp;<a href="' + (site.BASE_URI+item.uri) + '">' + 
                (item.title || item.uri) + '</a></strong>';
                if(item.excerpt){
                  html +='<div class="post-excerpt">'+item.excerpt+'</div>';
                }
                html +='</li>';
                
              }
              $(obj).html(html + '</ul>');
          }
          $('.random_posts_ul').each(function () {
              var c = this;
              if (!site.posts || !site.posts.length){
                  $.getJSON(site.BASE_URI + 'js/posts.js',function(json){site.posts = json;load_random_posts(c)});
              } 
               else{
                load_random_posts(c);
              }
          });
      </script>
    </div>
  </div>

    
<nav id="article-nav">
  
    <a href="/2018/05/11/工具命令/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">上一篇</strong>
      <div class="article-nav-title">
        
          工具命令
        
      </div>
    </a>
  
  
    <a href="/2018/05/02/jupyter学习笔记/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">下一篇</strong>
      <div class="article-nav-title">jupyter学习笔记</div>
    </a>
  
</nav>

  
</article>
 
     
  <div class="comments" id="comments">
    
     
       
      <div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div>
    
       
      
      
  </div>
 
  

</section>
           
    <aside id="sidebar">
  
    

  
    
    <div class="widget-wrap">
    
      <div class="widget" id="toc-widget-fixed">
      
        <strong class="toc-title">文章目录</strong>
        <div class="toc-widget-list">
              <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#CNN学习快速学习"><span class="toc-number">1.</span> <span class="toc-text">CNN学习快速学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#背景知识"><span class="toc-number">1.1.</span> <span class="toc-text">背景知识</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#一个新的激活函数–ReLU"><span class="toc-number">1.2.</span> <span class="toc-text">一个新的激活函数–ReLU</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#全连接网络-VS-卷积网络"><span class="toc-number">1.3.</span> <span class="toc-text">全连接网络 VS 卷积网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积神经网络是啥"><span class="toc-number">1.4.</span> <span class="toc-text">卷积神经网络是啥</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#网络架构"><span class="toc-number">1.4.1.</span> <span class="toc-text">网络架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#三维的层结构"><span class="toc-number">1.4.2.</span> <span class="toc-text">三维的层结构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积神经网络输出值的计算"><span class="toc-number">1.5.</span> <span class="toc-text">卷积神经网络输出值的计算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#卷积层输出值的计算"><span class="toc-number">1.5.1.</span> <span class="toc-text">卷积层输出值的计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#用卷积公式来表达卷积层计算-暂时跳过"><span class="toc-number">1.5.2.</span> <span class="toc-text">用卷积公式来表达卷积层计算(暂时跳过)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pooling层输出值的计算"><span class="toc-number">1.5.3.</span> <span class="toc-text">Pooling层输出值的计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#全连接层"><span class="toc-number">1.5.4.</span> <span class="toc-text">全连接层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积神经网络的训练"><span class="toc-number">1.6.</span> <span class="toc-text">卷积神经网络的训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#神经网络和反向传播算法"><span class="toc-number">1.6.1.</span> <span class="toc-text">神经网络和反向传播算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#卷积层的训练"><span class="toc-number">1.6.2.</span> <span class="toc-text">卷积层的训练</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#卷积层误差项的传递"><span class="toc-number">1.6.2.1.</span> <span class="toc-text">卷积层误差项的传递</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#最简单情况下误差项的传递"><span class="toc-number">1.6.2.1.1.</span> <span class="toc-text">最简单情况下误差项的传递</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#卷积步长为S时的误差传递"><span class="toc-number">1.6.2.2.</span> <span class="toc-text">卷积步长为S时的误差传递</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#输入层深度为D时的误差传递"><span class="toc-number">1.6.2.3.</span> <span class="toc-text">输入层深度为D时的误差传递</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#filter数量为N时的误差传递"><span class="toc-number">1.6.2.4.</span> <span class="toc-text">filter数量为N时的误差传递</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#卷积层filter权重梯度的计算"><span class="toc-number">1.6.2.5.</span> <span class="toc-text">卷积层filter权重梯度的计算</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pooling层的训练"><span class="toc-number">1.6.3.</span> <span class="toc-text">Pooling层的训练</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Max-Pooling误差项的传递"><span class="toc-number">1.6.3.1.</span> <span class="toc-text">Max Pooling误差项的传递</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Mean-Pooling误差项的传递"><span class="toc-number">1.6.3.2.</span> <span class="toc-text">Mean Pooling误差项的传递</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积神经网络的实现"><span class="toc-number">1.7.</span> <span class="toc-text">卷积神经网络的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#卷积层的实现"><span class="toc-number">1.7.1.</span> <span class="toc-text">卷积层的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#卷积层初始化"><span class="toc-number">1.7.1.1.</span> <span class="toc-text">卷积层初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#卷积层前向计算的实现"><span class="toc-number">1.7.1.2.</span> <span class="toc-text">卷积层前向计算的实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#卷积层反向传播算法的实现"><span class="toc-number">1.7.1.3.</span> <span class="toc-text">卷积层反向传播算法的实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#卷积层的梯度检查"><span class="toc-number">1.7.1.4.</span> <span class="toc-text">卷积层的梯度检查</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Max-Pooling层的实现"><span class="toc-number">1.7.2.</span> <span class="toc-text">Max Pooling层的实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积神经网络的应用"><span class="toc-number">1.8.</span> <span class="toc-text">卷积神经网络的应用</span></a></li></ol></li></ol>
          </div>
      </div>
    </div>

  
    

  
    
  
    
  
    

  
    
  
    <!--微信公众号二维码-->


  
</aside>

      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-left">
      &copy; 2014 - 2019 Fallenk Liu&nbsp;|&nbsp;
      主题 <a href="https://github.com/giscafer/hexo-theme-cafe/" target="_blank">Cafe</a>
    </div>
     <div id="footer-right">
      联系方式&nbsp;|&nbsp;fallenk_liu@yeah.com
    </div>
  </div>
</footer>
 <script src="/jquery/jquery.min.js"></script>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    <img class="back-to-top-btn" src="/images/fly-to-top.png"/>
<script>
// Elevator script included on the page, already.
window.onload = function() {
  var elevator = new Elevator({
    selector:'.back-to-top-btn',
    element: document.querySelector('.back-to-top-btn'),
    duration: 1000 // milliseconds
  });
}
</script>
      

  
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "e2fb4051c49842688ce669e634bc983f",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
    

  







<!-- author:forvoid begin -->
<!-- author:forvoid begin -->

<!-- author:forvoid end -->

<!-- author:forvoid end -->


  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      })
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      })
    </script>
    <script type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


 <script src="/js/is.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/elevator.js"></script>
  </div>
</body>
</html>