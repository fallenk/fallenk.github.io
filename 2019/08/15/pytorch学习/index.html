<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>pytorch学习 | Fallenk&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Introduction to Neural Network">
<meta name="keywords" content="pytorch学习">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch学习">
<meta property="og:url" content="https://fallenk.github.io/2019/08/15/pytorch学习/index.html">
<meta property="og:site_name" content="Fallenk&#39;s Blog">
<meta property="og:description" content="Introduction to Neural Network">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/1.png">
<meta property="og:image" content="https://fallenk.github.io/2019/08/15/pytorch学习/2.gif">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/3.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/4.png">
<meta property="og:image" content="https://morvanzhou.github.io/static/results/torch/2-3-1.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/7.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/8.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/9.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/10.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/11.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/12.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Library/Application%20Support/typora-user-images/image-20190818163532768.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Library/Application%20Support/typora-user-images/image-20190818163647605.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Library/Application%20Support/typora-user-images/image-20190818163807844.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/16.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Library/Application%20Support/typora-user-images/image-20190818163958622.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/18.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/19.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Library/Application%20Support/typora-user-images/image-20190819103829081.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/21.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Library/Application%20Support/typora-user-images/image-20190819104114779.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Library/Application%20Support/typora-user-images/image-20190822214243233.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Library/Application%20Support/typora-user-images/image-20190822214343843.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/35.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/36.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/37.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Library/Application%20Support/typora-user-images/image-20190822215958596.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Library/Application%20Support/typora-user-images/image-20190822220343752.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/40.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Library/Application%20Support/typora-user-images/image-20190824171847757.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/42.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/43.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Library/Application%20Support/typora-user-images/image-20190825164413250.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/45.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/23.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Library/Application%20Support/typora-user-images/image-20190819202632570.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Library/Application%20Support/typora-user-images/image-20190819202806837.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/26.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/27.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/28.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/29.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/30.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/32.gif">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/31.gif">
<meta property="og:updated_time" content="2019-08-25T08:47:18.877Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="pytorch学习">
<meta name="twitter:description" content="Introduction to Neural Network">
<meta name="twitter:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/1.png">
  
    <link rel="alternate" href="/atom.xml" title="Fallenk&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://fallenk.github.io"></form>
      </div>
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">首页</a>
        
          <a class="main-nav-link" href="/archives">归档</a>
        
          <a class="main-nav-link" href="/about">关于</a>
        
      </nav>
      
    </div>
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Fallenk&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">经历，心得，笔记，目标</a>
        </h2>
      
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-pytorch学习" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/15/pytorch学习/" class="article-date">
  <time datetime="2019-08-15T11:49:17.000Z" itemprop="datePublished">2019-08-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/pytorch/">pytorch</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      pytorch学习
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h1 id="Introduction-to-Neural-Network"><a href="#Introduction-to-Neural-Network" class="headerlink" title="Introduction to Neural Network"></a>Introduction to Neural Network</h1><a id="more"></a>
<blockquote>
<p>在人工神经网络里，没有产生新连接，网络固定不变</p>
</blockquote>
<blockquote>
<p>反向传播: 对比预测答案和真实答案的差别，再将差别去 反向传播 调整参数，提高正确率</p>
</blockquote>
<blockquote>
<p>详细的训练: 每个神经元都有一个自己的activate function, 刺激行为</p>
</blockquote>
<blockquote>
<p>神经网络: 梯度下降公式-》 优化问题optimization</p>
<p> 求导求微分，Gradient Descent </p>
</blockquote>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/1.png" alt="image-20190815204001644"></p>
<p>沿着梯度去下降，得到最小的W值 -&gt; 优化问题</p>
<p>迁移学习: 拆掉输出层，保留分辨能力，添加其他层，进行另外的功能</p>
<h2 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h2><p><a href="http://pytorch.org/" target="_blank" rel="noopener">PyTorch</a> 是 <a href="http://pytorch.org/" target="_blank" rel="noopener">Torch</a> 在 Python 上的衍生. 因为 <a href="http://pytorch.org/" target="_blank" rel="noopener">PyTorch</a> 是一个使用 <a href="http://pytorch.org/" target="_blank" rel="noopener">PyTorch</a> 语言的神经网络库, Torch 很好用, 但是 Lua 又不是特别流行, 所有开发团队将 Lua 的 Torch 移植到了更流行的语言 Python 上. 是的 PyTorch 一出生就引来了剧烈的反响. 为什么呢?</p>
<p>而且如果你知道 <a href="http://www.numpy.org/" target="_blank" rel="noopener">Numpy</a>, PyTorch 说他就是在神经网络领域可以用来替换 numpy 的模块.</p>
<h3 id="神经网络在做什么"><a href="#神经网络在做什么" class="headerlink" title="神经网络在做什么"></a>神经网络在做什么</h3><p>神经网络在学习拟合线条(回归):</p>
<p><img src="/2019/08/15/pytorch学习/2.gif" alt="Why Pytorch?"></p>
<h3 id="PyTorch-和-Tensorflow"><a href="#PyTorch-和-Tensorflow" class="headerlink" title="PyTorch 和 Tensorflow"></a>PyTorch 和 Tensorflow</h3><p>据 PyTorch 自己介绍, 他们家的最大优点就是<strong>建立的神经网络是动态的</strong>, 对比静态的 Tensorflow, 他能更有效地处理一些问题, 比如说 <strong>RNN 变化时间长度的输出</strong>. 而我认为, 各家有各家的优势和劣势, 所以我们要以中立的态度. 两者都是大公司, Tensorflow 自己说自己在分布式训练上下了很大的功夫, 那我就默认 Tensorflow 在这一点上要超出 PyTorch, 但是 Tensorflow 的静态计算图使得他在 RNN 上有一点点被动 (虽然它用其他途径解决了), 不过用 PyTorch 的时候, 你会对这种动态的 RNN 有更好的理解.</p>
<h2 id="Torch-或-Numpy"><a href="#Torch-或-Numpy" class="headerlink" title="Torch 或 Numpy"></a>Torch 或 Numpy</h2><p>Torch 自称为神经网络界的 Numpy, 因为他能将 torch 产生的 tensor 放在 GPU 中加速运算 (前提是你有合适的 GPU), 就像 Numpy 会把 array 放在 CPU 中加速运算. 所以神经网络的话, 当然是<strong>用 Torch 的 tensor 形式数据</strong>最好咯. 就像 Tensorflow 当中的 tensor 一样.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np_data = np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">torch_data = torch.from_numpy(np_data)</span><br><span class="line">tensor2array = torch_data.numpy()</span><br><span class="line"></span><br><span class="line">np.sin(data) == torch.sin(data)</span><br></pre></td></tr></table></figure>
<p>除了简单的计算, <strong>矩阵运算</strong>才是神经网络中最重要的部分. 所以我们展示下矩阵的乘法. 注意一下包含了一个 numpy 中可行, 但是 torch 中不可行的方式.</p>
<p>variable就是存放神经网络参数的东西，并且神经网络优化一般都是优化类型为variable的节点</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># matrix multiplication 矩阵点乘</span></span><br><span class="line">data = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">tensor = torch.FloatTensor(data)</span><br><span class="line"><span class="comment"># correct method</span></span><br><span class="line">print(</span><br><span class="line">    <span class="string">"\nmatrix multiplication (matmul)"</span>,</span><br><span class="line">    <span class="string">"\nnumpy"</span>, np.matmul(data, data),</span><br><span class="line">    <span class="string">"\ntorch"</span>, torch.mm(tensor, tensor)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="变量-Variable"><a href="#变量-Variable" class="headerlink" title="变量 (Variable)"></a>变量 (Variable)</h2><p>在 Torch 中的 Variable 就是一个存放会变化的值的地理位置. 里面的值会不停的变化. 就像一个裝鸡蛋的篮子, 鸡蛋数会不停变动. 那谁是里面的鸡蛋呢, 自然就是 Torch 的 Tensor 咯. 如果用一个 Variable 进行计算, 那返回的也是一个同类型的 Variable.</p>
<p>我们定义一个 Variable:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable  <span class="comment"># torch 中 Variable模块</span></span><br><span class="line"><span class="comment"># 先 生鸡蛋</span></span><br><span class="line">tensor = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="comment"># 把鸡蛋 放到篮子中， require_grad是参不参与误差反向传播,要不要计算梯度</span></span><br><span class="line">variable = Variable(tensor, requires_grad = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">print(</span><br><span class="line">	tensor</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="Variable-计算-梯度"><a href="#Variable-计算-梯度" class="headerlink" title="Variable 计算, 梯度"></a>Variable 计算, 梯度</h3><p>我们再对比下Tensor的计算和variable的计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line">tensor = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">t_out = torch.mean(tensor*tensor)</span><br><span class="line">v_out = torch.mean(variable*variable)</span><br><span class="line">print(t_out) <span class="comment"># 7.5</span></span><br><span class="line">print(v_out) <span class="comment"># 7.5</span></span><br></pre></td></tr></table></figure>
<p>到目前为止, 我们看不出什么不同, <strong>但是时刻记住, Variable 计算时, 它在背景幕布后面一步步默默地搭建着一个庞大的系统, 叫做计算图, computational graph. 这个图是用来干嘛的? 原来是将所有的计算步骤 (节点) 都连接起来, 最后进行误差反向传递的时候, 一次性将所有 variable 里面的修改幅度 (梯度) 都计算出来, 而 tensor 就没有这个能力啦.</strong></p>
<p><code>v_out = torch.mean(variable*variable)</code> 就是在计算图中添加的一个计算步骤, 计算误差反向传递的时候有他一份功劳, 我们就来举个例子:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable <span class="comment"># 导入 Variable 模块</span></span><br><span class="line">tensor = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]) </span><br><span class="line">variable = Variable(tensor, requires_grad = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">t_out = torch.mean(tensor*tensor)</span><br><span class="line">v_out = torch.mean(variable*variable)</span><br><span class="line">print(t_out)</span><br><span class="line">print(v_out)</span><br><span class="line"></span><br><span class="line">v_out.backward()  <span class="comment"># 模拟v_out误差反向传播</span></span><br><span class="line"><span class="comment"># Variable是计算图是的一部分</span></span><br><span class="line"><span class="comment"># v_out 是 1/4 * sum(variable*variable) 这是计算图中的 v_out 计算步骤</span></span><br><span class="line"><span class="comment"># 针对 v_out 的梯度是 d(v_out)/d(variable) = 1/2 * variable</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化的梯度</span></span><br><span class="line">print(</span><br><span class="line">    <span class="string">"variable.grad: "</span>,variable.grad</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="获取Variable-里面的数据"><a href="#获取Variable-里面的数据" class="headerlink" title="获取Variable 里面的数据"></a>获取Variable 里面的数据</h3><p>直接<code>print(variable)</code>只会输出Variable 形式的数据，很多时候用不了(plt画图)，转化成Tensor</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(variable) <span class="comment"># Variable 形式</span></span><br><span class="line">print(Variable.data) <span class="comment"># tensor 形式</span></span><br><span class="line">print(variable.data.numpy()) <span class="comment"># numpy形式</span></span><br></pre></td></tr></table></figure>
<h2 id="激励函数-Activation-Function"><a href="#激励函数-Activation-Function" class="headerlink" title="激励函数 Activation Function"></a>激励函数 Activation Function</h2><p>为什么需要？解决 日常生活总不能用 线性方程解决的问题</p>
<p>Linear;  NonLinear; <code>y = Wx</code>   =&gt;  <code>y = AF(Wx)</code> ; W 就是我们要求的参数, y 是预测值, x 是输入值.</p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/3.png" alt="image-20190817100050191"></p>
<h3 id="激励函数"><a href="#激励函数" class="headerlink" title="激励函数"></a>激励函数</h3><p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/4.png" alt="image-20190817100706478"></p>
<p> AF 就是指的激励函数(本身就是非线性的，必须可微分). 激励函数拿出自己最擅长的”掰弯利器”, 套在了原函数上 用力一扭, 原来的 Wx 结果就被扭弯了。</p>
<p>你甚至可以创造自己的激励函数来处理自己的问题, 不过要确保的是这些激励函数必须是可以微分的, 因为在 backpropagation 误差反向传递的时候, 只有这些可微分的激励函数才能把误差传递回去。当你使用特别多层的神经网络, 在掰弯的时候, 玩玩不得随意选择利器. 因为这会涉及到梯度爆炸, 梯度消失的问题。卷积神经网络 Convolutional neural networks 的卷积层中, 推荐的激励函数是 relu. 在循环神经网络中 recurrent neural networks, 推荐的是 tanh 或者是 relu</p>
<h3 id="pytorch-activation-function"><a href="#pytorch-activation-function" class="headerlink" title="pytorch activation function"></a>pytorch activation function</h3><p><img src="https://morvanzhou.github.io/static/results/torch/2-3-1.png" alt="æ¿å±å½æ° (./pytorch学习/6.png)"></p>
<p>神经网络中的每一层出来都是线性的函数关系，而在日常生活许多都是非线性关系。因此我们需要用激活函数将线性网络转成非线性结果。就是让神经网络可以描述非线性问题的步骤, 是神经网络变得更强大.</p>
<h3 id="Torch中的激励函数"><a href="#Torch中的激励函数" class="headerlink" title="Torch中的激励函数"></a>Torch中的激励函数</h3><p>Torch中的激励函数:  <code>relu, sigmoid, tanh, softplus</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.funcational <span class="keyword">as</span> F <span class="comment"># 激励函数在此</span></span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="comment"># 做数据</span></span><br><span class="line">x = torch.linespace(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">200</span>)  <span class="comment"># x data (Tensor), shape=(100, 1)</span></span><br><span class="line">x = Variable(x)</span><br></pre></td></tr></table></figure>
<p>接着就是做生成不同的激励函数数据:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x_np = x.data.numpy() <span class="comment"># 换成 numpy, array,出图用</span></span><br><span class="line"><span class="comment"># 常用的 激励函数</span></span><br><span class="line">y_relu = F.relu(x).data.numpy()  <span class="comment"># 0 -&gt; </span></span><br><span class="line">y_sigmoid = F.sigmoid(x).data.numpy() <span class="comment"># 0 ~ 1</span></span><br><span class="line">y_tanh = F.tanh(x).data.numpy() <span class="comment"># -1 ~ 1</span></span><br><span class="line">y_softplus = F.softplus(x).data.numpy() <span class="comment"># 0 ~</span></span><br><span class="line"><span class="comment"># y_softmax = F.softmax(x) softmax 比较特殊， 不能直接显示，他是关于概率的，用于分类</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># python 的可视化模块</span></span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">1</span>, figsize = (<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">221</span>)</span><br><span class="line">plt.plot(x_np, y_relu, c=<span class="string">'red'</span>, label=<span class="string">"relu"</span>)</span><br><span class="line">plt.ylim((<span class="number">-1</span>, <span class="number">5</span>))</span><br><span class="line">plt.legend(loc=<span class="string">"best"</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">223</span>)</span><br><span class="line">plt.plot(x_np, y_sigmoid, c=<span class="string">"blue"</span>, label=<span class="string">"sigmoid"</span>)</span><br><span class="line">plt.ylim((<span class="number">-0.1</span>, <span class="number">1.2</span>))</span><br><span class="line">plt.legend(loc=<span class="string">"best"</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">222</span>)</span><br><span class="line">plt.plot(x_np, y_tanh, c=<span class="string">"orange"</span>, label=<span class="string">"tanh"</span>)</span><br><span class="line">plt.ylim((<span class="number">-1.1</span>, <span class="number">1.1</span>))</span><br><span class="line">plt.legend(loc=<span class="string">"best"</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">224</span>)</span><br><span class="line">plt.plot(x_np, y_softplus, c=<span class="string">"yellow"</span>, label=<span class="string">"softplus"</span>)</span><br><span class="line">plt.ylim((<span class="number">-1</span>, <span class="number">5</span>))</span><br><span class="line">plt.legend(loc=<span class="string">"best"</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/7.png" alt="image-20190817160302989"></p>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>神经网络分为两类: 回归和分类； </p>
<p>回归，就是结果为一些系列连续的值，如f(房价) = W(大小，地点等)；</p>
<p>分类，就是判断结果的类别， 如图像中判断 是 猫还是狗</p>
<h2 id="关系拟合-回归"><a href="#关系拟合-回归" class="headerlink" title="关系拟合(回归)"></a>关系拟合(回归)</h2><p>来见证神经网络是如何通过简单的形式将一群数据用一条线条来表示. 或者说, 是如<strong>何在数据当中找到他们的关系, 然后用神经网络模型来建立一个可以代表他们关系的线条</strong>.</p>
<h3 id="建立数据集"><a href="#建立数据集" class="headerlink" title="建立数据集"></a>建立数据集</h3><p>我们创建一些假数据来模拟真实的情况. 比如一个一元二次函数: <code>y = a * x^2 + b</code>, 我们给 <code>y</code> 数据加上一点噪声来更加真实的展示它.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = torch.unsqueeze(torch.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>), dim=<span class="number">1</span>) <span class="comment"># x data (tensor),shape=(100, 1)</span></span><br><span class="line">y = x.pow(<span class="number">2</span>) + <span class="number">0.2</span>*torch.rand(x.size())  <span class="comment"># nosiy y data (tensor), shape=(100, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 动画</span></span><br><span class="line">plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="建立神经网络"><a href="#建立神经网络" class="headerlink" title="建立神经网络"></a>建立神经网络</h3><p>建立一个神经网络我们可以直接运用 torch 中的体系. <strong>先定义所有的层属性</strong>(<code>__init__()</code>), 然后再一层层搭建(<code>forward(x)</code>)<strong>层与层的关系链接</strong>. 建立关系的时候, 我们会用到激励函数, </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.funcational <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span> <span class="comment"># 1. 继承 torch 的Module  </span></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span>   <span class="comment"># 2. 初始化模块， 去继承， 搭建完成每层的定义，信息</span></span><br><span class="line">      super(Net, self).__init__() <span class="comment"># 2. 继承关系 __init__ 功能</span></span><br><span class="line">      <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span>  <span class="comment"># 3. 完成层与层之间的 forward的联系, 前向传播</span></span><br><span class="line">      <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.funcational <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span> <span class="comment"># 1. 继承 torch 的Module  </span></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_features, n_hidden, n_output)</span>:</span>   <span class="comment"># 2. 初始化模块， 去继承， 搭建完成每层的定义，信息; 神经元的个数</span></span><br><span class="line">      super(Net, self).__init__() <span class="comment"># 2. 继承关系 __init__ 功能</span></span><br><span class="line">      <span class="comment"># 4. 每一层的信息都是 模块的一个属性,属性的内容就是一层神经网络: 多少个输入，多少个输出，做什么操作</span></span><br><span class="line">      <span class="comment"># 4. 这是只是搭建了各层神经元的内容</span></span><br><span class="line">      self.hidden = torch.nn.Linear(n_features, n_hidden)</span><br><span class="line">      self.predict = torch.nn.Linear(n_hidden, n_output)</span><br><span class="line">      <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span>  <span class="comment"># 3. 完成层与层之间的 forward的联系, 前向传播流程</span></span><br><span class="line">      <span class="comment"># 5. 层与层之间的关系， 整个搭建过程，用激活函数激活</span></span><br><span class="line">      <span class="comment"># 正向传播输入值, 神经网络分析出输出值</span></span><br><span class="line">      x = F.relu(self.hidden(x))</span><br><span class="line">      x = self.predict(x)  <span class="comment"># 输出层不需要用激活函数去截断</span></span><br><span class="line">      <span class="keyword">return</span> x</span><br><span class="line">net = Net(n_features=<span class="number">1</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">1</span>)</span><br><span class="line">print(net)<span class="comment"># net 的结构</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Net (</span></span><br><span class="line"><span class="string">  (hidden): Linear (1 -&gt; 10)</span></span><br><span class="line"><span class="string">  (predict): Linear (10 -&gt; 1)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h3 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h3><p>训练步骤很简单:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置 优化函数；设置 误差函数； 迭代训练: 喂数据给模型，计算得到 loss，更新梯度，参数优化</span></span><br><span class="line"><span class="comment"># optimizer 是训练工具 优化参数</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.2</span>) <span class="comment"># 传入 net所有参数,学习率</span></span><br><span class="line">loss_func = torch.nn.MSELoss() <span class="comment"># 预测值和真实值的误差计算公式(均方差)回归 mean squre error； 分类: cross entropy</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">  prediction = net(x)  <span class="comment"># 喂给 net 训练数据 x； 输出 预测值</span></span><br><span class="line">  loss = loss_func(prediction, y)   <span class="comment"># 计算 误差, 拿去反向传播</span></span><br><span class="line">  </span><br><span class="line">  optimizer.zero_grad()  <span class="comment"># 清空上一步的残余更新参数值</span></span><br><span class="line">  loss.backward()  <span class="comment"># 误差反向传播，计算参数更新值</span></span><br><span class="line">  optimizer.step()  <span class="comment"># 用optimizer去优化 将参数更新到施加到 net 的parameters 上</span></span><br></pre></td></tr></table></figure>
<h3 id="可视化训练过程"><a href="#可视化训练过程" class="headerlink" title="可视化训练过程"></a>可视化训练过程</h3><p>理解如何训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.ion()  <span class="comment"># 画图</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.2</span>)</span><br><span class="line">loss_func = torch.nn.MSELoss()</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">200</span>):</span><br><span class="line">  prediction = net(x)</span><br><span class="line">  loss = loss_func(prediction, y)  </span><br><span class="line">  optimizer.zero_grad()</span><br><span class="line">  loss.backward()</span><br><span class="line">  optimizer.step()</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> t%<span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># plt and show learning process</span></span><br><span class="line">    plt.cla()</span><br><span class="line">    plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">    plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">'r-'</span>, lw=<span class="number">5</span>)</span><br><span class="line">    plt.text(<span class="number">0.5</span>, <span class="number">0.</span> <span class="string">"Loss=%.4f"</span> % loss.data.numpy(), fontdict=&#123;<span class="string">'size'</span>: <span class="number">20</span>, <span class="string">'color'</span>: <span class="string">'red'</span>&#125;)</span><br><span class="line">    plt.pause(<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="区分类型-分类"><a href="#区分类型-分类" class="headerlink" title="区分类型(分类)"></a>区分类型(分类)</h2><p>Classification 分类;  x 是32bit FloatTensor, y 64bit FloatTensor</p>
<h3 id="建立数据集-1"><a href="#建立数据集-1" class="headerlink" title="建立数据集"></a>建立数据集</h3><p>创建一些假数据来模拟真实的情况. 比如两个二次分布的数据, 不过他们的均值都不一样.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟分类数据</span></span><br><span class="line">n_data = torch.ones((<span class="number">100</span>, <span class="number">2</span>))  <span class="comment"># 数据基本形态， shape=(100, 2)</span></span><br><span class="line">x0 = torch.normal(<span class="number">2</span>*n_data, <span class="number">1</span>) <span class="comment"># 类型 0 x (tensor), shape=(100,2)</span></span><br><span class="line">y0 = torch.zeros((<span class="number">100</span>,)) <span class="comment"># y0 = torch.zeros((100, )) 类型 0 y data (tensor), shape=(100,)</span></span><br><span class="line">x1 = torch.normal(<span class="number">-2</span>*n_data, <span class="number">1</span>) <span class="comment"># 类型 1 x data (tensor),shape=(100, 2)</span></span><br><span class="line">y1 = torch.ones((<span class="number">100</span>,)) <span class="comment"># 类型1 y data (tensor),shape=(100,)</span></span><br><span class="line"><span class="comment"># pytorch的数据形式 </span></span><br><span class="line">x = torch.cat((x0, x1), <span class="number">0</span>).type(torch.FloatTensor) <span class="comment"># FloatTensor = 32-bit floating</span></span><br><span class="line">y = torch.cat((y0, y1),).type(torch.LongTensor) <span class="comment"># LongTensor = 64-bit integer</span></span><br><span class="line">plt.scatter(x.data.numpy()[:, <span class="number">0</span>], x.data.numpy()[:, <span class="number">1</span>], c=y.data.numpy(), s=<span class="number">100</span>, lw=<span class="number">0</span>, cmap=<span class="string">'RdYlGn'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/8.png" alt="image-20190818111523023"></p>
<h3 id="建立神经网络-1"><a href="#建立神经网络-1" class="headerlink" title="建立神经网络"></a>建立神经网络</h3><p>建立一个神经网络我们可以直接运用 torch 中的体系: </p>
<ol>
<li>先定义所有的层属性(<code>__init__()</code>) , 定义神经元个数</li>
<li>再一层层搭建(<code>forward(x)</code>)层于层的关系链接.建立关系的时候, 我们会用到激励函数</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F <span class="comment"># activation</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">  <span class="comment"># 神经元的节点数</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_feature, n_hidden, n_output)</span>:</span></span><br><span class="line">    super(Net, self).__init__() <span class="comment"># 继承 __init__ 功能</span></span><br><span class="line">    self.hidden = torch.nn.Linear(n_feature, n_hidden)</span><br><span class="line">    self.predict = torch.nn.Linear(n_hidden, n_output)</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>：</span></span><br><span class="line">  	x = F.relu(self.hidden(x))</span><br><span class="line">    x = self.predict(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">net = Net(n_feature=<span class="number">2</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">2</span>) <span class="comment"># 几个类别就几个 output</span></span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure>
<h3 id="训练网络-1"><a href="#训练网络-1" class="headerlink" title="训练网络"></a>训练网络</h3><p>用优化器 训练 网络参数；loss 反向传播</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.02</span>) <span class="comment"># set up optimizer, 传入参数，学习率</span></span><br><span class="line"><span class="comment"># 算误差时，注意真实值！ 不是one-hot形式的， 而是1D Tensor，(batch,)</span></span><br><span class="line"><span class="comment"># 但是预测值是2D tensor (batch, n_classes)</span></span><br><span class="line">loss_func = torch.nn.CrossEntropyLoss()</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">  out = net(x) <span class="comment"># 喂给 net 训练数据 x, 输出分析值</span></span><br><span class="line">  loss = loss_func(out, y)  <span class="comment"># 计算两者的误差</span></span><br><span class="line">  </span><br><span class="line">  optimizer.zero_grad() <span class="comment"># 清空上一步的残余更新参数值</span></span><br><span class="line">  loss.backward() <span class="comment"># 误差反向传播, 计算参数更新值 </span></span><br><span class="line">  optimizer.step()  <span class="comment"># 将参数更新值施加到 net 的 parameters 上</span></span><br></pre></td></tr></table></figure>
<h3 id="可视化训练过程-1"><a href="#可视化训练过程-1" class="headerlink" title="可视化训练过程"></a>可视化训练过程</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.ion()</span><br><span class="line">plt.show()</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">loss_func = torch.nn.CrossEntropyLoss()</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">	out = net(x)</span><br><span class="line">	loss = loss_func(out, y)</span><br><span class="line">	optimizer.zero_grad()</span><br><span class="line">	loss.backward()</span><br><span class="line">	optimizer.step()</span><br><span class="line">	<span class="keyword">if</span> t%<span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">		plt.cla()</span><br><span class="line">		<span class="comment"># 过了一道 softmax 的激励函数后的最大概率才是预测值</span></span><br><span class="line">		prediction = torch.max(F.softmax(out), <span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">		pred_y = prediction.data.numpy().squeeze()</span><br><span class="line">		target_y = y.data.numpy()</span><br><span class="line">    plt.scatter(x.data.numpy()[:,<span class="number">0</span>], x.data.numpy()[:,<span class="number">1</span>], c=pred_y, s=<span class="number">100</span>, lw=<span class="number">0</span>, cmap=<span class="string">'RdYlGn'</span>)</span><br><span class="line">    accuracy = sum(pred_y == target_y) /<span class="number">200</span></span><br><span class="line">    plt.text(<span class="number">1.5</span>, <span class="number">-4</span>, <span class="string">'Accuracy=%.2f'</span>%accuracy, fontdict=&#123;<span class="string">'size'</span>:<span class="number">20</span>, <span class="string">'color'</span>:<span class="string">'red'</span>&#125;)</span><br><span class="line">    plt.pause(<span class="number">0.1</span>)</span><br><span class="line">plt.ioff() <span class="comment">#停止画图</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/9.png" alt="image-20190818145029869"></p>
<h2 id="快速搭建"><a href="#快速搭建" class="headerlink" title="快速搭建"></a>快速搭建</h2><p>之前的搭法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_feature, n_hidden, n_output)</span>:</span></span><br><span class="line">		super(Net, self).__init__()</span><br><span class="line">    self.hidden = torch.nn.Linear(n_feature, n_hidden)</span><br><span class="line">    self.output = torch.nn.Linear(n_hidden, n_output)</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    x = F.relu(self.hidden(x))</span><br><span class="line">    x = self.output(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line">net1 = Net(<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">print(net) <span class="comment"># 这是我们用这种方式搭建的 net1</span></span><br></pre></td></tr></table></figure>
<p>我们<strong>用 class 继承了一个 torch 中的神经网络结构, 然后对其进行了修改</strong>, 不过还有更快的一招, 用一句话就概括了上面所有的内容!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">net2 = torch.nn.Sequential( <span class="comment"># Sequential 一层一层的积累神经层,激活函数也是一层神经</span></span><br><span class="line">	torch.nn.Linear(<span class="number">1</span>, <span class="number">10</span>),</span><br><span class="line">	torch.nn.ReLU(), <span class="comment"># 调用了类的构造方法</span></span><br><span class="line">	torch.mm.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line">print(net1)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Net (</span></span><br><span class="line"><span class="string">  (hidden): Linear (1 -&gt; 10)</span></span><br><span class="line"><span class="string">  (predict): Linear (10 -&gt; 1)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">print(net2)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Sequential (</span></span><br><span class="line"><span class="string">  (0): Linear (1 -&gt; 10)</span></span><br><span class="line"><span class="string">  (1): ReLU ()</span></span><br><span class="line"><span class="string">  (2): Linear (10 -&gt; 1)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<p>我们会发现 <code>net2</code> 多显示了一些内容, 这是为什么呢? 原来他把激励函数也一同纳入进去了, 但是 <code>net1</code> 中, 激励函数实际上是在 <code>forward()</code> 功能中才被调用的. 这也就说明了, 相比 <code>net2</code>, <code>net1</code> 的好处就是, 你可以根据你的个人需要更加个性化你自己的前向传播过程, 比如(RNN). 不过如果你不需要七七八八的过程, 相信 <code>net2</code> 这种形式更适合你.</p>
<h2 id="保存提取"><a href="#保存提取" class="headerlink" title="保存提取"></a>保存提取</h2><p>训练好了一个模型, 我们当然想要保存它, 留到下次要用的时候直接提取直接用</p>
<h3 id="保存"><a href="#保存" class="headerlink" title="保存"></a>保存</h3><p>我们快速建造数据，搭建神经网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># fake data</span></span><br><span class="line">x = torch.unsqueeze(torch.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>), dim=<span class="number">1</span>) <span class="comment"># x data (tensor),shape=(100, 1)</span></span><br><span class="line">y = x.pow(<span class="number">2</span>)+<span class="number">0.2</span>*torch.rand(x.size()) <span class="comment"># noisy y data (tensor), shape=(100, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="comment"># set up net</span></span><br><span class="line">  net1 = torch.nn.Sequential(</span><br><span class="line">  	torch.nn.Linear(<span class="number">1</span>, <span class="number">10</span>),</span><br><span class="line">    torch.nn.ReLu(),</span><br><span class="line">    torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">  )</span><br><span class="line">  optimizer = torch.optim.SGD(net1.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">  loss_func = torch.nn.MSELoss()</span><br><span class="line">  <span class="comment"># train</span></span><br><span class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">2000</span>):</span><br><span class="line">		prediction = net1(x)</span><br><span class="line">    loss = loss_func(prediction, y)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backkward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">  <span class="comment">#接下来我们有两种途径来保存</span></span><br><span class="line">  torch.save(net1, <span class="string">'net.pkl'</span>) <span class="comment"># save entire net</span></span><br><span class="line">  torch.save(net1.state_dict(), <span class="string">'net_params.pkl'</span>) <span class="comment"># 只保存网络中的参数(速度快，占内存少)</span></span><br><span class="line">  <span class="comment"># plot result</span></span><br><span class="line">  prediction = net1(x)</span><br><span class="line">  plt.figure(<span class="number">1</span>, figsize=(<span class="number">10</span>, <span class="number">3</span>))</span><br><span class="line">  plt.subplot(<span class="number">131</span>)</span><br><span class="line">  plt.title(<span class="string">'Net1'</span>)</span><br><span class="line">  plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">  plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">'r-'</span>, lw=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<h2 id="提取网络"><a href="#提取网络" class="headerlink" title="提取网络"></a>提取网络</h2><p>提取整个神经网络, 网络大的时候可能会比较慢.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">restore_net</span><span class="params">()</span>:</span></span><br><span class="line">	<span class="comment"># restore entire net1 to net2</span></span><br><span class="line">  net2 = torch.load(<span class="string">'net.pkl'</span>)</span><br><span class="line">  prediction = net2(x)</span><br><span class="line">  plt.subplot(<span class="number">132</span>)</span><br><span class="line">  plt.title(<span class="string">'Net2'</span>)</span><br><span class="line">  plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">  plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">'r-'</span>, lw=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<h2 id="只提取网络参数"><a href="#只提取网络参数" class="headerlink" title="只提取网络参数"></a>只提取网络参数</h2><p>提取所有的参数, 然后再放到你的新建网络中.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">restore_params</span><span class="params">()</span>:</span></span><br><span class="line">	<span class="comment"># set up net3</span></span><br><span class="line">  net3 = torch.nn.Sequential(</span><br><span class="line">  	torch.nn.Linear(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">    torch.nn.ReLU()</span><br><span class="line">    torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">  )</span><br><span class="line">  <span class="comment"># load parameters to net3</span></span><br><span class="line">  net3.load_state_dict(torch.load(<span class="string">'net_params.pkl'</span>))</span><br><span class="line">  prediction = net3(x)</span><br><span class="line">  plt.subplot(<span class="number">133</span>)</span><br><span class="line">  plt.title(<span class="string">'Net3'</span>)</span><br><span class="line">  plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">  plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">'r-'</span>, lw=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<h2 id="显示结果"><a href="#显示结果" class="headerlink" title="显示结果"></a>显示结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存 net1 (1. 整个网络, 2. 只有参数)</span></span><br><span class="line">save()</span><br><span class="line"><span class="comment"># 提取整个网络</span></span><br><span class="line">restore_net()</span><br><span class="line"><span class="comment"># 提取网络参数, 复制到新网络</span></span><br><span class="line">restore_params()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/10.png" alt="ä¿å­æå"></p>
<h2 id="批训练"><a href="#批训练" class="headerlink" title="批训练"></a>批训练</h2><p>Torch中有个帮助整理数据结构，<code>DataLoader</code>，用来包装自己的数据，进行批训练，批训练途径:</p>
<h3 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h3><p><code>DataLoader</code> 是 torch 给你用来<strong>包装你的数据的工具</strong>. 所以你要讲自己的 (numpy array 或其他) 数据形式装换成 <strong>Tensor</strong>, 然后<strong>再放进这个包装器</strong>中. 使用 <code>DataLoader</code> 有什么好处呢? 就是他们帮你有效地迭代数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">, ximport torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">5</span> <span class="comment"># 批训练的数据个数</span></span><br><span class="line"><span class="comment"># produce dataloader: tensor -&gt; dataset -&gt; torch dataset -&gt; dataloader</span></span><br><span class="line">x = torch.linspace(<span class="number">1</span>, <span class="number">10</span>, <span class="number">10</span>) <span class="comment"># x data (torch tensor)</span></span><br><span class="line">y = torch.linspace(<span class="number">10</span>, <span class="number">1</span>, <span class="number">10</span>) <span class="comment"># y data (torch tensor)</span></span><br><span class="line"><span class="comment"># 先转换成 torch 能识别的Dataset</span></span><br><span class="line">torch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)</span><br><span class="line"><span class="comment"># 把dataset 放入 DataLoder</span></span><br><span class="line">loader = Data.DataLoader(</span><br><span class="line">	dataset=torch_dataset, <span class="comment"># torch TensorDataset format</span></span><br><span class="line">  batch_size = BATCH_SIZE, <span class="comment"># mini batch size/</span></span><br><span class="line">  shuffle=<span class="keyword">True</span>, <span class="comment"># 打乱</span></span><br><span class="line">  num_workers=<span class="number">2</span>, <span class="comment"># 多线程</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">3</span>):  <span class="comment"># 训练所有 !整套! 数据 3 次</span></span><br><span class="line">	<span class="keyword">for</span> step, (batch_x, batch_y) <span class="keyword">in</span> enumerate(loader): <span class="comment"># 每一步 loader释放一小批数据来学习</span></span><br><span class="line">    <span class="comment"># 训练的地方</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 打印数据</span></span><br><span class="line">    print(<span class="string">'Epoch: '</span>, epoch, <span class="string">'| Step: '</span>, step, <span class="string">'| batch x: '</span>, batch_x.numpy(), <span class="string">'| batch y: '</span>, batch_y.numpy())</span><br></pre></td></tr></table></figure>
<p>可以看出, 每步都导出了5个数据进行学习. 然后每个 epoch 的导出数据都是先打乱了以后再导出.</p>
<p>真正方便的还不是这点. 如果我们改变一下 <code>BATCH_SIZE = 8</code>, 这样我们就知道, <code>step=0</code> 会导出8个数据, 但是, <code>step=1</code> 时数据库中的数据不够 8个, 这时怎么办呢:</p>
<p>这时, 在 <code>step=1</code> 就只给你返回这个 epoch 中剩下的数据就好了.</p>
<h2 id="加速神经网络训练-Speed-Up-Training"><a href="#加速神经网络训练-Speed-Up-Training" class="headerlink" title="加速神经网络训练 (Speed Up Training)"></a>加速神经网络训练 (Speed Up Training)</h2><p>包括以下几种模式:</p>
<ul>
<li>Stochastic Gradient Descent (SGD)</li>
<li>Momentum</li>
<li>AdaGrad</li>
<li>RMSProp</li>
<li>Adam</li>
</ul>
<h3 id="Stochastic-Gradient-Descent-SGD"><a href="#Stochastic-Gradient-Descent-SGD" class="headerlink" title="Stochastic Gradient Descent (SGD)"></a>Stochastic Gradient Descent (SGD)</h3><p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/11.png" alt="image-20190818163344387"></p>
<p>最基础的方法就是 SGD 啦, 想像红色方块是我们要训练的 data, 如果用普通的训练方法, 就需要重复不断的把整套数据放入神经网络 NN训练, 这样消耗的计算资源会很大.</p>
<p>我们换一种思路, 如果把这些数据拆分成小批小批的, 然后再分批不断放入 NN 中计算, 这就是我们常说的 SGD 的正确打开方式了. 每次使用批数据, 虽然不能反映整体数据的情况, 不过却很大程度上加速了 NN 的训练过程, 而且也不会丢失太多准确率.如果运用上了 SGD, 你还是嫌训练速度慢, 那怎么办?</p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/12.png" alt="image-20190818163501734"></p>
<p>事实证明, SGD 并不是最快速的训练方法, 红色的线是 SGD, 但它到达学习目标的时间是在这些方法中最长的一种. 我们还有很多其他的途径来加速训练.</p>
<h3 id="Momentum-更新方法"><a href="#Momentum-更新方法" class="headerlink" title="Momentum 更新方法"></a>Momentum 更新方法</h3><p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190818163532768.png" alt="image-20190818163532768"></p>
<p>大多数其他途径是在更新神经网络参数那一步上动动手脚. 传统的参数 W 的更新是把原始的 W 累加上一个负的学习率(learning rate) 乘以校正值 (dx).这种方法可能会让学习过程曲折无比, 看起来像 喝醉的人回家时, 摇摇晃晃走了很多弯路.</p>
<p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190818163647605.png" alt="image-20190818163647605"></p>
<p>所以我们把这个人从平地上放到了一个斜坡上, 只要他往下坡的方向走一点点, 由于向下的惯性, 他不自觉地就一直往下走, 走的弯路也变少了. 这就是 Momentum 参数更新. 另外一种加速方法叫AdaGrad.</p>
<h3 id="AdaGrad-更新方法"><a href="#AdaGrad-更新方法" class="headerlink" title="AdaGrad 更新方法"></a>AdaGrad 更新方法</h3><p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190818163807844.png" alt="image-20190818163807844"></p>
<p>这种方法是<strong>在学习率上面动手脚, 使得每一个参数更新都会有自己与众不同的学习率</strong>, 他的作用和 momentum 类似, 不过不是给喝醉酒的人安排另一个下坡, 而是给他一双不好走路的鞋子, 使得他一摇晃着走路就脚疼, 鞋子成为了走弯路的阻力, 逼着他往前直着走. 他的数学形式是这样的. 接下来又有什么方法呢? 如果把下坡和不好走路的鞋子合并起来, 是不是更好呢? 没错, 这样我们就有了 RMSProp 更新方法.</p>
<h3 id="RMSProp-更新方法"><a href="#RMSProp-更新方法" class="headerlink" title="RMSProp 更新方法"></a>RMSProp 更新方法</h3><p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/16.png" alt="image-20190818163916576"></p>
<p>有了 momentum 的惯性原则 , 加上 adagrad 的对错误方向的阻力, 我们就能合并成这样. 让 RMSProp同时具备他们两种方法的优势. 不过细心的同学们肯定看出来了, 似乎在 RMSProp 中少了些什么. 原来是我们还没把 Momentum合并完全, RMSProp 还缺少了 momentum 中的 这一部分. 所以, 我们在 Adam 方法中补上了这种想法.</p>
<h3 id="Adam-更新方法"><a href="#Adam-更新方法" class="headerlink" title="Adam 更新方法"></a>Adam 更新方法</h3><p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190818163958622.png" alt="image-20190818163958622"></p>
<p>计算m 时有 momentum 下坡的属性, 计算 v 时有 adagrad 阻力的属性, 然后再更新参数时 把 m 和 V 都考虑进去. 实验证明, 大多数时候, 使用 adam 都能又快又好的达到目标, 迅速收敛. 所以说, 在加速神经网络训练的时候, 一个下坡, 一双破鞋子, 功不可没.</p>
<h2 id="Optimizer-优化器"><a href="#Optimizer-优化器" class="headerlink" title="Optimizer 优化器"></a>Optimizer 优化器</h2><h3 id="伪数据"><a href="#伪数据" class="headerlink" title="伪数据"></a>伪数据</h3><p>为了对比各种优化器的效果, 我们需要有一些数据, 今天我们还是自己编一些伪数据, 这批数据是这样的:</p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/18.png" alt="image-20190818171449118"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">1</span>) <span class="comment"># 使每次随机产生的数一样</span></span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">EPOCH = <span class="number">12</span></span><br><span class="line"><span class="comment"># fake data</span></span><br><span class="line">x = torch.unsqueeze(torch.linpsace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">1000</span>), dim=<span class="number">1</span>)</span><br><span class="line">y = x.pow(<span class="number">2</span>) + <span class="number">0.1</span>*torch.normal(torch.zeros(*x.size()))</span><br><span class="line"><span class="comment"># plot dataset</span></span><br><span class="line">plt.scatter(x.numpy(), y.numpy())</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 使用上节提到 data loader: x,y -&gt; torch dataset -&gt; data loader</span></span><br><span class="line">torch_dataset = Data.TensorDataset(x, y)</span><br><span class="line">loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=<span class="keyword">True</span>, num_workers=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h3 id="每个优化器优化一个神经网络"><a href="#每个优化器优化一个神经网络" class="headerlink" title="每个优化器优化一个神经网络"></a>每个优化器优化一个神经网络</h3><p>为了对比每一种优化器, 我们给他们各自创建一个神经网络, 但这个神经网络都来自同一个 <code>Net</code> 形式.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认的 network 形式</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.hidden = torch.nn.Linear(<span class="number">1</span>, <span class="number">20</span>)   <span class="comment"># hidden layer</span></span><br><span class="line">        self.predict = torch.nn.Linear(<span class="number">20</span>, <span class="number">1</span>)   <span class="comment"># output layer</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.hidden(x))      <span class="comment"># activation function for hidden layer</span></span><br><span class="line">        x = self.predict(x)             <span class="comment"># linear output</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为每个优化器创建一个 net</span></span><br><span class="line">net_SGD         = Net()</span><br><span class="line">net_Momentum    = Net()</span><br><span class="line">net_RMSprop     = Net()</span><br><span class="line">net_Adam        = Net()</span><br><span class="line">nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]</span><br></pre></td></tr></table></figure>
<h3 id="优化器-Optimizer"><a href="#优化器-Optimizer" class="headerlink" title="优化器 Optimizer"></a>优化器 Optimizer</h3><p>接下来在创建不同的优化器, 用来训练不同的网络. 并创建一个 <code>loss_func</code> 用来计算误差. 我们用几种常见的优化器, <code>SGD</code>, <code>Momentum</code>, <code>RMSprop</code>, <code>Adam</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># different optimizers</span></span><br><span class="line">opt_SGD = torch.optim.SGD(net_SGD.parameters(), lr=LR)</span><br><span class="line">opt_Momentum = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=<span class="number">0.8</span>)</span><br><span class="line">opt_RMSprop = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=<span class="number">0.9</span>)</span><br><span class="line">opt_Adam = torch.optim.Adam(net.net_Adam.parameters(), lr=LR, betas=(<span class="number">0.9</span>, <span class="number">0.99</span>))</span><br><span class="line">optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]</span><br><span class="line">loss_func = torch.nn.MSELoss()</span><br><span class="line">loss_his = [[], [], [], []] <span class="comment"># 记录 training 时不同神经网络的 loss</span></span><br></pre></td></tr></table></figure>
<h3 id="训练-出图"><a href="#训练-出图" class="headerlink" title="训练/出图"></a>训练/出图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCH):</span><br><span class="line">  print(epoch)</span><br><span class="line">  <span class="keyword">for</span> step, (b_x, b_y) <span class="keyword">in</span> enumerate(loader):</span><br><span class="line">    <span class="comment"># 对每个优化器, 优化属于他的神经网络</span></span><br><span class="line">    <span class="keyword">for</span> net, opt, l_his <span class="keyword">in</span> zip(nets, optimizers, loss_his):</span><br><span class="line">      output = net(b_x)  <span class="comment"># get output for every net</span></span><br><span class="line">      loss = loss_func(output, b_y) <span class="comment"># compute loss for every net</span></span><br><span class="line">      opt.zero_grad() <span class="comment"># clear gradients for next train</span></span><br><span class="line">      loss.backward() <span class="comment"># backpropagation, compute gradients</span></span><br><span class="line">      opt.step() <span class="comment"># apply gradients</span></span><br><span class="line">      l_his.append(loss.data.numpy()) <span class="comment"># loss recoder</span></span><br></pre></td></tr></table></figure>
<h2 id="卷积神经网络-CNN-Convolutional-Neural-Network"><a href="#卷积神经网络-CNN-Convolutional-Neural-Network" class="headerlink" title="卷积神经网络 CNN (Convolutional Neural Network)"></a>卷积神经网络 CNN (Convolutional Neural Network)</h2><h3 id="卷积-和-神经网络"><a href="#卷积-和-神经网络" class="headerlink" title="卷积 和 神经网络"></a>卷积 和 神经网络</h3><p>卷积神经网络是如何运作的吧, 举一个识别图片的例子, 我们知道神经网络是由<strong>一连串的神经层组成,每一层神经层里面有存在有很多的神经元</strong>. 这些<strong>神经元就是神经网络识别事物的关键</strong>. <strong>每一种神经网络都会有输入输出值, 当输入值是图片的时候, 实际上输入神经网络的并不是那些色彩缤纷的图案,而是一堆堆的数字</strong>. 就比如说这个. 当神经网络需要处理这么多输入信息的时候, 也就是卷积神经网络就可以发挥它的优势的时候了. 那什么是卷积神经网络呢?</p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/19.png" alt="image-20190819103552681"></p>
<p>先把卷积神经网络这个词拆开来看. “卷积” 和 “神经网络”. 卷积也就是说神经网络<strong>不再是对每个像素的输入信息做处理了,而是图片上每一小块像素区域进行处理</strong>, 这种做法加强了图片信息的连续性. 使得神经网络能看到图形, 而非一个点. 这种做法同时也加深了神经网络对图片的理解. 具体来说, 卷积神经网络<strong>有一个批量过滤器, 持续不断的在图片上滚动收集图片里的信息,每一次收集的时候都只是收集一小块像素区域, 然后把收集来的信息进行整理</strong>, 这时候整理出来的信息有了一些实际上的呈现, 比如这时的神经网络能看到一些边缘的图片信息, 然后在以同样的步骤, 用类似的批量过滤器扫过产生的这些边缘信息, 神经网络从这些边缘信息里面总结出更高层的信息结构,比如说总结的边缘能够画出眼睛,鼻子等等. 再经过一次过滤, 脸部的信息也从这些眼睛鼻子的信息中被总结出来. 最后我们再把这些信息套入几层普通的全连接神经层进行分类, 这样就能得到输入的图片能被分为哪一类的结果了.</p>
<p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190819103829081.png" alt="image-20190819103829081"></p>
<p>我们截取一段 google 介绍卷积神经网络的视频, 具体说说图片是如何被卷积的. 下面是一张猫的图片, 图片有长, 宽, 高 三个参数. 对! 图片是有高度的! 这里的高指的是计算机用于产生颜色使用的信息. 如果是黑白照片的话, 高的单位就只有1, 如果是彩色照片, 就可能有红绿蓝三种颜色的信息, 这时的高度为3. 我们以彩色照片为例子. <strong>过滤器就是影像中不断移动的东西, 他不断在图片收集小批小批的像素块, 收集完所有信息后, 输出的值</strong>, 我们可以理解成是一个高度更高,长和宽更小的”图片”. 这个图片里就能包含一些边缘信息. 然后以同样的步骤再进行多次卷积, 将图片的长宽再压缩, 高度再增加, 就有了对输入图片更深的理解. 将压缩,增高的信息嵌套在普通的分类神经层上,我们就能对这种图片进行分类了.</p>
<h3 id="池化-pooling"><a href="#池化-pooling" class="headerlink" title="池化(pooling)"></a>池化(pooling)</h3><p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/21.png" alt="image-20190819104018442"></p>
<p>研究发现, 在每一次卷积的时候, 神经层可能会无意地丢失一些信息. 这时, 池化 (pooling) 就可以很好地解决这一问题. 而且池化是一个筛选过滤的过程, 能将 layer 中有用的信息筛选出来, 给下一个层分析. 同时也减轻了神经网络的计算负担 (<a href="http://cs231n.github.io/convolutional-networks/#pool" target="_blank" rel="noopener">具体细节参考</a>). 也就是说在卷集的时候, 我们不压缩长宽, 尽量地保留更多信息, 压缩的工作就交给池化了,这样的一项附加工作能够很有效的提高准确性. 有了这些技术,我们就可以搭建一个属于我们自己的卷积神经网络啦.</p>
<h2 id="流行的-CNN-结构"><a href="#流行的-CNN-结构" class="headerlink" title="流行的 CNN 结构"></a>流行的 CNN 结构</h2><p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190819104114779.png" alt="image-20190819104114779"></p>
<p>比较流行的一种搭建结构是这样, 从下到上的顺序, 首先是输入的图片(image), 经过一层卷积层 (convolution), 然后在用池化(pooling)方式处理卷积的信息, 这里使用的是 max pooling 的方式. 然后在经过一次同样的处理, 把得到的第二次处理的信息传入两层全连接的神经层 (fully connected),这也是一般的两层神经网络层,最后在接上一个分类器(classifier)进行分类预测.</p>
<h2 id="CNN-卷积神经网络"><a href="#CNN-卷积神经网络" class="headerlink" title="CNN 卷积神经网络"></a>CNN 卷积神经网络</h2><h3 id="MNIST手写数据"><a href="#MNIST手写数据" class="headerlink" title="MNIST手写数据"></a>MNIST手写数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torchvision  <span class="comment"># 数据库 模块 下载 处理数据</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Hyper parameters</span></span><br><span class="line">EPOCH = <span class="number">1</span> <span class="comment"># 训练整批数据的多少次，</span></span><br><span class="line">BATCH_ZISE = <span class="number">50</span></span><br><span class="line">LR = <span class="number">0.001</span></span><br><span class="line">DOWNLOAD_MNIST = <span class="keyword">True</span> <span class="comment"># </span></span><br><span class="line"><span class="comment"># Mnist 手写数字</span></span><br><span class="line"><span class="comment"># 1. use torchvision download mnist dataset</span></span><br><span class="line">train_data = torchvision.datasets.MNIST(</span><br><span class="line">	root=<span class="string">'./mnist/'</span> <span class="comment"># 保存位置</span></span><br><span class="line">  train=<span class="keyword">True</span>, <span class="comment"># this is training data</span></span><br><span class="line">  transform=torchvision.transforms.ToTensor(), <span class="comment"># 转换PIL.Image or numpy.ndarray 或 torch.FloaterTensor (C x H x W), 训练的时候 normalize成[0.0, 1.0]区间</span></span><br><span class="line">  download=DOWNLOAD_MNIST,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 2. package data with dataloader  </span></span><br><span class="line"><span class="comment"># 批训练 50 example, 1 channel, 28*28 (50, 1, 28, 28)</span></span><br><span class="line">train_loader = Data.Dataloader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#plot one example </span></span><br><span class="line">print(train_data.train_data.size())  <span class="comment"># (6000, 28, 28)</span></span><br><span class="line">print(train_data.train_labels.size()) <span class="comment"># (60000,)</span></span><br><span class="line">plt.imshow(train_data.train_data[<span class="number">0</span>].numpy(), cmap=<span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">"%i"</span> % train_data.train_data.labels[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同样, 我们除了训练数据, 还给一些测试数据, 测试看看它有没有训练好.</span></span><br><span class="line">test_data = torchvision.datasets.MNIST(root=<span class="string">'./mnist'</span>, train=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># 为了节约时间, 我们测试时只测试前2000个</span></span><br><span class="line"><span class="comment"># shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)</span></span><br><span class="line">test_x = torch.unsqueeze(test_data.test_data, dim=<span class="number">1</span>).type(torch.FloatTensor)[:<span class="number">2000</span>]/<span class="number">255.</span></span><br><span class="line">test_y = test_data.test_labels[:<span class="number">2000</span>]</span><br></pre></td></tr></table></figure>
<h2 id="CNN模型"><a href="#CNN模型" class="headerlink" title="CNN模型"></a>CNN模型</h2><p>和以前一样, 我们用一个 class 来建立 CNN 模型.这个 CNN 整体流程是 卷积(<code>Conv2d</code>) -&gt; 激励函数(<code>ReLU</code>) -&gt; 池化, 向下采样 (<code>MaxPooling</code>) -&gt; 再来一遍 -&gt; 展平多维的卷积成的特征图 -&gt; 接入全连接层 (<code>Linear</code>) -&gt; 输出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>：</span></span><br><span class="line"><span class="class">	<span class="title">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    super(CNN, self).__init__()</span><br><span class="line">    self.conv1 = nn.Sequential(  <span class="comment"># input shape (1, 28, 28)</span></span><br><span class="line">    	nn.Conv2d(</span><br><span class="line">      	in_channels=<span class="number">1</span>, <span class="comment"># input height</span></span><br><span class="line">        out_channels=<span class="number">16</span>, <span class="comment"># n_filters</span></span><br><span class="line">        kernel_size = <span class="number">5</span>, <span class="comment"># filter size</span></span><br><span class="line">        stride=<span class="number">1</span>, <span class="comment"># filter movement/step</span></span><br><span class="line">        padding=<span class="number">2</span>, <span class="comment"># 如想conv2d出来的图片长宽没有变化，padding=(kernel_size-1)/2 当 stride=1</span></span><br><span class="line">      )， <span class="comment"># out shape (16, 28, 28)</span></span><br><span class="line">      nn.ReLU(), <span class="comment"># activation</span></span><br><span class="line">      nn.MaxPool2d(<span class="number">2</span>), <span class="comment"># output (16, 14, 14)</span></span><br><span class="line">    )</span><br><span class="line">    self.conv2 = nn.Sequential( <span class="comment"># input shape (16, 14, 14)</span></span><br><span class="line">    	nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">      nn.ReLU(), <span class="comment"># activation</span></span><br><span class="line">      nn.MaxPool2d(<span class="number">2</span>) <span class="comment"># output shape (32, 7, 7)</span></span><br><span class="line">    )</span><br><span class="line">    self.out = nn.Linear(<span class="number">32</span>*<span class="number">7</span>*<span class="number">7</span>, <span class="number">10</span>) <span class="comment"># fully connected layer, output 10 classes</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    x = self.conv1(x)</span><br><span class="line">    x = self.conv2(x)</span><br><span class="line">    x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>) <span class="comment"># 展平多维平面图 成 (batch_size, 32*7*7)</span></span><br><span class="line">    output = self.out(x)</span><br><span class="line">    <span class="keyword">return</span> output, x</span><br><span class="line">cnn = CNN()</span><br><span class="line">print(cnn) <span class="comment"># net architecture</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">CNN (</span></span><br><span class="line"><span class="string">  (conv1): Sequential (</span></span><br><span class="line"><span class="string">    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span></span><br><span class="line"><span class="string">    (1): ReLU ()</span></span><br><span class="line"><span class="string">    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (conv2): Sequential (</span></span><br><span class="line"><span class="string">    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span></span><br><span class="line"><span class="string">    (1): ReLU ()</span></span><br><span class="line"><span class="string">    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (out): Linear (1568 -&gt; 10)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>下面我们开始训练, 将 <code>x</code> <code>y</code> 都用 <code>Variable</code> 包起来, 然后放入 <code>cnn</code> 中计算 <code>output</code>, 最后再计算误差. 下面代码省略了计算精确度 <code>accuracy</code> 的部分, 如果想细看 <code>accuracy</code> 代码的同学, 请去往我的 <a href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/401_CNN.py" target="_blank" rel="noopener">github</a> 看全部代码.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(cnn.parameters(), lr=LR) <span class="comment"># optimize all cnn parameters</span></span><br><span class="line">loss_func = nn.CrossEntropyLoss() <span class="comment"># the target label is not one-hotted</span></span><br><span class="line"><span class="comment"># training and testing</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCH):</span><br><span class="line">  <span class="keyword">for</span> step, (b_x, b_y) <span class="keyword">in</span> enumerate(train_loader): <span class="comment"># 分配batch data, normalize x when iterate train_loader</span></span><br><span class="line">    output = cnn(b_x)[<span class="number">0</span>]</span><br><span class="line">    loss = loss_func(output, b_y)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        test_output, last_layer = cnn(test_x)</span><br><span class="line">        pred_y = torch.max(test_output, <span class="number">1</span>)[<span class="number">1</span>].data.squeeze()</span><br><span class="line">        accuracy = (pred_y == test_y).sum().item() / float(test_y.size(<span class="number">0</span>))</span><br><span class="line">        print(<span class="string">'Epoch: '</span>, epoch, <span class="string">'| train loss: %.4f'</span> % loss.data, <span class="string">'| test accuracy: %.2f'</span> % accuracy)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0306 | test accuracy: 0.97</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0147 | test accuracy: 0.98</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0427 | test accuracy: 0.98</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0078 | test accuracy: 0.98</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># print 10 predictions from test data</span></span><br><span class="line">test_output, _ = cnn(test_x[:<span class="number">10</span>])</span><br><span class="line">pred_y = torch.max(test_output, <span class="number">1</span>)[<span class="number">1</span>].data.numpy().squeeze()</span><br><span class="line">print(pred_y, <span class="string">'prediction number'</span>)</span><br><span class="line">print(test_y[:<span class="number">10</span>].numpy(), <span class="string">'real number'</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[5 0 4 1 9 2 1 3 1 4] prediction number</span></span><br><span class="line"><span class="string">[5 0 4 1 9 2 1 3 1 4] real number</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h2 id="什么是循环神经网络-RNN-Recurrent-Neural-Network"><a href="#什么是循环神经网络-RNN-Recurrent-Neural-Network" class="headerlink" title="什么是循环神经网络 RNN (Recurrent Neural Network)"></a>什么是循环神经网络 RNN (Recurrent Neural Network)</h2><p>在语言分析, 序列化数据中穿梭自如的循环神经网络 RNN</p>
<h3 id="RNN-的用途"><a href="#RNN-的用途" class="headerlink" title="RNN 的用途"></a>RNN 的用途</h3><p>只想着斯蒂芬乔布斯这个名字 , 请你再把他逆序念出来. 斯布乔(*#&amp;, 有点难吧. 这就说明, <strong>对于预测, 顺序排列是多么重要</strong>. 我们可以预测下一个按照一定顺序排列的字, 但是打乱顺序, 我们就没办法分析自己到底在说什么了.</p>
<h2 id="序列数据"><a href="#序列数据" class="headerlink" title="序列数据"></a>序列数据</h2><p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190822214243233.png" alt="image-20190822214243233"></p>
<p>我们想象现在有一组序列数据 data 0,1,2,3. 在当预测 result0 的时候,我们基于的是 data0, 同样在预测其他数据的时候, 我们也都只单单基于单个的数据. 每次使用的神经网络都是同一个 NN. 不过这些数据是有关联 顺序的 , 就像在厨房做菜, 酱料 A要比酱料 B 早放, 不然就串味了. 所以普通的神经网络结构并不能让 NN 了解这些数据之间的关联.</p>
<h3 id="处理序列数据的神经网络"><a href="#处理序列数据的神经网络" class="headerlink" title="处理序列数据的神经网络"></a>处理序列数据的神经网络</h3><p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190822214343843.png" alt="image-20190822214343843"></p>
<p>那我们如何让数据间的关联也被 NN 加以分析呢? 想想我们人类是怎么分析各种事物的关联吧, 最基本的方式,<strong>就是记住之前发生的事情. 那我们让神经网络也具备这种记住之前发生的事的能力</strong>. 再分析 Data0 的时候, 我们把分析结果存入记忆. 然后当分析 data1的时候, NN会产生新的记忆, 但是新记忆和老记忆是没有联系的. 我们就简单的把老记忆调用过来, 一起分析. 如果继续分析更多的有序数据 , RNN就会把之前的记忆都累积起来, 一起分析.</p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/35.png" alt="image-20190822214449342"></p>
<p>我们再重复一遍刚才的流程, 不过这次是以加入一些数学方面的东西. <strong>每次 RNN 运算完之后都会产生一个对于当前状态的描述 , state</strong>. 我们用简写 <code>S(t)</code> 代替, 然后这个 RNN开始分析 x(t+1) , 他会根据 x(t+1)产生s(t+1), 不过此时 <strong>y(t+1) 是由 s(t) 和 s(t+1) 共同创造</strong>的. 所以我们通常看到的 RNN 也可以表达成这种样子.</p>
<h3 id="RNN-的应用"><a href="#RNN-的应用" class="headerlink" title="RNN 的应用"></a>RNN 的应用</h3><p>RNN 的形式不单单这有这样一种, 他的结构形式很自由. 如果<strong>用于分类问题, 比如说一个人说了一句话, 这句话带的感情色彩是积极的还是消极的. 那我们就可以用只有最后一个时间点输出判断结果的RNN.</strong></p>
<p>又或者这是图片描述 RNN, 我们只需要一个 X 来代替输入的图片, 然后生成对图片描述的一段话.</p>
<p>或者是语言翻译的 RNN, 给出一段英文, 然后再翻译成中文.</p>
<h2 id="什么是-LSTM-循环神经网络"><a href="#什么是-LSTM-循环神经网络" class="headerlink" title="什么是 LSTM 循环神经网络"></a>什么是 LSTM 循环神经网络</h2><h3 id="RNN-的弊端"><a href="#RNN-的弊端" class="headerlink" title="RNN 的弊端"></a>RNN 的弊端</h3><p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/36.png" alt="image-20190822215802638"></p>
<p>之前我们说过, <a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/2-3-RNN/" target="_blank" rel="noopener">RNN</a> 是在有顺序的数据上进行学习的. 为了记住这些数据, RNN 会像人一样产生对先前发生事件的记忆. 不过一般形式的 RNN 就像一个老爷爷, 有时候比较健忘. 为什么会这样呢?</p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/37.png" alt="image-20190822215849509"></p>
<p>想像现在有这样一个 RNN, 他的输入值是一句话: ‘我今天要做红烧排骨, 首先要准备排骨, 然后…., 最后美味的一道菜就出锅了’. 现在请 RNN 来分析, 我今天做的到底是什么菜呢. RNN可能会给出“辣子鸡”这个答案. 由于判断失误, RNN就要开始学习 这个长序列 X 和 ‘红烧排骨’ 的关系 , 而RNN需要的关键信息 ”红烧排骨”却出现在句子开头,</p>
<p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190822215958596.png" alt="image-20190822215958596"></p>
<p>再来看看 RNN是怎样学习的吧. <strong>红烧排骨这个信息原</strong>的记忆要进过长途跋涉才能抵达最后一个时间点. <strong>然后我们得到误差, 而且在 反向传递 得到的误差的时候, 他在每一步都会 乘以一个自己的参数 W</strong>. 如果这个 W 是一个小于1 的数, 比如0.9. 这个0.9 不断乘以误差, 误差传到初始时间点也会是一个接近于零的数, 所以对于初始时刻, 误差相当于就消失了. 我们把这个问题叫做梯度消失或者梯度弥散 Gradient vanishing. 反之如果 W 是一个大于1 的数, 比如1.1 不断累乘, 则到最后变成了无穷大的数, RNN被这无穷大的数撑死了, 这种情况我们叫做梯度爆炸, Gradient exploding. 这就是普通 RNN 没有办法回忆起久远记忆的原因.</p>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190822220343752.png" alt="image-20190822220343752"></p>
<p>LSTM 就是为了解决这个问题而诞生的. LSTM 和普通 RNN 相比, 多出了三个控制器. (<strong>输入控制, 输出控制, 忘记控制</strong>). 现在, LSTM RNN 内部的情况是这样.</p>
<p>他多了一个 控制全局的记忆, 我们用粗线代替. 为了方便理解, 我们把粗线想象成电影或游戏当中的 主线剧情. 而原本的 RNN 体系就是 分线剧情. 三个控制器都是在原始的 RNN 体系上, 我们先看 输入方面 , 如果此时的分线剧情对于剧终结果十分重要, 输入控制就会将这个分线剧情按重要程度 写入主线剧情 进行分析. 再看 忘记方面, 如果此时的分线剧情更改了我们对之前剧情的想法, 那么忘记控制就会将之前的某些主线剧情忘记, 按比例替换成现在的新剧情. 所以 主线剧情的更新就取决于输入 和忘记 控制. 最后的输出方面, 输出控制会基于目前的主线剧情和分线剧情判断要输出的到底是什么.基于这些控制机制, LSTM 就像延缓记忆衰退的良药, 可以带来更好的结果.</p>
<h2 id="什么是自编码-Autoencoder"><a href="#什么是自编码-Autoencoder" class="headerlink" title="什么是自编码 (Autoencoder)"></a>什么是自编码 (Autoencoder)</h2><p>自编码 autoencoder 是一种什么码呢. 他是不是 条形码? 二维码? 打码? 其中的一种呢? NONONONO. 和他们统统没有关系. <strong>自编码是一种神经网络的形式.</strong>如果你一定要把他们扯上关系, 我想也只能这样解释啦.</p>
<h3 id="压缩与解压"><a href="#压缩与解压" class="headerlink" title="压缩与解压"></a>压缩与解压</h3><p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/40.png" alt="image-20190824165233675"></p>
<p>有一个神经网络, 它在做的事情是 <strong>接收一张图片, 然后 给它打码, 最后 再从打码后的图片中还原</strong>. 太抽象啦? 行, 我们再具体点.</p>
<p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190824171847757.png" alt="image-20190824171847757"></p>
<p>假设刚刚那个神经网络是这样, 对应上刚刚的图片,看出图片其实是<strong>经过了压缩,再解压的这一道工序. 当压缩的时候, 原有的图片质量被缩减, 解压时用信息量小却包含了所有关键信息的文件恢复出原本的图片</strong>. 为什么要这样做呢?</p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/42.png" alt="image-20190825164048242"></p>
<p>原来有时神经网络要接受大量的输入信息, 比如输入信息是高清图片时, 输入信息量可能达到上千万, 让神经网络直接从上千万个信息源中学习是一件很吃力的工作. 所以, 何不压缩一下, <strong>提取出原图片中的最具代表性的信息, 缩减输入信息量</strong>, 再<strong>把缩减过后的信息放进神经网络学习</strong>. 这样学习起来就简单轻松了. 所以, 自编码就能在这时发挥作用. 通过<strong>将原数据白色的X 压缩, 解压 成黑色的X</strong>, 然后<strong>通过对比黑白 X</strong> ,<strong>求出预测误差, 进行反向传递, 逐步提升自编码的准确性</strong>. <strong>训练好的自编码中间这一部分就是能总结原数据的精髓</strong>. 可以看出, 从头到尾, 我们只用到了输入数据 X, 并没有用到 X 对应的数据标签, 所以也可以说自编码是一种<strong>非监督学习</strong>. 到了真正使用自编码的时候. 通常只会用到自编码前半部分.</p>
<h3 id="编码器-Encoder"><a href="#编码器-Encoder" class="headerlink" title="编码器 Encoder"></a>编码器 Encoder</h3><p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/43.png" alt="image-20190825164326962"></p>
<p>这 部分也叫作 encoder 编码器. <strong>编码器能得到原数据的精髓</strong>, 然后我们<strong>只需要再创建一个小的神经网络学习这个精髓的数据</strong>,不仅减少了神经网络的负担, 而且同样能达到很好的效果.</p>
<p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190825164413250.png" alt="image-20190825164413250"></p>
<p>这是一个<strong>通过自编码整理出来的数据</strong>, 他能<strong>从原数据中总结出每种类型数据的特征</strong>, 如果把这些特征类型都放在一张二维的图片上, 每种类型都已经被很好的用原数据的精髓区分开来. 如果你了解 PCA 主成分分析, 再提取主要特征时, 自编码和它一样,甚至超越了 PCA. 换句话说, <strong>自编码 可以像 PCA 一样 给特征属性降维</strong>.</p>
<h3 id="解码器-Decoder"><a href="#解码器-Decoder" class="headerlink" title="解码器 Decoder"></a>解码器 Decoder</h3><p>至于解码器 Decoder, 我们也能那它来做点事情. 我们知道, <strong>解码器在训练的时候是要将精髓信息解压成原始信息</strong>, 那么这就<strong>提供了一个解压器的作用, 甚至我们可以认为是一个生成器</strong> (类似于<a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/2-6-GAN/" target="_blank" rel="noopener">GAN</a>). 那做这件事的一种特殊自编码叫做 variational autoencoders, 你能在<a href="http://kvfrans.com/variational-autoencoders-explained/" target="_blank" rel="noopener">这里</a>找到他的具体说明.</p>
<p>有一个例子就是让它能模仿并生成手写数字.</p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/45.png" alt="image-20190825164602297"></p>
<h2 id="什么是批标准化-Batch-Normalization"><a href="#什么是批标准化-Batch-Normalization" class="headerlink" title="什么是批标准化 (Batch Normalization)"></a>什么是批标准化 (Batch Normalization)</h2><h3 id="普通数据标准化"><a href="#普通数据标准化" class="headerlink" title="普通数据标准化"></a>普通数据标准化</h3><p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/23.png" alt="image-20190819202503666"></p>
<p>Batch Normalization, 批标准化, 和普通的数据标准化类似, <strong>是将分散的数据统一的一种做法, 也是优化神经网络的一种方法</strong>. 在之前 Normalization 的简介视频中我们一提到, 具有统一规格的数据, 能让机器学习更容易学习到数据之中的规律.</p>
<h3 id="每层都做标准化"><a href="#每层都做标准化" class="headerlink" title="每层都做标准化"></a>每层都做标准化</h3><p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190819202632570.png" alt="image-20190819202632570"></p>
<p><strong>在神经网络中, 数据分布对训练会产生影响</strong>. 比如某个神经元 x 的值为1, 某个 Weights 的初始值为 0.1, 这样后一层神经元计算结果就是 Wx = 0.1; 又或者 x = 20, 这样 Wx 的结果就为 2. 现在还不能看出什么问题, 但是, 当我们加上一层激励函数, 激活这个 Wx 值的时候, 问题就来了. 如果使用 像 tanh 的激励函数, Wx 的激活值就变成了 ~0.1 和 ~1, 接近于 1 的部已经处在了 激励函数的饱和阶段, 也就是如果 x 无论再怎么扩大, tanh 激励函数输出值也还是 接近1. 换句话说<strong>, 神经网络在初始阶段已经不对那些比较大的 x 特征范围 敏感了</strong>. 这样很糟糕, 想象我轻轻拍自己的感觉和重重打自己的感觉居然没什么差别, 这就证明我的感官系统失效了. 当然我们是可以用之前提到的对数据做 normalization 预处理, 使得输入的 x 变化范围不会太大, 让输入值经过激励函数的敏感部分. 但刚刚这个不敏感问题不仅仅发生在神经网络的输入层, 而且在隐藏层中也经常会发生.</p>
<p><img src="/Users/liulifeng/Library/Application Support/typora-user-images/image-20190819202806837.png" alt="image-20190819202806837"></p>
<p>只是时候 x 换到了隐藏层当中, 我们能不能对隐藏层的输入结果进行像之前那样的normalization 处理呢? 答案是可以的, 因为大牛们发明了一种技术, 叫做 batch normalization, 正是处理这种情况.</p>
<h3 id="BN-添加位置"><a href="#BN-添加位置" class="headerlink" title="BN 添加位置"></a>BN 添加位置</h3><p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/26.png" alt="image-20190819202842876"></p>
<p>Batch normalization 的 batch 是批数据, 把数据分成小批小批进行 stochastic gradient descent. 而且在每批数据进行前向传递 forward propagation 的时候, 对每一层都进行 normalization 的处理,</p>
<h3 id="BN-效果"><a href="#BN-效果" class="headerlink" title="BN 效果"></a>BN 效果</h3><p>Batch normalization 也可以被看做一个层面. 在一层层的添加神经网络的时候, 我们先有数据 X, 再添加全连接层, 全连接层的计算结果会经过 激励函数 成为下一层的输入, 接着重复之前的操作. <strong>Batch Normalization (BN) 就被添加在每一个全连接和激励函数之间</strong>.</p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/27.png" alt="image-20190819203006098"></p>
<p>之前说过, 计算结果在进入激励函数前的值很重要, 如果我们不单单看一个值, 我们可以说, 计算结果值的分布对于激励函数很重要. 对于数据值大多分布在这个区间的数据, 才能进行更有效的传递. <strong>对比这两个在激活之前的值的分布. 上者没有进行 normalization, 下者进行了 normalization, 这样当然是下者能够更有效地利用 tanh 进行非线性化的过程.</strong></p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/28.png" alt="image-20190819203050813"></p>
<p>没有 normalize 的数据 使用 tanh 激活以后, 激活值大部分都分布到了饱和阶段, 也就是大部分的激活值不是-1, 就是1, 而 normalize 以后, 大部分的激活值在每个分布区间都还有存在. 再将这个激活后的分布传递到下一层神经网络进行后续计算, 每个区间都有分布的这一种对于神经网络就会更加有价值. Batch normalization 不仅仅 normalize 了一下数据, 他还进行了反 normalize 的手续. 为什么要这样呢?</p>
<h3 id="BN-算法"><a href="#BN-算法" class="headerlink" title="BN 算法"></a>BN 算法</h3><p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/29.png" alt="image-20190819203125107"></p>
<p>我们引入一些 batch normalization 的公式. 这三步就是我们在刚刚一直说的 normalization 工序, 但是公式的后面还有一个反向操作, <strong>将 normalize 后的数据再扩展和平移</strong>. 原来这是为了让神经网络自己去学着<strong>使用和修改这个扩展参数 gamma, 和 平移参数 β,</strong> 这样神经网络就能自己慢慢琢磨出前面的 normalization 操作到底有没有起到优化的作用, 如果没有起到作用, 我就使用 gamma 和 belt 来抵消一些 normalization 的操作.</p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/30.png" alt="image-20190819203221565"></p>
<p>最后我们来看看一张神经网络训练到最后, 代表了每层输出值的结果的分布图. 这样我们<strong>就能一眼看出 Batch normalization 的功效啦. 让每一层的值在有效的范围内传递下去</strong>.</p>
<p>BN就是通过<strong>一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布</strong>， 就是每次处理之后，用BN拉回N（0,1），这样就避免了梯度爆炸和梯度消失</p>
<h2 id="Batch-Normalization-批标准化"><a href="#Batch-Normalization-批标准化" class="headerlink" title="Batch Normalization 批标准化"></a>Batch Normalization 批标准化</h2><p>批标准化通俗来说就是对每一层神经网络进行标准化 (normalize) 处理, 我们知道对输入数据进行标准化能让机器学习有效率地学习。 如果把每一层后看成这种接受输入数据的模式, 那我们何不 “批标准化” 所有的层呢</p>
<p>那我们就看看下面的两个动图, 这就是在每层神经网络有无 batch normalization 的区别啦.</p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/32.gif" alt="Batch Normalization æ¹æ åå"></p>
<p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/pytorch学习/31.gif" alt="Batch Normalization æ¹æ åå"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://fallenk.github.io/2019/08/15/pytorch学习/" data-id="cjzqqm9qd003dy88ozrnku8ud" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/pytorch学习/">pytorch学习</a></li></ul>

    </footer>
  </div>
  
    
 <script src="/jquery/jquery.min.js"></script>
  <div id="random_posts">
    <h2>推荐文章</h2>
    <div class="random_posts_ul">
      <script>
          var random_count =4
          var site = {BASE_URI:'/'};
          function load_random_posts(obj) {
              var arr=site.posts;
              if (!obj) return;
              // var count = $(obj).attr('data-count') || 6;
              for (var i, tmp, n = arr.length; n; i = Math.floor(Math.random() * n), tmp = arr[--n], arr[n] = arr[i], arr[i] = tmp);
              arr = arr.slice(0, random_count);
              var html = '<ul>';
            
              for(var j=0;j<arr.length;j++){
                var item=arr[j];
                html += '<li><strong>' + 
                item.date + ':&nbsp;&nbsp;<a href="' + (site.BASE_URI+item.uri) + '">' + 
                (item.title || item.uri) + '</a></strong>';
                if(item.excerpt){
                  html +='<div class="post-excerpt">'+item.excerpt+'</div>';
                }
                html +='</li>';
                
              }
              $(obj).html(html + '</ul>');
          }
          $('.random_posts_ul').each(function () {
              var c = this;
              if (!site.posts || !site.posts.length){
                  $.getJSON(site.BASE_URI + 'js/posts.js',function(json){site.posts = json;load_random_posts(c)});
              } 
               else{
                load_random_posts(c);
              }
          });
      </script>
    </div>
  </div>

    
<nav id="article-nav">
  
    <a href="/2019/08/23/yaml入门/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">上一篇</strong>
      <div class="article-nav-title">
        
          yaml入门
        
      </div>
    </a>
  
  
    <a href="/2019/08/15/Golang总结/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">下一篇</strong>
      <div class="article-nav-title">Golang总结</div>
    </a>
  
</nav>

  
</article>
 
     
  <div class="comments" id="comments">
    
     
       
      <div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div>
    
       
      
      
  </div>
 
  

</section>
           
    <aside id="sidebar">
  
    

  
    
    <div class="widget-wrap">
    
      <div class="widget" id="toc-widget-fixed">
      
        <strong class="toc-title">文章目录</strong>
        <div class="toc-widget-list">
              <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction-to-Neural-Network"><span class="toc-number">1.</span> <span class="toc-text">Introduction to Neural Network</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Pytorch"><span class="toc-number">1.1.</span> <span class="toc-text">Pytorch</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#神经网络在做什么"><span class="toc-number">1.1.1.</span> <span class="toc-text">神经网络在做什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PyTorch-和-Tensorflow"><span class="toc-number">1.1.2.</span> <span class="toc-text">PyTorch 和 Tensorflow</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Torch-或-Numpy"><span class="toc-number">1.2.</span> <span class="toc-text">Torch 或 Numpy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#变量-Variable"><span class="toc-number">1.3.</span> <span class="toc-text">变量 (Variable)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Variable-计算-梯度"><span class="toc-number">1.3.1.</span> <span class="toc-text">Variable 计算, 梯度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#获取Variable-里面的数据"><span class="toc-number">1.3.2.</span> <span class="toc-text">获取Variable 里面的数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#激励函数-Activation-Function"><span class="toc-number">1.4.</span> <span class="toc-text">激励函数 Activation Function</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#激励函数"><span class="toc-number">1.4.1.</span> <span class="toc-text">激励函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pytorch-activation-function"><span class="toc-number">1.4.2.</span> <span class="toc-text">pytorch activation function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Torch中的激励函数"><span class="toc-number">1.4.3.</span> <span class="toc-text">Torch中的激励函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络"><span class="toc-number">1.5.</span> <span class="toc-text">神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#关系拟合-回归"><span class="toc-number">1.6.</span> <span class="toc-text">关系拟合(回归)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#建立数据集"><span class="toc-number">1.6.1.</span> <span class="toc-text">建立数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#建立神经网络"><span class="toc-number">1.6.2.</span> <span class="toc-text">建立神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#训练网络"><span class="toc-number">1.6.3.</span> <span class="toc-text">训练网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#可视化训练过程"><span class="toc-number">1.6.4.</span> <span class="toc-text">可视化训练过程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#区分类型-分类"><span class="toc-number">1.7.</span> <span class="toc-text">区分类型(分类)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#建立数据集-1"><span class="toc-number">1.7.1.</span> <span class="toc-text">建立数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#建立神经网络-1"><span class="toc-number">1.7.2.</span> <span class="toc-text">建立神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#训练网络-1"><span class="toc-number">1.7.3.</span> <span class="toc-text">训练网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#可视化训练过程-1"><span class="toc-number">1.7.4.</span> <span class="toc-text">可视化训练过程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#快速搭建"><span class="toc-number">1.8.</span> <span class="toc-text">快速搭建</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#保存提取"><span class="toc-number">1.9.</span> <span class="toc-text">保存提取</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#保存"><span class="toc-number">1.9.1.</span> <span class="toc-text">保存</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#提取网络"><span class="toc-number">1.10.</span> <span class="toc-text">提取网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#只提取网络参数"><span class="toc-number">1.11.</span> <span class="toc-text">只提取网络参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#显示结果"><span class="toc-number">1.12.</span> <span class="toc-text">显示结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#批训练"><span class="toc-number">1.13.</span> <span class="toc-text">批训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#DataLoader"><span class="toc-number">1.13.1.</span> <span class="toc-text">DataLoader</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#加速神经网络训练-Speed-Up-Training"><span class="toc-number">1.14.</span> <span class="toc-text">加速神经网络训练 (Speed Up Training)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Stochastic-Gradient-Descent-SGD"><span class="toc-number">1.14.1.</span> <span class="toc-text">Stochastic Gradient Descent (SGD)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Momentum-更新方法"><span class="toc-number">1.14.2.</span> <span class="toc-text">Momentum 更新方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AdaGrad-更新方法"><span class="toc-number">1.14.3.</span> <span class="toc-text">AdaGrad 更新方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RMSProp-更新方法"><span class="toc-number">1.14.4.</span> <span class="toc-text">RMSProp 更新方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adam-更新方法"><span class="toc-number">1.14.5.</span> <span class="toc-text">Adam 更新方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Optimizer-优化器"><span class="toc-number">1.15.</span> <span class="toc-text">Optimizer 优化器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#伪数据"><span class="toc-number">1.15.1.</span> <span class="toc-text">伪数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#每个优化器优化一个神经网络"><span class="toc-number">1.15.2.</span> <span class="toc-text">每个优化器优化一个神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#优化器-Optimizer"><span class="toc-number">1.15.3.</span> <span class="toc-text">优化器 Optimizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#训练-出图"><span class="toc-number">1.15.4.</span> <span class="toc-text">训练/出图</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积神经网络-CNN-Convolutional-Neural-Network"><span class="toc-number">1.16.</span> <span class="toc-text">卷积神经网络 CNN (Convolutional Neural Network)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#卷积-和-神经网络"><span class="toc-number">1.16.1.</span> <span class="toc-text">卷积 和 神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#池化-pooling"><span class="toc-number">1.16.2.</span> <span class="toc-text">池化(pooling)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#流行的-CNN-结构"><span class="toc-number">1.17.</span> <span class="toc-text">流行的 CNN 结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN-卷积神经网络"><span class="toc-number">1.18.</span> <span class="toc-text">CNN 卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MNIST手写数据"><span class="toc-number">1.18.1.</span> <span class="toc-text">MNIST手写数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN模型"><span class="toc-number">1.19.</span> <span class="toc-text">CNN模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#训练"><span class="toc-number">1.19.1.</span> <span class="toc-text">训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#什么是循环神经网络-RNN-Recurrent-Neural-Network"><span class="toc-number">1.20.</span> <span class="toc-text">什么是循环神经网络 RNN (Recurrent Neural Network)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN-的用途"><span class="toc-number">1.20.1.</span> <span class="toc-text">RNN 的用途</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#序列数据"><span class="toc-number">1.21.</span> <span class="toc-text">序列数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#处理序列数据的神经网络"><span class="toc-number">1.21.1.</span> <span class="toc-text">处理序列数据的神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN-的应用"><span class="toc-number">1.21.2.</span> <span class="toc-text">RNN 的应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#什么是-LSTM-循环神经网络"><span class="toc-number">1.22.</span> <span class="toc-text">什么是 LSTM 循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN-的弊端"><span class="toc-number">1.22.1.</span> <span class="toc-text">RNN 的弊端</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LSTM"><span class="toc-number">1.22.2.</span> <span class="toc-text">LSTM</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#什么是自编码-Autoencoder"><span class="toc-number">1.23.</span> <span class="toc-text">什么是自编码 (Autoencoder)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#压缩与解压"><span class="toc-number">1.23.1.</span> <span class="toc-text">压缩与解压</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#编码器-Encoder"><span class="toc-number">1.23.2.</span> <span class="toc-text">编码器 Encoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#解码器-Decoder"><span class="toc-number">1.23.3.</span> <span class="toc-text">解码器 Decoder</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#什么是批标准化-Batch-Normalization"><span class="toc-number">1.24.</span> <span class="toc-text">什么是批标准化 (Batch Normalization)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#普通数据标准化"><span class="toc-number">1.24.1.</span> <span class="toc-text">普通数据标准化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#每层都做标准化"><span class="toc-number">1.24.2.</span> <span class="toc-text">每层都做标准化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BN-添加位置"><span class="toc-number">1.24.3.</span> <span class="toc-text">BN 添加位置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BN-效果"><span class="toc-number">1.24.4.</span> <span class="toc-text">BN 效果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BN-算法"><span class="toc-number">1.24.5.</span> <span class="toc-text">BN 算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Batch-Normalization-批标准化"><span class="toc-number">1.25.</span> <span class="toc-text">Batch Normalization 批标准化</span></a></li></ol></li></ol>
          </div>
      </div>
    </div>

  
    

  
    
  
    
  
    

  
    
  
    <!--微信公众号二维码-->


  
</aside>

      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-left">
      &copy; 2014 - 2019 Fallenk Liu&nbsp;|&nbsp;
      主题 <a href="https://github.com/giscafer/hexo-theme-cafe/" target="_blank">Cafe</a>
    </div>
     <div id="footer-right">
      联系方式&nbsp;|&nbsp;fallenk_liu@yeah.com
    </div>
  </div>
</footer>
 <script src="/jquery/jquery.min.js"></script>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    <img class="back-to-top-btn" src="/images/fly-to-top.png"/>
<script>
// Elevator script included on the page, already.
window.onload = function() {
  var elevator = new Elevator({
    selector:'.back-to-top-btn',
    element: document.querySelector('.back-to-top-btn'),
    duration: 1000 // milliseconds
  });
}
</script>
      

  
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "e2fb4051c49842688ce669e634bc983f",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
    

  







<!-- author:forvoid begin -->
<!-- author:forvoid begin -->

<!-- author:forvoid end -->

<!-- author:forvoid end -->


  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      })
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      })
    </script>
    <script type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


 <script src="/js/is.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/elevator.js"></script>
  </div>
</body>
</html>