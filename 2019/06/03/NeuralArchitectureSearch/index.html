<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>综述-NeuralArchitectureSearch | Fallenk&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="综述- Nerual Architecture Search神经网络架构搜索，顾名思义，就是让机器自己去学习如何构架一个神经网络，得到比人类专家手工设计的网络更好的效果。这个思路是非常一脉相承的，就像机器学习到深度学习，也是完成了人类专家手工提取特征到由机器自己学习特征这样的步骤转换。">
<meta name="keywords" content="神经网络搜索">
<meta property="og:type" content="article">
<meta property="og:title" content="综述-NeuralArchitectureSearch">
<meta property="og:url" content="https://fallenk.github.io/2019/06/03/NeuralArchitectureSearch/index.html">
<meta property="og:site_name" content="Fallenk&#39;s Blog">
<meta property="og:description" content="综述- Nerual Architecture Search神经网络架构搜索，顾名思义，就是让机器自己去学习如何构架一个神经网络，得到比人类专家手工设计的网络更好的效果。这个思路是非常一脉相承的，就像机器学习到深度学习，也是完成了人类专家手工提取特征到由机器自己学习特征这样的步骤转换。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://fallenk.github.io/2019/06/03/NeuralArchitectureSearch/1.png">
<meta property="og:image" content="https://fallenk.github.io/2019/06/03/NeuralArchitectureSearch/2.png">
<meta property="og:image" content="https://fallenk.github.io/2019/06/03/NeuralArchitectureSearch/3.png">
<meta property="og:image" content="https://fallenk.github.io/2019/06/03/NeuralArchitectureSearch/4.png">
<meta property="og:image" content="https://fallenk.github.io/2019/06/03/NeuralArchitectureSearch/5.png">
<meta property="og:image" content="https://fallenk.github.io/2019/06/03/NeuralArchitectureSearch/6.png">
<meta property="og:image" content="https://fallenk.github.io/2019/06/03/NeuralArchitectureSearch/7.png">
<meta property="og:image" content="https://fallenk.github.io/2019/06/03/NeuralArchitectureSearch/8.png">
<meta property="og:image" content="https://fallenk.github.io/2019/06/03/NeuralArchitectureSearch/9.png">
<meta property="og:image" content="https://fallenk.github.io/2019/06/03/NeuralArchitectureSearch/10.png">
<meta property="og:image" content="https://fallenk.github.io/2019/06/03/NeuralArchitectureSearch/11.png">
<meta property="og:image" content="https://fallenk.github.io/2019/06/03/NeuralArchitectureSearch/12.png">
<meta property="og:image" content="https://fallenk.github.io/2019/06/03/NeuralArchitectureSearch/13.png">
<meta property="og:image" content="https://fallenk.github.io/2019/06/03/NeuralArchitectureSearch/14.png">
<meta property="og:image" content="https://fallenk.github.io/2019/06/03/NeuralArchitectureSearch/15.png">
<meta property="og:image" content="https://fallenk.github.io/2019/06/03/NeuralArchitectureSearch/16.png">
<meta property="og:image" content="https://fallenk.github.io/Users/liulifeng/Workspaces/hexo/source/_posts/NeuralArchitectureSearch/17.png">
<meta property="og:updated_time" content="2019-06-03T13:41:13.886Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="综述-NeuralArchitectureSearch">
<meta name="twitter:description" content="综述- Nerual Architecture Search神经网络架构搜索，顾名思义，就是让机器自己去学习如何构架一个神经网络，得到比人类专家手工设计的网络更好的效果。这个思路是非常一脉相承的，就像机器学习到深度学习，也是完成了人类专家手工提取特征到由机器自己学习特征这样的步骤转换。">
<meta name="twitter:image" content="https://fallenk.github.io/2019/06/03/NeuralArchitectureSearch/1.png">
  
    <link rel="alternate" href="/atom.xml" title="Fallenk&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://fallenk.github.io"></form>
      </div>
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">首页</a>
        
          <a class="main-nav-link" href="/archives">归档</a>
        
          <a class="main-nav-link" href="/about">关于</a>
        
      </nav>
      
    </div>
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Fallenk&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">经历，心得，笔记，目标</a>
        </h2>
      
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-NeuralArchitectureSearch" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/03/NeuralArchitectureSearch/" class="article-date">
  <time datetime="2019-06-03T02:25:21.000Z" itemprop="datePublished">2019-06-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/论文阅读/">论文阅读</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      综述-NeuralArchitectureSearch
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h1 id="综述-Nerual-Architecture-Search"><a href="#综述-Nerual-Architecture-Search" class="headerlink" title="综述- Nerual Architecture Search"></a>综述- Nerual Architecture Search</h1><p>神经网络架构搜索，顾名思义，就是让<strong>机器自己去学习如何构架一个神经网络</strong>，得到比人类专家手工设计的网络更好的效果。这个思路是非常一脉相承的，就像机器学习到深度学习，也是<strong>完成了人类专家手工提取特征到由机器自己学习特征</strong>这样的步骤转换。</p>
<a id="more"></a>
<p>而现在的深度学习网络架构，虽然获得了不错的效果，但是细究起来，其实是没有非常牢固的理论根据的，依靠的是人类的先验与设计。</p>
<p>所以，在考虑有限制的空间内获得效果更好的网络这一问题上，架构搜索能够给出一些答案。</p>
<h1 id="Introduction-of-NAS"><a href="#Introduction-of-NAS" class="headerlink" title="Introduction of NAS"></a>Introduction of NAS</h1><p>TODO 主参考论文：Neural Architecture Search: A Survey 【<a href="https://arxiv.org/abs/1808.05377" target="_blank" rel="noopener">pdf</a>】</p>
<p>作者：<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Elsken%2C+T" target="_blank" rel="noopener">Thomas Elsken</a>, <a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Metzen%2C+J+H" target="_blank" rel="noopener">Jan Hendrik Metzen</a>, <a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Hutter%2C+F" target="_blank" rel="noopener">Frank Hutter</a></p>
<p>首先，我们先整体讨论一下神经网络架构搜索到底是怎样进行的。通常来说，我们需要考虑三个方面：<strong>搜索空间</strong>、<strong>搜索策略</strong>和<strong>评价策略</strong>。</p>
<p><img src="/2019/06/03/NeuralArchitectureSearch/1.png" alt="image-20190603103544132"></p>
<p><strong>搜索空间（SearchSpace）</strong>: 搜索空间定义了<strong>搜索的范围，其实就是在哪搜索</strong>。通过结合一些过去研究者架构设计方面的经验，可以通过减小搜索空间和简化搜索过程来提高搜索的性能。当然，这样同时也引入了人为的主观臆断，可能会妨碍寻找到超越当前人类知识的新的架构构建块（building blocks）。</p>
<p><strong>搜索策略（SearchStrategy）</strong>：搜索策略定义的则<strong>怎样去搜索</strong>。一方面，我们希望能快速找到性能良好的架构，另一方面，也应避免过早收敛到次优架构（sub optimal architeture）区域。</p>
<p><strong>性能评估策略（Performaceestimation strategy）</strong>：NAS 的<strong>目标是希望能够自动的在给定的数据集上找到一个高性能的架构</strong>。性能评估则是指<strong>评估此性能的过程</strong>：最简单的方式是按照<strong>通常的方式对一个标准架构训练和验证来获得结果</strong>，但遗憾的是这样的计算成本太高了，并且同时限制了可以搜索的网络架构的数量。因此，最近的许多研究都集中在探索新的方法来降低这些性能评估的成本。</p>
<h3 id="搜索空间（Search-Space）"><a href="#搜索空间（Search-Space）" class="headerlink" title="搜索空间（Search Space）"></a>搜索空间（Search Space）</h3><p><img src="/2019/06/03/NeuralArchitectureSearch/2.png" alt="image-20190603110224437"></p>
<p>如图所示的就是一个相对简单的搜索空间，称为<strong>链式神经网络（Chain-stuctured Neural Network）</strong>，很简单就是一个链表，第$ 𝑖−1$层的输出作为第 $i$ 层的输入，可以表示为$ 𝐴=𝐿<em>{n}∘…𝐿</em>{1}∘𝐿_{0}$。而针对于这样一个 search space，我们就需要考虑这些参数：</p>
<ul>
<li>网络的(最大)<strong>层数𝑛</strong></li>
<li>每一层执行的<strong>操作类型</strong>，比如 pooling， convolution 这样的基础操作，或者更高级的一些操作，如depthwise separable convolutions 或 dilated convolutions。</li>
<li>每一层与这个操作相关的 <strong>hyperparameters</strong>，比如对于一个一般的 convolutional 层来说，有 filter 的 numbers，keneral 的 size 和 strides 的 length，而对于 fully-connected 层来说就说是 units 的 number 了。</li>
</ul>
<p>而需要注意的是，与<strong>每层相关的 hyperparameters 是取决于这一层的操作类型</strong>，因此对于 Search Space 的参数化的结果并不是一个固定的长度（fixed-length），而是一个条件空间（conditioanl space）。</p>
<p><img src="/2019/06/03/NeuralArchitectureSearch/3.png" alt="image-20190603110853320"></p>
<p>最近的很多关于 NAS 的研究中都引入了人工设计出的如跳跃连接(skip connections)这样的架构元素，可以用来构建如图 2 右所示的<strong>复杂的多分支网络（multi-branch networks）</strong>。对于这样的结构，第 i 层的输入不再仅仅是前一层的输入，而需要表示为一个组合函数的形式。</p>
<p><img src="/2019/06/03/NeuralArchitectureSearch/4.png" alt="image-20190603111059553"></p>
<p>这种 cell-based 的 search space 也应用在了很多后来的研究中。然而，当使用基于 cell 的搜索空间时，出现了一种新的选择问题，即<strong>如何选择元架构（Micro-architecture）</strong>：应该使用多少 cells 以及如何连接它们来构建实际模型？</p>
<p>理想情况下，Meta-architecture 也应作为 NAS 的一部分自动优化;否则，如果大多数复杂性已经由 meta-architecture 解决，那么很容易就把问题变成进行 meta-architecture engineer，这样对 Cell 的搜索就变得过于简单了。</p>
<h3 id="搜索策略（Search-Strategy）"><a href="#搜索策略（Search-Strategy）" class="headerlink" title="搜索策略（Search Strategy）"></a>搜索策略（Search Strategy）</h3><p>到现在，已经有许多不同的搜索策略用于 NAS，主要有如下这些：</p>
<ul>
<li>随机搜索（random search）</li>
<li>贝叶斯优化（Bayesian optimazation）</li>
<li>进化方法（evolutionaray methods）</li>
<li>强化学习（Reinforcement Learning, RL）</li>
<li>梯度方法（gradient-based methods）</li>
</ul>
<p>自 2013 年开始，Bayesian optimazation（BO）就在 NAS 研究中取得了一些成功，<strong>基于 BO</strong> 发现了当时最优的 vison architectures，在 CIFAR-10 上取得最优 architeture，以及实现了第一个超过人类专家赢得竞赛的 automaticallytuned 神经网络。</p>
<p>在 16 年 Google Brain 发表的 [10] 中通过<strong>强化学习的搜索策略</strong>在 CIFAR-10 和 Penn Treebank 数据集上取得很好表现后，NAS 成为机器学习社区的主流研究课题之一。</p>
<p>关于其他几个算法，会在后续介绍，这边简单介绍一下进化算法和贝叶斯优化。</p>
<p><strong>贝叶斯优化（BO）</strong>是<strong>用于 hyperparameters 优化中最流行的方法</strong>，但是由于 typical BO toolboxes 是<strong>基于高斯过程且主要集中于低维连续性优化问题</strong>，所以它并没有被很多组应用于 NAS 中。[17] 中派生了 kennel function 来使用基于 GP 的 BO 方法，但还没有实现过最优的结果。相比之下，一些研究中使用<strong>基于树的模型或随机森林来搜索非常高维的条件空间时实现了在很多问题上的最优性能</strong>，这些方法同时优化神经架构和它们的 hyperparameters。虽然缺乏完整的比较，但初步证据表明这些方法可以超过进化算法 [18]。</p>
<p>关于 进化算法：</p>
<p>更近期的一些研究中<strong>有不是使用基于梯度的方法来优化权重，而使用进化算法来优化神经结构本身</strong>。是用进化算法演化一组模型，在每个进化步骤中，对来自群体的至少一个模型进行采样，并将它们作为父母通过繁衍或者突变来产生后代。在 NAS 中，突变表示本地化的操作，例如添加或移除层，改变层的超参数，添加跳跃连接，以及改变训练超参数。在对后代进行训练之后，评估它们的适应性（例如，在验证集上的表现）后再将它们添加到种群中。</p>
<h3 id="性能评估策略（Performace-Estimation-Strategy）"><a href="#性能评估策略（Performace-Estimation-Strategy）" class="headerlink" title="性能评估策略（Performace Estimation Strategy）"></a>性能评估策略（Performace Estimation Strategy）</h3><p>上一节讨论的搜索策略旨在找到<strong>某些性能度量（如准确度）最大化的架构 A</strong>，为了引导它们的搜索过程，这些策略需要<strong>考虑如何评判给定架构的性能高低</strong>。最简单的方法是在训练数据上训练 A 并评估其在验证数据上的表现。然而，从头开始训练每个要评估的架构经常会产生大约数千 GPU 天的计算需求</p>
<p>为了加快搜索过程，<strong>需要在相对较少的评估的基础</strong>上并在相对较大的搜索空间中进行良好的预测</p>
<p><img src="/2019/06/03/NeuralArchitectureSearch/5.png" alt="image-20190603111811624"></p>
<ul>
<li>Lower Fidelit Estimates</li>
</ul>
<p>这种 lower fidelities 可以表示<strong>为较短的训练时间</strong>，对训练子集的训练，使用较低分辨率的图像，或者是用更少的 filter。虽然这些 lower fidelities 降低了计算成本，但与此同时它们也会在测试中引入偏差，因为性能通常会被低估。当然只要搜索策略只依赖于相对排名时，这不会有什么问题。</p>
<ul>
<li>Learning Curve Extrapolation</li>
</ul>
<p>提出方法来<strong>推断初始学习曲线并终止那些预测表现不佳以加速架构搜索过程</strong>。另外一些研究者则通过基于架构的超参数来预测哪些部分学习曲线最有希望。</p>
<ul>
<li>Weight Inheritance/ Network Morphisms</li>
</ul>
<p>??<strong>基于之前已经训练过的其他架构的权重来初始化新架构的权重</strong>。实现这一目标的一种方法，称为 network morphisms，它允许修改架构的同时保持网络所代表的功能不变。这就允许连续的增加网络容量并保持高性能而无需从头开始训练。对几个时期的持续训练也可以利用 network morphisms 引入额外容量。</p>
<ul>
<li>One-Shot Models/ Weight Sharing</li>
</ul>
<p>??它将<strong>所有架构都视为一个超级图的不同子图，并在架构之间共享权重</strong>。那么只需要一次性训练单个模型的权重，然后通过继承权重来评估架构（它们只是一次性模型的子图），而无需再进行任何单独的训练。比如ENAS、Darts。</p>
<h2 id="NAS-amp-NASNet-amp-ENAS"><a href="#NAS-amp-NASNet-amp-ENAS" class="headerlink" title="NAS &amp; NASNet &amp; ENAS"></a>NAS &amp; NASNet &amp; ENAS</h2><h3 id="NAS"><a href="#NAS" class="headerlink" title="NAS"></a>NAS</h3><p>Neural Architecture Search with Reinforcement Learning 【<a href="https://arxiv.org/abs/1611.01578" target="_blank" rel="noopener">pdf</a>】</p>
<p>它是NSANet的前置，比较详细的介绍了Google的<strong>NAS的整体设计</strong>，包括<strong>具体的目标函数</strong>以及<strong>实现skip connection的具体方法</strong>，主要是提出了<strong>用强化学习的方法来完成搜索神经网络架构</strong>。</p>
<p><img src="/2019/06/03/NeuralArchitectureSearch/6.png" alt="image-20190603112503684"></p>
<p>通过一个controllerRNN在搜索空间（search space）中得到一个网络结构（论文中称为child network），然后用这个网络结构在数据集上训练，在验证集上测试得到准确率R，再将这个准确率回传给controller，controller继续优化得到另一个网络结构，如此反复进行直到得到最佳的结果，整个过程称为Neural Architecture Search。</p>
<h4 id="How-to-use-a-controller-RNN-to-generate-an-CNN-model"><a href="#How-to-use-a-controller-RNN-to-generate-an-CNN-model" class="headerlink" title="How to use a controller RNN to generate an CNN model"></a><strong>How to use a controller RNN to generate an CNN model</strong></h4><p><strong>控制器生成的是网络架构中的超参数，NAS中假定预测的inference中只含卷积层</strong>。</p>
<p><img src="/2019/06/03/NeuralArchitectureSearch/7.png" alt="image-20190603112640743"></p>
<p>对于Layer N，控制器可以预测<strong>该层的filter width，filter height，stride height，stride width，filter的个数，每一个预测都是通过softmax分类</strong>，并作为下一个预测的输入。实验中的停止条件是，<strong>层的个数超过了某个指定的数值</strong>。当然这个数值在训练的过程中也可以变化。</p>
<p>最终得到的是LSTM产生的一个序列化的tokens，记为$θ_{c}$。</p>
<h4 id="How-to-train-a-controller-RNN-with-REINFORCE"><a href="#How-to-train-a-controller-RNN-with-REINFORCE" class="headerlink" title="How to train a controller RNN with REINFORCE"></a><strong>How to train a controller RNN with REINFORCE</strong></h4><p>将NAS与强化学习做一个对应，那么控制器LSTM就是Agent，控制器预测产生$θ<em>{c}$对应为活动$𝑎</em>{1:T}$，子网络在验证集上的准确率R作为reward signal。</p>
<p>那么，就得到了目标函数：</p>
<p>$\boldsymbol{J}\left(\boldsymbol{\theta}<em>{c}\right)=\boldsymbol{E}</em>{P\left(a_{1 : T} ; \theta_{c}\right)}[\boldsymbol{R}]$</p>
<p>鉴于R是不连续的，我们采用policy gradient来迭代更新$θ_{c}$，具体采用了REINFORCE规则：</p>
<p>$\nabla_{\theta_{c}} J\left(\theta_{c}\right)=\sum_{t=1}^{T} E_{P\left(a_{1 : T} ; \theta_{c}\right)}\left[\nabla_{\theta_{c}} \log P\left(a_{t} | a_{(t-1) : 1} ; \theta_{c}\right) R\right]$</p>
<p>对上述公式作一个先验近似：</p>
<p>$\frac{1}{m} \sum_{k=1}^{m} \sum_{t=1}^{T} \nabla \theta_{c} \log P\left(a_{t} | a_{(t-1) : 1} ; \theta_{c}\right) R_{k}$</p>
<p>由于baseline fuction b不依赖与当下action，本式依然是一个无偏估计。b采用的之前架构准确率的指数加权平均（exponential moving average）。</p>
<h4 id="How-to-add-skip-connections-and-other-layer-types"><a href="#How-to-add-skip-connections-and-other-layer-types" class="headerlink" title="How to add skip connections and other layer types"></a><strong>How to add skip connections and other layer types</strong></h4><p>NAS采用注意力机制。具体来说，通过添加在第N层添加N-1个anchor，来确定是否要与之前的某一层跳跃连接。anchor通过两层（本层和上一层）的hidden state，用sigmoid来判断：</p>
<p>$\mathrm{P}(\text { Layer j is an input to layer i })=\operatorname{sigmoid}\left(v^{\mathrm{T}} \tanh \left(W_{\mathrm{prev}} <em> h_{j}+W_{\mathrm{curr}} </em> h_{i}\right)\right)$</p>
<p>公式中的$𝑊<em>{𝑝}𝑟𝑒𝑣$、$𝑊</em>{𝑐}𝑢𝑟𝑟$和$𝑣$是可训练变量。这些行为依然是概率分布的，因此REINFORCE方法不会有大的改动。</p>
<p>由下图可以形象的了解anchor point是怎样添加跳跃连接的。</p>
<p><img src="/2019/06/03/NeuralArchitectureSearch/8.png" alt="image-20190603113639125"></p>
<p>本篇的实验结果并无太多参考意义，因为很快作者就出了NASNet。NAS最大的意义还是在提出了一种神经网络搜索的框架结构。</p>
<p>实验中采用了2层35个隐藏单元的LSTM，使用800个GPU训练了28天，一共22400 GPU-Hours，一共训练了12800个架构。</p>
<h3 id="NASNet"><a href="#NASNet" class="headerlink" title="NASNet"></a>NASNet</h3><p>论文： Learning Transferable Architectures for Scalable Image Recognition 【<a href="https://arxiv.org/abs/1707.07012" target="_blank" rel="noopener">pdf</a>】</p>
<p><img src="/2019/06/03/NeuralArchitectureSearch/9.png" alt="image-20190603212540904"></p>
<p>相较于NAS的优化：</p>
<p>一种新的搜索空间- NASNet search space—-a generic convolutional cell</p>
<p>一种新的正则化技术，ScheduledDropPath，使得NASNet模型的泛化能力更好(实验细节，不太重要)</p>
<h4 id="NASNet-cell"><a href="#NASNet-cell" class="headerlink" title="NASNet cell"></a><strong>NASNet cell</strong></h4><p>不再进行整个网络的搜索，而是<strong>定义了cell单元作为building block</strong>。通过堆叠cell形成最后的网络。</p>
<p>具体讨论的话，对于CNN网络，采用了两种cell，<strong>normal cell</strong>和<strong>reduction cell</strong>。normal cell的输出保持维度不变，而reduction cell用于收缩维度，具体来说就是步长为2。两种单元是不同的架构，堆叠在一起形成了最后的网络架构。</p>
<p>控制器RNN输出的是两种cell的超参，最后以下图形式堆叠为子网络，进行训练和验证，得到验证集准确率R回馈到控制器。</p>
<p><img src="/2019/06/03/NeuralArchitectureSearch/10.png" alt="img"></p>
<p>在NASNet的搜索空间中，每个cell的有两个初始隐藏状态输入$ℎ𝑖<em>{i}$和$ℎ</em>{𝑖−1}$，分别是前两个层的输出。控制器RNN会跟根据这两个initial hidden states预测卷积cell的剩余结构。</p>
<p>控制器对每个cell的预测分为B个bolck（此处的block可以理解为一个操作），每个block又有5个不同的softmax分类器进行5个预测步骤。</p>
<p><img src="/2019/06/03/NeuralArchitectureSearch/11.png" alt="image-20190603213110274"></p>
<p>在算法中，用之前已经存在的隐藏态作为序列分块的可能输入来填入新建的hidden state。B=5时效果较好，即一个cell中包含5个操作。</p>
<p>为了使RNN能同时预测Normal Cell和Reduction Cell，我们简单设定控制器有2*5B个预测步骤。前5B给Normal Cell，后5B给Reduction Cell。</p>
<h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><strong>实验结果</strong></h4><p>在具体训练阶段，控制器采用Proximal Policy Optimazation（PPO）策略，用RNN控制一个全局工作序列系统来生成一个子网络的备选池。</p>
<p>训练时长为500个GPU训练4天，总计2000GPU时。比初始NAS快了7倍，但是还是非常奢侈。</p>
<p>训练得到的卷积cell：</p>
<p><img src="/2019/06/03/NeuralArchitectureSearch/12.png" alt="image-20190603213200358"></p>
<p>CIFAR-10上的训练效果：</p>
<p><img src="/2019/06/03/NeuralArchitectureSearch/13.png" alt="image-20190603213233993"></p>
<p>cell泛化到ImageNet的图像分类任务：</p>
<p><img src="/2019/06/03/NeuralArchitectureSearch/14.png" alt="image-20190603213302539"></p>
<h3 id="ENAS"><a href="#ENAS" class="headerlink" title="ENAS"></a>ENAS</h3><p>论文：Efficient Neural Architecture Search via Parameter Sharing 【<a href="https://arxiv.org/abs/1802.03268" target="_blank" rel="noopener">pdf</a>】【<a href="https://github.com/melodyguan/enas" target="_blank" rel="noopener">code-tf</a>】【<a href="https://github.com/carpedm20/ENAS-pytorch" target="_blank" rel="noopener">code-python</a>】</p>
<p>在ENAS中，控制器通过在大型计算图中<strong>搜索最优子图来发现神经网络结构</strong>。利用<strong>策略梯度对控制器进行训练</strong>，通过<strong>权重共享</strong>，快速选择验证集上期望报酬最大的子图。</p>
<p><strong>最关键是利用子模型之间共享参数，大大减少了训练时长。</strong>采用了控制器参数与子网络参数相互迭代的方式进行训练。</p>
<p><img src="/2019/06/03/NeuralArchitectureSearch/15.png" alt="image-20190603213354368"></p>
<p>本处为了通篇逻辑，也使用ENAS中CNN cell的训练过程来描述。RNN网络训练过程请参考原文。</p>
<h4 id="搜索空间"><a href="#搜索空间" class="headerlink" title="搜索空间"></a><strong>搜索空间</strong></h4><p>基本沿用NASNet。</p>
<p><img src="/2019/06/03/NeuralArchitectureSearch/16.png" alt="image-20190603213436437"></p>
<p>用一个例子来说明搜索空间的机制，这里是B = 4个节点(参见图5)。</p>
<ol>
<li>节点1,2是输入节点，因此不需要为它们做任何决策。设ℎ1h1,ℎ2h2为这些节点的输出。</li>
<li>在节点3:控制器采样前两个节点和两个操作。在左上角的图5中，它对节点1、节点2分别选取了操作sep conv 5x5和identity。这意味着$h_{3}=\operatorname{sep}<em>{c} o n v</em>{5} x 5\left(h_{2}\right)+i d\left(h_{2}\right)$。</li>
<li>在节点4:控制器采样节点3、节点1为 avg pooling3x3和sep conv 3x3。这意味着$h_{4}=a v g_{p} o o l_{3} x 3\left(h_{3}\right)+\operatorname{sep}<em>{c} o n v</em>{3} \times 3\left(h_{1}\right)$。</li>
<li>由于除h4之外的所有节点都被用作至少另一个节点的输入，因此惟一的松散端h4被视为单元的输出。如果有多个松散的端点，它们将沿着深度维度连接起来，形成单元格的输出。</li>
</ol>
<h4 id="架构搜索与训练权重迭代进行"><a href="#架构搜索与训练权重迭代进行" class="headerlink" title="架构搜索与训练权重迭代进行"></a><strong>架构搜索与训练权重迭代进行</strong></h4><p>在ENAS,有两套可学的参数:控制器LSTM的参数θ,和子模型的共享参数ω。ENAS的培训过程包括两个交叉阶段。</p>
<ol>
<li>第一阶段通过一整个训练集训练子模型共享参数$w$。在本文的Penn Treebank实验中，ω是训练大约400 steps,每个minibatch使用64个样本,采用梯度反向传播+梯度截断（<strong>Truncated Gradient</strong>）进行优化，每隔35 steps进行一次隔断。同时,在CIFAR-10上,ω是45,000张训练图像,分为minibatches大小128,∇ω计算使用标准的反向传播。</li>
<li>第二阶段训练参数控制器LSTM的参数θ, 对于固定数量的步骤,通常在我们的实验设置为2000。这两个阶段在ENAS的培训期间交替进行。</li>
</ol>
<p><strong>Deriving Architectures.</strong></p>
<p>如何在训练好的ENAS模型中派生出全新的结构。首先从训练好的策略π(m, θ)中采样几个模型。针对每个模型，通过验证集的一个minibatch计算reward。选取其中reward最高的模型从零开始预训练。</p>
<h4 id="实验结果-1"><a href="#实验结果-1" class="headerlink" title="实验结果"></a><strong>实验结果</strong></h4><p><img src="/Users/liulifeng/Workspaces/hexo/source/_posts/NeuralArchitectureSearch/17.png" alt="image-20190603214044256"></p>
<h2 id="Latest-architecture-search-algorithms"><a href="#Latest-architecture-search-algorithms" class="headerlink" title="Latest architecture search algorithms"></a>Latest architecture search algorithms</h2><p>上述NAS三篇论文是一脉相承，一点点改进过来的。其实我最开始接触的论文是Darts，当时感觉这个结构是全新的。再一路看下来，会发现Darts也是在ENAS上又做了搜索策略部分的革新，整体的架构也还是继承下来的。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://fallenk.github.io/2019/06/03/NeuralArchitectureSearch/" data-id="ck0258kbv001opq8op4ef3eap" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/神经网络搜索/">神经网络搜索</a></li></ul>

    </footer>
  </div>
  
    
 <script src="/jquery/jquery.min.js"></script>
  <div id="random_posts">
    <h2>推荐文章</h2>
    <div class="random_posts_ul">
      <script>
          var random_count =4
          var site = {BASE_URI:'/'};
          function load_random_posts(obj) {
              var arr=site.posts;
              if (!obj) return;
              // var count = $(obj).attr('data-count') || 6;
              for (var i, tmp, n = arr.length; n; i = Math.floor(Math.random() * n), tmp = arr[--n], arr[n] = arr[i], arr[i] = tmp);
              arr = arr.slice(0, random_count);
              var html = '<ul>';
            
              for(var j=0;j<arr.length;j++){
                var item=arr[j];
                html += '<li><strong>' + 
                item.date + ':&nbsp;&nbsp;<a href="' + (site.BASE_URI+item.uri) + '">' + 
                (item.title || item.uri) + '</a></strong>';
                if(item.excerpt){
                  html +='<div class="post-excerpt">'+item.excerpt+'</div>';
                }
                html +='</li>';
                
              }
              $(obj).html(html + '</ul>');
          }
          $('.random_posts_ul').each(function () {
              var c = this;
              if (!site.posts || !site.posts.length){
                  $.getJSON(site.BASE_URI + 'js/posts.js',function(json){site.posts = json;load_random_posts(c)});
              } 
               else{
                load_random_posts(c);
              }
          });
      </script>
    </div>
  </div>

    
<nav id="article-nav">
  
    <a href="/2019/06/13/论文阅读-RandWiredNN/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">上一篇</strong>
      <div class="article-nav-title">
        
          论文阅读-RandWiredNN
        
      </div>
    </a>
  
  
    <a href="/2019/05/29/彩票假设paper阅读笔记/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">下一篇</strong>
      <div class="article-nav-title">彩票假设paper阅读笔记</div>
    </a>
  
</nav>

  
</article>
 
     
  <div class="comments" id="comments">
    
     
       
      <div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div>
    
       
      
      
  </div>
 
  

</section>
           
    <aside id="sidebar">
  
    

  
    
    <div class="widget-wrap">
    
      <div class="widget" id="toc-widget-fixed">
      
        <strong class="toc-title">文章目录</strong>
        <div class="toc-widget-list">
              <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#综述-Nerual-Architecture-Search"><span class="toc-number">1.</span> <span class="toc-text">综述- Nerual Architecture Search</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction-of-NAS"><span class="toc-number">2.</span> <span class="toc-text">Introduction of NAS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#搜索空间（Search-Space）"><span class="toc-number">2.0.1.</span> <span class="toc-text">搜索空间（Search Space）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#搜索策略（Search-Strategy）"><span class="toc-number">2.0.2.</span> <span class="toc-text">搜索策略（Search Strategy）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#性能评估策略（Performace-Estimation-Strategy）"><span class="toc-number">2.0.3.</span> <span class="toc-text">性能评估策略（Performace Estimation Strategy）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NAS-amp-NASNet-amp-ENAS"><span class="toc-number">2.1.</span> <span class="toc-text">NAS &amp; NASNet &amp; ENAS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#NAS"><span class="toc-number">2.1.1.</span> <span class="toc-text">NAS</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#How-to-use-a-controller-RNN-to-generate-an-CNN-model"><span class="toc-number">2.1.1.1.</span> <span class="toc-text">How to use a controller RNN to generate an CNN model</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#How-to-train-a-controller-RNN-with-REINFORCE"><span class="toc-number">2.1.1.2.</span> <span class="toc-text">How to train a controller RNN with REINFORCE</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#How-to-add-skip-connections-and-other-layer-types"><span class="toc-number">2.1.1.3.</span> <span class="toc-text">How to add skip connections and other layer types</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NASNet"><span class="toc-number">2.1.2.</span> <span class="toc-text">NASNet</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#NASNet-cell"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">NASNet cell</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#实验结果"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">实验结果</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ENAS"><span class="toc-number">2.1.3.</span> <span class="toc-text">ENAS</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#搜索空间"><span class="toc-number">2.1.3.1.</span> <span class="toc-text">搜索空间</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#架构搜索与训练权重迭代进行"><span class="toc-number">2.1.3.2.</span> <span class="toc-text">架构搜索与训练权重迭代进行</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#实验结果-1"><span class="toc-number">2.1.3.3.</span> <span class="toc-text">实验结果</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Latest-architecture-search-algorithms"><span class="toc-number">2.2.</span> <span class="toc-text">Latest architecture search algorithms</span></a></li></ol></li></ol>
          </div>
      </div>
    </div>

  
    

  
    
  
    
  
    

  
    
  
    <!--微信公众号二维码-->


  
</aside>

      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-left">
      &copy; 2014 - 2019 Fallenk Liu&nbsp;|&nbsp;
      主题 <a href="https://github.com/giscafer/hexo-theme-cafe/" target="_blank">Cafe</a>
    </div>
     <div id="footer-right">
      联系方式&nbsp;|&nbsp;fallenk_liu@yeah.com
    </div>
  </div>
</footer>
 <script src="/jquery/jquery.min.js"></script>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    <img class="back-to-top-btn" src="/images/fly-to-top.png"/>
<script>
// Elevator script included on the page, already.
window.onload = function() {
  var elevator = new Elevator({
    selector:'.back-to-top-btn',
    element: document.querySelector('.back-to-top-btn'),
    duration: 1000 // milliseconds
  });
}
</script>
      

  
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "e2fb4051c49842688ce669e634bc983f",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
    

  







<!-- author:forvoid begin -->
<!-- author:forvoid begin -->

<!-- author:forvoid end -->

<!-- author:forvoid end -->


  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      })
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      })
    </script>
    <script type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


 <script src="/js/is.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/elevator.js"></script>
  </div>
</body>
</html>